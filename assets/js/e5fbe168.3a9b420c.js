"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[3338],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>g});var a=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var p=a.createContext({}),l=function(e){var t=a.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=l(e.components);return a.createElement(p.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,p=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),u=l(n),g=r,h=u["".concat(p,".").concat(g)]||u[g]||d[g]||i;return n?a.createElement(h,o(o({ref:t},c),{},{components:n})):a.createElement(h,o({ref:t},c))}));function g(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,o=new Array(i);o[0]=u;var s={};for(var p in t)hasOwnProperty.call(t,p)&&(s[p]=t[p]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var l=2;l<i;l++)o[l]=n[l];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}u.displayName="MDXCreateElement"},55465:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>p,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>s,toc:()=>l});var a=n(87462),r=(n(67294),n(3905));const i={sidebar_position:1,title:"Apache Spark"},o=void 0,s={unversionedId:"integrations/spark/spark",id:"integrations/spark/spark",title:"Apache Spark",description:"This integration is known to work with Apache Spark 2.4 and later.",source:"@site/docs/integrations/spark/spark.md",sourceDirName:"integrations/spark",slug:"/integrations/spark/",permalink:"/docs/integrations/spark/",draft:!1,editUrl:"https://github.com/OpenLineage/docs/tree/main/docs/integrations/spark/spark.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1,title:"Apache Spark"},sidebar:"tutorialSidebar",previous:{title:"OpenLineage Integrations",permalink:"/docs/integrations/about"},next:{title:"Installation",permalink:"/docs/integrations/spark/installation"}},p={},l=[{value:"Collecting Lineage in Spark",id:"collecting-lineage-in-spark",level:2},{value:"About the Integration",id:"about-the-integration",level:2},{value:"How to Use the Integration",id:"how-to-use-the-integration",level:2},{value:"Scheduling from Airflow",id:"scheduling-from-airflow",level:3}],c={toc:l};function d(e){let{components:t,...i}=e;return(0,r.kt)("wrapper",(0,a.Z)({},c,i,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("admonition",{type:"info"},(0,r.kt)("p",{parentName:"admonition"},"This integration is known to work with Apache Spark 2.4 and later.")),(0,r.kt)("p",null,"Spark jobs typically run on clusters of machines. A single machine hosts the \"driver\" application,\nwhich constructs a graph of jobs - e.g., reading data from a source, filtering, transforming, and\njoining records, and writing results to some sink- and manages execution of those jobs. Spark's\nfundamental abstraction is the Resilient Distributed Dataset (RDD), which encapsulates distributed\nreads and modifications of records. While RDDs can be used directly, it is far more common to work\nwith Spark Datasets or Dataframes, which is an API that adds explicit schemas for better performance\nand the ability to interact with datasets using SQL. The Dataframe's declarative API enables Spark\nto optimize jobs by analyzing and manipulating an abstract query plan prior to execution."),(0,r.kt)("h2",{id:"collecting-lineage-in-spark"},"Collecting Lineage in Spark"),(0,r.kt)("p",null,"Collecting lineage requires hooking into Spark's ",(0,r.kt)("inlineCode",{parentName:"p"},"ListenerBus")," in the driver application and\ncollecting and analyzing execution events as they happen. Both raw RDD and Dataframe jobs post events\nto the listener bus during execution. These events expose the structure of the job, including the\noptimized query plan, allowing the Spark integration to analyze the job for datasets consumed and\nproduced, including attributes about the storage, such as location in GCS or S3, table names in a\nrelational database or warehouse, such as Redshift or Bigquery, and schemas. In addition to dataset\nand job lineage, Spark SQL jobs also report logical plans, which can be compared across job runs to\ntrack important changes in query plans, which may affect the correctness or speed of a job."),(0,r.kt)("p",null,"A single Spark application may execute multiple jobs. The Spark OpenLineage integration maps one\nSpark job to a single OpenLineage Job. The application will be assigned a Run id at startup and each\njob that executes will report the application's Run id as its parent job run. Thus, an application\nthat reads one or more source datasets, writes an intermediate dataset, then transforms that\nintermediate dataset and writes a final output dataset will report three jobs- the parent application\njob, the initial job that reads the sources and creates the intermediate dataset, and the final job\nthat consumes the intermediate dataset and produces the final output. As an image:\n",(0,r.kt)("img",{alt:"image",src:n(83189).Z,width:"1289",height:"204"})),(0,r.kt)("h2",{id:"about-the-integration"},"About the Integration"),(0,r.kt)("p",null,"This integration employs the ",(0,r.kt)("inlineCode",{parentName:"p"},"SparkListener")," interface through ",(0,r.kt)("inlineCode",{parentName:"p"},"OpenLineageSparkListener"),", offering\na comprehensive monitoring solution. It examines SparkContext-emitted events to extract metadata\nassociated with jobs and datasets, utilizing the RDD and DataFrame dependency graphs. This method\neffectively gathers information from various data sources, including filesystem sources (e.g., S3\nand GCS), JDBC backends, and data warehouses such as Redshift and Bigquery."),(0,r.kt)("h2",{id:"how-to-use-the-integration"},"How to Use the Integration"),(0,r.kt)("p",null,"Incorporating OpenLineage metadata collection into existing Spark jobs is designed to be simple and\nminimally invasive."),(0,r.kt)("h3",{id:"scheduling-from-airflow"},"Scheduling from Airflow"),(0,r.kt)("p",null,"The same parameters passed to ",(0,r.kt)("inlineCode",{parentName:"p"},"spark-submit")," can be supplied from Airflow and other schedulers. If\nusing the ",(0,r.kt)("a",{parentName:"p",href:"/docs/integrations/airflow/"},"openlineage-airflow")," integration, each task in the DAG has its own Run id\nwhich can be connected to the Spark job run via the ",(0,r.kt)("inlineCode",{parentName:"p"},"spark.openlineage.parentRunId")," parameter. For example,\nhere is an example of a ",(0,r.kt)("inlineCode",{parentName:"p"},"DataProcPySparkOperator")," that submits a Pyspark application on Dataproc:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'t1 = DataProcPySparkOperator(\n    task_id=job_name,\n    gcp_conn_id=\'google_cloud_default\',\n    project_id=\'project_id\',\n    cluster_name=\'cluster-name\',\n    region=\'us-west1\',\n    main=\'gs://bucket/your-prog.py\',\n    job_name=job_name,\n    dataproc_pyspark_properties={\n      "spark.extraListeners": "io.openlineage.spark.agent.OpenLineageSparkListener",\n      "spark.jars.packages": "io.openlineage:openlineage-spark:1.0.0+",\n      "spark.openlineage.transport.url": f"{openlineage_url}/api/v1/namespaces/{openlineage_namespace}/jobs/dump_orders_to_gcs/runs/{{{{lineage_run_id(run_id, task)}}}}?api_key={api_key}",\n      "spark.openlineage.namespace": openlineage_namespace,\n      "spark.openlineage.parentJobName": job_name,\n      "spark.openlineage.parentRunId": f"{{{{lineage_run_id(run_id, task)}}}}\n    },\n    dag=dag)\n')))}d.isMDXComponent=!0},83189:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/spark-job-creation.dot-d3fd1094587dcacc0c8a1566dac60ed5.png"}}]);