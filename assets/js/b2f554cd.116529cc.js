"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1477],{30010:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/warsaw-meetup","metadata":{"permalink":"/blog/warsaw-meetup","source":"@site/blog/warsaw-meetup/index.mdx","title":"Meet Us in Warsaw on November 29th!","description":"Our first Warsaw OpenLineage Meetup will take place at Google\'s Warsaw HQ.","date":"2023-10-13T00:00:00.000Z","formattedDate":"October 13, 2023","tags":[],"readingTime":0.82,"hasTruncateMarker":true,"authors":[{"name":"Michael Robinson","title":"OpenLineage Community Manager","url":"https://github.com/merobi-hub","imageURL":"https://github.com/merobi-hub.png","key":"Robinson"}],"frontMatter":{"title":"Meet Us in Warsaw on November 29th!","date":"2023-10-13T00:00:00.000Z","authors":["Robinson"],"description":"Our first Warsaw OpenLineage Meetup will take place at Google\'s Warsaw HQ."},"nextItem":{"title":"Metaphor\'s Integration with OpenLineage Enhances Data Governance and Collaboration","permalink":"/blog/metaphor-integration"}},"content":"Join us on Wednesday, November 29th, 2023, from 17:30-20:30 CET in Warsaw, Poland, to \\ncontribute to a discussion of the future of OpenLineage. On the tentative agenda:\\n1. *Mary Idamkina on OpenLineage in GCP Dataplex*\\n2. *Pawe\u0142 Leszczynski on recent developments in the Spark Integration*\\n3. *Jakub Dardzi\u0144ski on Extracting lineage from PythonOperator - how come this is possible?*\\n4. *Pawe\u0142 Leszczynski on How to Become a Spark-OpenLineage Contributor in 5 Steps*\\n\\n\x3c!--truncate--\x3e\\n\\nFood and beverages will be provided. Don\'t miss this opportunity to help set the course of OpenLineage. You\'ll also learn about the spec from some of its key contributors and recent adopters.\\n\\n**Please [sign up](https://www.meetup.com/warsaw-openlineage-meetup-group/events/296705558/?utm_medium=referral&utm_campaign=share-btn_savedevents_share_modal&utm_source=link) to let us know you\'re coming.**\\n\\n### Time, Place & Format\\n\\nDate: November 29th, 2023  \\nFormat: Hybrid  \\nTime: 17:30-20:30 CET  \\nAddress: Google Warsaw, The Hub, [Rondo Daszy\u0144skiego 2c, 00-843 Warsaw, Poland](https://maps.app.goo.gl/YAbrkbf1zNAKi3RG7)\\n\\n### Joining Virtually\\n\\nA Zoom link will be provided to attendees in the days leading up to the event.\\n\\n### Hope to see you there!"},{"id":"/metaphor-integration","metadata":{"permalink":"/blog/metaphor-integration","source":"@site/blog/metaphor-integration/index.mdx","title":"Metaphor\'s Integration with OpenLineage Enhances Data Governance and Collaboration","description":"Metaphor\u2019s journey towards data governance excellence led to the adoption of OpenLineage.","date":"2023-10-02T00:00:00.000Z","formattedDate":"October 2, 2023","tags":[],"readingTime":2.515,"hasTruncateMarker":true,"authors":[{"name":"Yi Wang","title":"Guest Blogger & Founding Engineer at Metaphor","url":"https://www.linkedin.com/in/yi-alan-wang-26a53b15/","imageURL":"https://media.licdn.com/dms/image/C5603AQHC2SXEkAwK1Q/profile-displayphoto-shrink_800_800/0/1587594579539?e=1702512000&v=beta&t=pQqmLSybiRwX-ZwhmnTjqI__oKkS2JJ2TlPQEAZEYlk","key":"Wang"},{"name":"Mars Lan","title":"Guest Blogger & Co-founder/CTO at Metaphor","url":"https://www.linkedin.com/in/marslan/","imageURL":"https://media.licdn.com/dms/image/C4D03AQHDPQa4giocFQ/profile-displayphoto-shrink_800_800/0/1656448936694?e=1702512000&v=beta&t=QXvdUMkVHPhB8Nb1DsQYRjbqCujRbbxWw0AZcm0QXdc","key":"Lan"}],"frontMatter":{"title":"Metaphor\'s Integration with OpenLineage Enhances Data Governance and Collaboration","date":"2023-10-02T00:00:00.000Z","authors":["Wang","Lan"],"description":"Metaphor\u2019s journey towards data governance excellence led to the adoption of OpenLineage."},"prevItem":{"title":"Meet Us in Warsaw on November 29th!","permalink":"/blog/warsaw-meetup"},"nextItem":{"title":"The OpenLineage Airflow Provider is Here","permalink":"/blog/airflow-provider"}},"content":"In the ever-evolving landscape of data management and governance, organizations constantly seek innovative solutions to streamline their processes, foster collaboration, and maximize the value of their data assets. [Metaphor](https://metaphor.io/), born out of the minds behind LinkedIn\'s DataHub, has emerged as a modern data catalog and social platform for data. We take a unique approach by combining technical metadata with social collaboration, making data governance accessible and engaging for everyone in the organization. In this blog post, we explain the motivation behind Metaphor\u2019s adoption of OpenLineage, delve into the integration methodology, and discuss its current status and benefits.\\n\\n\x3c!--truncate--\x3e\\n\\n## Embracing OpenLineage\\n[Metaphor](https://metaphor.io/)\u2019s journey towards data governance excellence led to the adoption of OpenLineage, a standardized metadata model that enables seamless integration with various data systems. The motivation behind this decision is threefold:\\n1. **Streamlined Metadata Collection**: OpenLineage provides a standardized framework for collecting and managing metadata, simplifying the process of gathering lineage information from diverse data processing systems.\\n2. **Interoperability**: OpenLineage boasts a range of clients for popular data processing frameworks like Apache Spark and Apache Airflow. By adopting OpenLineage, we gain the ability to seamlessly integrate with these systems, ensuring data lineage tracking across the entire data ecosystem.\\n3. **Enhanced Data Understanding**: Integration with OpenLineage enhances Metaphor\'s core mission of making data understandable and actionable. It empowers users to gain deeper insights into data assets by tracking their lineage, from source to consumption.  \\n\\nThe Metaphor team recognized the immense potential of OpenLineage and quickly decided to adopt OpenLineage as a crucial component of their platform.\\n\\n## Integration with OpenLineage\\n\\nTo integrate with OpenLineage, we developed our own OpenLineage-compatible REST endpoint. This endpoint allows Metaphor to consume metadata events emitted by OpenLineage clients, including the ones for Apache Spark and Airflow. The integration process involves transforming this metadata into Metaphor\u2019s own data models, which are then associated with the corresponding data assets within the platform, creating a cohesive global knowledge graph.\\n\\nThe user interface of Metaphor plays a pivotal role in making this integration accessible. Users can easily drill down into data jobs and data lineages, facilitating their work and enriching their understanding of data assets.\\n\\n![Metaphor architecture](./metaphor-integration-arch.png)\\n\\n## Realizing the Benefits\\n\\nWe have rolled out the OpenLineage integration to production environments. Early adopters have already begun to reap the benefits of Spark and Airflow lineage graphs, especially the column-level lineage. Users can now gain deeper insights into their data assets, making it easier to make informed decisions, optimize data usage, and maximize ROI.\\n\\nIn conclusion, the integration of OpenLineage with Metaphor represents a significant step forward in the world of data management and governance. It combines the power of standardized metadata with an intuitive social platform, offering organizations a comprehensive solution for their data needs. As more organizations embrace this integration, we can expect to see even greater advancements in data governance and collaboration, unlocking the full potential of their data assets.\\n\\n## Additional Resources\\n- Metaphor Data: https://metaphor.io/\\n- Try Metaphor: https://metaphor.io/try \\n- Metaphor Documentation: https://docs.metaphor.io/docs\\n- Metaphor OSS Connectors: https://github.com/MetaphorData/connectors"},{"id":"/airflow-provider","metadata":{"permalink":"/blog/airflow-provider","source":"@site/blog/airflow-provider/index.mdx","title":"The OpenLineage Airflow Provider is Here","description":"Built-in OpenLineage support in Airflow means big improvements in reliability, lineage output, and custom operator implementation.","date":"2023-08-23T00:00:00.000Z","formattedDate":"August 23, 2023","tags":[],"readingTime":4.365,"hasTruncateMarker":true,"authors":[{"name":"Michael Robinson","title":"OpenLineage Community Manager","url":"https://github.com/merobi-hub","imageURL":"https://github.com/merobi-hub.png","key":"Robinson"},{"name":"Maciej Obuchowski","title":"OpenLineage Committer","url":"https://github.com/mobuchowski","imageURL":"https://github.com/mobuchowski.png","key":"Obuchowski"},{"name":"Julien Le Dem","title":"OpenLineage Project Lead","url":"https://www.github.com/julienledem/","imageURL":"https://avatars.githubusercontent.com/u/367841?v=4","key":"Le Dem"}],"frontMatter":{"title":"The OpenLineage Airflow Provider is Here","date":"2023-08-23T00:00:00.000Z","authors":["Robinson","Obuchowski","Le Dem"],"banner":"./banner.svg","description":"Built-in OpenLineage support in Airflow means big improvements in reliability, lineage output, and custom operator implementation."},"prevItem":{"title":"Metaphor\'s Integration with OpenLineage Enhances Data Governance and Collaboration","permalink":"/blog/metaphor-integration"},"nextItem":{"title":"Meet Us in Toronto on September 18th!","permalink":"/blog/airflow-summit-meetup"}},"content":"This one is big. With the release of Airflow 2.7.0, the Airflow integration is now officially an Airflow Provider. This means the `openlineage-airflow` package is now `apache-airflow-providers-openlineage` in Airflow itself \u2013 a built-in feature of Airflow rather than an externally managed integration. Why does it matter where the integration \u201clives\u201d? The short answer: as an Airflow Provider, the integration will offer *improved reliability, broader support for operators, enhanced lineage, and easier implementation in custom operators* going forward. \\n\\nAlthough still a work in progress in some key respects, the OpenLineage Provider promises to pay dividends to users and contributors alike while accelerating the growth of the OpenLineage Ecosystem.\\n\\n\x3c!--truncate--\x3e\\n\\n### Critical Improvements\\n\\nBefore 2.7.0, OpenLineage metadata was only available via a plugin implementation maintained in the OpenLineage project. In other words, the integration was an external package getting lineage from the outside. Being external to Airflow, the integration had to use extractors to get lineage \u2013 special classes created for all supported operators. In order to function, these locally maintained extractors had to understand operators\u2019 internals and know where to look for data. While being the best possible approach under the circumstances, this solution was hardly ideal. On the one hand, it was brittle because it depended on both operators\u2019 and Airflow\u2019s internals. On the other, it required extra work to maintain compatibility with new versions of providers and Airflow itself. We had to keep up with changes to not only operators *but also Airflow* \u2013 which is not exactly a small, slowly-moving project.\\n\\nImprovements coming with the provider are not limited to fixes, however. The OpenLineage Provider promises to enable some long-sought-after enhancements, including support for one of the most-used Airflow operators \u2013 more about which below. \\n\\n### High-level Design\\n\\nThe provider approach solves these maintenance and reliability issues by moving the extraction logic, along with unit tests, to each provider. Although a lot of up-front work has gone into creating the provider, full implementation of this solution has actually been distributed (and necessarily remains a work in progress). No longer self-contained, the integration is now part of the operator contract and belongs to every provider that supports OpenLineage. Relocating the extraction logic in this way makes the integration more robust by ensuring the stability of the lineage contract in each operator. Another benefit of the approach: adding lineage coverage to custom operators is now easier.    \\n\\n### Implementation\\n\\nThe OpenLineage Provider has been implemented in Airflow by reimplementing the [`openlineage-airflow`](https://github.com/OpenLineage/OpenLineage/tree/main/integration/airflow) package from the OpenLineage project in the `apache-airflow-providers-openlineage` provider in the base Airflow Docker image, where it can be easily enabled by configuration. Furthermore, lineage extraction logic that was included in [Extractors](https://github.com/OpenLineage/OpenLineage/tree/main/integration/airflow/openlineage/airflow/extractors) in that package is now implemented in operators living in their provider package along with unit tests, eliminating the need for Extractors in most cases. For this purpose, a new optional API for Operators (`get_openlineage_facets_on_{start(), complete(ti), failure(ti)}`, documented [here](https://openlineage.io/docs/integrations/airflow/default-extractors)) can be used.\\n\\n#### Example Operator\\n\\nThe Google Cloud Provider in Airflow is one of the providers to which extraction logic has been added. The `get_openlineage_facets_on_complete()` function in the [`gcs_to_gcs`](https://github.com/apache/airflow/blob/main/airflow/providers/google/cloud/transfers/gcs_to_gcs.py#L556) operator shows how easy adding OpenLineage support to an operator can be.\\n   \\n```\\n    def get_openlineage_facets_on_complete(self, task_instance):\\n        \\"\\"\\"\\n        Implementing _on_complete because the execute method does preprocessing on internals.\\n\\n        This means we won\'t have to normalize self.source_object and self.source_objects,\\n        destination bucket and so on.\\n        \\"\\"\\"\\n        from openlineage.client.run import Dataset\\n\\n        from airflow.providers.openlineage.extractors import OperatorLineage\\n\\n        return OperatorLineage(\\n            inputs=[\\n                Dataset(namespace=f\\"gs://{self.source_bucket}\\", name=source)\\n                for source in sorted(self.resolved_source_objects)\\n            ],\\n            outputs=[\\n                Dataset(namespace=f\\"gs://{self.destination_bucket}\\", name=target)\\n                for target in sorted(self.resolved_target_objects)\\n            ],\\n        )\\n```\\nIn this case, the operator itself presents us with the source and target buckets, and objects which will be copied. Implementing OpenLineage support requires only properly initializing the name and namespace of the object according to the [naming schema](https://github.com/OpenLineage/docs/blob/main/docs/spec/naming.md) \\n\\n#### Implementing the Provider in Custom Operators\\n\\nThe OpenLineage Provider in Airflow makes implementing support for custom operators easy. In fact, now there is nothing stopping you from adding OpenLineage support to your own custom operator. The provider detects OpenLineage methods and calls them when appropriate \u2013 before task execution, after success, or after complete. Also, you don\u2019t have to add all three \u2013 the failure method falls back to the complete method if it\u2019s not present, and the complete method to the start method. \\n\\n### Future Development\\n\\nThe OpenLineage Provider makes possible several sought-after enhancements, including:\\n\\n- Integration with XCom datasets (Airflow AIP-48)\\n- Coverage of `PythonOperator`, the most-used operator in Airflow, including Task Flow support\\n- Support for Hooks, which would track their own lineage to be collected by the `PythonOperator` and presented as its own lineage\\n\\n### Supported Operators\\n\\nThe OpenLineage Provider currently supports the following operators, with support for additional operators coming soon:\\n\\n- Apache Kafka\\n- AWS SageMaker\\n- GCS\\n- Common-SQL, including support for multiple databases like Postgres and MySQL\\n- MS Azure\\n- Snowflake\\n\\nWe welcome contributions and feedback on operator support and will be happy to help anyone get started adding extraction logic to an existing or custom operator.\\n\\n### Additional Resources\\n\\nIf you are interested in participating in the effort to add support for more operators, reach out to us on [Slack](https://bit.ly/lineageslack).\\n\\nFor background on the architecture and implementation plan, read the [proposal](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-53+OpenLineage+in+Airflow).\\n\\nFor guides on getting started with OpenLineage, read the [docs](https://openlineage.io/docs/)."},{"id":"/airflow-summit-meetup","metadata":{"permalink":"/blog/airflow-summit-meetup","source":"@site/blog/airflow-summit-meetup/index.mdx","title":"Meet Us in Toronto on September 18th!","description":"Our first Toronto OpenLineage Meetup will take place alongside Airflow Summit on September 18th.","date":"2023-08-18T00:00:00.000Z","formattedDate":"August 18, 2023","tags":[],"readingTime":1.32,"hasTruncateMarker":true,"authors":[{"name":"Michael Robinson","title":"OpenLineage Community Manager","url":"https://github.com/merobi-hub","imageURL":"https://github.com/merobi-hub.png","key":"Robinson"}],"frontMatter":{"title":"Meet Us in Toronto on September 18th!","date":"2023-08-18T00:00:00.000Z","authors":["Robinson"],"description":"Our first Toronto OpenLineage Meetup will take place alongside Airflow Summit on September 18th."},"prevItem":{"title":"The OpenLineage Airflow Provider is Here","permalink":"/blog/airflow-provider"},"nextItem":{"title":"Announcing OpenLineage 1.0","permalink":"/blog/1.0-release"}},"content":"Join us on Monday, September 18th, 2023, from 5:00-8:00 pm PT ET in Toronto to \\ncontribute to a discussion of the future of OpenLineage. On the tentative agenda: \\n- *Intros*\\n- *Evolution of spec presentation/discussion (project background/history)*\\n- *State of the community*\\n- *Integrating OpenLineage with [Metaphor](https://metaphor.io/) (by special guests [Ye](https://www.linkedin.com/in/yeliu84/) & [Ivan](https://www.linkedin.com/in/ivanperepelitca/))*\\n- *Spark/Column lineage update*\\n- *Airflow Provider update*\\n- *Roadmap Discussion*\\n- *Action items review/next steps*\\n\\nBring your ideas and vision for OpenLineage!\\n\\n\x3c!--truncate--\x3e\\n\\nFood will be provided, and the meetup is open to all. Don\'t miss this opportunity \\nto influence the direction of the standard! We hope to see you there. \\n\\n**Please note that the meetup is not being held at the Airflow Summit conference \\nlocation but at a separate space nearby. See below for details.** \\n\\n**Please [sign up](https://www.meetup.com/openlineage/events/295488014/?utm_medium=referral&utm_campaign=share-btn_savedevents_share_modal&utm_source=link)\\nto let us know you\'re coming.**\\n\\n### Time, Place & Format\\n\\nDate: September 18th, 2023  \\nFormat: In-person  \\nTime: 5:00-8:00 PM ET  \\nAddress: [Canarts Media Studio](https://www.canarts.com/), [600 Bay Street, Unit 410, Toronto, ON M5G 1M6](https://goo.gl/maps/Q4k6MQ9AhsmPUUkX9)  \\nPhone: 416-805-2286\\n\\n#### Getting There\\nCanarts Media Studio is a 3-minute walk away from the Marriott Downtown CF at \\nEaton Centre and accessible via two subway lines, a streetcar line and a bus route.\\n\\n- Subway: Dundas subway station\\n- Subway: St Patrick subway station\\n- Streetcar: Dundas - 505 eastbound and westbound\\n- Bus: Bay - 19 southbound and northbound\\n\\n#### Getting In \\nTake the elevator to the fourth floor. Canarts is in #410.  \\nStuck outside or in the lobby? Post a message in [Slack](https://bit.ly/lineageslack), and someone will come down.\\n\\n### Hope to see you there!"},{"id":"/1.0-release","metadata":{"permalink":"/blog/1.0-release","source":"@site/blog/1.0-release/index.mdx","title":"Announcing OpenLineage 1.0","description":"We are pleased to announce the first major release of OpenLineage. This release completes the first version of a new static lineage feature.","date":"2023-08-04T00:00:00.000Z","formattedDate":"August 4, 2023","tags":[],"readingTime":2.735,"hasTruncateMarker":true,"authors":[{"name":"Shirley Lu","title":"Guest Blogger","url":"https://www.linkedin.com/in/juye-shirley-lu/","imageURL":"https://media.licdn.com/dms/image/C5603AQGTYZE937669w/profile-displayphoto-shrink_800_800/0/1654740220042?e=1702512000&v=beta&t=VcRV18rk31jIfGEwLE2EZkQQUIY5okbfqCS7l9J1p0Y","key":"Lu"}],"frontMatter":{"title":"Announcing OpenLineage 1.0","date":"2023-08-04T00:00:00.000Z","authors":["Lu"],"description":"We are pleased to announce the first major release of OpenLineage. This release completes the first version of a new static lineage feature."},"prevItem":{"title":"Meet Us in Toronto on September 18th!","permalink":"/blog/airflow-summit-meetup"},"nextItem":{"title":"Meet Us in San Francisco on August 30th!","permalink":"/blog/sf-meetup-2"}},"content":"Announcing OpenLineage [v1.0](https://github.com/OpenLineage/OpenLineage/releases/tag/1.0.0)! We\u2019re officially in 1.x territory!\\n\\n\x3c!--truncate--\x3e\\n\\nIt has become more and more apparent to us that managing data has become an O(n^2) problem. Every company is a data company, and every department within an organization is then again a mini data ecosystem in and of itself.  When departments interact, there\u2019s a duplication of effort across pipeline tooling, and deployment of new tools can break existing lineage workflows. \\n\\nThis is why we\u2019re seeing increasing demand for OpenLineage support across various tools, and we\u2019re super excited to see more and more data engineers, developers and database managers joining our community. \\n\\nMomentum seems to be growing behind OpenLineage as the industry standard for runtime lineage collection. At the same time, a consensus appears to be forming that an open spec will get us closest to 100% coverage of data ecosystem tooling, one that is highly granular and also general-purpose enough to be consistent across various data workloads.\\n\\n### An Evolving Spec\\n\\nNow that we\u2019re at v1.0, static lineage has made its way to OpenLineage! Up till now, OpenLineage has focused on \u201cruntime\u201d lineage - i.e., metadata generated when jobs are run. Capturing information as transformations of datasets occur enables precise descriptions of those transformations. \\n\\nThe 1.0 release reflects the fact that there is demand for \\"design-time\\" lineage. The concept behind this is that even when datasets are not being touched yet, lineage metadata about them can still be useful and valuable.\\n\\nAlthough operational lineage covers many use cases, some scenarios call for lineage about jobs that have not run - and might never do so. \\n\\nAlso, in many cases, a combination of both static and runtime approaches provides the best operational results. For example, imagine that a dataset exists in a data warehouse and dashboarding tool for which a pipeline has always been broken. Static lineage will show not only that the dataset exists but also that the pipeline for it has never run or always fails.  \\n\\n\\n### Implementing Static Lineage\\n\\nFor an overview of the implementation, read the [release preview](static-lineage) by Michael Robinson.\\n\\nThe first part of the implementation was authored by Pawe\u0142 Leszczy\u0144ski in [OpenLineage 0.29.2](https://github.com/OpenLineage/OpenLineage/releases/tag/0.29.2), which included two new event types along with support for them in the Python client.\\n\\nAdditional work, contributed by Julien Le Dem and Jakub Dardzi\u0144ski, involved:\\n- adding facet deletion to handle the case in which a user adds and deletes a dataset in the same request ([0.30.1](https://github.com/OpenLineage/OpenLineage/releases/tag/0.30.1))\\n- removing references to facets from the core spec that broke compatibility with the Json schema specification ([1.0.0](https://github.com/OpenLineage/OpenLineage/releases/tag/1.0.0)).\\n\\nOn the Marquez side, adding support for static lineage is ongoing. [Marquez 0.37.0](https://github.com/MarquezProject/marquez/releases/tag/0.37.0) includes support in the API for decoding static events via a new [`EventTypeResolver`](https://github.com/MarquezProject/marquez/pull/2495).\\n\\n### Supporting New Use Cases\\n\\nWith the release of 1.0, we now support use cases like:\\n\\n- bootstrapping of a lineage graph with prospective runs for auditing\\n- capturing dataset ownership changes outside of runs\\n- consuming facets from external systems\\n- creating dataset symlinks more easily\\n\\nStatic lineage promises to fill the blind spots that dynamic lineage alone could not reach, offering a macroscopic view of how data flows and is accessed throughout an entire organization.\\n\\n### Additional Resources\\n\\n* [Getting Started Guide](../getting-started)\\n* [OpenLineage 1.0 Release](https://github.com/OpenLineage/OpenLineage/releases/tag/1.0.0)\\n* [JsonSchema Specification](https://github.com/OpenLineage/OpenLineage/tree/main/spec/OpenLineage.json)\\n* [OpenAPI Specification for HTTP-based Implementation](https://openlineage.io/apidocs/openapi/)"},{"id":"/sf-meetup-2","metadata":{"permalink":"/blog/sf-meetup-2","source":"@site/blog/sf-meetup-2/index.mdx","title":"Meet Us in San Francisco on August 30th!","description":"Our second SF OpenLineage Meetup will take place on August 30th.","date":"2023-08-02T00:00:00.000Z","formattedDate":"August 2, 2023","tags":[],"readingTime":1.005,"hasTruncateMarker":true,"authors":[{"name":"Michael Robinson","title":"OpenLineage Community Manager","url":"https://github.com/merobi-hub","imageURL":"https://github.com/merobi-hub.png","key":"Robinson"}],"frontMatter":{"title":"Meet Us in San Francisco on August 30th!","date":"2023-08-02T00:00:00.000Z","authors":["Robinson"],"description":"Our second SF OpenLineage Meetup will take place on August 30th."},"prevItem":{"title":"Announcing OpenLineage 1.0","permalink":"/blog/1.0-release"},"nextItem":{"title":"OpenLineage 1.0, Featuring Static Lineage, is Coming Soon","permalink":"/blog/static-lineage"}},"content":"Join us on Wednesday, August 30th, 2023, from 5:30-8:30 pm PT at the Astronomer offices \\nin San Francisco to learn more about the present and future of OpenLineage. Meet \\nother members of the ecosystem, learn about the project\u2019s goals and fundamental \\ndesign, and participate in a discussion about the future of the project. Bring \\nyour ideas and vision for OpenLineage!\\n\\nAlso on the agenda: a presentation by new contributor/partner [John Lukenoff](https://github.com/jlukenoff), \\nwho will be speaking about his lineage use case.\\n\\n\x3c!--truncate--\x3e\\n\\nFood will be provided, and the meetup is open to all. Don\'t miss this opportunity \\nto influence the direction of this important new standard! We hope to see you \\nthere. \\n\\n**Please [sign up](https://www.meetup.com/meetup-group-bnfqymxe/events/295195280/?utm_medium=referral&utm_campaign=share-btn_savedevents_share_modal&utm_source=link)\\nto let us know you\'re coming.**\\n\\n### Time, Place & Format\\n\\nDate: August 30th, 2023  \\nFormat: In-person  \\nTime: 5:30-8:30 pm PT  \\nAddress: Astronomer, [8 California Street, 7th Floor, San Francisco, CA 94111](https://goo.gl/maps/7UxfePDNPkneyc8v5?coh=178571&entry=tt)\\n\\n#### Getting There\\nThe Astronomer SF office is in the Financial District at the corner of California\\nand Drumm Streets, catty-cornered from the Market/Drumm Street cable car stop.\\n\\n#### Getting In \\nAn Astronomer team member in the lobby will direct you to the Astronomer offices \\non the seventh floor.\\n\\n### Hope to see you there!"},{"id":"/static-lineage","metadata":{"permalink":"/blog/static-lineage","source":"@site/blog/static-lineage/index.mdx","title":"OpenLineage 1.0, Featuring Static Lineage, is Coming Soon","description":"The release of OpenLineage 1.0 will add static lineage support.","date":"2023-07-18T00:00:00.000Z","formattedDate":"July 18, 2023","tags":[],"readingTime":2.285,"hasTruncateMarker":true,"authors":[{"name":"Michael Robinson","title":"OpenLineage Community Manager","url":"https://github.com/merobi-hub","imageURL":"https://github.com/merobi-hub.png","key":"Robinson"}],"frontMatter":{"title":"OpenLineage 1.0, Featuring Static Lineage, is Coming Soon","date":"2023-07-18T00:00:00.000Z","authors":["Robinson"],"description":"The release of OpenLineage 1.0 will add static lineage support."},"prevItem":{"title":"Meet Us in San Francisco on August 30th!","permalink":"/blog/sf-meetup-2"},"nextItem":{"title":"Join us in New York on June 22nd","permalink":"/blog/nyc-collibra-meetup"}},"content":"The first major release of OpenLineage, 1.0, will add static lineage support.\\n\\n\x3c!--truncate--\x3e\\n\\n### Static, AKA \\"Design,\\" Lineage is Coming Soon \\n\\nOpenLineage 1.0, which is expected early in August, will add support for static lineage to the project.\\n\\nAn initiative to add the provision of static lineage, sometimes also called \\"design\\" or \\"design-time\\" lineage, to OpenLineage came out of conversations with community members at Collibra, Manta and Marquez.\\n\\nData catalogs like those offered by Collibra and Manta will benefit from static lineage support, but so will other users. In one way, this addition represents an exciting new chapter in the history of the project. In another, it represents a return to our roots. Before OpenLineage focused on operational lineage, it supported a form of static lineage.\\n\\n#### What is Static Lineage?\\n\\nOpenLineage has traditionally supported only operational, or \\"runtime,\\" lineage -- metadata emitted when jobs run. In other words, OpenLineage has been engineered to capture information as transformations of datasets are happening, enabling precise descriptions of those transformations.\\n\\nAs part of this process, OpenLineage has nonetheless also captured some static metadata -- specifically, information about jobs (such as the current version of the code) and datasets (such as the schema).\\n\\nWhat was called for was a way to collect such static metadata outside of the run context. \\n\\n#### What Use Cases are Served by Static Lineage?\\n\\nUse cases include:\\n\\n- bootstrapping of a lineage graph with prospective runs for auditing\\n- capturing dataset ownership changes \\n- consuming facets from external systems\\n- creating dataset symlinks more easily\\n\\n#### Implementation Details \\n\\nIn order to add static lineage to the spec, two new event types were proposed: `DatasetEvent` and `JobEvent`.\\n\\nThese new events will add facets at a point in time that will apply to an entity until a new version of the same facet is produced. \\n\\nThe first step in implementing static lineage was completed with the release of [OpenLineage 0.29.2](https://github.com/OpenLineage/OpenLineage/releases/tag/0.29.2), which included two types in the spec for \\"runless\\" metadata: a `DatasetEvent` and `JobEvent` (along with support for the new types in the Python client).\\n\\nThe next steps will include changing the event lifecycle (from running to complete, failed, or aborted) to handle events of the new types, and adding facet deletion to handle the case in which a user adds and deletes a dataset in the same request.\\n\\nAdding support for static lineage in Marquez is also ongoing, and we are excited about the progress there. [Marquez 0.37.0](https://github.com/MarquezProject/marquez/releases/tag/0.37.0) includes support in the API for decoding static events via a new [`EventTypeResolver`](https://github.com/MarquezProject/marquez/pull/2495).\\n\\n\\n#### More Information \\n\\nFor more details including the code changes, see:\\n\\n- the [static lineage proposal](https://github.com/OpenLineage/OpenLineage/blob/main/proposals/1837/static_lineage.md) by Julien Le Dem, Maciej Obuchowski, Benji Lampel and Ross Turk \\n- the initial [pull request](https://github.com/OpenLineage/OpenLineage/pull/1880) by Pawe\u0142 Leszczy\u0144ski"},{"id":"/nyc-collibra-meetup","metadata":{"permalink":"/blog/nyc-collibra-meetup","source":"@site/blog/nyc-collibra-meetup/index.mdx","title":"Join us in New York on June 22nd","description":"A meetup is happening on June 22nd in New York. Please join us!","date":"2023-06-08T00:00:00.000Z","formattedDate":"June 8, 2023","tags":[],"readingTime":0.75,"hasTruncateMarker":true,"authors":[{"name":"Michael Robinson","title":"OpenLineage Community Manager","url":"https://github.com/merobi-hub","imageURL":"https://github.com/merobi-hub.png","key":"Robinson"}],"frontMatter":{"title":"Join us in New York on June 22nd","date":"2023-06-08T00:00:00.000Z","authors":["Robinson"],"description":"A meetup is happening on June 22nd in New York. Please join us!"},"prevItem":{"title":"OpenLineage 1.0, Featuring Static Lineage, is Coming Soon","permalink":"/blog/static-lineage"},"nextItem":{"title":"2023 Ecosystem Survey","permalink":"/blog/ecosystem-survey"}},"content":"Join us on Thursday, June 22nd, 2023, from 6:00-8:00 pm ET at Collibra\'s HQ in \\nNew York to discuss the present and future of OpenLineage. Meet other members of \\nthe ecosystem, learn about the project\u2019s goals and fundamental design, and \\nparticipate in a discussion about the future of the project. Bring your ideas \\nand vision for OpenLineage!\\n\\n\x3c!--truncate--\x3e\\n\\nThe meetup is open to all. Snacks, non-alcoholic drinks and pizza will be \\nprovided. Don\'t miss this opportunity to influence the direction of this \\nimportant new standard! We hope to see you there. \\n\\n**Please [sign up](https://www.meetup.com/data-lineage-meetup/events/294065396/) \\nby 6/20 to let us know you\'re coming.**\\n\\n### Time, Place & Format\\n\\nDate: June 22nd, 2023  \\nFormat: In-person  \\nTime: 6:00-8:00 pm ET  \\nAddress: 61 Broadway, New York, NY 10006\\n\\n#### Getting In \\nCheck in will be in the lobby with ID. Then, take the elevator to the 31st floor.\\n\\n### Hope to see you there!"},{"id":"/ecosystem-survey","metadata":{"permalink":"/blog/ecosystem-survey","source":"@site/blog/ecosystem-survey/index.mdx","title":"2023 Ecosystem Survey","description":"The 2023 Ecosystem Survey will provide us with important information about our users, their opinions and needs.","date":"2023-05-23T00:00:00.000Z","formattedDate":"May 23, 2023","tags":[],"readingTime":0.865,"hasTruncateMarker":true,"authors":[{"name":"Michael Robinson","title":"OpenLineage Community Manager","url":"https://github.com/merobi-hub","imageURL":"https://github.com/merobi-hub.png","key":"Robinson"}],"frontMatter":{"title":"2023 Ecosystem Survey","date":"2023-05-23T00:00:00.000Z","authors":["Robinson"],"description":"The 2023 Ecosystem Survey will provide us with important information about our users, their opinions and needs."},"prevItem":{"title":"Join us in New York on June 22nd","permalink":"/blog/nyc-collibra-meetup"},"nextItem":{"title":"Why an Open Standard for Lineage Metadata?","permalink":"/blog/why-open-standard"}},"content":"Our first Ecosystem Survey is now live. Please take a moment to [share your opinions and hopes for OpenLineage](http://bit.ly/ecosystem_survey). \\n\\n\x3c!--truncate--\x3e\\n\\n### An Important Measure and Milestone \\n\\nThis Ecosystem Survey, our first ever, is an effort to gauge how well we are serving our partners and users and to gain a better understanding of your needs.\\n\\nWe plan to conduct a survey like this on an annual basis.\\n\\nThe questions focus on users\':\\n\\n- views on how we should prioritize supporting various tooling categories (e.g., orchestrators),\\n- views on how we should prioritize supporting specific tools in each category,\\n- organizations and roles,\\n- use cases,\\n- motivations for choosing the project,\\n- experience of this community,\\n- alternatives you explored,\\n- and more (but not much!).\\n\\nNote: it might appear longer than it is due to the large number of optional questions. Not all questions apply to all use cases.\\n\\nThank you in advance for taking the time to help us [chart the course of OpenLineage](http://bit.ly/ecosystem_survey)!\\n\\nWe look forward to sharing the results."},{"id":"/why-open-standard","metadata":{"permalink":"/blog/why-open-standard","source":"@site/blog/why-open-standard/index.mdx","title":"Why an Open Standard for Lineage Metadata?","description":"An open standard offers the best route to a universally adopted, persistent metadata specification.","date":"2023-05-22T00:00:00.000Z","formattedDate":"May 22, 2023","tags":[],"readingTime":4.235,"hasTruncateMarker":true,"authors":[{"name":"Michael Robinson","title":"OpenLineage Community Manager","url":"https://github.com/merobi-hub","imageURL":"https://github.com/merobi-hub.png","key":"Robinson"}],"frontMatter":{"title":"Why an Open Standard for Lineage Metadata?","date":"2023-05-22T00:00:00.000Z","authors":["Robinson"],"description":"An open standard offers the best route to a universally adopted, persistent metadata specification."},"prevItem":{"title":"2023 Ecosystem Survey","permalink":"/blog/ecosystem-survey"},"nextItem":{"title":"Meet Us in San Francisco on June 27th!","permalink":"/blog/sf-meetup"}},"content":"We make much of the fact that OpenLineage is an *open* standard. It\u2019s right there in our name. But it shouldn\u2019t go without saying why an open standard for lineage metadata is preferable to a privately held one. The chief advantage of an open standard is precisely the fact that no one person or entity owns it. Hence, it offers the best avenue to a universally adopted, persistent specification.\\n\\n\x3c!--truncate--\x3e\\n\\n### Background\\n\\nIt\u2019s called OpenLineage for a reason \u2013 it\u2019s an open-source [spec](https://github.com/OpenLineage/OpenLineage/blob/main/spec/OpenLineage.md) for the collection of lineage metadata. This is a large part of its appeal. If you ask our users why they chose OpenLineage, they are likely to cite, in addition to its simplicity and desirable [integrations](https://github.com/OpenLineage/OpenLineage/tree/main/integration), the fact that it provides an open spec for lineage. (But please note that lineage metadata is not the only kind of metadata OpenLineage supports, event time and run state being two additional forms of metadata provided by the core spec, and [facets](https://github.com/OpenLineage/OpenLineage/tree/main/spec/facets) provide even more. OpenLineage is easily extensible and offers more than just lineage out of the box!)\\n\\nAn open spec for lineage metadata will be more likely to succeed because it will foster collaboration and hasten wide adoption across the data ecosystem. Advantages of a collaborative, open approach include faster innovation, reduced duplication of effort, and better interoperability between systems. In fact, to our thinking, an open standard is the only way to approach the constantly moving target of 100% coverage of tooling in the fast-moving data space. We also believe that the pursuit of total coverage is worth the short-term challenges involved in getting buy-in across the industry.\\n\\n### Bluetooth: Evidence that Open Standards Work\\n\\nRoss Turk ([@rossturk](https://github.com/rossturk)), one of the early evangelists for OpenLineage, has often cited the example of Bluetooth, a spec, when making the case for OpenLineage. The example is a salutary one.\\n\\nAccording to [Bluetooth](https://www.bluetooth.com/2023-market-update/), 5.4 billion Bluetooth-equipped devices will ship this year. That\u2019s a lot of headsets and waterproof speakers, among the many other things that use Bluetooth, but why did the standard become the dominant spec for short-range wireless connectivity? One possible explanation stands out: it started as an open standard.\\n\\nThe Bluetooth standard has been in development since the late 1990s, [when Nokia, Ericsson and Intel began work on it](https://www.bluetooth.com/about-us/bluetooth-origin/). They knew that only an open standard would make wireless connectivity across devices and industries a reality, but they collaborated on the spec because [neither company was the leader of its market segment](https://en.wikipedia.org/wiki/Bluetooth). Unable to use market dominance to impose a standard, they joined forces instead. When the Bluetooth SIG (Special Interest Group) launched in 1998, it had five members: Ericsson, IBM, Toshiba, Nokia and Intel. Today, [membership stands at over 38,000 companies](https://www.bluetooth.com/about-us/).\\n\\n### OK, but What\u2019s in It for Me?\\n\\nPlaying devil\u2019s advocate, it\u2019s one thing to argue that companies straddling multiple industries and dealing with complex hardware-related challenges can benefit from open standards. It\u2019s perhaps another to argue that companies in the data space can benefit from open standards \u2013 especially when the competition will, too.\\n\\nIf one\u2019s focus is only on short-term gains and losses, then this concern has some merit. A truly open standard is open to all, meaning partners and competitors alike reap the benefits. (Even in the short term, there are ways to differentiate, however.) If one takes a broader view, though, it becomes clear that lineage metadata is only truly valuable to anyone if it offers end-to-end and fully agnostic pipeline visibility. The best way to get total coverage that is reliable and persistent is to get the participation of the metadata producers themselves. \\n\\nThis reality means that, absent a dominant open standard, one\u2019s own stakeholders \u2013 from internal engineering teams to customers to external partners \u2013 will feel the pain of incomplete coverage. This will have long-term implications for productivity, product quality, user experience and, ultimately, profitability. \\n\\nFair enough, but won\u2019t a shared standard dilute member companies\u2019 value propositions? Not if their products are adequately differentiated. Competition through differentiated products, combined with collaboration on a shared standard, is the solution.\\n\\n### What\u2019s in It for the Ecosystem?\\n\\nThe data ecosystem is evolving continuously, with new tools being added daily. Given this constant rate of change, a spec that is open \u2013 and, therefore, more likely to become technology-agnostic \u2013 offers the fastest route to comprehensive and up-to-date pipeline observability.\\n\\nThe speed with which the ecosystem is evolving has meant that, ironically enough, some legacy systems, particularly in the Big Data space, have remained viable for many years. An open standard is better positioned not only to support new tools as they emerge but also to maintain support for legacy systems.\\n\\nIn short, the way to ensure that a standard is tool-agnostic and resilient is to make it a community effort owned by all.     \\n\\n### How Can I Get Involved?\\n\\nAnyone can contribute to OpenLineage by forking the GitHub repository and opening a pull request. For more information about getting started as a contributor, read the [new contributor guide](https://github.com/OpenLineage/OpenLineage/blob/main/CONTRIBUTING.md). Prefer to get your feet wet first? Try our [quickstart guide](https://openlineage.io/getting-started)."},{"id":"/sf-meetup","metadata":{"permalink":"/blog/sf-meetup","source":"@site/blog/sf-meetup/index.mdx","title":"Meet Us in San Francisco on June 27th!","description":"The first SF OpenLineage Meetup will take place on June 27th.","date":"2023-05-16T00:00:00.000Z","formattedDate":"May 16, 2023","tags":[],"readingTime":0.905,"hasTruncateMarker":true,"authors":[{"name":"Michael Robinson","title":"OpenLineage Community Manager","url":"https://github.com/merobi-hub","imageURL":"https://github.com/merobi-hub.png","key":"Robinson"}],"frontMatter":{"title":"Meet Us in San Francisco on June 27th!","date":"2023-05-16T00:00:00.000Z","authors":["Robinson"],"description":"The first SF OpenLineage Meetup will take place on June 27th."},"prevItem":{"title":"Why an Open Standard for Lineage Metadata?","permalink":"/blog/why-open-standard"},"nextItem":{"title":"Meet Us in NYC Later This Month!","permalink":"/blog/nyc-meetup"}},"content":"Join us on Tuesday, June 27th, 2023, from 5:30-8:30 pm PT at the Astronomer offices \\nin San Francisco to learn more about the present and future of OpenLineage. Meet \\nother members of the ecosystem, learn about the project\u2019s goals and fundamental \\ndesign, and participate in a discussion about the future of the project. Bring \\nyour ideas and vision for OpenLineage!\\n\\n\x3c!--truncate--\x3e\\n\\nFood will be provided, and the meetup is open to all. Don\'t miss this opportunity \\nto influence the direction of this important new standard! We hope to see you \\nthere. \\n\\n**Please [sign up](https://www.meetup.com/meetup-group-bnfqymxe/events/293448130/)\\nto let us know you\'re coming.**\\n\\n### Time, Place & Format\\n\\nDate: June 27th, 2023  \\nFormat: In-person  \\nTime: 5:30-8:30 pm PT  \\nAddress: Astronomer, [8 California Street, 7th Floor, San Francisco, CA 94111](https://goo.gl/maps/7UxfePDNPkneyc8v5?coh=178571&entry=tt)\\n\\n#### Getting There\\nThe Astronomer SF office is in the Financial District at the corner of California\\nand Drumm Streets, catty-cornered from the Market/Drumm Street cable car stop.\\n\\n#### Getting In \\nAn Astronomer team member in the lobby will direct you to the Astronomer offices \\non the seventh floor.\\n\\n### Hope to see you there!"},{"id":"/nyc-meetup","metadata":{"permalink":"/blog/nyc-meetup","source":"@site/blog/nyc-meetup/index.mdx","title":"Meet Us in NYC Later This Month!","description":"The next OpenLineage Meetup will take place on April 26th in NYC.","date":"2023-04-04T00:00:00.000Z","formattedDate":"April 4, 2023","tags":[],"readingTime":1.035,"hasTruncateMarker":true,"authors":[{"name":"Michael Robinson","title":"OpenLineage Community Manager","url":"https://github.com/merobi-hub","imageURL":"https://github.com/merobi-hub.png","key":"Robinson"}],"frontMatter":{"title":"Meet Us in NYC Later This Month!","date":"2023-04-04T00:00:00.000Z","authors":["Robinson"],"description":"The next OpenLineage Meetup will take place on April 26th in NYC."},"prevItem":{"title":"Meet Us in San Francisco on June 27th!","permalink":"/blog/sf-meetup"},"nextItem":{"title":"Meet Us at Data Council Austin","permalink":"/blog/data-council-meetup"}},"content":"Join us on Wednesday, April 26th, 2023 from 5:30-8:30 pm at the Astronomer offices \\nin New York, NY, to learn more about the present and future of OpenLineage. Meet \\nother members of the ecosystem, learn about the project\u2019s goals and fundamental \\ndesign, and participate in a discussion about the future of the project. Bring \\nyour ideas and vision for OpenLineage!\\n\\n\x3c!--truncate--\x3e\\n\\nFood will be provided, and the meetup is open to all. Don\'t miss this opportunity \\nto influence the direction of this important new standard! We hope to see you \\nthere. \\n\\n**Please [sign up](https://www.meetup.com/data-lineage-meetup/events/292897496) \\nto let us know you\'re coming.**\\n\\n### Time, Place & Format\\n\\nDate: April 26th, 2023  \\nFormat: In-person  \\nTime: 5:30-8:30 pm ET  \\nAddress: Astronomer, [636 6th Avenue, 3rd Floor, New York, NY 10011](https://goo.gl/maps/y2GZdg3St4PWHzh66)\\n\\n#### Getting There\\nThe Astronomer NY offices are in the Flatiron District at the intersection of 6th \\nAvenue and 19th Street, one block east of the 18th Street Metro station. The entrance \\nto the building is on 19th Street.\\n\\n#### Getting In \\n- If you arrive before 6 pm, simply come on up to the third floor! Otherwise, post \\na message in [Slack](http://bit.ly/OpenLineageSlack) to let us know you\'re here, \\nand someone will let you in.\\n\\n### Hope to see you there!"},{"id":"/data-council-meetup","metadata":{"permalink":"/blog/data-council-meetup","source":"@site/blog/data-council-meetup/index.mdx","title":"Meet Us at Data Council Austin","description":"The OpenLineage community will be gathering on March 30th at Data Council Austin -- join us!","date":"2023-02-28T00:00:00.000Z","formattedDate":"February 28, 2023","tags":[],"readingTime":1.085,"hasTruncateMarker":true,"authors":[{"name":"Michael Robinson","title":"OpenLineage Community Manager","url":"https://github.com/merobi-hub","imageURL":"https://github.com/merobi-hub.png","key":"Robinson"}],"frontMatter":{"title":"Meet Us at Data Council Austin","date":"2023-02-28T00:00:00.000Z","authors":["Robinson"],"description":"The OpenLineage community will be gathering on March 30th at Data Council Austin -- join us!"},"prevItem":{"title":"Meet Us in NYC Later This Month!","permalink":"/blog/nyc-meetup"},"nextItem":{"title":"Happening Soon - Our First Meetup!","permalink":"/blog/data-lineage-meetup"}},"content":"[Data Council](https://www.datacouncil.ai/), known for putting on wonderful \\nevents that attract the best and brightest speakers in the data ecosystem, will \\nbe holding its only [conference of 2023](https://www.datacouncil.ai/austin) in \\nAustin, Texas, on March 28-30th, and OpenLineage will be there. \\n\\n\x3c!--truncate--\x3e\\n\\nAmong the speakers at this year\'s event will be OpenLineage Project Lead and \\n[Astronomer](https://astronomer.io) Chief Architect [Julien Le Dem](https://www.datacouncil.ai/talks/ten-years-of-building-open-source-standards?hsLang=en).\\nJulien will be speaking on March 30th at 10 am CST.\\n\\nAlso, we are planning an OpenLineage event for conference attendees. Join Julien \\nand other members of the OpenLineage community on March 30th from 12:15-1:30 pm \\nat the [AT&T Hotel and Conference Center](https://meetattexas.com/hotel) on the \\ncampus of UT Austin for a community meetup.\\n\\nYou can expect a wide-ranging, open-ended discussion of the past, present and \\nfuture of the OpenLineage spec. You\'ll also meet other members of the data \\necosystem, learn more about the project\'s goals and design, and be able to \\nparticipate in a conversation about what\'s next.\\n\\nIf you haven\'t registered for this event yet, click \\n[this link](https://www.tickettailor.com/events/datacouncil/747883?hsCtaTracking=dc0af239-1ec5-4c9f-a175-f487bb858074%7Ca9aa7625-43fb-436d-b05d-9666bab6e414&_ga=2.239870401.316757827.1677608801-1529956930.1676647160) \\nand use the code OpenLineage20. Also, watch this space for additional details \\nabout the meetup as the date draws near. We hope to see you there.\\n\\n### Meetup Details\\n- Date: March 30, 2023\\n- Time: 12:15-1:30 pm CST \\n- Place: AT&T Hotel and Conference Center, UT Austin, Room 103"},{"id":"/data-lineage-meetup","metadata":{"permalink":"/blog/data-lineage-meetup","source":"@site/blog/data-lineage-meetup/index.mdx","title":"Happening Soon - Our First Meetup!","description":"The inaugural Data Lineage Meetup will take place on March 9th in Providence, RI.","date":"2023-02-08T00:00:00.000Z","formattedDate":"February 8, 2023","tags":[],"readingTime":1.38,"hasTruncateMarker":true,"authors":[{"name":"Michael Robinson","title":"OpenLineage Community Manager","url":"https://github.com/merobi-hub","imageURL":"https://github.com/merobi-hub.png","key":"Robinson"}],"frontMatter":{"title":"Happening Soon - Our First Meetup!","date":"2023-02-08T00:00:00.000Z","authors":["Robinson"],"description":"The inaugural Data Lineage Meetup will take place on March 9th in Providence, RI."},"prevItem":{"title":"Meet Us at Data Council Austin","permalink":"/blog/data-council-meetup"},"nextItem":{"title":"OpenLineage Advances to Incubation Stage with the LFAI & Data","permalink":"/blog/incubation-stage-lfai"}},"content":"Join us on Thursday, March 9, 2023 from 6-8 pm in Providence, Rhode Island, to learn more about the present and future of OpenLineage. Meet other members of the ecosystem, learn about the project\u2019s goals and fundamental design, and participate in a discussion about the future of the project. Bring your ideas and vision for OpenLineage!\\n\\n\x3c!--truncate--\x3e\\n\\nFood will be provided, and the meetup is open to all. Don\'t miss this opportunity to influence the direction of this important new standard! We hope to see you there. Please [sign up](https://www.meetup.com/providence-data-lineage-meetup/events/291847062/) to let us know you\'re coming.\\n\\n### Time, Place & Format\\n\\nDate: March 9, 2023  \\nFormat: In-person  \\nTime: 6-8 pm ET  \\nAddress: [CIC](https://cic.com/providence), [225 Dyer Street](https://goo.gl/maps/NYBbs4ht91dWWaRs9), Providence, RI, US 02903  \\n<p align=\\"center\\"><iframe src=\\"https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d2973.371855436124!2d-71.41054938451008!3d41.82029027700877!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x89e445d43ee241e7%3A0x2f2bda0de0f233c9!2sCIC%20Providence!5e0!3m2!1sen!2sus!4v1676559511501!5m2!1sen!2sus\\" width=\\"600\\" height=\\"450\\" style={{border:0}} allowfullscreen=\\"\\" loading=\\"lazy\\" referrerpolicy=\\"no-referrer-when-downgrade\\"></iframe></p>\\n\\n#### Getting There\\n- Air: the nearest airport is T.F. Green/PVD. Boston Logan is also within 1.5-2 hours\' driving distance.  \\n- Rail: Amtrak serves [PVD](https://goo.gl/maps/eAMNPcpQVJkqwyS76), which is within walking distance of CIC.  \\n- Road: garages and lots are a short walk away from the venue, and metered street parking is also available nearby.\\n    - [Richmond Garage](https://goo.gl/maps/nG8j8Vk6ko75GXaH9) \\n    - [South Street Landing garage](https://goo.gl/maps/QYrbo1oaiU4J3fi69) \\n    - [Clifford parking lot](https://goo.gl/maps/qHoLkRcGmWpxeHhJ8) \\n\\n#### Getting In \\n- Check in with the CIC concierge inside the north entrance. The concierge will direct you to the Hope Island Room on the 3rd floor.\\n\\n#### Arriving Early?\\n- Come to the coffee bar in [Plant City](https://www.plantcitypvd.com/) at 334 South Water Street, Providence RI 02903, which is a short walk from CIC. Other out-of-towners will be meeting up there between 3 and 6 pm.\\n<p align=\\"center\\"><iframe src=\\"https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d2973.3487782837556!2d-71.40636338451014!3d41.82078727697768!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x89e4453e4ce11105%3A0x16bdf76c62b9a960!2sPlant%20City!5e0!3m2!1sen!2sus!4v1676559397689!5m2!1sen!2sus\\" width=\\"600\\" height=\\"450\\" style={{border:0}} allowfullscreen=\\"\\" loading=\\"lazy\\" referrerpolicy=\\"no-referrer-when-downgrade\\"></iframe></p>\\n\\n### Hope to see you there!"},{"id":"/incubation-stage-lfai","metadata":{"permalink":"/blog/incubation-stage-lfai","source":"@site/blog/incubation-stage-lfai/index.mdx","title":"OpenLineage Advances to Incubation Stage with the LFAI & Data","description":"OpenLineage has achieved Incubation status with the LFAI & Data.","date":"2023-01-17T00:00:00.000Z","formattedDate":"January 17, 2023","tags":[],"readingTime":1.37,"hasTruncateMarker":true,"authors":[{"name":"Michael Robinson","title":"OpenLineage Community Manager","url":"https://github.com/merobi-hub","imageURL":"https://github.com/merobi-hub.png","key":"Robinson"}],"frontMatter":{"title":"OpenLineage Advances to Incubation Stage with the LFAI & Data","date":"2023-01-17T00:00:00.000Z","authors":["Robinson"],"description":"OpenLineage has achieved Incubation status with the LFAI & Data."},"prevItem":{"title":"Happening Soon - Our First Meetup!","permalink":"/blog/data-lineage-meetup"},"nextItem":{"title":"What\'s in a Namespace?","permalink":"/blog/whats-in-a-namespace"}},"content":"OpenLineage has achieved Incubation status with the LFAI & Data.\\n\\n\x3c!--truncate--\x3e\\n\\nAt the December meeting of the LFAI & Data Foundation TAC, members voted to advance OpenLineage to the Incubation stage of their program! This required us to earn a Silver Badge from the [OpenSSF](https://bestpractices.coreinfrastructure.org/en), get 300+ stars on GitHub (we have over 1100), and make an in-depth presentation about the project and our community to the TAC. \\n\\n### What It Means\\n\\nNow that we\u2019ve cleared this hurdle, we have access to additional services from the foundation, including assistance with creative work, marketing and communication support, and event-planning assistance. Graduation from the program, which will earn us a voting seat on the TAC, is on the horizon. Stay tuned for updates on our progress with the foundation. \\n\\n### About the LFAI & Data\\n\\nLF AI & Data is an umbrella foundation of the Linux Foundation that supports open source innovation in artificial intelligence (AI) and data. LF AI & Data was created to support open source AI and data, and to create a sustainable open source AI and data ecosystem that makes it easy to create AI and data products and services using open source technologies. They foster collaboration under a neutral environment with an open governance in support of the harmonization and acceleration of open source technical projects.\\n\\nFor more info about the foundation and other LFAI & Data projects, visit their [website](https://lfaidata.foundation/).\\n\\n### What\'s Next \\n\\nThe next step for the project is Graduation, which we expect to happen early this summer. Requirements for Graduation include 1000 stars on GitHub and the OpenSSF Gold Badge. Watch this space for updates on our progress."},{"id":"/whats-in-a-namespace","metadata":{"permalink":"/blog/whats-in-a-namespace","source":"@site/blog/whats-in-a-namespace/index.mdx","title":"What\'s in a Namespace?","description":"Namespaces enable powerful insights into distributed workflows.","date":"2023-01-13T00:00:00.000Z","formattedDate":"January 13, 2023","tags":[],"readingTime":10.775,"hasTruncateMarker":true,"authors":[{"name":"Michael Robinson","title":"OpenLineage Community Manager","url":"https://github.com/merobi-hub","imageURL":"https://github.com/merobi-hub.png","key":"Robinson"}],"frontMatter":{"title":"What\'s in a Namespace?","date":"2023-01-13T00:00:00.000Z","authors":["Robinson"],"description":"Namespaces enable powerful insights into distributed workflows."},"prevItem":{"title":"OpenLineage Advances to Incubation Stage with the LFAI & Data","permalink":"/blog/incubation-stage-lfai"},"nextItem":{"title":"At Manta, OpenLineage Opens Doors to New Insights","permalink":"/blog/manta-integration"}},"content":"Namespaces enable powerful insights into distributed workflows.\\n\\n\x3c!--truncate--\x3e\\n\\n### Background\\n\\nWith all due respect to Shakespeare\'s [Juliet](https://shakespeare.folger.edu/shakespeares-works/romeo-and-juliet/act-2-scene-2/?search=rose/#line-2.2.41), in the [OpenLineage](https://github.com/OpenLineage/OpenLineage) spec at least, names in general -- and namespaces in particular -- are everything.\\n\\nOK, that\u2019s an exaggeration, but not by much. The function of namespaces is to provide unique IDs for everything in the lineage graph so that jobs and datasets can be rendered as nodes. This means namespaces make stitching input and output datasets together as pipelines possible \u2013 which is to say they effectively make lineage possible. In the broader context of the spec, namespaces reflect the importance of naming and naming conventions to the way OpenLineage constructs lineage. \\n\\n![Namespace Selector](./namespace_selector.png)\\n\\nIn creating pipelines organized according to data sources (in the case of datasets) or schedulers (in the case of jobs), namespaces enable focused insight into data flows, even when datasets and workflows are distributed across an organization. This focus enabled by namespaces is key to the production of useful lineage. If everything lived in a single namespace, every lineage graph would show everything that happened in an ecosystem \u2013 and be too cluttered to be useful. \\n\\n### Namespaces in the Spec \\n\\nA look at the spec provides additional detail about namespaces. In the spec, namespaces are at the top of the hierarchy, which means that they have priority over datasets, jobs, and the graphs that connect them. Namespaces contain graphs, in fact, along with just about everything else in a datasource or scheduler\u2019s domain. Ultimately, this reflects the spec\u2019s bias towards tracking dataset and job transformations. \\n\\nTo wit: the same code applied to different input datasets results in different jobs (not different runs of the same job). If those jobs share a scheduler, they will also share a namespace \u2013 but not a graph, which makes tracking the transformations much easier. Similarly, if different input datasets share a datasource, they will also share a namespace (but not a graph). \\n\\nAs you can see, the track switching can get a little complicated, but the namespace abstraction has some clear advantages.\\n\\n### Namespaces in the Wild\\n\\nConsider the common scenario in which multiple teams in an organization maintain pipelines that access the same datasets. Now, imagine trying to collect and display lineage from the organization\u2019s ecosystem without having a way to distinguish between the different pipelines that use the same datasets. The ambiguous metadata would make any graph so cluttered as to be practically meaningless. \\n\\nSuffice it to say, without a strategy for naming at that macro level of the ecosystem, creating a meaningful graph and tracking transformations is much more difficult. Namespaces also ensure that lineage is meaningful irrespective of the various sources of a job\u2019s metadata. A scope above the dataset and run makes heterogeneous, holistic lineage possible in the case of datasets and jobs.\\n\\n```\\nWe define the unique name strategy per resource to ensure it is followed uniformly independently from who is producing metadata and we can connect lineage from various sources.\\n```\\n\\nIn sum, namespaces make operational lineage possible \u2013 which is, while maybe not everything, close to it.  \\n\\n### Consulting the Marquez API \\n\\nThanks to the [Marquez API reference](https://marquezproject.github.io/marquez/openapi.html), we know that a namespaces endpoint is available that we can query for all namespaces.\\n\\nIf you use curl to do so, here\u2019s what you\u2019ll get after building Marquez from source with seed data (using `./docker/up.sh --build --seed`):\\n\\n```\\n\u279c  marquez git:(main) \u2717 curl -v http://localhost:5000/api/v1/namespaces | jq\\n{\\n  \\"namespaces\\": [\\n    {\\n      \\"name\\": \\"default\\",\\n      \\"createdAt\\": \\"2022-12-07T15:02:24.135154Z\\",\\n      \\"updatedAt\\": \\"2022-12-07T15:02:24.135154Z\\",\\n      \\"ownerName\\": \\"anonymous\\",\\n      \\"description\\": \\"The default global namespace for dataset, job, and run metadata not belonging to a user-specified namespace.\\",\\n      \\"isHidden\\": false\\n    },\\n    {\\n      \\"name\\": \\"food_delivery\\",\\n      \\"createdAt\\": \\"2020-02-22T22:42:42Z\\",\\n      \\"updatedAt\\": \\"2020-02-22T22:42:42Z\\",\\n      \\"ownerName\\": \\"anonymous\\",\\n      \\"description\\": null,\\n      \\"isHidden\\": false\\n    }\\n  ]\\n}\\n``` \\n\\nThe namespaces endpoint returns all the available namespaces, which is helpful because, as we\u2019ll see, so much of the information available from the API requires a namespace. For this reason alone, you might say the namespace is the \u201croot\u201d of the object model.\\n\\nSay you want to retrieve one or more datasets from the API. First, you\u2019ll need a namespace:\\n\\n```\\n\u279c  marquez git:(main) \u2717 curl -v http://localhost:5000/api/v1/namespaces/food_delivery/datasets/public.drivers | jq\\n{\\n  \\"id\\": {\\n    \\"namespace\\": \\"food_delivery\\",\\n    \\"name\\": \\"public.drivers\\"\\n  },\\n  \\"type\\": \\"DB_TABLE\\",\\n  \\"name\\": \\"public.drivers\\",\\n  \\"physicalName\\": \\"public.drivers\\",\\n  \\"createdAt\\": \\"2020-02-22T22:42:42Z\\",\\n  \\"updatedAt\\": \\"2020-02-22T22:42:42Z\\",\\n  \\"namespace\\": \\"food_delivery\\",\\n  \\"sourceName\\": \\"default\\",\\n  \\"fields\\": [\\n    {\\n      \\"name\\": \\"id\\",\\n      \\"type\\": \\"INTEGER\\",\\n      \\"tags\\": [],\\n      \\"description\\": \\"The unique ID of the driver.\\"\\n    }, \u2026\\n```\\nSay you want to retrieve a job. You\u2019ll need a namespace:\\n\\n```\\n\u279c  marquez git:(main) \u2717 curl -v http://localhost:5000/api/v1/namespaces/food_delivery/jobs/etl_order_status | jq\\n{\\n  \\"id\\": {\\n    \\"namespace\\": \\"food_delivery\\",\\n    \\"name\\": \\"etl_order_status\\"\\n  },\\n  \\"type\\": \\"BATCH\\",\\n  \\"name\\": \\"etl_order_status\\",\\n  \\"simpleName\\": \\"etl_order_status\\",\\n  \\"parentJobName\\": null,\\n  \\"createdAt\\": \\"2020-02-22T22:42:42Z\\",\\n  \\"updatedAt\\": \\"2020-02-22T22:44:52Z\\",\\n  \\"namespace\\": \\"food_delivery\\",\\n  \\"inputs\\": [],\\n  \\"outputs\\": [\\n    {\\n      \\"namespace\\": \\"food_delivery\\",\\n      \\"name\\": \\"public.order_status\\"\\n    }\\n  ], \u2026\\n```\\nRuns? You\u2019ll need a namespace for those:\\n\\n```\\n\u279c  marquez git:(main) \u2717 curl -v http://localhost:5000/api/v1/namespaces/food_delivery/jobs/etl_order_status/runs | jq\\n{\\n  \\"runs\\": [\\n    {\\n      \\"id\\": \\"b7098939-87f0-4207-878f-dfd8e8804d8a\\",\\n      \\"createdAt\\": \\"2020-02-22T22:42:42Z\\",\\n      \\"updatedAt\\": \\"2020-02-22T22:44:52Z\\",\\n      \\"nominalStartTime\\": null,\\n      \\"nominalEndTime\\": null,\\n      \\"state\\": \\"COMPLETED\\",\\n      \\"startedAt\\": \\"2020-02-22T22:42:42Z\\",\\n      \\"endedAt\\": \\"2020-02-22T22:44:52Z\\",\\n      \\"durationMs\\": 130000,\\n      \\"args\\": {},\\n      \\"jobVersion\\": {\\n        \\"namespace\\": \\"food_delivery\\",\\n        \\"name\\": \\"etl_order_status\\",\\n        \\"version\\": \\"44ca508b-43cc-392f-bbd2-9ca77d501afa\\"\\n      },\\n      \\"inputVersions\\": [],\\n      \\"outputVersions\\": [\\n        {\\n          \\"namespace\\": \\"food_delivery\\",\\n          \\"name\\": \\"public.order_status\\",\\n          \\"version\\": \\"0c16298c-cbe2-3547-8429-309917290570\\"\\n        }\\n      ],\\n      \\"context\\": {\\n        \\"sql\\": \\"INSERT INTO order_status (id, transitioned_at, status, order_id, customer_id, restaurant_id, driver_id)\\\\n SELECT id, transitioned_at, status, order_id, customer_id, restaurant_id, driver_id\\\\n   FROM tmp_order_status;\\"\\n      },\\n      \\"facets\\": {}\\n    }\\n  ]\\n}\\n```\\nAs the API reveals, namespaces really are key in the spec: they organize and unlock most of the insights OpenLineage has to offer.\\n\\n### Dataset Namespaces\\n\\nHaving explored the thinking behind namespaces and their role in the spec, we can get into how they organize datasets and jobs. Let\u2019s start with datasets, the simpler of the two cases due to the more straightforward way datasets\u2019 namespaces are constructed.\\n\\nIn short, a dataset\u2019s namespace is always tied to its datasource. As the spec says, `the namespace for a dataset is the unique name for its datasource.`  \\n\\nData sources vary, however, so the specific construction of dataset namespaces also varies across datasource types. (But they tend to map to databases.)\\n\\nIn the case of Postgres, Mysql, and Trino, for example, we derive the namespace of a dataset from a combination of the scheme, host, and port of the service instance:\\n\\n```\\n* Namespace: postgres://{host}:{port} of the service instance.  \\n    * Scheme = postgres\\n    * Authority = {host}:{port}\\n* Namespace: mysql://{host}:{port} of the service instance.  \\n    * Scheme = mysql\\n    * Authority = {host}:{port}\\n* Namespace: trino://{host}:{port} of the service instance.  \\n    * Scheme = trino\\n    * Authority = {host}:{port}\\n```\\nRedshift requires a different strategy. It\u2019s possible to interact with Redshift via SQL and an API, so a Redshift namespace is composed of a cluster identifier, region name and port \u2013 the only common unique ID available to both methods: \\n\\n```\\n* Namespace: redshift://{cluster_identifier}.{region_name}:{port} of the cluster instance.  \\n    * Scheme = redshift\\n    * Authority = {cluster_identifier}:{port}\\n```\\nSnowflake and Amazon\u2019s serverless Athena warehouse service, which do not require a port, are even simpler:\\n\\n```\\n* Namespace: awsathena://athena.{region_name}.amazonaws.com of the service instance.  \\n    * Scheme = awsathena\\n    * Authority = athena.{region_name}.amazonaws.com\\n* Namespace: snowflake://{account name}  \\n    * Scheme = snowflake\\n    * Authority = {account name}\\n```\\nAnd so on. As you can see, the provenance of dataset namespaces is pretty straightforward: it\u2019s the data source.\\n\\nThat said, sometimes deriving the data source of a dataset is not a simple operation. Some datasets can be identified two different ways, for example. A Spark dataset can be written using a Hive metastore and table name but read using the physical location of the data. Before we added the [`SymlinksDatasetFacet`](https://github.com/OpenLineage/OpenLineage/blob/main/spec/facets/SymlinksDatasetFacet.json), this naming conflict could break the lineage graph. Symlinks both provide alternative dataset names as a workaround in such cases and contain extra information about the datasets.\\n\\n### Job Namespaces \\n\\nWe\u2019ve seen that for datasets the namespace is determined by the data source. But jobs are a different animal, so their namespaces are also different. How is a job\u2019s namespace derived? \\n\\nAs in the case of datasets, the unique naming of jobs is essential, and a job\u2019s unique name consists of a namespace and name. Unlike datasets, jobs descend from schedulers, not data sources. Also unlike datasets, jobs are reducible: a job is composed of executions, or runs (as you can see from the \u201cHistorial de Ejecuciones\u201d tab \u2013 if you were to take the new language switcher for a spin and select Spanish, that is!).\\n\\n![Historial de Ejecuciones Tab](./historial_de_ejecuciones.png)\\n\\nConsulting [the spec](https://github.com/OpenLineage/OpenLineage/blob/main/spec/Naming.md), we find more detail about the naming of jobs:\\n\\n```\\nJobs have a name that is unique to them in their namespace by construction.\\nThe Namespace is the root of the naming hierarchy. The job name is constructed to identify the job within that namespace.\\nExample:  \\n* Airflow:  \\n    * Namespace: the namespace is assigned to the airflow instance. Ex: airflow-staging, airflow-prod\\n    * Job: each task in a DAG is a job. name: {dag name}.{task name}\\n* Spark:  \\n    * Namespace: the namespace is provided as a configuration parameter as in airflow. If there\'s a parent job, we use the same namespace, otherwise it is provided by configuration.\\n    * Spark app job name: the spark.app.name\\n    * Spark action job name: {spark.app.name}.{node.name}\\n```\\nSo, while for datasets it\u2019s all about the datasource, for jobs it\u2019s all about the scheduler. And the `ParentRun` facet makes tracking job namespaces possible.\\n\\n```\\n{\\n  \\"run\\": {\\n    \\"runId\\": \\"run_uuid\\"\\n  },\\n  \\"job\\": {\\n    \\"namespace\\": \\"job_namespace\\",\\n    \\"name\\": \\"job_name\\"\\n  }\\n}\\n```\\nFor all events, if a parent job exists, the facet\u2019s `namespace` value is used to assign a namespace. Otherwise, one is provided by configuration.\\n\\n### What\'s the Point? \\n\\nBut why bother with dataset namespaces in the first place? One answer to this question gets to what is, for some users, a primary value proposition of OpenLineage. A common use case for lineage collection involves tracking dataset access and transformation across jobs and teams in an organization \u2013 for monitoring the use of PII, for example. Tags are supported by OpenLineage and can be used to meet this need, but, depending on how an organization\u2019s ecosystem is constructed, namespaces can also help with this common governance use case.\\n\\nLet\u2019s explore a simple example constructed using the [Python client](https://openlineage.io/docs/client/python/). Imagine that a library\u2019s website has two components, a catalog and a blog, and that both features access the same user and profile tables, both of which contain PII.\\n\\n![Website Schematic](./website_schematic.png)\\n\\nIn the spec, a dataset is unique only within a namespace \u2013 not across multiple namespaces \u2013 so a number of different graphs are possible depending on how the datasets are produced and accessed across an organization. \\n\\nFor example, if for some reason the `users` and `profiles` tables shared two *different* data sources, the tables would belong to two different namespaces (let\u2019s call them `catalog_project` and `blog_project`). While not a typical scenario, this would result in two different, uncluttered graphs of the multiple flows making use of the shared datasets:\\n\\n![Catalog with Different Namespaces](./catalog_with_different_namespaces.png)\\n\\n![Blog with Different Namespaces](./blog_with_different_namespaces.png)\\n\\nThe reason for the simplicity? The `users` and `profiles` tables belong to *both* the `catalog_project` and `blog_project` namespaces.\\n\\nA more typical scenario might involve single versions of the tables being produced by one data source, which would assign them to a single OpenLineage namespace. Ironically, a simpler approach like this results in a more complicated visualization. Notice that the graph remains the same regardless of the namespace selected:\\n\\n![Catalog with One Namespace](./catalog_with_one_namespace.png)\\n\\n![Blog with One Namespace](./blog_with_one_namespace.png)\\n\\nOne advantage of this architecture is that it results in graphs making clear that the datasets containing PII are shared by the two jobs. Depending on an organization\u2019s needs, developers might also find it more convenient to be able to see both jobs and their shared datasets in the same graph.\\n\\nA third scenario might involve the isolation of PII by the use of a dedicated database and, by extension, a dedicated namespace (e.g., `user_data`). In the resulting visualization, the job views above remain the same, but the shared datasets containing PII are now collected in the `user_data` namespace on the datasets page of the Marquez UI:\\n\\n![Datasets in Dedicated Namespace](./pii_namespace.png)\\n\\nNamespaces offer organizations a range of insights into how their teams are accessing and transforming sensitive data.\\n\\n### How to Learn More \\n\\nIf you\u2019re new to OpenLineage and want to check out namespaces in action, a good entry point is the [Getting Started guide](https://openlineage.io/docs/getting-started). There you can learn about the core model, collect run-level metadata using [Marquez](https://marquezproject.ai/) as the [HTTP backend](https://github.com/OpenLineage/OpenLineage#scope), and explore lineage in the Marquez UI.\\n\\nHelpful resources for learning more about the namespaces include the [spec](https://github.com/OpenLineage/OpenLineage/tree/main/spec), where [Naming.md](https://github.com/OpenLineage/OpenLineage/blob/main/spec/Naming.md) is the Rosetta stone for namespace construction and naming conventions in the project.\\n\\nWe also welcome contributions to the project. One of the existing [integrations](https://github.com/OpenLineage/OpenLineage/tree/main/integration) might be a good place to start. Our growing list of partners includes Airflow, dbt, Dagster and Flink.\\n\\nSound fun? Check out the [new contributor guide](https://github.com/MarquezProject/marquez/blob/main/CONTRIBUTING.md) to get started. \\n\\n### Acknowledgments\\n\\nRoss Turk ([@rossturk](https://github.com/rossturk)) and Pawe\u0142 Leszczy\u0144ski ([@pawel-big-lebowski](https://github.com/pawel-big-lebowski)) contributed valuable feedback and suggestions. Any faults are the author\'s own."},{"id":"/manta-integration","metadata":{"permalink":"/blog/manta-integration","source":"@site/blog/manta-integration/index.mdx","title":"At Manta, OpenLineage Opens Doors to New Insights","description":"Adopting OpenLineage as part of our portfolio allows MANTA to bring detailed run-time lineage to our customers.","date":"2022-10-31T00:00:00.000Z","formattedDate":"October 31, 2022","tags":[],"readingTime":1.76,"hasTruncateMarker":true,"authors":[{"name":"Ernie Ostic","title":"SVP of Product at Manta","key":"Ostic"}],"frontMatter":{"title":"At Manta, OpenLineage Opens Doors to New Insights","date":"2022-10-31T00:00:00.000Z","authors":["Ostic"],"description":"Adopting OpenLineage as part of our portfolio allows MANTA to bring detailed run-time lineage to our customers."},"prevItem":{"title":"What\'s in a Namespace?","permalink":"/blog/whats-in-a-namespace"},"nextItem":{"title":"Pursuing Lineage from Airflow using Custom Extractors","permalink":"/blog/extractors"}},"content":"Adopting OpenLineage as part of our portfolio allows MANTA to bring detailed run-time lineage to our customers.\\n\\n\x3c!--truncate--\x3e\\n\\nHere at [MANTA](https://getmanta.com/?utm_source=partner&utm_medium=referral&utm_campaign=OpenLineage), we are very excited to be working closely with [OpenLineage](https://openlineage.io/) and, more importantly, with the OpenLineage Community.  As a leader in lineage analysis, we see first-hand the complexity required to achieve effective lineage, and the benefits of having an accepted standard for the sharing of operational lineage metadata. OpenLineage moves everything in the direction of enhanced interoperability, and helps to ensure that enterprises have maximum flexibility for current and future tool selection.  \\n\\nAdopting OpenLineage as part of our portfolio allows MANTA to bring detailed run-time lineage to our customers, many of whom are enterprise organizations and need this level of granularity. This is especially important for new technologies such as [Apache Airflow](https://airflow.apache.org/), whose integration with OpenLineage continues to evolve.  Apache Airflow, as an example, is increasingly being utilized by our customers as part of their process orchestration portfolio; as such, these companies need lineage coverage for these operations. \\n\\nHaving a recognized industry standard for lineage capture and reporting is an enabler for enhanced metadata management and governance. OpenLineage helps to ensure increased consistency in pipeline analysis, especially as more and more solutions appear in the Cloud, and in the general marketplace, for the transformation, enrichment, and overall movement of information through new and future dataflows. Vendors like MANTA will continue to offer creative and purposeful solutions that answer key questions and meet the end-to-end requirements of the business. For selected technologies, OpenLineage enables us to do this faster and simpler.   \\n\\nEighteen months ago, we started our investigation into OpenLineage. After working with various customers earlier this year, we decided to double down on our investment and get more involved with the OpenLineage Community.  Throughout our journey, we\u2019ve found this growing community to be welcoming, helpful, and collaborative. MANTA is pleased to contribute however we can to this important open source project.  Are you ready to join?\\n\\nFor more information about MANTA\'s data lineage solution, visit [our website](https://getmanta.com/?utm_source=partner&utm_medium=referral&utm_campaign=OpenLineage).\\n\\nTo learn more about contributing to OpenLineage, check out the project\'s [new contributor guide](https://github.com/OpenLineage/OpenLineage/blob/main/CONTRIBUTING.md)."},{"id":"/extractors","metadata":{"permalink":"/blog/extractors","source":"@site/blog/extractors/index.mdx","title":"Pursuing Lineage from Airflow using Custom Extractors","description":"Built-in support for custom extractors makes OpenLineage a highly adaptable solution for pipelines that use Airflow.","date":"2022-09-08T00:00:00.000Z","formattedDate":"September 8, 2022","tags":[],"readingTime":4.16,"hasTruncateMarker":true,"authors":[{"name":"Maciej Obuchowski","title":"OpenLineage Committer","url":"https://github.com/mobuchowski","imageURL":"https://github.com/mobuchowski.png","key":"Obuchowski"},{"name":"Michael Robinson","title":"OpenLineage Community Manager","url":"https://github.com/merobi-hub","imageURL":"https://github.com/merobi-hub.png","key":"Robinson"}],"frontMatter":{"title":"Pursuing Lineage from Airflow using Custom Extractors","date":"2022-09-08T00:00:00.000Z","authors":["Obuchowski","Robinson"],"description":"Built-in support for custom extractors makes OpenLineage a highly adaptable solution for pipelines that use Airflow."},"prevItem":{"title":"At Manta, OpenLineage Opens Doors to New Insights","permalink":"/blog/manta-integration"},"nextItem":{"title":"How Operators and Extractors Work Under-the-Hook","permalink":"/blog/operators-and-extractors-technical-deep-dive"}},"content":"Built-in support for custom extractors makes OpenLineage a highly adaptable solution for pipelines that use Airflow.\\n\\n\x3c!--truncate--\x3e\\n\\n### Overview \\n\\nAirflow is built around operators, each having a different function and requiring a different approach to lineage. The OpenLineage Airflow integration detects which Airflow operators your DAG is using and extracts lineage data from them using extractors.\\n\\nThe community has already authored a number of extractors to support Airflow\u2019s Great Expectations, BigQuery, Python, Postgres, SQL and Bash operators (and more \u2013 you can find all the extractors [here](https://github.com/OpenLineage/OpenLineage/tree/main/integration/airflow/openlineage/airflow/extractors).) Nevertheless, in the course of pursuing lineage, you may find yourself needing to write custom extractors. \\n\\nSome teams use custom extractors to automate repeatable work \u2013 using the same code from `PythonOperator` across a project, for example. Another common use case is that a team needs to use an operator for which a pre-built extractor does not yet exist. Airflow has literally hundreds of operators. \\n\\nBuilt-in support for custom extractors makes OpenLineage a flexible, highly adaptable solution for pipelines that use Airflow for orchestration. \\n\\n### How it works \\n\\nAs we explain in the OpenLineage [docs](https://openlineage.io/docs/integrations/airflow/extractor), custom extractors must be derived from the `BaseExtractor` class (import it from `openlineage.airflow.extractors.base`).\\n\\nExtractors have methods they can implement: `extract`, `extract_on_complete` and `get_operator_classnames`. Either `extract` or `extract_on_complete` is required. The `get_operator_classnames` method, also required, is for providing a list of operators from which your extractor can get lineage.\\n\\nFor example:\\n\\n```\\n@classmethod\\ndef get_operator_classnames(cls) -> List[str]:\\n  return [\'PostgresOperator\']\\n```\\n\\nIf the name of the operator matches one of the names on the list, the extractor will be instantiated \u2013 using the operator passed to the extractor as a parameter and stored in the `self.operator` property \u2013 and both the `extract` and `extract_on_complete` methods will be called. They both return information used by the OpenLineage integration to emit OpenLineage events. The difference is that `extract` is called before the operator\'s `execute` method to generate a `START` event, while `extract_on_complete` is called afterward to generate a `COMPLETE` event. The latter has access to any additional information that the operator leaves behind following execution. A good example of this is the `SnowflakeOperator`, which sets `query_id`s after execution.\\n\\nBoth methods return a `TaskMetadata` structure:\\n\\n```\\n@attr.s\\nclass TaskMetadata:\\n    name: str = attr.ib()  # deprecated\\n    inputs: List[Dataset] = attr.ib(factory=list)\\n    outputs: List[Dataset] = attr.ib(factory=list)\\n    run_facets: Dict[str, BaseFacet] = attr.ib(factory=dict)\\n    job_facets: Dict[str, BaseFacet] = attr.ib(factory=dict)\\n```\\n\\nThe inputs and outputs are lists of plain [OpenLineage datasets](https://openlineage.io/docs/client/python).\\n\\nThe `run_facets` and `job_facets` are dictionaries of optional [`JobFacets`](https://openlineage.io/docs/client/python) and [`RunFacets`](https://openlineage.io/docs/client/python) that accompany a job. For example, you might want to attach a `SqlJobFacet` if your operator is executing SQL.\\n\\nNote: in order for a custom extractor to work, it must be registered first, so the OpenLineage integration can import it. You can read about how to use environment variables to do this [here](https://openlineage.io/docs/integrations/airflow/extractor#registering-custom-extractor).\\n\\n### Example: the RedshiftDataExtractor\\n\\nIn the `RedshiftDataExtractor`, the `extract_on_complete` method parses SQL, obtains task `stats` using the `get_facets` method of the `RedshiftDataDatasetsProvider` class, and returns a `TaskMetadata` instance. We can see usage of a SQL statement, and the connection is provided by an actual operator. \\n\\n```\\ndef extract_on_complete(self, task_instance) -> Optional[TaskMetadata]:\\n        log.debug(f\\"extract_on_complete({task_instance})\\")\\n        job_facets = {\\"sql\\": SqlJobFacet(self.operator.sql)}\\n\\n        log.debug(f\\"Sending SQL to parser: {self.operator.sql}\\")\\n        sql_meta: Optional[SqlMeta] = parse(self.operator.sql, self.default_schema)\\n        log.debug(f\\"Got meta {sql_meta}\\")\\n        try:\\n            redshift_job_id = self._get_xcom_redshift_job_id(task_instance)\\n            if redshift_job_id is None:\\n                raise Exception(\\n                    \\"Xcom could not resolve Redshift job id. Job may have failed.\\"\\n                )\\n        except Exception as e:\\n            log.error(f\\"Cannot retrieve job details from {e}\\", exc_info=True)\\n            return TaskMetadata(\\n                name=get_job_name(task=self.operator),\\n                run_facets={},\\n                job_facets=job_facets,\\n            )\\n\\n        client = self.operator.hook.conn\\n\\n        redshift_details = [\\n            \\"database\\",\\n            \\"cluster_identifier\\",\\n            \\"db_user\\",\\n            \\"secret_arn\\",\\n            \\"region\\",\\n        ]\\n\\n        connection_details = {\\n            detail: getattr(self.operator, detail) for detail in redshift_details\\n        }\\n\\n        stats = RedshiftDataDatasetsProvider(\\n            client=client, connection_details=connection_details\\n        ).get_facets(\\n            job_id=redshift_job_id,\\n            inputs=sql_meta.in_tables if sql_meta else [],\\n            outputs=sql_meta.out_tables if sql_meta else [],\\n        )\\n\\n        return TaskMetadata(\\n            name=get_job_name(task=self.operator),\\n            inputs=[ds.to_openlineage_dataset() for ds in stats.inputs],\\n            outputs=[ds.to_openlineage_dataset() for ds in stats.output],\\n            run_facets=stats.run_facets,\\n            job_facets={\\"sql\\": SqlJobFacet(self.operator.sql)},\\n        )\\n```\\n\\n### Common issues \\n\\nThere are two common issues associated with custom extractors. \\n\\nFirst, when the wrong path is provided to `OPENLINEAGE_EXTRACTORS`, the extractor isn\u2019t imported and OpenLineage events aren\u2019t emitted. The path needs to be exactly the same as the one you are using in your code. Also, make sure that the extractor code is available to import from Airflow\u2019s Python interpreter.\\n\\nSecond, imports from Airflow can be unnoticeably cyclical. This is due to the fact that OpenLineage code gets instantiated when the Airflow worker itself starts, in contrast to DAG code. OpenLineage extraction can fail as a result. To avoid this issue, make sure that all imports from Airflow are local \u2013 in the `extract` or `extract_on_complete` methods. If you need imports for type checking, guard them behind `typing.TYPE_CHECKING`.\\n\\n### How to get started\\n\\nCheck out the existing extractors [here](https://github.com/OpenLineage/OpenLineage/tree/main/integration/airflow/openlineage/airflow/extractors).\\n\\nRead the docs about the Airflow integration, including tips on registering and debugging your custom extractor, [here](https://openlineage.io/docs/integrations/airflow/). \\n\\n### How to contribute\\n\\nWe welcome your contributions! One of our existing [integrations](https://github.com/OpenLineage/OpenLineage/tree/main/integration) might be a good place to start. OpenLineage\u2019s growing list of partners includes Airflow, dbt, Dagster and Flink.\\n\\nSounds fun? Check out our [new contributor guide](https://github.com/OpenLineage/OpenLineage/blob/main/CONTRIBUTING.md) to get started."},{"id":"/operators-and-extractors-technical-deep-dive","metadata":{"permalink":"/blog/operators-and-extractors-technical-deep-dive","source":"@site/blog/operators-and-extractors-technical-deep-dive/index.mdx","title":"How Operators and Extractors Work Under-the-Hook","description":"A technical deep-dive on how the Airflow OSS and OpenLineage OSS projects interact.","date":"2022-09-07T00:00:00.000Z","formattedDate":"September 7, 2022","tags":[],"readingTime":3.685,"hasTruncateMarker":true,"authors":[{"name":"Benji Lampel","title":"OpenLineage Committer","url":"https://www.github.com/denimalpaca","iamge_url":"https://www.github.com/denimalpaca.png","key":"Lampel"}],"frontMatter":{"title":"How Operators and Extractors Work Under-the-Hook","date":"2022-09-07T00:00:00.000Z","authors":["Lampel"],"description":"A technical deep-dive on how the Airflow OSS and OpenLineage OSS projects interact."},"prevItem":{"title":"Pursuing Lineage from Airflow using Custom Extractors","permalink":"/blog/extractors"},"nextItem":{"title":"The Current State of Column-level Lineage","permalink":"/blog/column-lineage"}},"content":"A technical deep-dive on how the Airflow OSS and OpenLineage OSS projects interact. \\n\\n\x3c!--truncate--\x3e\\n\\n## Overview\\n\\nAirflow Operators and OpenLineage Extractors have a specific, if quirky, way of working together. Recently, the way they work together has seen a bit of an overhaul, and the new SQL Check Extractors added a new and unique way that the extractors work and interact with operators. In this blog, we\'ll demystify these relationships.\\n\\nNote: This blog post describes the relationships of the operators and extractors only for `Airflow>=2.3` and `OpenLineage>=0.12.0`.\\n\\n## Integration\\n\\n### The Operator and the Extractor\\n\\nSome quick definitions are in order before we continue.\\n\\nThe Airflow Operator defines a task, which is the unit of work in Airflow. All operators inherit from the `BaseOperator`, and in addition to taking the arguments of the `BaseOperator`, they can take arguments specific to the kind of task they are going to perform, such as a specific `conn_id` to connect to a datasource or a dictionary of checks to perform on that datasource.\\n\\nThe OpenLineage Extractor is somewhat analogous to the Airflow Operator: it is a unit of work in OpenLineage, which takes the relevant input and output data from an operator, creates OpenLineage data facets, and sends those facets to be displayed in Marquez or Datakin. Each extractor maps to a specific set of operators via the `get_operator_classnames()` class method. The extractors all inherit from a `BaseExtractor`, which defines a few abstract methods, importantly `execute()` and `execute_on_complete()`.\\n\\nBriefly, the two other major OpenLineage constructs in this story are the `ExtractorManager`, which is responsible for identifying the correct extractor to use, and the `Listener`, which is the connecting piece between OpenLineage and Airflow.\\n\\n### Interplay\\n\\nNext, we\'ll walk through what happens when an Airflow instance with OpenLineage support runs a DAG, and how that operator data makes it to the Marquez or Datakin UI.\\n\\nFirst, a DAG is born. When the DAG is run, the scheduler runs the operators in order by calling their `execute()` method. This is the first time the OpenLineage `Listener` responds. Triggered by the `execute()` event, it calls the `Manager`, which identifies the correct extractor based on the `task_id`. Then the `Extractor`\'s `execute()` method is run, potentially returning lineage data in the form of a metadata object. When the operator is done--either succeeded or failed--, the `Listener` calls the `Manager` again, and this time the `Manager` triggers the `Extractor`\'s `execute_on_complete()` method, which may also return metadata based on the result of the task. The metadata object is then sent to Marquez or Datakin, where the data is displayed.\\n\\n### SQL Check Operators/Extractors\\n\\nThe `SQLCheckOperators` and `SQLCheckExtractors` work slightly differently than the interplay outlined above. The biggest difference is that the `SQLCheckExtractors` all inherit from a `BaseSqlCheckExtractor`, which in turn *dynamically* inherits from the appropriate extractor at run time. The appropriate extractor is always some existing SQL database extractor. The `BaseSqlCheckExtractor` itself only implements the `extract_on_complete()` method by determining whether the super class\u2019s `extract()` or `extract_on_complete()` method should be run to gather metadata. The `_get_input_facets()` methods are all implemented by the particular check extractors, and are called in the super class\u2019s `extract()` or `extract_on_complete()` method.\\n\\nThe dynamic inheritance is done by defining the `SQLCheckExtractors` inside the `get_check_extractors()` function that takes a class as a parameter and passes that class to the constructor of the `BaseSqlCheckExtractor`. When a `SQLCheckOperator` is run, and the `Manager` searches for the correct extractor to use, it calls `instantiate_abstract_extractors()` with the given task instance.\\n\\nIn this function, the task instance is used to find the correct extractor that will be the superclass of the `BaseSqlCheckOperator`. To do this, the function uses a set of hard-coded operator names whose extractors will dynamically inherit from the found superclass. Currently, this list is the set of `SQLCheckOperator`s, which corresponds to a dictionary of extractor keys and `conn_type` values. The given task\u2019s class name is checked against the set of operator names, and if it is in the set, a loop compares the existing extractor\u2019s corresponding `conn_type` from the aforementioned dictionary to the given task instance\u2019s `conn_type` retrieved from Airflow connections. If there\u2019s a match, the `get_check_extractors()` method is called with the matched extractor, instantiating all the operators with the correct superclass.\\n\\nThe `SQLCheckExtractors` only rely on the `extract_on_complete()` method, as the values needed from the operators, i.e. the results of the query and the success or failures of the checks, are only available after the operator completes."},{"id":"/column-lineage","metadata":{"permalink":"/blog/column-lineage","source":"@site/blog/column-lineage/index.mdx","title":"The Current State of Column-level Lineage","description":"Column-level lineage helps organizations navigate a complex regulatory landscape.","date":"2022-09-02T00:00:00.000Z","formattedDate":"September 2, 2022","tags":[],"readingTime":4.815,"hasTruncateMarker":true,"authors":[{"name":"Michael Robinson","title":"OpenLineage Community Manager","url":"https://github.com/merobi-hub","imageURL":"https://github.com/merobi-hub.png","key":"Robinson"}],"frontMatter":{"title":"The Current State of Column-level Lineage","date":"2022-09-02T00:00:00.000Z","authors":["Robinson"],"description":"Column-level lineage helps organizations navigate a complex regulatory landscape."},"prevItem":{"title":"How Operators and Extractors Work Under-the-Hook","permalink":"/blog/operators-and-extractors-technical-deep-dive"},"nextItem":{"title":"The Python Client -- the Foundation of OpenLineage Integrations","permalink":"/blog/python-client"}},"content":"Column-level lineage helps organizations navigate a complex regulatory landscape.\\n\\n\x3c!--truncate--\x3e\\n\\n### Overview & background\\n\\nLong one of our most requested new features, column-level lineage was added to the [Spark integration](https://github.com/OpenLineage/OpenLineage/tree/main/integration/spark) with the release of [OpenLineage 0.9.0](https://github.com/OpenLineage/OpenLineage/releases/tag/0.9.0). Project committer Pawe\u0142 Leszczy\u0144ski ([@pawel-big-lebowski](https://github.com/pawel-big-lebowski)) authored the relevant pull requests ([#645](https://github.com/OpenLineage/OpenLineage/pull/645), [#698](https://github.com/OpenLineage/OpenLineage/pull/698), [#738](https://github.com/OpenLineage/OpenLineage/pull/738) and [#772](https://github.com/OpenLineage/OpenLineage/pull/772)). In its current form, column-level lineage in OpenLineage is limited to the Spark integration and not yet visible in the [Marquez](https://marquezproject.ai/) UI. But this is only the first step in a broader, ongoing project to implement the feature across the project, and we\u2019d love your help. \\n\\nColumn-level lineage is a worthy pursuit. It dramatically extends the reach of OpenLineage\u2019s metadata capture, providing finely grained information about datasets\' dependencies. As Pawe\u0142 and project lead Julien Le Dem ([@julienledem](https://github.com/julienledem)) wrote in the initial proposal, \u201cNot only can we know that a dependency exists, but we are also able to understand which input columns are used to produce output columns. This allows [for] answering questions like \u2018Which root input columns are used to construct column x?\u2019\u201d\\n\\nAnother reason to pursue column-level lineage: the demands of regulatory compliance. Bodies such as the [GDPR](https://gdpr-info.eu/), [HIPAA](https://www.hhs.gov/hipaa/index.html), [CCPA](https://oag.ca.gov/privacy/ccpa), [BCBS](https://www.bis.org/bcbs/) and [PCI](https://www.pcisecuritystandards.org/) have instituted requirements for data accuracy and integrity that compel companies and organizations to obtain deeper insight into their datasets and pipelines. \\n\\n### Why start with the Spark integration?\\n\\nAs Julien and Pawe\u0142\'s proposal [suggests](https://github.com/OpenLineage/OpenLineage/tree/main/proposals/148), the Spark integration was a logical starting point for adding column-level lineage. This is so because the integration relies on implementing visitors that traverse a `LogicalPlan` and extract meaningful information when encountered. These data include outputs and inputs with their schemas (which we were already identifying, in fact). The `LogicalPlan` also exposes the expressions that derive the output columns from the input columns. They can be inspected to derive column-level lineage. Traversing the `LogicalPlan` allows for the capturing of all the dependencies required to build column-level lineage.\\n\\n### A new facet in the spec \\n\\nIn the process of implementing column-level lineage, Pawe\u0142 and Julien contributed a new facet schema, `ColumnLineageDatasetFacet`, to the OpenLineage spec. This facet uses fields to relay data points about dependencies. These are properties of items in the `InputField` property of the facet (`namespace`, `name` and `field`), as well as two human-readable string fields (`transformationDescription`, `transformationType`) for conveying information about dataset transformations. The last field, `transformationType`, may be especially useful for those whose companies or organizations need to track the usage of sensitive personal information.\\n\\nAn example of a `columnLineage` facet in the outputs array of a lineage event:\\n\\n    {\\n      \\"namespace\\": \\"{namespace of the outputdataset}\\",\\n      \\"name\\": \\"{name of the output dataset}\\",\\n      \\"facets\\": {\\n        \\"schema\\": {\\n          \\"fields\\": [\\n            { \\"name\\": \\"{first column of the output dataset}\\", \\"type\\": \\"{its type}\\"},\\n            { \\"name\\": \\"{second column of the output dataset}\\", \\"type\\": \\"{its type}\\"},\\n            ...\\n          ]\\n        },\\n        \\"columnLineage\\": {\\n          \\"{first column of the output dataset}\\": {\\n            \\"inputFields\\": [\\n              { \\"namespace\\": \\"{input dataset namespace}\\", name: \\"{input dataset name}\\", \\"field\\": \\"{input dataset column name}\\"},\\n              ... other inputs\\n            ],\\n            \\"transformationDescription\\": \\"identical\\",\\n            \\"transformationType\\": \\"IDENTITY\\"\\n          },\\n          \\"{second column of the output dataset}\\": ...,\\n          ...\\n        }\\n      }\\n    }\\n\\n### How it works\\n\\nAs we\u2019ve seen, column-level lineage is being collected via the new `columnLineage` dataset facet. For each output, this facet contains a list of the output\'s fields along with the input fields used to create it. The input fields are identified by a `namespace`, `name` and `field`. But how is OpenLineage obtaining the data about dependencies that the facet relays?\\n\\nIn PR [#698](https://github.com/OpenLineage/OpenLineage/pull/698), Pawe\u0142 describes the mechanism this way:\\n1. The core mechanism first gets an output schema and logical plan as inputs.\\n2. Then, the `OutputFieldsCollector` class traverses the plan to gather the outputs. Outputs can be extracted from Aggregate or Project, and each output field has an `ExprId` (expression ID) that is attached from the plan.\\n3. Next, the `InputFieldsCollector` class is used to collect inputs that can be extracted from `DataSourceV2Relation`, `DataSourceV2ScanRelation`, `HiveTableRelation` or `LogicalRelation`. Each input field takes its `ExprId` from the plan, and each input is identified by a `DatasetIdentifier`, which means it contains the name and namespace of a dataset and an input field.\\n4. Finally, the `FieldDependenciesCollector` traverses the plan to identify dependencies between different `ExprIds`. Dependencies map parent expressions to children expressions. This is used to identify the inputs used to evaluate certain outputs.\\n\\n### What\u2019s next?\\n\\nWork on extending column-level lineage in the project is ongoing. For example, project committer Will Johnson ([@wjohnson](https://github.com/wjohnson)) has opened a PR ([#963](https://github.com/OpenLineage/OpenLineage/issues/963)) to add support for common dataframe operations not covered due to the initial focus on Spark. As Will writes in the PR,\\n\\n> Currently, the Column Lineage Input Field Collectors work mainly for Spark SQL operations and Data Source V2.\\n> This leaves out normal dataframe operations like inserting into HDFS without the use of a Hive table.\\n> Column Lineage should support this scenario as many users will want to see column lineage for operations outside of SQL and Hive Metastore backed tables.\\n\\nAlso, Pawe\u0142 has written enhancements that will [enable column-level lineage in the case of altered table and column names](https://github.com/OpenLineage/OpenLineage/issues/993) and [allow one to extend column-level lineage without contributing to OpenLineage](https://github.com/OpenLineage/OpenLineage/issues/738) (to avoid exposing proprietary code, for example). \\n\\nMeanwhile, over in Marquez, Julien has contributed a [proposal](https://github.com/MarquezProject/marquez/issues/2045) to add a column-level endpoint to the project that would leverage OpenLineage\u2019s `ColumnLineageDatasetFacet`. This approach would add column lineage to an existing endpoint by embedding the `columnLineage` facet in the data section of the `DATASET` nodes.\\n\\n### How can I contribute?\\n\\nWe welcome contributions to this ongoing effort at implementing column-level lineage in OpenLineage! If you\u2019re interested in contributing, one of our existing [integrations](https://github.com/OpenLineage/OpenLineage/tree/main/integration) might be a good place to start. OpenLineage\u2019s growing list of integrations includes Airflow, dbt, Dagster and Flink.\\n\\nSounds fun? Check out our [new contributor guide](https://github.com/OpenLineage/OpenLineage/blob/main/CONTRIBUTING.md) to get started."},{"id":"/python-client","metadata":{"permalink":"/blog/python-client","source":"@site/blog/python-client/index.mdx","title":"The Python Client -- the Foundation of OpenLineage Integrations","description":"The Python client enables users to create custom integrations.","date":"2022-07-29T00:00:00.000Z","formattedDate":"July 29, 2022","tags":[],"readingTime":3.38,"hasTruncateMarker":true,"authors":[{"name":"Michael Robinson","title":"OpenLineage Community Manager","url":"https://github.com/merobi-hub","imageURL":"https://github.com/merobi-hub.png","key":"Robinson"}],"frontMatter":{"title":"The Python Client -- the Foundation of OpenLineage Integrations","date":"2022-07-29T00:00:00.000Z","authors":["Robinson"],"description":"The Python client enables users to create custom integrations."},"prevItem":{"title":"The Current State of Column-level Lineage","permalink":"/blog/column-lineage"},"nextItem":{"title":"Microsoft Purview Accelerates Lineage Extraction from Azure Databricks","permalink":"/blog/openlineage-microsoft-purview"}},"content":"The Python client enables users to create custom integrations.\\n\\n\x3c!--truncate--\x3e\\n\\n### Introduction\\n\\nThanks to the [OpenLineage](https://github.com/OpenLineage/OpenLineage) community\u2019s active work on [integrations](https://github.com/OpenLineage/OpenLineage/tree/main/integration), the pursuit of lineage is getting more efficient and effective all the time. And our growing list of partners and adapters makes OpenLineage plenty powerful out of the box. At the same time, the nature of the data engineering field means that lineage capture is an ongoing process \u2013 simply put, the work of lineage is never done.\\n\\nHence, as lineage capture becomes integral to your pipelines, situations can arise that require new custom integrations. Enter the [Python client](https://github.com/OpenLineage/OpenLineage/tree/main/client/python), one of two built-in clients included in the project (the other being the [Java client](https://github.com/OpenLineage/OpenLineage/tree/main/client/java)). The OpenLineage spec is defined using JSON schema, but we have created these clients so that new integrations do not have to reinvent the wheel. \\n\\nOpenLineage\u2019s Python client enables the creation of lineage metadata events with Python code. The core data structures currently offered by the client include the `RunEvent`, `RunState`, `Run`, `Job`, `Dataset`, and `Transport` classes. These either configure or collect data for the emission of lineage events.\\n\\nIn the history of the project, the client has been useful in helping us avoid unnecessary duplication of code. It is also integral to OpenLineage\u2019s existing integrations, serving as the basis of the Airflow and dbt integrations, for example. It could also act as the foundation of your own custom integration should you need to write one. (Another use case: tests for a new Airflow extractor.)\\n\\nFor this reason, an existing integration can serve as a useful example of how to use the client to write a new integration (and, hopefully, [contribute it](https://github.com/OpenLineage/OpenLineage/blob/main/CONTRIBUTING.md) back to the project!). What follows is an overview of the Python client and the dbt integration, which uses the Python client. You\u2019ll see how the client receives metadata from dbt to make it available to a consumer of OpenLineage such as Microsoft Purview, Amundsen, Astronomer, Egeria or Marquez.\\n\\n### Python Client Data Structures\\n\\nThe core structures of the client organize metadata about the foundational objects of the OpenLineage spec: runs, jobs and datasets.\\n\\nA `dataset` is a class consisting of a `name`, `namespace` and, optionally, `facets` array:\\n\\n```\\n@attr.s\\nclass Dataset:\\n\\tnamespace: str = attr.ib()\\n\\tname: str - attr.ib()\\n\\tFacets: Dict = attr.ib(factory=dict)\\n```\\n\\nThe same goes for a `job`:\\n\\n```\\n@attr.s\\nclass Job:\\n\\tnamespace: str = attr.ib()\\n\\tname: str - attr.ib()\\n\\tFacets: Dict = attr.ib(factory=dict)\\n```\\n\\nA `RunEvent` sends the time, state, job, run, producer, input and output information needed to construct an OpenLineage job run event:\\n\\n```\\n@attr.s \\nclass RunEvent:\\n\\teventType: RunState = attr.ib(validator=attr.validators.in_(RunState))\\n\\teventTime: str = attr.ib()\\n\\trun: Run = attr.ib()\\n\\tjob: Job = attr.ib()\\n\\tproducer: str = attr.ib()\\n\\tinputs: Optional[List[Dataset]] = attr.ib(factory=list)\\n\\toutputs: Optional[List[Dataset]] = attr.ib(factory=list)\\n```\\n### The OpenLineage-dbt Integration\\n\\nAt a high level, the [dbt integration](https://github.com/OpenLineage/OpenLineage/tree/main/integration/dbt) uses the Python client to push metadata to the OpenLineage backend. The metadata it makes available comprises the run lifecycle, including any dataset inputs and outputs accessed during a job run.\\n\\nIn the `dbt-ol` [script](https://github.com/OpenLineage/OpenLineage/blob/main/integration/dbt/scripts/dbt-ol), the integration uses the project\u2019s `ParentRunMetadata` and `DbtArtifactProcessor` classes, both of which can be found in the OpenLineage common integration, to parse metadata from the dbt `manifest` and `run_result` in order to produce OpenLineage events:  \\n\\n```\\nfrom openlineage.common.provider.dbt import DbtArtifactProcessor, ParentRunMetadata\\n\\n#\u2026\\n\\nif parent_id:\\n\\tparent_namespace, parent_job_name, parent_run_id = parent_id.split(\u2018/\u2019)\\n\\tparent_run_metadata = ParentRunMetadata(\\n\\t\\trun_id=parent_run_id,\\n\\t\\tjob_name=parent_job_name,\\n\\t\\tjob_namespace=parent_namespace\\n\\t)\\n\\nprocessor = DbtArtifactProcessor(\\n\\tproducer=PRODUCER,\\n\\ttarget=target,\\n\\tjob_namespace=job_namespace,\\n\\tproject_dir=project_dir,\\n\\tprofile_name=profile_name,\\n\\tlogger=logger\\n)\\n```\\n\\nThe integration uses a wrapper for dbt runs because start and complete events are not emitted until execution concludes:\\n\\n```\\ndbt_run_event = dbt_run_event_start(\\n\\tjob_name=f\u201cdbt-run-{processor.project[\u2018name\u2019]}\u201d,\\n\\tjob_namespace=job_namespace,\\n\\tparent_run_metadata=parent_run_metadata\\n)\\n\\ndbt_run_metadata = ParentRunMetadata(\\n\\trun_id=dbt_run_event.run.runId,\\n\\tjob_name=dbt_run_event.job.name,\\n\\tjob_namespace=dbt_run_event.job.namespace\\n)\\n\\nprocessor.dbt_run_metadata = dbt_run_metadata\\n```\\n\\nAfter executing dbt, the script parses the metadata using the processor and emits a run event:\\n\\n```\\nevents = processor.parse().events()\\n\\nclient.emit(dbt_run_event_end(\\n\\trun_id=dbt_run_metadata.run_id,\\n\\tjob_namespace=dbt_run_metadata.job_namespace,\\n\\tjob_name=dbt_run_metadata.job_name,\\n\\tparent_run_metadata=parent_run_metadata\\n\\t))\\nlogger.info(f\\"Emitted {len(events) + 2} openlineage events\\")\\n```\\n\\n### Additional Resources\\n\\nCheck out the source code here: https://github.com/OpenLineage/OpenLineage/tree/main/client/python.\\n\\nInterested in contributing to the project? Read our guide for new contributors: https://github.com/OpenLineage/OpenLineage/blob/main/CONTRIBUTING.md.\\n\\nJoin us on Slack: http://bit.ly/OpenLineageSlack.\\n\\nAttend a community meeting: https://bit.ly/OLwikitsc."},{"id":"/openlineage-microsoft-purview","metadata":{"permalink":"/blog/openlineage-microsoft-purview","source":"@site/blog/openlineage-microsoft-purview/index.mdx","title":"Microsoft Purview Accelerates Lineage Extraction from Azure Databricks","description":"A new collaboration between Microsoft and OpenLineage is making lineage extraction possible for Azure Databricks and Microsoft Purview users.","date":"2022-06-14T00:00:00.000Z","formattedDate":"June 14, 2022","tags":[],"readingTime":7.66,"hasTruncateMarker":true,"authors":[{"name":"Chandru Sugunan","title":"Guest Blogger and OpenLineage Contributor","key":"Sugunan"},{"name":"Will Johnson","title":"Guest Blogger and OpenLineage Committer","key":"Johnson"},{"name":"Michael Robinson","title":"OpenLineage Community Manager","url":"https://github.com/merobi-hub","imageURL":"https://github.com/merobi-hub.png","key":"Robinson"}],"frontMatter":{"title":"Microsoft Purview Accelerates Lineage Extraction from Azure Databricks","date":"2022-06-14T00:00:00.000Z","authors":["Sugunan","Johnson","Robinson"],"description":"A new collaboration between Microsoft and OpenLineage is making lineage extraction possible for Azure Databricks and Microsoft Purview users."},"prevItem":{"title":"The Python Client -- the Foundation of OpenLineage Integrations","permalink":"/blog/python-client"},"nextItem":{"title":"Data Lineage with Snowflake","permalink":"/blog/openlineage-snowflake"}},"content":"A new collaboration between Microsoft and OpenLineage is making lineage extraction possible for Azure Databricks and Microsoft Purview users.\\n\\n\x3c!--truncate--\x3e\\n\\nA new collaboration between Microsoft and OpenLineage is making lineage extraction possible for Azure Databricks and Microsoft Purview users. Thanks to a robust OpenLineage Spark integration, users can both extract and visualize lineage from their Databricks notebooks and jobs inside Microsoft Purview. This blog post shares the history and future of this exciting open-source project, describes the solution, and shows you how to get started. \\n\\n### Summary\\n\\n- [Microsoft Purview](https://azure.microsoft.com/en-us/services/purview/) provides a comprehensive platform to populate native and custom data lineage metadata from on-prem, OSS, SaaS, and multi-cloud data systems. \\n- The [Azure Databricks to Microsoft Purview Solution Accelerator](https://github.com/microsoft/Purview-ADB-Lineage-Solution-Accelerator) takes advantage of the robust Spark integration inside [OpenLineage](https://github.com/OpenLineage/OpenLineage) and translates OpenLineage events into Microsoft Purview native assets supporting:\\n    - Azure Data Lake Gen 2\\n    -  Azure Blob Storage\\n    - Azure SQL\\n    - Azure Synapse SQL Pools\\n- Customers of Azure Databricks and Microsoft Purview can [try the solution today](https://github.com/microsoft/Purview-ADB-Lineage-Solution-Accelerator) by following the [demo instructions](https://github.com/microsoft/Purview-ADB-Lineage-Solution-Accelerator/blob/main/deploy-demo.md) or [connector only instructions](https://github.com/microsoft/Purview-ADB-Lineage-Solution-Accelerator/blob/main/deploy-base.md).\\n\\n### What is Microsoft Purview?\\n\\n[Microsoft Purview](https://docs.microsoft.com/en-us/azure/purview/overview) provides an ambient data governance solution that helps you unify and manage your data wherever it exists \u2013 on-premises, in the cloud, or on a software-as-a-service (SaaS) platform. With Microsoft Purview, you can:\\n- create a holistic, up-to-date map of your data landscape with automated data discovery, sensitive data classification, and end-to-end data lineage.\\n- enable data curators to manage and secure your data estate.\\n- empower data consumers to find valuable, trustworthy data.\\n\\n![High-level overview](./high-level-overview.png)\\nFigure 1. Microsoft Purview is an ambient data governance platform for an enterprise.\\n\\nMicrosoft Purview automates data discovery by providing data scanning and classification as a service for assets across your data estate. Microsoft Purview integrates metadata and descriptions of discovered data assets into a holistic map of your data ecosystem. Layered on this map are purpose-built apps that create environments for data discovery, policy management, and insights into your data landscape.\\n\\n### Data Lineage in Microsoft Purview\\n\\nOrganizations need data to conduct business, and they need trustworthy data to perform analysis and make key decisions. Data lineage and provenance provide insights into data pedigree, which relates to operational information, runtime analysis, historical lineage, and ownership information. Users rely on pedigree when taking insights from data. Critical scenarios involving root cause analysis, impact analysis, quality control, compliance, and audit tracing are served by data lineage and provenance.\\n\\n[Data Lineage](https://docs.microsoft.com/en-us/azure/purview/catalog-lineage-user-guide) in Microsoft Purview is a core platform capability that populates the Microsoft Purview Data Map with data movement and transformations across systems. With the backing of [Apache Atlas 2.2](https://atlas.apache.org/#/), lineage is captured as it flows in the enterprise and stitched without gaps irrespective of its source. Data lineage in Microsoft Purview enables data analysts and data stewards to conduct root cause analysis, troubleshoot, and perform impact analysis of data moving upstream and downstream in data estates. With a combined platform and interactive lineage visualization tool, data investigations related to quality, trust, and compliance can be self-served in a few clicks rather than requested from a third party.\\n\\nMicrosoft Purview has native data lineage support for [20+ sources](https://docs.microsoft.com/en-us/azure/purview/catalog-lineage-user-guide#lineage-collection), many of which are integrated at engine runtimes. For example, data lineage is pushed from Azure Data Factory when pipelines are run. This deep integration allows Microsoft Purview to capture operational metadata such as job start/end times, the number of rows impacted, job run status and more. In addition to native support, the [open APIs](https://docs.microsoft.com/en-us/rest/api/purview/catalogdataplane/lineage) can be used to integrate with enterprise systems to support custom lineage.\\n\\n<img src=\\"./screenshot.gif\\" style={{padding: \\"3rem\\"}}/>\\n\\nFigure 2. Native data lineage visualization in Microsoft Purview.\\n\\n### OpenLineage + Microsoft\\n\\nThis integration came about because Microsoft Purview sought a lineage solution for Azure Databricks users, ideally one that would support all Azure data repository types, from Azure Blob Storage to Azure SQL. The team that took on this challenge was the Early Access Engineering (EAE) team, a group of data experts at Microsoft who forge competitive differentiation and value by using groundbreaking technology and features before they become available to the general enterprise landscape. \\n\\n#### A History of Contributions to OpenLineage\\n\\nThe EAE team at Microsoft has a long history of contributions to open source projects in general and to OpenLineage in particular. In December of 2021, Will Johnson contributed a [PR](https://github.com/OpenLineage/OpenLineage/pull/425) to OpenLineage to add support for arbitrary parameters in the OpenLineage URL. This change supported key-based authentication via URL and eased the process of sending metadata from OpenLineage to repositories other than [Marquez](https://github.com/MarquezProject), OpenLineage\u2019s sister project. This in turn supported additional integrations and collaboration and has helped to increase adoption of the OpenLineage standard.\\n\\nOver the course of seven months, the Microsoft team contributed eight pull requests to enable:\\n- better support for the Azure Blob File System (Azure Data Lake Gen 2).\\n- use of an Azure Function as the lineage endpoint.\\n- lineage extraction for [Azure Synapse](https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/synapse-analytics) as a data source.\\n- extraction of Databricks environment properties such as notebook paths and job ids.\\n\\nContributing open source integrations to OpenLineage benefits not only Microsoft Purview but also the data landscape as a whole. Collaborations like this one help increase adoption of the OpenLineage standard across the industry, which gets us closer to the single standard we need for consistently powerful and reliable lineage across the wide diversity of tooling in today\u2019s data pipelines.  \\n\\nAt Microsoft, this kind of work is not unique to the EAE team. Across the company, cross-functional, community-driven teams foster innovation through open source collaboration.\\n\\n### Why Contribute to OpenLineage?\\n\\nMost enterprise data environments are convoluted, with data systems spread across on-prem, multi-cloud, SaaS, and open-source platforms. The data moves between a variety of storage, processing, analytical, and SaaS data systems. Azure Databricks is one such data system in an enterprise with a lakehouse platform in the cloud that combines data warehouses and data lakes to offer an open and unified platform for data and AI. Microsoft Purview customers have long asked for the ability to populate and govern Azure Databricks assets in the Microsoft Purview DataMap. With OpenLineage, we are bringing runtime Data Lineage capture from Azure Databricks Spark workloads to Microsoft Purview. \\n\\nIn addition, by contributing to OpenLineage, Microsoft can offer users of OpenLineage on other platforms the ability to represent metadata models of Microsoft data sources accurately in their lineage graphs. For example, users of Spark on any other platform can represent the metadata models of Microsoft data sources more accurately.\\n\\nLastly, OpenLineage benefits from Microsoft\u2019s contributions as they will add hundreds if not thousands of new users to the OpenLineage standard. This will spur more  contributions by the OpenLineage community as more users request that new implementations and features be added to the specification.\\n\\n### About the Solution\\n\\n![Flow of metadata using OpenLineage](./purview-lineage.png)\\nFigure 3. The flow of metadata from Azure Databricks to Microsoft Purview using OpenLineage.\\n\\n1. An Azure administrator deploys an Azure Function (serverless C# application) and an Event Hub (to store OpenLineage events) by running a deployment script.\\n2. An administrator configures a Databricks cluster as per the [OpenLineage install instructions](https://github.com/OpenLineage/OpenLineage/tree/main/integration/spark/databricks) along with the Azure Function key and OpenLineage host pointing to the Azure Function.\\n3. The OpenLineage Spark jar extracts the necessary inputs and outputs and emits them to the Azure Function.\\n4. The Azure Functions transform the OpenLineage payload and push lineage to Microsoft Purview through the Apache Atlas REST APIs.\\n5. Databricks Lineage is then visible inside Microsoft Purview!\\n\\n\\n## Getting Started with Microsoft Purview\\n<div style={{backgroundColor: \\"rgb(211, 211, 211)\\", padding: \\"15px 15px 5px 15px\\"}}>\\nQuickly and easily create a <a href=\\"https://azure.microsoft.com/en-us/services/purview/#get-started\\">Microsoft Purview</a> account to explore the new features.\\n\\nTry out the [Azure Databricks to Microsoft Purview Solution Accelerator](https://github.com/microsoft/Purview-ADB-Lineage-Solution-Accelerator).\\n\\nLearn how to [deploy the solution](https://www.youtube.com/watch?v=pLF0iykhruY&feature=youtu.be).\\n</div>\\n\\n### What the Future Holds\\n\\nMicrosoft plans to continue contributing to OpenLineage to ensure that users can extract lineage from additional Azure data sources such as [Azure Data Explorer (Kusto)](https://docs.microsoft.com/en-us/azure/data-explorer/spark-connector), [Azure Cosmos DB](https://docs.microsoft.com/en-us/azure/cosmos-db/sql/create-sql-api-spark), and [Azure Event Hubs](https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/streaming-event-hubs), and that OpenLineage continues to perform well on Azure Databricks.\\n\\nIn addition, Microsoft plans to keep up-to-date with advancements made by the OpenLineage community, such as the exciting recent contribution of [column-level lineage](https://github.com/OpenLineage/OpenLineage/pull/698) to the project.\\n\\n### Acknowledging the Contributors\\n\\nThe OpenLineage Spark integration is the product of hard work by teams inside and outside Microsoft.\\n\\nContributors from the Microsoft Early Access Engineering team include:\\n- Mark Taylor, Principal Technical Specialist ([@marktayl1](https://github.com/Marktayl1))\\n- Will Johnson, Global Black Belt - Big Data, Analytics, and ML Specialist ([@wjohnson](https://github.com/wjohnson))\\n- Rodrigo Monteiro, Global Black Belt - Big Data, Analytics ([@rodrigomonteiro-gbb](https://github.com/rodrigomonteiro-gbb))\\n- Travis Hilbert, Technical Specialist ([@travishilbert](https://github.com/TravisHilbert))\\n- Matt Savarino, Sr. Technical Specialist ([@mattsavarino](https://github.com/mattsavarino))\\n\\nOutside Microsoft, contributors to the OpenLineage Spark integration are based at a range of internationally distributed companies and organizations. Additional contributors to the integration include:\\n- Michael Collado, Staff Software Engineer, Astronomer ([@collado-mike](https://github.com/collado-mike))\\n- Oleksandr Dvornik, Senior Java Developer, UBS ([@OleksandrDvornik](https://github.com/OleksandrDvornik))\\n- Pawe\u0142 Leszczy\u0144ski, Data Engineer, GetInData ([@pawel-big-lebowski](https://github.com/pawel-big-lebowski))\\n- Tomasz Nazarewicz, Data Engineer, GetInData ([@tnazarew](https://github.com/tnazarew))\\n- Maciej Obuchowski, Software Engineer, GetInData ([@mobuchowski](https://github.com/mobuchowski)) \\n- Kengo Seki, PMC Member and Committer, Apache Software Foundation ([@sekikn](https://github.com/sekikn))\\n- Ziyoiddin Yusupov, Senior Software Engineer, UBS ([@mr-yusupov](https://github.com/mr-yusupov))\\n\\nTry the [Azure Databricks to Microsoft Purview Solution Accelerator](https://github.com/microsoft/Purview-ADB-Lineage-Solution-Accelerator) today!"},{"id":"/openlineage-snowflake","metadata":{"permalink":"/blog/openlineage-snowflake","source":"@site/blog/openlineage-snowflake/index.mdx","title":"Data Lineage with Snowflake","description":"The OpenLineage Adapter offers Snowflake\'s enterprise users a powerful tool for analyzing their pipelines.","date":"2022-04-27T00:00:00.000Z","formattedDate":"April 27, 2022","tags":[],"readingTime":8.06,"hasTruncateMarker":true,"authors":[{"name":"Michael Robinson","title":"OpenLineage Community Manager","url":"https://github.com/merobi-hub","imageURL":"https://github.com/merobi-hub.png","key":"Robinson"}],"frontMatter":{"title":"Data Lineage with Snowflake","date":"2022-04-27T00:00:00.000Z","authors":["Robinson"],"description":"The OpenLineage Adapter offers Snowflake\'s enterprise users a powerful tool for analyzing their pipelines."},"prevItem":{"title":"Microsoft Purview Accelerates Lineage Extraction from Azure Databricks","permalink":"/blog/openlineage-microsoft-purview"},"nextItem":{"title":"OpenLineage Support in Egeria","permalink":"/blog/openlineage-egeria"}},"content":"The OpenLineage Adapter offers Snowflake\'s enterprise users a powerful tool for analyzing their pipelines.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nWe are excited to reveal a new way to gather lineage metadata directly from Snowflake: the OpenLineage Adapter. This integration offers Snowflake\u2019s enterprise users a powerful tool for analyzing and diagnosing issues with their data pipelines.\\n\\nThis new integration will add new diagnostic capability to one of the world\u2019s largest data platforms. Snowflake\u2019s Data Cloud currently empowers more than 5,900 companies, including 241 of the Fortune 500 as of January 2022, to unite siloed data, securely share data, and execute diverse analytic workloads across their organizations. Legacy platforms struggled to provide a single, secure, and universally accessible platform for organizations to warehouse and analyze their data, but Snowflake\u2019s Data Cloud provides a global ecosystem where customers, providers, and partners can finally break down data silos and derive value from rapidly growing data sets in secure, compliant, and governed ways.\\n\\n## Background\\n\\nAn open source [LF AI & Data Foundation](https://lfaidata.foundation/projects/openlineage) sandbox project, OpenLineage provides an open standard for metadata and lineage collection that instruments jobs as they are running. OpenLineage not only automates the process of generating lineage and metadata about datasets, jobs, and runs in a data flow, but also does this in real time behind the scenes. With OpenLineage\u2019s open standard and extensible backend, users can easily identify the root causes of slow or failing jobs and issues with data quality in their ecosystems without parsing queries. The magic of OpenLineage is its standard API for capturing lineage events. Any number of tools \u2013 from schedulers to SQL engines \u2013 can send metadata from this endpoint to a compatible tool such as [Marquez](https://github.com/MarquezProject/marquez) for visualization and further analysis of a pipeline.\\n\\nHistorically, the process of producing lineage and collecting metadata has been laborious and error-prone. Extracting data from query logs via parsing, for example, required one to reimplement database parsing logic, which added complexity and introduced opportunities for user error. In addition, the lineage collected was incomplete. One could learn about the view that was queried but not about the underlying tables in the pipeline, much less about the upstream and downstream dependencies of the datasets. OpenLineage, by contrast, exploits what the database already knows and does to maintain an up-to-date, end-to-end graph of a pipeline \u2013 and makes the graph available via an API.     \\n\\nOpenLineage and Snowflake play nicely because the latter is unusual among cloud data platforms for offering lineage information out of the box in a view ([ACCESS_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/access_history.html)). The integration of OpenLineage builds on this foundation to offer automated generation of lineage and metadata.\\n\\nThe value proposition of Snowflake + OpenLineage lies in the combination of an open standard tool, which supports multiple data systems to provide lineage in a single format, to Snowflake\u2019s existing production of lineage information on an enterprise scale. The integration gives customers the ability to consume enterprise-wide table lineage and process lineage together in a consolidated OpenLineage format. \\n\\n## Approach\\n\\nThe process of integrating OpenLineage benefited from an existing query logging tool already available to Snowflake enterprise customers: the `ACCESS_HISTORY` view. As its name suggests, this feature, designed initially for governance use cases, offers users a detailed view of read operations conducted on Snowflake objects (e.g., tables, views, and columns) on an on-demand basis in response to SQL queries. (Write operations are viewable as a preview feature.)\\n\\nAs developed primarily by former Snowflake intern Aly Hirani with support from Datakin Senior Engineer Minkyu Park, the OpenLineage integration makes Access History the basis of automated production of lineage and metadata. But rather than produce a view for querying, OpenLineage produces a holistic lineage graph. To create the graph, the integration takes the data used to populate the Access History view and sends it to the OpenLineage backend as a standard OpenLineage event. Events in OpenLineage are JSON objects that employ a consistent naming strategy for database entities and enrich those entities with facets:\\n\\n```\\n{\\n    \\"eventType\\": \\"COMPLETE\\",\\n    \\"eventTime\\": \\"2020-12-28T20:52:00.001+10:00\\",\\n    \\"run\\": {\\n        \\"runId\\": \\"d46e465b-d358-4d32-83d4-df660ff614dd\\"\\n    },\\n    \\"job\\": {\\n        \\"namespace\\": \\"my-namespace\\",\\n        \\"name\\": \\"my-job\\"\\n    },\\n    \\"outputs\\": [{\\n        \\"namespace\\": \\"my-namespace\\",\\n        \\"name\\": \\"my-output\\",\\n        \\"facets\\": {\\n        \\"schema\\": {\\n            \\"_producer\\": \\"https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client\\",\\n            \\"_schemaURL\\": \\"https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/spec/OpenLineage.json#/definitions/SchemaDatasetFacet\\",\\n            \\"fields\\": [\\n            { \\"name\\": \\"a\\", \\"type\\": \\"VARCHAR\\"},\\n            { \\"name\\": \\"b\\", \\"type\\": \\"VARCHAR\\"}\\n            ]\\n        }\\n        }\\n    }],    \\n    \\"producer\\": \\"https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client\\"\\n}\\n```\\n\\n## A DAG-based Solution\\n\\nAutomating lineage production from the Access History view required a two-DAG solution. Minkyu had initially planned to use one DAG to scan the view and produce the lineage graph, but the timing of the logs used for the view precluded the production of lineage data with a single DAG. The solution Minkyu found was a separate DAG with a schedule for scanning the Access History view on a regular interval.\\n\\n```\\ndef send_ol_events():\\n   client = OpenLineageClient.from_environment()\\n\\n   with connect(user=SNOWFLAKE_USER,\\n                password=SNOWFLAKE_PASSWORD,\\n                account=SNOWFLAKE_ACCOUNT,\\n                database=\'OPENLINEAGE\',\\n                schema=\'PUBLIC\') as conn:\\n       with conn.cursor() as cursor:\\n           ol_view = \'OPENLINEAGE_ACCESS_HISTORY\'\\n           ol_event_time_tag = \'OL_LATEST_EVENT_TIME\'\\n\\n           var_query = f\'\'\'\\n               set current_organization=\'{SNOWFLAKE_ACCOUNT}\';\\n           \'\'\'\\n\\n           cursor.execute(var_query)\\n\\n           ol_query = f\'\'\'\\n               SELECT * FROM {ol_view}\\n               WHERE EVENT:eventTime > system$get_tag(\'{ol_event_time_tag}\', \'{ol_view}\', \'table\')\\n               ORDER BY EVENT:eventTime ASC;\\n           \'\'\'\\n\\n           cursor.execute(ol_query)\\n           ol_events = [json.loads(ol_event[0]) for ol_event in cursor.fetchall()]\\n\\n           for ol_event in ol_events:\\n               client.emit(ol_event)\\n\\n           if len(ol_events) > 0:\\n               latest_event_time = ol_events[-1][\'eventTime\']\\n               cursor.execute(f\'\'\'\\n                   ALTER VIEW {ol_view} SET TAG {ol_event_time_tag} = \'{latest_event_time}\';\\n               \'\'\')\\n\\ndefault_args = {\\n   \'owner\': \'openlineage\',\\n   \'depends_on_past\': False,\\n   \'start_date\': days_ago(1),\\n   \'email_on_failure\': False,\\n   \'email_on_retry\': False,\\n   \'email\': [\'demo@openlineage.io\'],\\n   \'snowflake_conn_id\': \'openlineage_snowflake\'\\n}\\n\\nwith DAG(\'etl_openlineage\',\\n    schedule_interval=\'@hourly\',\\n    catchup=False,\\n    default_args=default_args,\\n    description=\'Send OL events every minutes\',\\n    tags=[\\"extract\\"]) as dag:\\n        t1 = PythonOperator(task_id=\'ol_event\', python_callable=send_ol_events)\\n```\\n\\n## Getting Started with an Example\\n\\nThis example uses Airflow to run a collection of Snowflake queries for a fictional food delivery service. Lineage data for these queries will be recorded within Snowflake `ACCESS_HISTORY` and, using the OpenLineage Access History View, emitted to an OpenLineage backend.\\n \\nThis is done using a series of DAGs in `dags/etl` that each use SnowflakeOperator to run queries, along with a DAG in `dags/lineage` that uses PythonOperator to send generated OpenLineage events to the configured backend.\\n\\n### Prerequisites\\n\\n#### Installing Marquez\\n\\nFirst, checkout the Marquez repository:\\n\\n```bash\\n% git clone https://github.com/MarquezProject/marquez.git\\n% cd marquez\\n```\\n\\nThen, run Marquez in detached mode:\\n\\n```bash\\n% docker/up.sh -d\\n%\\n```\\n\\n#### Preparing Snowflake\\n\\nFirst, check out the OpenLineage Access History View repository:\\n\\n```bash\\n% git clone https://github.com/Snowflake-Labs/OpenLineage-AccessHistory-Setup.git\\n% cd OpenLineage-AccessHistory-Setup\\n```\\n\\nThe `OPENLINEAGE` database and `FOOD_DELIVERY` schema in Snowflake need to be created. This can be done using the SnowSQL command-line tool, or by pasting the queries into a new Snowflake Worksheet. This example uses SnowSQL.\\n\\n```bash\\n% snowsql -u <snowflake-user> -a <snowflake-account>\\nSnowSQL> CREATE DATABASE OPENLINEAGE;\\nSnowSQL> CREATE SCHEMA OPENLINEAGE.FOOD_DELIVERY;\\n```\\n\\nThe view defined in `open_lineage_access_history.sql` also needs to be created. This view represents the entries in `ACCESS_HISTORY` as specially-constructed JSON objects containing RunEvents that can be emitted to an OpenLineage backend. To create it, use SnowSQL to set the current_organization session variable and execute the SQL file.\\n\\n```\\nSnowSQL> SET current_organization=\'<snowflake-organization>\';\\nSnowSQL> USE SCHEMA OPENLINEAGE.PUBLIC;\\nSnowSQL> !source open_lineage_access_history.sql\\n```\\n\\nFinally, our lineage extraction DAG relies upon a tag on the view to keep track of which lineage events have been processed. This tag needs to be initialized:\\n\\n```\\nSnowSQL> CREATE TAG OL_LATEST_EVENT_TIME;\\nSnowSQL> ALTER VIEW OPENLINEAGE.PUBLIC.OPENLINEAGE_ACCESS_HISTORY SET TAG OL_LATEST_EVENT_TIME = \'1970-01-01T00:00:00.000\';\\nSnowSQL> !quit\\n%\\n```\\n\\n### Preparing the Environment\\n\\nThe following environment variables need to be set in order for the query DAGs to connect to Snowflake, and so that the extraction DAG can send lineage events to your OpenLineage backend:\\n- SNOWFLAKE_USER\\n- SNOWFLAKE_PASSWORD\\n- SNOWFLAKE_ACCOUNT\\n- OPENLINEAGE_URL\\n- AIRFLOW_CONN_OPENLINEAGE_SNOWFLAKE\\n \\nTo do this, copy the `.env-example` file to `.env`, and edit it to provide the appropriate values for your environment. The variables in this file will be set for each service in the Airflow deployment.\\n\\n```bash\\n% cd examples/airflow\\n% cp .env-example .env\\n% vi .env\\n```\\n\\n### Preparing Airflow\\n\\nOnce the environment is prepared, initialize Airflow with docker-compose:\\n\\n```bash\\n% docker-compose up airflow-init\\n```\\n\\nThis will take several minutes. When it has finished, bring up the Airflow services:\\n\\n```\\n% docker-compose up\\n```\\n\\nThis will also take several minutes. Eventually, the webserver will be up at [http://localhost:8080](http://localhost:8080). Log in using the default credentials (airflow/airflow) and navigate to the DAGs page. When you see 12 DAGs in the list, you can be confident that Airflow has completed its initialization of the example.\\n\\n### Running the Example\\n\\nEach of the DAGs is paused by default. Enable each one, skipping the `etl_openlineage` DAG for now. They may not all run successfully on the first try, since they have interdependencies that this example leaves unmanaged.\\n \\n![Airflow DAG list](./snowflake-airflow-example.png)\\n\\nAfter each DAG has completed at least one successful run, enable `etl_openlineage`. Wait for it to complete its run.\\n\\n### Result\\n\\nNavigate to your Marquez deployment and view the resulting lineage graph:\\n\\n![Lineage graph](./snowflake-openlineage-example.png)\\n\\n## Potential Improvements\\n\\nThis new integration paves the way for an exciting set of potential future capabilities. These include support for `Object_Dependencies` and the addition of Granular Lineage (column-level lineage). We are interested in feedback from users, which will help the team at Snowflake and the members of the OpenLineage community prioritize future work.\\n\\n## Conclusion\\n\\nSnowflake\u2019s integration of the OpenLineage standard promises to dramatically improve enterprise users\u2019 ability to diagnose issues with quality and performance in their pipelines. This project is cause for optimism about future collaboration with OpenLineage. The fit between Snowflake\u2019s enterprise product and OpenLineage is already fairly seamless. Further collaboration would likely yield additional features and, by extension, more value for Snowflake\u2019s customers. Also, the fact that OpenLineage is an open standard offers opportunities for fruitful integrations with other partners. Supporters of OpenLineage already include Spark, Airflow, and dbt, and the list is growing. For more information or to contribute to OpenLineage, reach out on [twitter](https://twitter.com/OpenLineage/) or [Slack](https://join.slack.com/t/openlineage/shared_invite/zt-oko79982-4bHHhxTUDQ9KXgQWXyWVxg), and check out the repositories on [Github](https://github.com/OpenLineage/)."},{"id":"/openlineage-egeria","metadata":{"permalink":"/blog/openlineage-egeria","source":"@site/blog/openlineage-egeria/index.mdx","title":"OpenLineage Support in Egeria","description":"The Egeria project uses OpenLineage to enhance its production of holistic metadata about an organization\'s operations.","date":"2022-04-25T00:00:00.000Z","formattedDate":"April 25, 2022","tags":[],"readingTime":2.06,"hasTruncateMarker":true,"authors":[{"name":"Mandy Chessel","title":"Guest Blogger and OpenLineage Committer","key":"Chessell"}],"frontMatter":{"title":"OpenLineage Support in Egeria","date":"2022-04-25T00:00:00.000Z","authors":["Chessell"],"description":"The Egeria project uses OpenLineage to enhance its production of holistic metadata about an organization\'s operations."},"prevItem":{"title":"Data Lineage with Snowflake","permalink":"/blog/openlineage-snowflake"},"nextItem":{"title":"Video - OpenLineage at Data Agility Day","permalink":"/blog/data-agility-day"}},"content":"The Egeria project uses OpenLineage to enhance its production of holistic metadata about an organization\'s operations.\\n\\n\x3c!--truncate--\x3e\\n\\n## OpenLineage Support in Egeria \\n\\n[Egeria](https://egeria-project.org) is a sister open source project to OpenLineage in the LF AI and Data Foundation. Egeria provides Open Metadata and Governance standard types and integration technology to exchange metadata between different technologies. It stitches together different standards to create a complete landscape of metadata about an organization\u2019s digital operations.\\n\\nOpenLineage is very welcome to the Egeria team since it defines a standard for dynamic lineage capture.  This means Egeria can capture open lineage events to detect new assets and activity around them, link this new knowledge into the existing metadata and distribute it to the open metadata ecosystem.\\n\\nEgeria also executes governance processes for maintaining both metadata and the data sources it describes. Since it is running processes, it also makes sense that Egeria produces open lineage for its processes.\\n\\nThe diagram below is a big animal picture showing the different features relating to open lineage that Egeria offers. With Egeria\u2019s plug-and-play architecture you can pick and choose which pieces you need.\\n\\n![Egeria architecture](./open-lineage-blog.svg)\\n\\nThe numbers on the diagram refer to the notes below.\\n1. Egeria can capture open lineage events directly through HTTP or via the proxy backend.\\n2. OpenLineage metadata is correlated and matched to existing metadata captured through a variety of mechanisms from direct metadata extraction from the hosting data platforms, to updates through dev ops pipelines to metadata discovery analytic tools.\\n3. Egeria can publish OpenLineage events. These include the OpenLineage events it received (potentially augmented with additional facets), or events generated from its own governance processes. Published OpenLineage events can go to Egeria\u2019s OpenLineage file-based log store for later processing or to any application that supports the OpenLineage API (Marquez, for example -- another project from LF AI and Data).\\n4. The metadata extracted from OpenLineage events can be distributed to the open metadata ecosystem using standard approaches. This means it can be picked up by connected data science, governance and lineage tools.\\n5. Governance processes linked to the open metadata ecosystem can use OpenLineage events to validate that their originating processes are operating as frequently and as accurately as expected.\\n\\nMore information on Egeria\u2019s open lineage support can be found [here](https://egeria-project.org/features/lineage-management/overview/#the-openlineage-standard).\\n\\nThe Egeria community would like to thank the OpenLineage community for their great support while we created this integration. We look forward to continuing to work together as both our projects mature."},{"id":"/data-agility-day","metadata":{"permalink":"/blog/data-agility-day","source":"@site/blog/data-agility-day/index.mdx","title":"Video - OpenLineage at Data Agility Day","description":"At Data Agility Day 2021, Julien Le Dem and Kevin Mellott outlined their approach to data lineage and discussed various approaches to implementing it in the real world.","date":"2021-11-17T00:00:00.000Z","formattedDate":"November 17, 2021","tags":[],"readingTime":0.77,"hasTruncateMarker":true,"authors":[{"name":"Ross Turk","title":"OpenLineage Committer","url":"https://www.github.com/rossturk","imageURL":"https://www.github.com/rossturk.png","key":"Turk"}],"frontMatter":{"title":"Video - OpenLineage at Data Agility Day","date":"2021-11-17T00:00:00.000Z","authors":["Turk"],"description":"At Data Agility Day 2021, Julien Le Dem and Kevin Mellott outlined their approach to data lineage and discussed various approaches to implementing it in the real world."},"prevItem":{"title":"OpenLineage Support in Egeria","permalink":"/blog/openlineage-egeria"},"nextItem":{"title":"Tracing Data Lineage with OpenLineage and Apache Spark","permalink":"/blog/openlineage-spark"}},"content":"At Data Agility Day 2021, Julien Le Dem and Kevin Mellott outlined their approach to data lineage and discussed various approaches to implementing it in the real world.\\n\\n\x3c!--truncate--\x3e\\n\\nOpenLineage made an appearance at Data Agility Day 2021, when contributors Julien Le Dem and Kevin Mellott took the virtual stage for a casual conversation about data lineage. The result was both informative and enjoyable.\\n\\nIf you couldn\'t make the event this year, that\'s okay! The video is now available, and it\'s *almost* as good as being there in person.\\n\\n<iframe id=\\"llo_fn3f45lpc900s9awi\\" src=\\"//embed.vidello.com/4630/n3f45lpc900s9awi/player.html\\" allowfullscreen style={{width:\\"850px\\", height:\\"480px\\", margin:\\"0 auto\\", marginTop: \\"20px\\", marginBottom: \\"20px\\", border: \\"none\\"}}></iframe>\\n\\nJulien Le Dem is the creator and lead engineer of OpenLineage. Kevin Mellott implemented the Enterprise Data Platform at Northwestern Mutual, and recently shared [a post detailing his team\u2019s experiences](/blog/openlineage-at-northwestern-mutual). \\n\\nThe video is also available at the [Data Agility Day site](https://dataagility.io/october2021/nw-mutual-datakin/), where you can keep an eye out for future events."},{"id":"/openlineage-spark","metadata":{"permalink":"/blog/openlineage-spark","source":"@site/blog/openlineage-spark/index.mdx","title":"Tracing Data Lineage with OpenLineage and Apache Spark","description":"Spark ushered in a brand new age of data democratization... and left us with a mess of hidden dependencies, stale datasets, and failed jobs.","date":"2021-11-05T00:00:00.000Z","formattedDate":"November 5, 2021","tags":[],"readingTime":12.26,"hasTruncateMarker":true,"authors":[{"name":"Michael Collado","title":"OpenLineage Committer","url":"https://www.github.com/collado-mike","imageURL":"https://www.github.com/collado-mike.png","key":"Collado"}],"frontMatter":{"title":"Tracing Data Lineage with OpenLineage and Apache Spark","date":"2021-11-05T00:00:00.000Z","authors":["Collado"],"description":"Spark ushered in a brand new age of data democratization... and left us with a mess of hidden dependencies, stale datasets, and failed jobs."},"prevItem":{"title":"Video - OpenLineage at Data Agility Day","permalink":"/blog/data-agility-day"},"nextItem":{"title":"How Northwestern Mutual Simplified Data Observability with OpenLineage & Marquez","permalink":"/blog/openlineage-at-northwestern-mutual"}},"content":"Spark ushered in a brand new age of data democratization... and left us with a mess of hidden dependencies, stale datasets, and failed jobs.\\n\\n\x3c!--truncate--\x3e\\n\\n## The Age of Data Democratization\\n\\nIn 2015, Apache Spark seemed to be taking over the world. Many of us had spent the prior few years moving our large\\ndatasets out of the Data Warehouse into \\"Data Lakes\\"- repositories of structured and unstructured data in\\ndistributed file systems or object stores, like HDFS or S3. This enabled us to build analytic systems that could\\nhandle traditional, table-structured data alongside flexible, unstructured JSON blobs, giving us access to more data\\nand allowing us to move much faster than we\u2019d previously been able to.\\n\\nThe problem was that taking the data out of Data Warehouses meant that the people who really needed access to the\\ndata, analysts and statisticians, could no longer use the tools they were comfortable with in order to read that data.\\nWhere previously, SQL and Python were all that was needed to start exploring and analyzing a dataset, now people needed\\nto write Java or use specialized scripting languages, like Pig, to get at the data. Systems that did support SQL, such\\nas Hive, were unbearably slow for any but the most basic operations. In many places, the statisticians were dependent on\\nsoftware engineers to build custom tools for access, meaning the bottleneck had moved from the systems that\\nneeded to store and process the data to the humans who were supposed to tell us what systems to build.\\n\\nThen along came Apache Spark, which gave back to analysts the ability to use their beloved Python (and eventually SQL)\\ntools to process raw data in object stores without the dependency on software engineers. While others were\\nattracted to its ability to perform multiple operations on data without the I/O overhead of alternatives, like Pig or\\nHive, data scientists were thrilled to start piping that data through their NumPy and Pandas scripts. \\n\\n## A Colossal Mess\\n\\nOf course, the natural consequence of this data democratization is that it becomes difficult to keep track of who is\\nusing the data and for what purpose. Hidden dependencies and Hyrum\u2019s Law suddenly meant that changes to the data schema\\nwould inadvertently break downstream processes or that stale, deprecated datasets were still being consumed, and that\\ncorrupted datasets would leak into unknown processes making recovery difficult or even impossible.\\n\\n[![XKCD 1172](./workflow.png)](https://xkcd.com/1172/)\\n\\nThe goal of OpenLineage is to reduce issues and speed up recovery by exposing those hidden dependencies and informing\\nboth producers and consumers of data about the state of that data and the potential blast radius of changes and software\\nbugs. Naturally, support for Apache Spark seemed like a good idea and, while the Spark 2.4 branch has been supported for\\nsome time, the recent OpenLineage 0.3 release has explicit support for Spark 3.1. \ud83c\udf89 \\n\\n## Getting Started\\n\\nOur approach to integrating with Spark is not super novel nor is it complicated to integrate into your own system. Spark\\nhas had a SparkListener interface since before the 1.x days. If you\'re a heavy Spark user, it\'s likely you\'re already\\nfamiliar with it and how it\'s used in Spark applications. OpenLineage integrates with Spark by implementing that\\ninterface and collecting information about jobs that are executed inside a Spark application. To activate the\\nlistener, add the following properties to your Spark configuration:\\n```\\nspark.jars.packages     io.openlineage:openlineage-spark:0.3.+\\nspark.extraListeners\\tio.openlineage.spark.agent.OpenLineageSparkListener\\n```\\nThis can be added to your cluster\u2019s `spark-defaults.conf` file, in which case it will record lineage for every job executed on the cluster, or added to specific jobs on submission via the `spark-submit` command. Once the listener is activated, it needs to know where to report lineage events, as well as the namespace of your jobs. Add the following additional configuration lines to your `spark-defaults.conf` file or your Spark submission script:\\n```\\nspark.openlineage.host\\t\\t{your.openlineage.host}\\nspark.openlineage.namespace\\t{your.openlineage.namespace}\\n```\\n## The Demo\\n\\nTrying out the Spark integration is super easy if you already have Docker Desktop and git installed. To follow along\\nwith this demo, you\u2019ll also need a Google Cloud account and a Service Account JSON key file for an account that has\\naccess to BigQuery and read/write access to your GCS bucket. I added mine to a file called bq-spark-demo.json.\\n\\nNote: If you\'re on macOS Monterey (macOS 12) you\'ll have to release port 5000 before beginning by [disabling the AirPlay Receiver](https://developer.apple.com/forums/thread/682332).\\n\\nCheck out the OpenLineage project into your workspace with:\\n```bash\\ngit clone https://github.com/OpenLineage/OpenLineage\\n```\\n\\nThen cd into the integration/spark directory. Run `mkdir -p docker/notebooks/gcs` and copy your service account credentials\\nfile into that directory. Then run:\\n\\n```bash\\ndocker-compose up\\n```\\n\\nThis launches a Jupyter notebook with Spark already installed as well as a Marquez API endpoint to report lineage. Once the notebook server is up and running, you should see something like the following text in the logs:\\n```\\nnotebook_1  | [I 21:43:39.014 NotebookApp] Jupyter Notebook 6.4.4 is running at:\\nnotebook_1  | [I 21:43:39.014 NotebookApp] http://082cb836f1ec:8888/?token=507af3cf9c22f627f6c5211d6861fe0804d9f7b19a93ca48\\nnotebook_1  | [I 21:43:39.014 NotebookApp]  or http://127.0.0.1:8888/?token=507af3cf9c22f627f6c5211d6861fe0804d9f7b19a93ca48\\nnotebook_1  | [I 21:43:39.015 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\\n```\\n\\nCopy the URL with 127.0.0.1 as the hostname from your own log (the token will be different from mine) and paste it into\\nyour browser window. You should have a blank Jupyter notebook environment ready to go.\\n\\n![Jupyter home screen](./jupyter_home.png)\\n\\nOnce your notebook environment is ready, click on the notebooks directory, then click on the New button to create a new\\nPython 3 notebook.\\n\\n![Jupyter create new notebook](./jupyter_new_notebook.png)\\n\\nIn the first cell in the window paste the following text. Update the GCP project and bucket names and the \\nservice account credentials file, then run the code:\\n\\n```python\\nfrom pyspark.sql import SparkSession\\nimport urllib.request\\n\\n# download dependencies for BigQuery and GCS\\ngc_jars = [\'https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.1.1/gcs-connector-hadoop3-2.1.1-shaded.jar\',\\n          \'https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/bigquery-connector/hadoop3-1.2.0/bigquery-connector-hadoop3-1.2.0-shaded.jar\',\\n          \'https://repo1.maven.org/maven2/com/google/cloud/spark/spark-bigquery-with-dependencies_2.12/0.22.2/spark-bigquery-with-dependencies_2.12-0.22.2.jar\']\\n\\nfiles = [urllib.request.urlretrieve(url)[0] for url in gc_jars]\\n\\n\\n# Set these to your own project and bucket\\nproject_id = \'bq-openlineage-spark-demo\'\\ngcs_bucket = \'bq-openlineage-spark-demo-bucket\'\\ncredentials_file = \'/home/jovyan/notebooks/gcs/bq-spark-demo.json\'\\n\\nspark = (SparkSession.builder.master(\'local\').appName(\'openlineage_spark_test\')\\n             .config(\'spark.jars\', \\",\\".join(files))\\n             \\n             # Install and set up the OpenLineage listener\\n             .config(\'spark.jars.packages\', \'io.openlineage:openlineage-spark:0.3.+\')\\n             .config(\'spark.extraListeners\', \'io.openlineage.spark.agent.OpenLineageSparkListener\')\\n             .config(\'spark.openlineage.host\', \'http://marquez-api:5000\')\\n             .config(\'spark.openlineage.namespace\', \'spark_integration\')\\n             \\n             # Configure the Google credentials and project id\\n             .config(\'spark.executorEnv.GCS_PROJECT_ID\', project_id)\\n             .config(\'spark.executorEnv.GOOGLE_APPLICATION_CREDENTIALS\', \'/home/jovyan/notebooks/gcs/bq-spark-demo.json\')\\n             .config(\'spark.hadoop.google.cloud.auth.service.account.enable\', \'true\')\\n             .config(\'spark.hadoop.google.cloud.auth.service.account.json.keyfile\', credentials_file)\\n             .config(\'spark.hadoop.fs.gs.impl\', \'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\')\\n             .config(\'spark.hadoop.fs.AbstractFileSystem.gs.impl\', \'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\')\\n             .config(\\"spark.hadoop.fs.gs.project.id\\", project_id)\\n             .getOrCreate())\\n```\\n\\nMost of this is boilerplate- we need the BigQuery and GCS libraries installed in the notebook environment, then we need\\nto set the configuration parameters to tell the libraries what GCP project we want to use and how to authenticate with\\nGoogle. The parameters specific to OpenLineage are the four we already covered- `spark.jars.packages`,\\n`spark.extraListeners`, `spark.openlineage.host`, `spark.openlineage.namespace`. Here, we\u2019ve configured the host to be\\nthe marquez-api container started by Docker.\\n\\nGoogle has a wealth of information available as public datasets in BigQuery. If you\u2019re ever bored one Saturday night,\\nbrowse the datasets available- you\u2019ll find census data, crime data, liquor sales, and even a black hole database. For\\nthe demo, I thought I\u2019d browse some of the Covid19 related datasets they have. Specifically, there\u2019s a dataset that\\nreports the likelihood of people in a given county to wear masks (broken up into five categories: `always`, `frequently`,\\n`sometimes`, `rarely`, and `never`). There\u2019s also a giant dataset called `covid19_open_data` that contains things like\\nvaccination rates, current totals of confirmed cases, hospitalizations, deaths, population breakdowns, and policies on\\nmask-wearing, contact tracing, and vaccination-mandates. Both datasets contain the county FIPS code for US counties,\\nmeaning we can join the two datasets and start exploring.\\n\\nCreate a new cell in the notebook and paste the following code:\\n\\n```python\\nfrom pyspark.sql.functions import expr, col\\n\\nmask_use = spark.read.format(\'bigquery\') \\\\\\n    .option(\'parentProject\', project_id) \\\\\\n    .option(\'table\', \'bigquery-public-data:covid19_nyt.mask_use_by_county\') \\\\\\n    .load() \\\\\\n    .select(expr(\\"always + frequently\\").alias(\\"frequent\\"),\\n            expr(\\"never + rarely\\").alias(\\"rare\\"),\\n            \\"county_fips_code\\")\\n\\nopendata = spark.read.format(\'bigquery\') \\\\\\n    .option(\'parentProject\', project_id) \\\\\\n    .option(\'table\', \'bigquery-public-data.covid19_open_data.covid19_open_data\') \\\\\\n    .load() \\\\\\n    .filter(\\"country_name == \'United States of America\'\\") \\\\\\n    .filter(\\"date == \'2021-10-31\'\\") \\\\\\n    .select(\\"location_key\\",\\n            expr(\'cumulative_deceased/(population/100000)\').alias(\'deaths_per_100k\'),\\n            expr(\'cumulative_persons_fully_vaccinated/(population - population_age_00_09)\').alias(\'vaccination_rate\'),\\n            col(\'subregion2_code\').alias(\'county_fips_code\'))\\njoined = mask_use.join(opendata, \'county_fips_code\')\\n\\njoined.write.mode(\'overwrite\').parquet(f\'gs://{gcs_bucket}/demodata/covid_deaths_and_mask_usage/\')\\n```\\n\\nAgain, this is standard Spark DataFrame usage. The particulars are completely irrelevant to the OpenLineage data\\ncollection- we don\u2019t need to call any new APIs or change our code in any way. Here, I\u2019ve filtered\\nthe `covid19_open_data` table to include only U.S. data and to include the data for Halloween 2021. That dataset has a\\nlarge number of columns, but for my own purposes, I\u2019m only interested in a few of them. I calculate `deaths_per_100k`\\nusing the existing `cumulative_deceased` and `population` columns and I calculate the `vaccination_rate` using the total\\npopulation, subtracting the 0-9 year olds, since they weren\u2019t eligible for vaccination at the time. For\\nthe `mask_use_by_county` data, I don\'t really care about the difference between `rarely` and `never`, so I combine them\\ninto a single number. I do the same for `frequently` and `always`. I join the few columns I want from the two datasets\\nand store the combined result in GCS.\\n\\nAdd one more cell to the notebook and paste the following:\\n\\n```python\\nspark.read.parquet(f\'gs://{gcs_bucket}/demodata/covid_deaths_and_mask_usage/\').count()\\n```\\n\\nThe notebook will likely spit out a warning and a stacktrace (it should probably be a debug statement), then give you a\\ntotal of 3142 records.\\n\\nSo far, so good. Now what? If this was a data science blog, we might start generating some scatter plots or doing a \\nlinear regression to determine whether frequent mask usage was a predictor of high death rates or vaccination rates.\\nBut since we\'re really focused on lineage collection, I\'ll leave the rest of the analysis up to those with the time and\\ninclination to dig further. Instead, let\'s switch to exploring the lineage records we just created.\\n\\nThe `docker-compose.yml` file that ships with the OpenLineage repo includes only the Jupyter notebook and the Marquez API.\\nFor exploring visually, we\u2019ll also want to start up the Marquez web project. Without terminating the existing docker\\ncontainers, run the following command in a new terminal:\\n\\n```bash\\ndocker run --network spark_default -p 3000:3000 -e MARQUEZ_HOST=marquez-api -e MARQUEZ_PORT=5000 --link marquez-api:marquez-api marquezproject/marquez-web:0.19.1\\n```\\n\\nNow open a new browser tab and navigate to `http://localhost:3000`. You should see a screen like the following:\\n\\n![Marquez home page](./marquez_home.png)\\n\\nNote the spark_integration namespace was found for us and automatically chosen, since there are no other namespaces\\navailable. We can see three jobs listed on the jobs page of the UI. They all start with `openlineage_spark_test`, which\\nis the `appName` we passed to the SparkSession we were building in the first cell of the notebook. Each query execution\\nor RDD action is represented as a distinct job and the name of the action is appended to the application name to form\\nthe name of the job. Clicking on the `openlineage_spark_test.execute_insert_into_hadoop_fs_relation_command` node, we\\ncan see the lineage graph for our notebook:\\n\\n![Marquez job lineage](./marquez_job_graph.png)\\n\\nThe graph shows the `openlineage_spark_test.execute_insert_into_hadoop_fs_relation_command` job reads from two input\\ndatasets, `bigquery-public-data.covid19_nyt.mask_use_by_county`\\nand `bigquery-public-data.covid19_open_data.covid19_open_data`, and writes to a third dataset,\\n`/demodata/covid_deaths_and_mask_usage`. The namespace is missing from that third dataset- the fully qualified name is\\n`gs://<your_bucket>/demodata/covid_deaths_and_mask_usage`.\\n\\nBefore clicking on the datasets, though, the bottom bar shows some really interesting data that was collected from the\\nSpark job. Dragging the bar up expands the view so we can get a better look at that data.\\n\\n![Marquez job facets](./marquez_job_facets.png)\\n\\nTwo [facets](https://openlineage.io/blog/extending-with-facets/) that are always collected from Spark jobs are\\nthe `spark_version` and the `spark.logicalPlan`. The first simply reports what version of Spark was executing, as well\\nas the version of the `openlineage-spark` library. This is helpful information to collect when trying to debug a job\\nrun.\\n\\nThe second facet is the serialized optimized `LogicalPlan` Spark reports when the job runs. Spark\u2019s query optimization\\ncan have dramatic effects on the execution time and efficiency of the query job. Tracking how query plans change over\\ntime can significantly aid in debugging slow queries or OutOfMemory errors in production.\\n\\nClicking on the first BigQuery dataset gives us information about the data we read:\\n\\n![Marquez dataset latest facet](./marquez_bigquery_dataset_latest.png)\\n\\nHere, we can see the schema of the dataset as well as the datasource \u2014 namely BigQuery.\\n\\nWe can get similar information about the dataset written to in GCS:\\n\\n![Marquez output dataset latest facet](./marquez_output_dataset_latest.png)\\n\\nAs in the BigQuery dataset, we can see the output schema and the datasource \u2014 here, the `gs://` scheme and the name of\\nthe bucket we wrote to. \\n\\nIn addition to the schema, we also see a `stats` facet, reporting the number of output records and bytes as -1. A \\nsomewhat recent change to the OpenLineage schema resulted in output facets being recorded in a new field- one that \\nMarquez is not yet reading from. The old, deprecated facet reports the output stats incorrectly. An upcoming bugfix \\nshould correct the stats view so that we can see the number of rows written as well as the number of output bytes (the\\nstatistics are actually recorded correctly- the API simply needs to start returning the correct values).\\n\\nYou may have noticed the `VERSIONS` tab on the bottom bar. We can click it, but since the job has only ever run once,\\nwe\u2019ll only see one version. Clicking on the version, we\u2019ll see the same schema and statistics facets, but specific\\nto the version we clicked.\\n\\n![Marquez output dataset version info](./marquez_output_dataset_version.png)\\n\\nIn production, this dataset would have many versions, as each time the job runs a new version of the dataset is created.\\nThis allows us to track changes to the statistics and schema over time, again aiding in debugging slow jobs (suddenly,\\nwe started writing 50% more records!) or data quality issues (suddenly, we\u2019re only writing half as many records as\\nnormal!) and job failures (somebody changed the output schema and downstream jobs are failing!).\\n\\nThe final job in the UI is a HashAggregate job- this represents the `count()` method we called at the end to show the\\nnumber of records in the dataset. Rather than a `count()`, this could easily be a `toPandas()` call or some other job\\nthat reads and processes that data- perhaps storing output back into GCS or updating a Postgres database or publishing a\\nnew model, etc. Regardless of where the output gets stored, the OpenLineage integration allows you to see the entire\\nlineage graph, unifying datasets in object stores, relational databases, and more traditional data warehouses.\\n\\n[![XKCD 2347](./dependency.png)](https://xkcd.com/2347/)\\n\\nThe Spark integration is still a work in progress, but users are already getting insights into their graphs of datasets\\nstored in object stores like S3, GCS, and Azure Blob Storage, as well as BigQuery and relational databases like\\nPostgres. Now with Spark 3.1 supported, we can gain visibility into more environments, like Databricks, EMR, and\\nDataproc clusters.\\n\\nData lineage gives visibility to the (hopefully) high quality, (hopefully) regularly updated datasets that everyone\\ndepends on, maybe without even realizing it. Spark helped usher in a welcome age of data democratization. Now data\\nobservability can help ensure we\u2019re making the best possible use of the data available."},{"id":"/openlineage-at-northwestern-mutual","metadata":{"permalink":"/blog/openlineage-at-northwestern-mutual","source":"@site/blog/openlineage-at-northwestern-mutual/index.mdx","title":"How Northwestern Mutual Simplified Data Observability with OpenLineage & Marquez","description":"Northwestern Mutual is building an Enterprise Data Platform. In this guest blog, learn about the experiences and decisions that led them to embrace the OpenLineage and Marquez communities.","date":"2021-10-22T00:00:00.000Z","formattedDate":"October 22, 2021","tags":[],"readingTime":5.93,"hasTruncateMarker":true,"authors":[{"name":"Kevin Mellott","title":"Guest Blogger and OpenLineage Contributor","key":"Mellott"}],"frontMatter":{"title":"How Northwestern Mutual Simplified Data Observability with OpenLineage & Marquez","date":"2021-10-22T00:00:00.000Z","authors":["Mellott"],"description":"Northwestern Mutual is building an Enterprise Data Platform. In this guest blog, learn about the experiences and decisions that led them to embrace the OpenLineage and Marquez communities."},"prevItem":{"title":"Tracing Data Lineage with OpenLineage and Apache Spark","permalink":"/blog/openlineage-spark"},"nextItem":{"title":"Using Marquez to Visualize dbt Models","permalink":"/blog/dbt-with-marquez"}},"content":"Northwestern Mutual is building an Enterprise Data Platform. In this guest blog, learn about the experiences and decisions that led them to embrace the OpenLineage and Marquez communities.\\n\\n\x3c!--truncate--\x3e\\n\\nI joined Northwestern Mutual last year to oversee the implementation and delivery of their Enterprise Data Platform (Unified Data Platform). With over 160 years of history, Northwestern Mutual has been serving our clients with insurance and investment products, as well as financial planning, advisory and consultation services. It goes without saying that the company has accumulated a vast amount of data over this time. Our team\u2019s objective is to empower data analysts, data scientists, and data engineers with the platform capabilities they need to derive insights and garner value from many disparate data sources.\\n\\n# Ready...Set...Go!\\n\\nSo, where do you start? The industry has taught us a lot over the past 10+ years - *remember when on-premises Hadoop clusters were all the rage*? When revisiting the approach we took within our Data Platform Engineering teams, I see a lot of alignment to the [Data Engineering Manifesto](https://connectingdots.xyz/blog/posts/2021/05/the-data-engineering-manifesto/). A few principles really jump out:\\n\\n### Embrace cloud managed services\\n\\nMany of the foundational needs of an Enterprise Data Platform can be accomplished using a cloud-first mindset. While we may not all agree which cloud provider is best, we can all agree that the level of scale and sophistication accomplished around things like storage, compute, and redundancy are going to be MUCH greater when relying on a cloud provider than when rolling your own solution.\\n\\n### We are software engineers\\n\\nThe Data Mesh evolution has reminded the industry that centralized data teams do not scale or empower anybody. With this principle in mind, our platform teams embraced full automation from the beginning and designed for self-service workflows. We do not want to become the bottleneck to insights; rather, we want to enable data owners to manage and share their datasets throughout the company. We want to empower data engineers with transformation and machine learning capabilities, so that they can author pipelines and deliver insights.\\n\\n### Aim for simplicity through consistency\\n\\nTraditionally, data platforms have gathered and constructed technical metadata based on events of the past. For example, there are many crawlers that will examine various database systems and build a catalog to make those datasets \u201cdiscoverable.\u201d Logs from various jobs can be parsed in *extremely specific ways* to identify datasets consumed and produced by a given pipeline to infer data lineage.\\n\\nWe viewed these traditional methods as a massive impediment to activating DataOps, due to differing technology solutions and the historical-based approach of their designs. Our platform aimed to achieve dynamic decisions based on what *is* happening *as it is* happening.\\n\\nWe also recognize and appreciate the complexity of this portion of the platform and did not find it wise to build from the ground up. Especially with the industry momentum towards real-time data observability, why add another custom solution to the stack? With such an evolving technical landscape, it was important for us to avoid vendor lock to allow us flexibility in future decisions.\\n\\n# NM hearts OL\\n\\nWhen we first learned of the OpenLineage specification, we were very intrigued and hopeful. An open specification focused on observing real-time events AND unifying tools and frameworks?!? Fast forward nine months, and we cannot believe how much capability we have developed around data observability in such a brief time. Let me back up a little...\\n\\nMarquez is a metadata management framework that implements the OpenLineage specification. It transforms your data runtime events into a searchable catalog of technical metadata. It was a perfect fit to the skills of our Platform Data Engineers - it is written in Java, runs in Kubernetes, and integrates well with our backend services via web-based APIs.\\n\\nWe were able to quickly deploy this framework into our own environment, which provided us with several immediate wins.\\n\\n### Flexible framework\\n\\nSince it is aligned with the OpenLineage framework, Marquez can process messages from ANY data producer that is publishing compliant events. The Marquez and OpenLineage communities have been doing an excellent job maturing the integration library, which allows you to tackle this challenge at the infrastructure level. This is the ultimate easy button approach and our own ideal state; configure an environment on behalf of your user base and sit back while it automatically detects and logs the activity within!\\n\\nIn the cases when an integration either does not exist or you need to address a more custom workflow, you can construct and emit your own OpenLineage event messages. Marquez will still be able to process and store custom OpenLineage events, provided they meet the requirements of the open standard.\\n\\nFor example, our teams have been able to programmatically construct OpenLineage messages within code that pulls data from various on-premises database servers and publishes it into our Data Platform. Using the OpenLineage specification, we extract the actual table schema from the source system as part of the `Dataset` entity and log the executing SQL query as part of the `Job` entity. This code was simplistic and allowed us to meet our immediate needs around observing data movement and recording those event details.\\n\\n### Alignment with enterprise\\n\\nMarquez already supported Kubernetes when we got involved, which provided us with many different deployment options. Our first contributions to the project were made to mature the [Helm chart](https://github.com/MarquezProject/marquez/tree/main/chart) and to enhance security around the base images and Kubernetes secrets usage.\\n\\nThese changes allowed us to fully automate our deployments using GitOps and incorporate internal security measures involving container vulnerability management.\\n\\nThe flexibility offered by the Marquez deployment architecture and our ability to customize its details allowed us to activate new production use cases in about a month. We were happy with this timeline, given the series of security checkpoints that were validated and the wealth of functionality we had just unlocked.\\n\\n### Collaborative working group\\n\\nBoth the Marquez and OpenLineage communities have been extremely welcoming, and that has been a huge factor in our success at Northwestern Mutual. Our feedback and ideas have been encouraged and heard, which is evidenced by evolving project roadmaps and accepted developer contributions.\\n\\nWe have learned quite a bit from the community members and feel fortunate to be a part of this group. Monthly community meetings are informative yet have an amazingly informal feel to them.\\n\\n# Where are we headed?\\n\\nThe Unified Data Platform at Northwestern Mutual relies on the OpenLineage standard to formulate technical metadata within our various platform services. Publishing these events into Marquez has provided us with an effortless way to understand our running jobs. We can easily trace a downstream dataset to the job that produced it, as well as examine individual runs of that job or any preceding ones.\\n\\nGaining the ability to observe lineage throughout our platform has been huge, and we are just getting started. Our teams are working to apply standard OpenLineage integrations into our environment and introduce data quality facets into our events. We have also been establishing operational workflows using job run information, to allow our DataOps team to monitor durations and measure against SLAs."},{"id":"/dbt-with-marquez","metadata":{"permalink":"/blog/dbt-with-marquez","source":"@site/blog/dbt-with-marquez/index.mdx","title":"Using Marquez to Visualize dbt Models","description":"Each time dbt runs, it generates a trove of metadata about datasets and the work it performs with them. In this post, I\u2019d like to show you how to harvest this metadata and put it to good use.","date":"2021-09-21T00:00:00.000Z","formattedDate":"September 21, 2021","tags":[],"readingTime":10.095,"hasTruncateMarker":true,"authors":[{"name":"Ross Turk","title":"OpenLineage Committer","url":"https://www.github.com/rossturk","imageURL":"https://www.github.com/rossturk.png","key":"Turk"}],"frontMatter":{"title":"Using Marquez to Visualize dbt Models","date":"2021-09-21T00:00:00.000Z","authors":["Turk"],"description":"Each time dbt runs, it generates a trove of metadata about datasets and the work it performs with them. In this post, I\u2019d like to show you how to harvest this metadata and put it to good use."},"prevItem":{"title":"How Northwestern Mutual Simplified Data Observability with OpenLineage & Marquez","permalink":"/blog/openlineage-at-northwestern-mutual"},"nextItem":{"title":"Introducing OpenLineage 0.1.0","permalink":"/blog/0.1-release"}},"content":"Each time dbt runs, it generates a trove of metadata about datasets and the work it performs with them. In this post, I\u2019d like to show you how to harvest this metadata and put it to good use.\\n\\n\x3c!--truncate--\x3e\\n\\n```toc\\n```\\n\\nThe first time I built a data warehouse was in a completely different era, even though it wasn\u2019t all that long ago. It was a few dozen tables + a collection of loader scripts and an ETL tool. If I\u2019m honest, calling the whole thing a \u201cdata warehouse\u201d is a bit grandiose, but it worked.\\n\\nAt the time, my defining question was \u201chow can I make all of my most important data available for study without spending more than it\u2019s worth?\u201d Because my database capacity wasn\u2019t infinite, I couldn\u2019t keep all of my data forever. The jobs I wrote would pull data from operational data stores, perform a bunch of slicing and aggregation, and load summary data into the warehouse. They shoveled bits every night from one server to another, performing calculations in between - and that meant they had to run on a beefy server with close proximity to my data.\\n\\nSkip forward to the current day and here I am, building and running models from a cafe over pretty shaky wifi. **My, how things have changed.**\\n\\nCloud data warehouses like [Google BigQuery](https://cloud.google.com/bigquery/), [Amazon Redshift](https://aws.amazon.com/redshift/), and [Snowflake](https://www.snowflake.com) have created a new economic and technological possibility: we can now pretty much just load everything - including our entire operational data stores - into a single warehouse. Once everything is in one place, data can be sliced up and analyzed much more quickly. This is where [dbt](https://www.getdbt.com) shines, at making transformations within a cloud data warehouse easy. And we all know what happens when you make something easy: it finds a way to happen a lot. People are doing more complex transformations than ever before, and the need for lineage context is becoming greater than ever.\\n\\nFortunately, each time dbt runs it generates a trove of metadata about datasets and the work it performs with them. In this post, I\u2019d like to show you how to harvest this metadata and put it to good use.\\n\\n# Our Example\\n\\nFor our example, let\u2019s choose the kind of experiment that I might run in my day-to-day life. I\u2019m the head of marketing at [Datakin](https://datakin.com), which means the metrics I\u2019m most interested in are usually about some sort of human behavior.\\n\\nI ask questions like:\\n* Does [x] technology space matter, and to whom? Is it waxing or waning?\\n* Are there adjacent ecosystems we should be collaborating with?\\n* Who are the influencers in this space? Who are the major contributors?\\n* What challenges are users facing? What does successful adoption look like?\\n\\nThere are a lot of ways to try to answer these questions. None of them are any more reliable than human behavior itself, and every resulting metric requires analysis and judgment. But there are still some pretty fun things to discover. And what better data source to mine to understand technical audiences than [Stack Overflow](https://www.stackoverflow.com)?\\n\\nSo let\u2019s see what we can learn from the Stack Overflow [public data set in BigQuery](https://cloud.google.com/blog/topics/public-datasets/google-bigquery-public-datasets-now-include-stack-overflow-q-a). But not the whole thing; it is very large, so let\u2019s study just a part of it. I created a [sample dbt project](http://github.com/rossturk/stackostudy/) that contains a handful of models to study all of the questions and answers we can find about the topic of ELT. These models:\\n\\n* Create slices of the key Stack Overflow tables, pulling them into a separate BigQuery project. These slices only contain the rows that are related to questions tagged with \u201celt\u201d. That way, we can query them tortuously all day long without scanning through gobs of partitions and running up our bill.\\n* Augment these slices by performing some helpful calculations - in this case, the number of upvotes/downvotes per question.\\n* Populate two summary tables for consumption by a BI system of some sort: a daily summary table that can be used to study trends and a user summary table that can be used to learn about the most influential contributors.\\n\\nThis is exactly the kind of experiment I have run multiple times over the years, across numerous stacks. It\u2019s usually pretty messy. But this time, after running all of these models, we will be rewarded with a gorgeous [Marquez](https://marquezproject.ai/) lineage graph. We\u2019ll be able to see how everything fits together.\\n\\n# Setting Everything Up\\n\\nFirst, if you haven\u2019t already, run through the excellent [dbt tutorial](https://docs.getdbt.com/tutorial/setting-up). It will show you how to create a BigQuery project, provision a service account, download a JSON key, and set up your local dbt environment. The rest of this example assumes that you have created a BigQuery project where our models can be run, and you know how to properly configure dbt to connect to it.\\n\\nNext, let\u2019s start a local Marquez instance to store our lineage metadata. Make sure you have Docker running, and then:\\n\\n```bash\\ngit clone https://github.com/MarquezProject/marquez.git && cd marquez\\n./docker/up.sh\\n```\\n\\nCheck to make sure Marquez is up by visiting [http://localhost:3000](http://localhost:3000). You should see an empty Marquez instance with a message saying there isn\u2019t any data. Also, you should be able to see the server output from your requests in the terminal window where Marquez is running. Keep this window open until we\u2019re done.\\n\\nNow, let\u2019s open a new terminal window/pane and clone the GitHub project containing our models:\\n\\n```bash\\ngit clone https://github.com/rossturk/stackostudy.git && cd stackostudy\\n```\\n\\nNext we need to install dbt and its integration with OpenLineage. I like to do this in a Python virtual environment because I make mistakes - as we all do - and I enjoy knowing that I can burn everything down and start over quickly if I need to. Virtual environments make this easy. To create one and install everything we need, run the following commands:\\n\\n```bash\\npython -m venv virtualenv\\nsource virtualenv/bin/activate\\npip install dbt dbt-openlineage\\n```\\n\\ndbt learns how to connect to your BigQuery project by looking for a matching profile in `~/.dbt/profiles.yml`. Create or edit this file so it contains a section with your BigQuery connection details. You will need to point to the location of a file containing the JSON key for your service account. If you aren\u2019t sure, you can follow [this section](https://docs.getdbt.com/tutorial/create-a-project-dbt-cli#connect-to-bigquery) in the dbt documentation. My `profiles.yml` looked like this when I was done:\\n\\n```yaml\\nstackostudy:\\n  target: dev\\n  outputs:\\n    dev:\\n      type: bigquery\\n      method: service-account\\n      keyfile: /Users/rturk/.dbt/dbt-example.json\\n      project: dbt-example\\n      dataset: stackostudy\\n      threads: 1\\n      timeout_seconds: 300\\n      location: US\\n      priority: interactive\\n```\\n\\nRun `dbt debug` to make sure that you have everything configured correctly.\\n\\n```bash\\n% dbt debug\\nRunning with dbt=0.20.1\\ndbt version: 0.20.1\\npython version: 3.8.12\\npython path: /opt/homebrew/Cellar/dbt/0.20.1_1/libexec/bin/python3\\nos info: macOS-11.5.2-arm64-arm-64bit\\nUsing profiles.yml file at /Users/rturk/.dbt/profiles.yml\\nUsing dbt_project.yml file at /Users/rturk/projects/stackostudy/dbt_project.yml\\n\\nConfiguration:\\n  profiles.yml file [OK found and valid]\\n  dbt_project.yml file [OK found and valid]\\n\\nRequired dependencies:\\n - git [OK found]\\n\\nConnection:\\n  method: service-account\\n  database: stacko-study\\n  schema: stackostudy\\n  location: US\\n  priority: interactive\\n  timeout_seconds: 300\\n  maximum_bytes_billed: None\\n  Connection test: OK connection ok\\n```\\n\\n# A Few Important Details\\n\\nThere are a couple of considerations to make when designing dbt models for use with OpenLineage. By following these conventions, you can help OpenLineage collect the most complete metadata possible.\\n\\nFirst, when working with datasets outside of your dbt project, define them in a schema YAML file inside the `models/` directory:\\n\\n```yaml\\nversion: 2\\n\\nsources:\\n  - name: stackoverflow\\n    database: bigquery-public-data\\n    schema: stackoverflow\\n    tables:\\n      - name: posts_questions\\n      - name: posts_answers\\n      - name: users\\n      - name: votes\\n```\\n\\nThis contains the name of the external dataset - in this case, `bigquery-public-datasets`, and lists the tables that are used by the models in this project. It doesn\u2019t matter what the file is named, as long as it ends with `.yml` and is inside the `models/` directory, so I called mine `schema.yml` \ud83e\udd37\u200d\u2642\ufe0f If you hardcode dataset and table names into your queries instead, dbt will likely run successfully but dataset metadata will be incompletely collected.\\n\\nWhen writing queries, be sure to use the `{{ ref() }}` and `{{ source() }}` jinja functions when referring to data sources. The `{{ ref() }}` function can be used to refer to tables within the same model, and the `{{ source() }}` function refers to tables we have defined in `schema.yml`. That way, dbt will properly keep track of the relationships between datasets. For example, to select from both an external dataset and one in this model:\\n\\n```sql\\nselect * from {{ source(\'stackoverflow\', \'posts_answers\') }}\\nwhere parent_id in (select id from {{ ref(\'filtered_questions\') }} )\\n```\\n\\n# Performing a Run\\n\\nOkay! We are ready to perform a run. Before we do, though, there\u2019s one last step we need to take.\\n\\nRun `dbt docs generate`. This will cause dbt to create a `target/catalog.json` file containing the schemas of each dataset referred to in the models. This file will be parsed by the dbt OpenLineage integration and sent to our Marquez server. If it doesn\u2019t exist, a lineage graph will still be generated but schema details won\u2019t be available in Marquez.\\n\\n```bash\\ndbt docs generate\\nRunning with dbt=0.20.1\\nFound 8 models, 0 tests, 0 snapshots, 0 analyses, 164 macros, 0 operations, 0 seed files, 4 sources, 0 exposures\\n\\n12:15:10 | Concurrency: 1 threads (target=\'dev\')\\n12:15:10 |\\n12:15:10 | Done.\\n12:15:10 | Building catalog\\n12:15:26 | Catalog written to /Users/rturk/projects/stackostudy/target/catalog.json\\n```\\n\\nThe OpenLineage integration for dbt is implemented as a wrapper, `dbt-ol`. This wrapper runs dbt and, after it completes, analyzes the `target/catalog.json`, `target/run_results.json` and `target/manifest.json` files. It sends corresponding OpenLineage events to the endpoint specified in the `OPENLINEAGE_URL` environment variable.\\n\\nTo run the models: \\n\\n```bash\\n% OPENLINEAGE_URL=http://localhost:5000 dbt-ol run\\nRunning with dbt=0.20.1\\nFound 8 models, 0 tests, 0 snapshots, 0 analyses, 164 macros, 0 operations, 0 seed files, 4 sources, 0 exposures\\n\\n12:35:41 | Concurrency: 1 threads (target=\'dev\')\\n12:35:41 |\\n12:35:41 | 1 of 8 START incremental model stackostudy.filtered_questions........ [RUN]\\n12:35:46 | 1 of 8 OK created incremental model stackostudy.filtered_questions... [MERGE (0.0 rows, 34.6 GB processed) in 4.52s]\\n12:35:46 | 2 of 8 START incremental model stackostudy.filtered_answers.......... [RUN]\\n12:35:51 | 2 of 8 OK created incremental model stackostudy.filtered_answers..... [MERGE (0.0 rows, 26.8 GB processed) in 5.22s]\\n12:35:51 | 3 of 8 START incremental model stackostudy.filtered_votes............ [RUN]\\n12:36:05 | 3 of 8 OK created incremental model stackostudy.filtered_votes....... [MERGE (0.0 rows, 6.5 GB processed) in 14.58s]\\n12:36:05 | 4 of 8 START incremental model stackostudy.filtered_users............ [RUN]\\n12:36:21 | 4 of 8 OK created incremental model stackostudy.filtered_users....... [MERGE (0.0 rows, 2.5 GB processed) in 16.09s]\\n12:36:21 | 5 of 8 START view model stackostudy.summary_daily.................... [RUN]\\n12:36:23 | 5 of 8 OK created view model stackostudy.summary_daily............... [OK in 1.01s]\\n12:36:23 | 6 of 8 START view model stackostudy.answer_stats..................... [RUN]\\n12:36:23 | 6 of 8 OK created view model stackostudy.answer_stats................ [OK in 0.96s]\\n12:36:23 | 7 of 8 START view model stackostudy.question_stats................... [RUN]\\n12:36:24 | 7 of 8 OK created view model stackostudy.question_stats.............. [OK in 0.88s]\\n12:36:24 | 8 of 8 START view model stackostudy.user_stats....................... [RUN]\\n12:36:26 | 8 of 8 OK created view model stackostudy.user_stats.................. [OK in 1.21s]\\n12:36:26 |\\n12:36:26 | Finished running 4 incremental models, 4 view models in 45.39s.\\n\\nCompleted successfully\\n\\nDone. PASS=8 WARN=0 ERROR=0 SKIP=0 TOTAL=8\\nEmitted 16 openlineage events\\n```\\n\\nNote the output showing the number of OpenLineage events emitted to Marquez.\\n\\n# Reviewing the Output\\n\\nIf everything ran successfully you should be able to see a list of jobs when you navigate to http://localhost:3000. Upon clicking a job, you will see a lineage graph that looks similar to this:\\n\\n![The stackostudy Marquez lineage graph](./graph.png)\\n\\nOur set of models, previously represented by SQL inside text files, has become more easily digestible. The dependencies between datasets are now completely obvious. Data engineers can throw away their remaining whiteboards, hooray!\\n\\nThere\u2019s something satisfying about seeing models represented in two-dimensional space. But more importantly, this integration allows us to capture the state of a dbt pipeline as it runs. Using a long-running instance of Marquez (or another OpenLineage-compatible metadata repository) this information can be studied as it changes over time.\\n\\nTo see how the OpenLineage dbt integration works, visit its [GitHub repository](https://github.com/OpenLineage/OpenLineage/tree/main/integration/dbt)."},{"id":"/0.1-release","metadata":{"permalink":"/blog/0.1-release","source":"@site/blog/0.1-release/index.mdx","title":"Introducing OpenLineage 0.1.0","description":"We are pleased to announce the initial release of OpenLineage. This release includes the core specification, data model, clients, and integrations with common data tools.","date":"2021-09-03T00:00:00.000Z","formattedDate":"September 3, 2021","tags":[],"readingTime":1.15,"hasTruncateMarker":true,"authors":[{"name":"Julien Le Dem","title":"OpenLineage Project Lead","url":"https://www.github.com/julienledem/","imageURL":"https://avatars.githubusercontent.com/u/367841?v=4","key":"Le Dem"}],"frontMatter":{"title":"Introducing OpenLineage 0.1.0","date":"2021-09-03T00:00:00.000Z","authors":["Le Dem"],"description":"We are pleased to announce the initial release of OpenLineage. This release includes the core specification, data model, clients, and integrations with common data tools."},"prevItem":{"title":"Using Marquez to Visualize dbt Models","permalink":"/blog/dbt-with-marquez"},"nextItem":{"title":"Expecting Great Quality with OpenLineage Facets","permalink":"/blog/dataquality_expectations_facet"}},"content":"We are pleased to announce the initial release of OpenLineage. This release includes the core specification, data model, clients, and integrations with common data tools.\\n\\n\x3c!--truncate--\x3e\\n\\nWe are pleased to announce the initial release of OpenLineage. This is the culmination of a broad community effort, and establishes a common framework for data lineage collection and analysis.\\n\\nWe want to thank [all the contributors](https://github.com/OpenLineage/OpenLineage/graphs/contributors) as well all the projects and companies involved in the design (in alphabetical order): [Airflow](https://airflow.apache.org), [Astronomer](https://www.astronomer.io), [Datakin](https://datakin.com), [Data Mesh](https://datameshlearning.com), [dbt](https://www.getdbt.com), [Egeria](https://egeria.odpi.org), [GetInData](https://getindata.com), [Great Expectations](https://greatexpectations.io), [Iceberg](https://iceberg.apache.org) (and others that I am probably forgetting).\\n\\nThis release includes:\\n* The initial 1-0-0 release of the [OpenLineage specification](https://github.com/OpenLineage/OpenLineage/blob/main/spec/OpenLineage.md)\\n* A core lineage model of Jobs, Runs and Datasets\\n  * Core facets\\n  * Data Quality Metrics and statistics\\n  * Dataset schema\\n  * Source code location\\n  * SQL\\n* Clients that send OpenLineage events to an HTTP backend\\n  * Java\\n  * Python\\n* [Integrations](https://github.com/OpenLineage/OpenLineage/tree/main/integration) that collect lineage metadata as OpenLineage events\\n  * Apache Airflow with support for BigQuery, Great Expectations, Postgres, Redshift, Snowflake\\n  * Apache Spark\\n  * dbt\\n\\nThis is only the beginning. We invite everyone interested to [consult and contribute to the roadmap](https://github.com/OpenLineage/OpenLineage/projects). The roadmap currently contains, among other things: adding support for [Kafka](https://github.com/OpenLineage/OpenLineage/issues/152), [BI dashboards](https://github.com/OpenLineage/OpenLineage/issues/207), and [column level lineage](https://github.com/OpenLineage/OpenLineage/issues/148)...but you can influence it by participating!\\n\\nFollow the [repo](https://github.com/OpenLineage/OpenLineage) to stay updated. And, as always, you can [join the conversation](http://bit.ly/OpenLineageSlack) on Slack."},{"id":"/dataquality_expectations_facet","metadata":{"permalink":"/blog/dataquality_expectations_facet","source":"@site/blog/dataquality_expectations_facet/index.mdx","title":"Expecting Great Quality with OpenLineage Facets","description":"Good data is paramount to making good decisions- but how can you trust the quality of your data and its dependencies?","date":"2021-08-12T00:00:00.000Z","formattedDate":"August 12, 2021","tags":[],"readingTime":12.745,"hasTruncateMarker":true,"authors":[{"name":"Michael Collado","title":"OpenLineage Committer","url":"https://www.github.com/collado-mike","imageURL":"https://www.github.com/collado-mike.png","key":"Collado"}],"frontMatter":{"title":"Expecting Great Quality with OpenLineage Facets","date":"2021-08-12T00:00:00.000Z","authors":["Collado"],"description":"Good data is paramount to making good decisions- but how can you trust the quality of your data and its dependencies?"},"prevItem":{"title":"Introducing OpenLineage 0.1.0","permalink":"/blog/0.1-release"},"nextItem":{"title":"Extending OpenLineage with Facets","permalink":"/blog/extending-with-facets"}},"content":"Good data is paramount to making good decisions- but how can you trust the quality of your data and its dependencies?\\n\\n\x3c!--truncate--\x3e\\n\\n## The Parable of Bad Data\\n\\nSeveral years ago, I worked as a developer on the experimentation team at Amazon, which owned the code libraries and\\ndata processing systems that supported experimentation on the amazon.com website (among other systems). Developers used\\nour libraries and a microservice we maintained to \u201ctrigger\u201d an experiment for a customer- that is, the customer was\\nrandomized into either control or treatment and the resulting assignment was recorded in the logs, which my team\\nconsumed in the analysis of the data later on. One of the interesting parts of my job was helping our users diagnose\\nproblems with their experiment results.\\n\\nA classic example was a Kindle developer who was prototyping a new feature for book pages that would make the site more\\nengaging for Kindle owners- perhaps a \u201clook inside\u201d feature, or maybe some better recommendations. A customer would come\\nto the website and the developer\u2019s code would determine if the customer belonged in _Control_ or _Treatment_. If _Control_,\\nthe assignment was logged and no feature was shown- the site looked to the customer as it always had. But if the\\nassignment was _Treatment_, the code would check the customer\u2019s account to determine if they owned a Kindle device and,\\nif yes, the assignment was logged and the customer saw the fancy new feature on the books page. \\n\\nThe experiment showed the developer\u2019s feature would be wildly successful- an increase of over $10 in Kindle book\\npurchases per customer on average over the course of the 2 weeks the experiment ran- projected to be tens of billions of\\ndollars in annual revenue due to this one feature!\\n\\nWith data in hand, the developer requested tons of resources to build the feature up to production standards. After\\nthree months and two dozen people\u2019s labor, the feature was ready to ship. The developers deployed their new service and\\nthe incredible feature was unleashed. For days afterward, everyone watched the metrics dashboards waiting for that\\nhockey stick uptick in the revenue graphs \ud83d\udcc8. But it never materialized! The graph was flat. No change at all! Weeks\\nwent by. Nothing. How could the experiment results be so far from reality?\\n\\nOf course, if you\u2019ve ever run A/B tests, you probably already recognized the developer\u2019s mistake. In their randomization\\nlogic, customers who were assigned control were logged and forgotten, while customers who were assigned treatment were\\nlogged only after validating that they owned a Kindle device. It turned out the total number of customers who came to\\namazon.com was far greater than the number of customers who owned a Kindle device. And if you divide the total sum of\\nKindle book sales by all of the amazon.com customers, regardless of whether they own a Kindle device, that average\\nwill come out quite a lot lower than if you calculate the average Kindle book revenue from only customers who own Kindles.\\n\\n![Yoda says, poorly run experiments lead to bad data. Bad data leads to bad insights. Bad insights lead to bad decisions. Bad decisions lead to suffering](./bad_experiment.png)\\n\\nIn reality, this story never happened. Why? Because we knew the adage- Bad Data is Worse than No Data. In the story,\\npeople took data of poor quality and used it to justify bad decisions. In our system, we checked the quality of the data\\nand, if we detected assignment imbalances, we simply invalidated the experiment and hid the results. Over the years, I\\ncan\u2019t count the number of times our users asked us to just give them partial results or just exclude certain segments or\\nto let them know if things were \u201ctrending\u201d the right way. Our policy was firm- if we couldn\u2019t trust the quality of the\\ndata, the results were meaningless and we would not surface them in our system.\\n\\n## Data-Driven Depends On Data-Quality\\n\\nToday, most businesses consider themselves data-driven. The stereotype of the maverick CEO leading with his or her gut\\nis mostly antiquated, with a handful of exceptions. And yet, even though people know intellectually that data is only\\nuseful if it is correct, we tend to stop digging once we find some data that confirms our pre-existing assumptions. We\\njustify bad decisions by claiming that they are \u201cdata-based\u201d without ever validating the quality of our sources. Where\\ndid that data come from? How old is it? Is the logic that generated it correct? Is it derived from some other dataset?\\nWhat is the quality of that dataset?\\n![Data-driven: You keep using that word. I do not think it means what you think it means](./data_driven.png)\\n\\nThankfully, data quality validation is becoming more and more common in data engineering organizations. In part, this is\\ndue to the prevalence of new tools and their integration with common workflow engines which we already use to schedule\\nthe jobs that generate and process our data. One such tool that has been gaining in popularity is called [_Great\\nExpectations_](https://docs.greatexpectations.io), a Python-based framework for defining assertions about data sets\\nwhich easily integrates with existing workflow tools, such as Airflow. \\n\\nIn software development, testing the behavior of our code with unit and integration tests has been common practice for\\nyears. Similarly, using Great Expectations, a data engineer can assert that a dataset has a row count that falls within\\nan expected range, that column values are not null, or that values match a specified regular expression. One can even\\ncreate custom expectations, such as validating that the number of records in treatment is roughly the same as the number\\nof records in control (this post is not intended to be an in-depth tutorial on setting up Great Expectations; if you\\nwant to read more on its capabilities and to get started, I recommend the going through\\nthe [Quick Start tutorial](https://docs.greatexpectations.io/en/latest/guides/tutorials/quick_start.html)).\\n\\n### A Sample Assertion Suite\\n\\nAs a simple example, imagine a table of new customers that you need to import into your Data Warehouse. Before\\nimporting, we want to check the data quality of this new batch of customers. One example suite of assertions we could\\ntest is below:\\n\\n```json\\n{\\n  \\"data_asset_type\\": \\"Dataset\\",\\n  \\"expectation_suite_name\\": \\"customers_suite\\",\\n  \\"expectations\\": [\\n    {\\n      \\"expectation_type\\": \\"expect_table_row_count_to_be_between\\",\\n      \\"kwargs\\": {\\n        \\"max_value\\": 1000,\\n        \\"min_value\\": 100\\n      },\\n      \\"meta\\": {}\\n    },\\n    {\\n      \\"expectation_type\\": \\"expect_table_column_count_to_equal\\",\\n      \\"kwargs\\": {\\n        \\"value\\": 8\\n      },\\n      \\"meta\\": {}\\n    },\\n    {\\n      \\"expectation_type\\": \\"expect_table_columns_to_match_ordered_list\\",\\n      \\"kwargs\\": {\\n        \\"column_list\\": [\\n          \\"id\\",\\n          \\"created_at\\",\\n          \\"updated_at\\",\\n          \\"name\\",\\n          \\"email\\",\\n          \\"address\\",\\n          \\"phone\\",\\n          \\"city_id\\"\\n        ]\\n      },\\n      \\"meta\\": {}\\n    },\\n    {\\n      \\"expectation_type\\": \\"expect_column_values_to_be_unique\\",\\n      \\"kwargs\\": {\\n        \\"column\\": \\"email\\"\\n      },\\n      \\"meta\\": {}\\n    }\\n  ],\\n  \\"meta\\": {\\n    // ...\\n  }\\n}\\n\\n```\\n\\nThis sample suite contains 4 data quality assertions- that the dataset contains between 100 and 1000 rows, that the\\ntable contains exactly 8 columns, that they match the explicit list of column names we expect, and that the `email` column\\ncontains only distinct values.\\n\\n### Adding Data Quality Checks to an Airflow pipeline\\n\\nWith a suite of assertions in hand, we can update our Airflow DAG to only import data into our Data Warehouse if it\\nmatches our expectations. A simple DAG might look like this\\n\\n```python\\nfrom airflow import DAG\\nfrom airflow.contrib.operators.bigquery_operator import BigQueryOperator\\nfrom airflow.utils.dates import days_ago\\nfrom great_expectations_provider.operators.great_expectations import GreatExpectationsOperator\\n\\ndag = DAG(\\n    \'etl_customers\',\\n    schedule_interval=\'@daily\',\\n    catchup=False,\\n    default_args=default_args,\\n    description=\'Loads newly registered customers daily.\'\\n)\\n\\nt1 = BigQueryOperator(\\n    task_id=\'if_not_exists\',\\n    sql=\'\'\'\\n  CREATE TABLE IF NOT EXISTS food_delivery.customers (\\n  id         INT64,\\n  created_at TIME,\\n  updated_at TIME,\\n  name       STRING,\\n  email      STRING,\\n  address    STRING,\\n  phone      STRING,\\n  city_id    INT64\\n  )\\n  \'\'\',\\n    use_legacy_sql=False,\\n    dag=dag\\n)\\n\\nt2 = GreatExpectationsOperator(\\n    expectation_suite_name=\'customers_suite\',\\n    batch_kwargs={\\n      \'table\': \'tmp_customers\',\\n      \'datasource\': \'food_delivery_db\'\\n    },\\n    dag=dag\\n    task_id=\'customers_expectation\',\\n)\\n\\nt3 = BigQueryOperator(\\n    task_id=\'etl\',\\n    sql=\'\'\'\\n    SELECT id, created_at, updated_at, name, email, address, phone, city_id\\n    FROM food_delivery.tmp_customers\\n  \'\'\',\\n    destination_dataset_table=\'airflow_marquez.food_delivery.customers\',\\n    use_legacy_sql=False,\\n    dag=dag\\n)\\n\\nt1 >> t2 >> t3\\n```\\n\\nThis is great! Our DAG creates our target table in BigQuery (if it doesn\u2019t already exist), checks the quality of the\\n`tmp_customers` table by running the `customers_suite` defined earlier, then imports _only if all_ of the data quality\\nchecks pass.\\n\\nAnd thus ended all data quality problems forever.\\n\\nJust kidding.\\n\\nBecause reality is never so straightforward. In reality, the recommendations team wanted to start generating\\nrecommendations for new customers without waiting until the next day, so they built a data pipeline to start consuming\\nfrom the `tmp_customers` table directly. And the supply chain folks wanted to start detecting what recipes are gaining\\npopularity so they can predict what supplies will need to be restocked sooner, so they started reading from the both\\n`tmp_orders` table and the `tmp_customers` table before they\u2019re available in the DW. Oh, and the scheduling team needs\\nan idea of the geography of the various customers that are ordering and what the distances are between restaurants and\\ncustomers so they can get the right number of drivers in the right neighborhoods and of course the marketing team wants\\nto use all of this data to make predictions about how much to spend on the right search engine and social media ads and\\nthey absolutely cannot wait until tomorrow at 8AM to update their models.\\n![I am the gatekeeper](./gatekeeper.png)\\n\\n## Tracing Data Quality With OpenLineage Facets\\n\\nUsers are never satisfied with the way things are supposed to work. There\u2019s always a reason to work around gatekeepers-\\noftentimes, very good reasons that have real business impact- and data engineering is full of creative and resourceful\\npeople who absolutely will find a way to get at that data. Even at Amazon, the experiment data was available in the\\nclick stream logs, so resourceful users could (and sometimes did) calculate their own experiment results if they really\\nwanted to. So it\u2019s important not just to have data quality checks, but to trace the impact of that data throughout an\\norganization.\\n\\nThe OpenLineage standard uses [Facets](https://openlineage.io/blog/extending-with-facets/) to augment the core data\\nmodel with useful information about the jobs, runs, and datasets reported on. One interesting detail about facets is\\nthat they can be attached to an entity after the fact. In the Marquez reference implementation, a dataset version is\\ncreated every time a job run writes to or otherwise modifies a dataset. _Output_ facets, such as the new record count or\\nthe number of bytes written, are attached directly to the dataset version when the job run completes. But consuming\\njobs can also attach facets to the version of the dataset that exists at the start time of the job\u2019s execution.\\n\\nIn the OpenLineage Airflow integration, Great Expectations tasks, such as the one in our example DAG above, are\\nevaluated after they run and the expectation results (as well as some other data quality metrics) are collected\\ninto a\\n[DataQuality Metrics Input Dataset Facet](https://github.com/OpenLineage/OpenLineage/blob/main/spec/OpenLineage.json#L446-L499)\\n, which is reported to the server along with the rest of the lineage metadata. In Marquez, we recognize the version of\\nthe dataset that was read by the job run and the data quality metadata is permanently associated with that dataset\\nversion. The impact of this is that any job that reads that data, whether it happens before or after the dataset quality\\nassertion, can be linked to the data quality facet recorded (provided that the dataset version doesn\u2019t change between\\nthe data quality check and the read job).\\n\\nThis integration is extremely straightforward to get working. If you already have the Marquez Airflow DAG running in\\nyour Airflow workflows, there\u2019s nothing to do! Great Expectations tasks are already being detected and the metrics and\\nassertion statuses are already being reported to your configured instance of Marquez.\\n\\nIf you\u2019ve never integrated Marquez with your Airflow setup, add a couple\\nof [environment variables](https://github.com/OpenLineage/OpenLineage/tree/main/integration/airflow#configuration)\\nand [change one line of code](https://github.com/OpenLineage/OpenLineage/tree/main/integration/airflow#usage):\\n\\n```diff\\n- from airflow import DAG\\n+ from marquez_airflow import DAG\\nfrom airflow.contrib.operators.bigquery_operator import BigQueryOperator\\nfrom airflow.utils.dates import days_ago\\nfrom great_expectations_provider.operators.great_expectations import GreatExpectationsOperator\\n```\\n\\nI\u2019ve previously written\\nabout [how to determine the version of the dataset that was read by a particular job run](https://openlineage.io/blog/explore-lineage-api/)\\n. With Great Expectations now integrated into my Airflow DAG, I want to see what the data quality metrics are for the\\nlatest version of the `customers` dataset that was processed by my ETL job. I\u2019ll hit my datakin demo instance:\\n\\n```bash\\n$ curl \\"https://demo.datakin.com/api/v1/namespaces/food_delivery/jobs/etl.etl_delivery_7_days\\" | jq | less\\n{\\n  \\"id\\": {\\n    \\"namespace\\": \\"food_delivery\\",\\n    \\"name\\": \\"etl.etl_delivery_7_days\\"\\n  },\\n  \\"type\\": \\"BATCH\\",\\n  \\"name\\": \\"etl.etl_delivery_7_days\\",\\n  \\"createdAt\\": \\"2021-07-23T19:32:03.401782Z\\",\\n  \\"updatedAt\\": \\"2021-08-06T05:11:03.604573Z\\",\\n  \\"namespace\\": \\"food_delivery\\",\\n  \\"inputs\\": [\\n    {\\n      \\"namespace\\": \\"food_delivery\\",\\n      \\"name\\": \\"public.customers\\"\\n    },\\n    //...\\n  ],\\n  \\"latestRun\\": {\\n    \\"id\\": \\"1043e596-ccb8-4bfb-8fc2-7ee066253248\\",\\n    \\"jobVersion\\": {\\n      \\"namespace\\": \\"food_delivery\\",\\n      \\"name\\": \\"etl.etl_delivery_7_days\\",\\n      \\"version\\": \\"bc6c294b-b0eb-3160-a06d-1ff9ba3a4e1c\\"\\n    },\\n    \\"inputVersions\\": [\\n      {\\n        \\"namespace\\": \\"food_delivery\\",\\n        \\"name\\": \\"public.customers\\",\\n        \\"version\\": \\"4c33f292-40a9-304d-b43f-c7ffb2256e7f\\"\\n      },\\n      // ...\\n    ],\\n    // ...\\n  }\\n}\\n```\\n\\nWith the input version of the `public.customers` dataset, I can query Marquez for all the metadata we have about that\\nspecific version of the dataset.\\n\\n```bash\\n$ curl \\"https://demo.datakin.com/api/v1/namespaces/food_delivery/datasets/public.customers/versions/4c33f292-40a9-304d-b43f-c7ffb2256e7f\\" | jq | less\\n{\\n  \\"id\\": {\\n    \\"namespace\\": \\"food_delivery\\",\\n    \\"name\\": \\"public.customers\\"\\n  },\\n  \\"type\\": \\"DB_TABLE\\",\\n  \\"name\\": \\"public.customers\\",\\n  \\"physicalName\\": \\"public.customers\\",\\n  \\"createdAt\\": \\"2021-08-06T05:02:59.189118Z\\",\\n  \\"version\\": \\"4c33f292-40a9-304d-b43f-c7ffb2256e7f\\",\\n  \\"namespace\\": \\"food_delivery\\",\\n  \\"sourceName\\": \\"analytics_db\\",\\n  \\"fields\\": [\\n    {\\n      \\"name\\": \\"id\\",\\n      \\"type\\": \\"INTEGER\\",\\n      \\"tags\\": [],\\n      \\"description\\": \\"The unique ID of the customer.\\"\\n    },\\n    // ...\\n  ],\\n  \\"facets\\": {\\n    \\"stats\\": {\\n      \\"size\\": 53362712,\\n      \\"rowCount\\": 4969\\n    },\\n    \\"dataSource\\": {\\n      \\"uri\\": \\"jdbc:postgresql://localhost:3306/deliveries\\",\\n      \\"name\\": \\"analytics_db\\"\\n    },\\n    \\"description\\": \\"A table for customers.\\",\\n    \\"dataQuality\\": {\\n      \\"bytes\\": 53362712,\\n      \\"rowCount\\": 4969,\\n      \\"columnMetrics\\": {\\n        \\"id\\": {\\n          \\"nullCount\\": 0,\\n          \\"distinctCount\\": 4969\\n        },\\n        \\"name\\": {\\n          \\"nullCount\\": 0,\\n          \\"distinctCount\\": 4969\\n        },\\n        \\"email\\": {\\n          \\"nullCount\\": 0,\\n          \\"distinctCount\\": 4969\\n        }\\n      }\\n    },\\n    \\"greatExpectations_assertions\\": {\\n      \\"assertions\\": [\\n        {\\n          \\"success\\": true,\\n          \\"expectationType\\": \\"expect_table_row_count_to_be_between\\"\\n        },\\n        {\\n          \\"success\\": true,\\n          \\"expectationType\\": \\"expect_column_to_exist\\"\\n        },\\n        {\\n          \\"success\\": true,\\n          \\"columnId\\": \\"id\\",\\n          \\"expectationType\\": \\"expect_column_values_to_be_unique\\"\\n        },\\n        {\\n          \\"success\\": true,\\n          \\"columnId\\": \\"id\\",\\n          \\"expectationType\\": \\"expect_column_values_to_not_be_null\\"\\n        },\\n        {\\n          \\"success\\": true,\\n          \\"columnId\\": \\"created_at\\",\\n          \\"expectationType\\": \\"expect_column_values_to_not_be_null\\"\\n        },\\n        //....\\n      ]\\n    }\\n  }\\n}\\n```\\n\\nNote the `facets` field contains several properties- `stats`, `dataSource`, `description`, `dataQuality`\\nand `greatExpectations_assertions`. Each of those describes some attribute about the dataset version. Some of the facets\\nare attached at write-time, some are attached later- when the dataset is read.\\n\\nIn our [Datakin demo](https://demo.datakin.com), we have a lot more assertions than what I included in the sample suite\\nabove and that can be seen in this response. In addition to counting rows and columns, we also validate that id columns\\nare unique and non-null, timestamps fall within specified ranges (did you know that if you accidentally write a\\ntimestamp too far in the future, certain JDBC drivers will overflow the Calendar instance they use for converting\\ntimezones?), and emails match expected regular expressions.\\n\\nWith the ability to attach data quality facets to dataset versions and the ability to trace the specific versions of\\ndatasets read by and written to by specific job runs, I can trust whether the data I\u2019m looking at is good data or bad\\ndata. And if my data quality checks fail, I can find out whether I need to contact somebody over in marketing or\\nrecommendations to [backfill their pipelines](https://openlineage.io/blog/backfilling-airflow-dags-using-marquez/) once\\nthe issue has been corrected.\\n\\n![Good data, bad data](./bad_data.png)\\n\\nWhether your business is an e-commerce shop that wants to improve its customer experience or a music streaming service\\nthat wants to make better listening recommendations or an autonomous vehicle company trying to improve the car\u2019s ability\\nto detect double parked vehicles, the quality of your data is paramount to making good decisions. Quality testing tools\\nare out there and, chances are, they already work with the pipeline workflow tool you\u2019re using today. And with\\nOpenLineage support, you can be confident in the quality of the data at every stage in your pipeline."},{"id":"/extending-with-facets","metadata":{"permalink":"/blog/extending-with-facets","source":"@site/blog/extending-with-facets/index.mdx","title":"Extending OpenLineage with Facets","description":"Facets are a self-contained definition of one aspect of a job, dataset, or run at the time the event happened. They make the OpenLineage model extensible.","date":"2021-07-27T00:00:00.000Z","formattedDate":"July 27, 2021","tags":[],"readingTime":7.66,"hasTruncateMarker":true,"authors":[{"name":"Julien Le Dem","title":"OpenLineage Project Lead","url":"https://www.github.com/julienledem/","imageURL":"https://avatars.githubusercontent.com/u/367841?v=4","key":"Le Dem"}],"frontMatter":{"title":"Extending OpenLineage with Facets","date":"2021-07-27T00:00:00.000Z","authors":["Le Dem"],"description":"Facets are a self-contained definition of one aspect of a job, dataset, or run at the time the event happened. They make the OpenLineage model extensible."},"prevItem":{"title":"Expecting Great Quality with OpenLineage Facets","permalink":"/blog/dataquality_expectations_facet"},"nextItem":{"title":"OpenLineage joins the LF AI & Data Foundation","permalink":"/blog/joining-lfai"}},"content":"Facets are a self-contained definition of one aspect of a job, dataset, or run at the time the event happened. They make the OpenLineage model extensible.\\n\\n\x3c!--truncate--\x3e\\n\\n# Building consensus\\n\\nOpenLineage is an open source project, part of the LFAI&Data foundation, that standardizes lineage collection in the data ecosystem. In this increasingly rich ecosystem - that includes SQL-driven data warehouses, programmatic data processing frameworks like Spark or Pandas, and machine learning - it is near-impossible to maintain a clear and sane view of data lineage across everything without the collaboration of the ecosystem around a shared standard. Open source collaboration is a very powerful mechanism that can produce widely-adopted standard APIs.\\n\\n[OpenLineage draws a clear parallel with OpenTelemetry](https://openlineage.io/blog/openlineage-takes-inspiration-from-opentelemetry/) which provides a standard API to collect traces and metrics in the service world. It also draws from the experience of the Apache Parquet and Apache Arrow projects, which aim to define standard columnar data representations at-rest and in-memory.\\n\\n## Open Source\\nStandardizing an API through open source collaboration can be challenging. On one end, you need to get input and feedback from the people who will use the API in different contexts. On the other, you want to avoid getting stuck in disagreements arising from the different and sometimes incompatible viewpoints that inevitably drive these discussions. Thankfully, there are mechanisms to help organize and decouple those disagreements and drive discussions towards conclusion.\\n\\nA community driven open source project works very differently from a product you buy off the shelf. At the very moment you start using it - maybe starting by reading the doc - you become part of the community and start sharing a little bit of ownership. As with any software, you might encounter problems... but in this case, you immediately become part of the solution. In a healthy community, how much of the solution you become is entirely up to you.\\n\\nMaybe you spotted a typo and reported it. Maybe you opened a pull request to fix it. You might propose an improvement, or even build one yourself. All of those contributions, no matter how small, make the project better for everyone. That very powerful flywheel motion gathers momentum and drives very successful open source projects.\\n\\nOne of the success factors of such an open source project is how much it can minimize the friction for new community members who want to contribute. The easier it is to contribute, the faster the project will acquire momentum. It\u2019s not about getting other people\u2019s input, it\u2019s about giving them a share of ownership and encouraging them to drive the areas where they can most effectively contribute.\\n\\nIn a multi-faceted domain like data lineage, enabling others to lead discussions is critical.\\n\\n## Making progress\\nIn this context, we need mechanisms to converge often and make incremental progress. \\n\\nYou definitely want to avoid having a big monolithic spec that takes a long time to reach consensus on - if you ever do. A discussion around a large ultra-spec that combines specifications from multiple related domains will lose steam. We need to keep conversations focused on the topics that individual contributors care about. It is critical to subdivide the specification in concrete and granular decision points where consistent and significant progress can be made.\\n\\nNot everyone will care about all the aspects of the specification, and that is fine. We need to make sure contributors can easily focus on the aspects they do care about. This need for a very granular decision making process, one where we can make progress independently on different aspects of the spec, leads naturally into decomposition of the specification into smaller independent subsets.\\n\\nThis will keep conversations focused and moving. It also decouples workstreams where consensus can be reached from those that are more contentious. \\n\\nFor example the contributors interested in data quality might be different from the ones interested in column-level lineage or query performance.\\n\\n## Embracing different points of view\\nDepending on their perspective, contributors may have very different opinions on how to model a certain aspect of data. Or they may have different use-cases in mind. Instead of pitting different view-points against each other and forcing alignment on every point, it is sometimes beneficial to allow them to be expressed separately. \\n\\nFor example, when you ask a data practitioner \\"what is data lineage?\\" they may have very different definitions for it. \\n- Some care about how a specific metric is derived from the raw data, and need column level lineage. \\n- Some will care about compliance with privacy regulations and need relevant metadata to locate sensitive data and trace its movement.\\n- Some will care about the reliability of data delivery and need data freshness and quality metrics - in particular, how they change over time in correlation with changes in the system.\\n\\nAll those are valid view points that deserve to be captured appropriately and can be defined independently in a framework that allows them to cohabitate.\\n\\n# Mechanics\\n\\nOpenLineage is purposefully providing a faceted model around a minimalistic core spec to enable this granular decision making, minimize friction in contributing, and favor community-driven improvements.\\n\\nThe core spec focuses on high-level modeling of jobs, runs, datasets, and their relation. Each OpenLineage event refers to a run of a job and its input and output datasets.\\n- A job is a recurring transformation that reads from datasets and writes to datasets. It has a unique name that identifies it across runs.\\n- A run identifies an individual execution of a job. It might be an incremental or full batch process. It could also be a streaming job.\\n- A dataset could be a table in a warehouse or a folder in a blob store. It is consumed or written to by jobs.\\n\\n**Facets** are pieces of metadata that can be attached to those core entities. Facets have their own schema and capture various aspects of those entities.\\n\\n## Facets are individual atomic specs\\nLike the core model, facets are defined by a `JSONSchema`. They are a self-contained definition of one aspect of a job, a dataset, or a run at the time the event happened. They make the model extensible. The notion of facets is powerful because it makes it easy to add more information to the model - you just define a new facet. There\u2019s a clear compatibility model when introducing a new facet, since fields that are defined at the same time are grouped together.\\n\\nFor example, there\u2019s a facet to capture the schema of a dataset. There\u2019s a facet to capture the version of a job in source control. There\u2019s a facet to capture the parameters of a run. Facets are optional and may not apply to every instance of an entity.\\n\\n## Facets enable specialization of models\\nThe core entities are fairly generic. A dataset might be a table in a warehouse or a topic in a Kafka broker. A job might be a SQL query or a machine learning training job.\\n\\nThis generic high level model of lineage can be specialized by adding facets for that particular type of entity. At-rest data might be versioned, enabling transactions at the run level. Streaming data might capture the offsets and partitions where a streaming job started reading. Datasets might have a schema like a warehouse table, or not (for example, in the case of a machine learning model).\\n\\nBy capturing a generic representation of lineage and allowing progressive specialization of those entities, this approach offers a lot of flexibility. \\n\\n## Facets allow expressing different point of views\\nThere can be divergent points of view on how to model a certain aspect of metadata. Facets allow these models to cohabitate in a common framework.\\n\\nOne example of this is capturing the physical plan of a query execution. Each data warehouse might have its own unique way of describing execution plans. It is very valuable to be able to capture both a precise (but maybe too specific) model as well as a generic (but possibly imprecise or lossy) representation. They can be captured as two different facets. This also gives us opportunities to define several competing models and use the resulting information to collaborate on a more unified and generic representation. This emergent modeling is actually extremely useful in an open source setting, and as a way to make incremental progress.\\n\\n## Custom facets make the model decentralized\\nMost importantly, the OpenLineage spec allows custom facets that are defined elsewhere, completely outside of the spec. This allows others to extend the spec as-needed without having to coordinate with anyone or ask any permission from a governing body. They can make their own opinionated definition of an aspect of metadata. All that is required is that they publish a `JSONSchema` that describes their facets, prefixed by a unique namespace. This lowers the barrier to experimentation and encourages incremental progress by making the experimentation of others visible. The facets that become broadly useful can eventually be represented in the core spec.\\n\\n# Contribute!\\nAs a community, we\u2019ve done our best to minimize friction when experimenting with or contributing to OpenLineage. We\u2019re looking forward to seeing you join us as we make data lineage transparent across the data ecosystem."},{"id":"/joining-lfai","metadata":{"permalink":"/blog/joining-lfai","source":"@site/blog/joining-lfai/index.mdx","title":"OpenLineage joins the LF AI & Data Foundation","description":"Becoming a LF AI & Data project ensures that OpenLineage can never belong to a company, or even a group of developers; it belongs to us all.","date":"2021-07-22T00:00:00.000Z","formattedDate":"July 22, 2021","tags":[],"readingTime":1.095,"hasTruncateMarker":true,"authors":[{"name":"Julien Le Dem","title":"OpenLineage Project Lead","url":"https://www.github.com/julienledem/","imageURL":"https://avatars.githubusercontent.com/u/367841?v=4","key":"Le Dem"}],"frontMatter":{"title":"OpenLineage joins the LF AI & Data Foundation","date":"2021-07-22T00:00:00.000Z","authors":["Le Dem"],"description":"Becoming a LF AI & Data project ensures that OpenLineage can never belong to a company, or even a group of developers; it belongs to us all."},"prevItem":{"title":"Extending OpenLineage with Facets","permalink":"/blog/extending-with-facets"},"nextItem":{"title":"Exploring Lineage History via the Marquez API","permalink":"/blog/explore-lineage-api"}},"content":"Becoming a LF AI & Data project ensures that OpenLineage can never belong to a company, or even a group of developers; it belongs to us all.\\n\\n\x3c!--truncate--\x3e\\n\\nI am pleased to share that the OpenLineage project is joining the [LF AI & Data foundation](https://lfaidata.foundation) as a Sandbox Project! This is an important step towards the development of an open ecosystem for lineage metadata collection.\\n\\nThe LF AI & Data Foundation provides a vendor-neutral governance structure that can help the project grow broad industry collaboration. Even more importantly, becoming a LF AI & Data project ensures that OpenLineage can never belong to a company, or even a group of developers; it belongs to us all. The license can\u2019t be changed to protect the business interests of a subset of the community. That\u2019s important, because in order to succeed we need a whole lot of software projects - open source and proprietary - to adopt this standard and allow their users to begin collecting lineage metadata.\\n\\nIn the [full announcement](https://lfaidata.foundation/blog/2021/07/22/openlineage-joins-lf-ai-data-as-new-sandbox-project/), Ibrahim Haddad, Executive Director of LF AI & Data, writes:\\n\\n> \u201cWe look forward to working with the OpenLineage project to grow the project\u2019s footprint in the ecosystem, expand its community of adopters and contributors, and to foster the creation of collaboration opportunities with our members and other related projects.\u201d"},{"id":"/explore-lineage-api","metadata":{"permalink":"/blog/explore-lineage-api","source":"@site/blog/explore-lineage-api/index.mdx","title":"Exploring Lineage History via the Marquez API","description":"Taking advantage of recent changes to the Marquez API, this post shows how to diagnose job failures and explore the impact of code changes on downstream dependents.","date":"2021-07-08T00:00:00.000Z","formattedDate":"July 8, 2021","tags":[],"readingTime":13.685,"hasTruncateMarker":true,"authors":[{"name":"Michael Collado","title":"OpenLineage Committer","url":"https://www.github.com/collado-mike","imageURL":"https://www.github.com/collado-mike.png","key":"Collado"}],"frontMatter":{"title":"Exploring Lineage History via the Marquez API","date":"2021-07-08T00:00:00.000Z","authors":["Collado"],"description":"Taking advantage of recent changes to the Marquez API, this post shows how to diagnose job failures and explore the impact of code changes on downstream dependents."},"prevItem":{"title":"OpenLineage joins the LF AI & Data Foundation","permalink":"/blog/joining-lfai"},"nextItem":{"title":"Backfilling Airflow DAGs using Marquez","permalink":"/blog/backfilling-airflow-dags-using-marquez"}},"content":"Taking advantage of recent changes to the Marquez API, this post shows how to diagnose job failures and explore the impact of code changes on downstream dependents.\\n\\n\x3c!--truncate--\x3e\\n\\nManaging a data pipeline means tracking changes. Sometimes changes to your code, sometimes changes to\\nsomebody else\u2019s schema, sometimes to the contents of the data itself. Sometimes you need to\\ntrace the root cause of a problem- somebody changed an int to a string and all the downstream consumers\\nbroke.\\n\\nSometimes you want to make a change and see how your consumers were affected- do all the jobs\\nrun significantly faster after you filter out &ldquo;unused&rdquo; records? Or did somebody [rely on those\\n&ldquo;unused&rdquo; records](https://www.hyrumslaw.com/) to be present in the data?\\n\\nDo the recommendation models perform better after you \\"improved\\" the data cleaning job upstream? Can\\nyou be certain it was your change that improved the performance?\\n\\nSometimes the data itself just looks wrong and you need a way to verify that nothing has broken. Why\\nwas there a huge drop in traffic to the food delivery site yesterday? Was there an outage you didn\'t\\nhear about? Competitors outbidding your ads? Or did the website developers simply stop logging some\\ncritical event, corrupting every table in your data warehouse?\\n\\nTypically, we think of data lineage in static terms-\\n\\n> Job A produces Dataset X, which is consumed by Job B which joins it with Dataset Y and produces\\nDataset Z, which is consumed by&#8230;\\n\\nIt\u2019s a map\\nthat we use to get our heads around the dependencies that exist between the datasets we use to make\\ngood decisions (how much inventory should I stock in the warehouse to ensure customers get timely\\ndeliveries?) or to make technical features our customers will love (how can I compile the perfect\\nroad trip playlist given this customer\'s listening history?).\\n\\nBut data lineage is much more than a static map of inputs and outputs. _Real time_ lineage and faceted\\nmetadata give us visibility into how the map changes over time and even allow us to look back in\\nhistory to see how changes in one part of the map cause ripples in other areas. Taking advantage of some\\nrecent changes to the Marquez API, we\u2019ll demonstrate how to diagnose job failures and how to explore\\nthe impact of code changes on downstream dependents.\\n\\n## Getting Started\\n\\nTo get started, we need a running instance of Marquez with a little bit of seed data. For these\\nexercises, we\'ll assume you have a terminal with the following programs installed\\n\\n* [docker](https://www.docker.com/products/docker-desktop)\\n* [git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)\\n* [curl](https://curl.se/download.html)\\n* [jq](https://stedolan.github.io/jq/download/)\\n* less (optional)\\n\\nDownload and install any dependencies you don\'t already have. You\'ll need the docker daemon running\\n(see the docs for your platform to get that started). Then check out the Marquez repository\\nfrom Github and run the docker image locally:\\n\\n```bash\\ngit clone https://github.com/MarquezProject/marquez\\ncd marquez\\n./docker/up.sh --seed\\n```\\n\\nThis script uses `docker-compose` to spin up a self-contained installation of Marquez, including a\\nlocal database container, web frontend, and service instance. Additionally, it populates a set of\\nsample data that\'s useful for exploring the API. You\'ll know when the seed job is done when you see\\nthe following line in the output logs\\n```\\nseed-marquez-with-metadata exited with code 0\\n```\\n\\nOnce the seed job is done, we can begin exploring the API.\\n\\n### The Jobs\\nIn a separate terminal window, type the following command\\n```bash\\ncurl \\"http://localhost:5000/api/v1/namespaces/food_delivery/jobs/\\" | jq | less\\n```\\n\\nThe output returned should look something like the following\\n```json\\n{\\n  \\"jobs\\": [\\n    {\\n      \\"id\\": {\\n        \\"namespace\\": \\"food_delivery\\",\\n        \\"name\\": \\"example.delivery_times_7_days\\"\\n      },\\n      \\"type\\": \\"BATCH\\",\\n      \\"name\\": \\"example.delivery_times_7_days\\",\\n      \\"createdAt\\": \\"2021-06-24T21:50:39.229759Z\\",\\n      \\"updatedAt\\": \\"2021-06-24T22:05:45.321952Z\\",\\n      \\"namespace\\": \\"food_delivery\\",\\n      \\"inputs\\": [\\n        {\\n          \\"namespace\\": \\"food_delivery\\",\\n          \\"name\\": \\"public.delivery_7_days\\"\\n        }\\n      ],\\n      \\"outputs\\": [],\\n      \\"location\\": \\"https://github.com/example/jobs/blob/2294bc15eb49071f38425dc927e48655530a2f2e/delivery_times_7_days.py\\",\\n      \\"context\\": {\\n        \\"sql\\": \\"INSERT INTO top_delivery_times (order_id, order_placed_on, order_dispatched_on, order_delivered_on, order_delivery_time,\\\\n    customer_email, restaurant_id, driver_id)\\\\n  SELECT order_id, order_placed_on, order_delivered_on, DATEDIFF(minute, order_placed_on, order_delivered_on) AS order_delivery_time,\\\\n    customer_email, restaurant_id, driver_id\\\\n    FROM delivery_7_days\\\\nGROUP BY restaurant_id\\\\nORDER BY order_delivery_time DESC\\\\n   LIMIT 1;\\"\\n      },\\n      \\"description\\": \\"Determine weekly top delivery times by restaurant.\\",\\n      \\"latestRun\\": {\\n        \\"id\\": \\"f4fada30-dfcc-400c-9391-2d7a506b9139\\",\\n        \\"createdAt\\": \\"2021-06-24T21:50:59.509739Z\\",\\n        \\"updatedAt\\": \\"2021-06-24T22:05:45.321952Z\\",\\n        \\"nominalStartTime\\": \\"2021-06-24T22:02:00Z\\",\\n        \\"nominalEndTime\\": \\"2021-06-24T22:05:00Z\\",\\n        \\"state\\": \\"FAILED\\",\\n        \\"startedAt\\": \\"2021-06-24T22:02:39.321952Z\\",\\n        \\"endedAt\\": \\"2021-06-24T22:05:45.321952Z\\",\\n        \\"durationMs\\": 186000,\\n        \\"args\\": {},\\n        \\"jobVersion\\": {\\n          \\"namespace\\": \\"food_delivery\\",\\n          \\"name\\": \\"example.delivery_times_7_days\\",\\n          \\"version\\": \\"e9eafa5b-e334-358d-a3b4-61c8d3de75f3\\"\\n        },\\n        \\"inputVersions\\": [\\n          {\\n            \\"namespace\\": \\"food_delivery\\",\\n            \\"name\\": \\"public.delivery_7_days\\",\\n            \\"version\\": \\"a40ec54f-b8e1-35f7-b868-58b27383b5ff\\"\\n          }\\n        ],\\n        \\"outputVersions\\": [],\\n        \\"context\\": {\\n          \\"sql\\": \\"INSERT INTO top_delivery_times (order_id, order_placed_on, order_dispatched_on, order_delivered_on, order_delivery_time,\\\\n    customer_email, restaurant_id, driver_id)\\\\n  SELECT order_id, order_placed_on, order_delivered_on, DATEDIFF(minute, order_placed_on, order_delivered_on) AS order_delivery_time,\\\\n    customer_email, restaurant_id, driver_id\\\\n    FROM delivery_7_days\\\\nGROUP BY restaurant_id\\\\nORDER BY order_delivery_time DESC\\\\n   LIMIT 1;\\"\\n        },\\n        \\"facets\\": {}\\n      },\\n      \\"facets\\": {}\\n    },\\n   ...\\n  ]\\n}\\n```\\nFor brevity, I only included a single job- in this case, a job called `example.delivery_times_7_days`\\nin the `food_delivery` namespace (which we specified in the curl command). Your output will include\\nmany more jobs.\\n\\nThere are a few things in the job output worth noting. The first is the id of the job:\\n\\n```json\\n      \\"id\\": {\\n        \\"namespace\\": \\"food_delivery\\",\\n        \\"name\\": \\"example.delivery_times_7_days\\"\\n      },\\n```\\nThere is no version information in the id, as this API refers to the unversioned job information. The\\njob itself is mutable, in the sense that each time you query the API, the content of the job may\\nchange as new versions are created.\\n\\nThe response includes the set of input and output datasets, as well as the current job source location:\\n```json\\n      \\"inputs\\": [\\n        {\\n          \\"namespace\\": \\"food_delivery\\",\\n          \\"name\\": \\"public.delivery_7_days\\"\\n        }\\n      ],\\n      \\"outputs\\": [],\\n      \\"location\\": \\"https://github.com/example/jobs/blob/2294bc15eb49071f38425dc927e48655530a2f2e/delivery_times_7_days.py\\",\\n```\\nIf a new version of the job is created, any or all of these fields can change.\\n\\n### The Job Run\\nThe next thing to notice is the `latestRun` field. This includes information about the latest Run\\nof this job:\\n```json\\n      \\"latestRun\\": {\\n        \\"id\\": \\"f4fada30-dfcc-400c-9391-2d7a506b9139\\",\\n        \\"createdAt\\": \\"2021-06-24T21:50:59.509739Z\\",\\n        \\"updatedAt\\": \\"2021-06-24T22:05:45.321952Z\\",\\n        \\"nominalStartTime\\": \\"2021-06-24T22:02:00Z\\",\\n        \\"nominalEndTime\\": \\"2021-06-24T22:05:00Z\\",\\n        \\"state\\": \\"FAILED\\",\\n        \\"startedAt\\": \\"2021-06-24T22:02:39.321952Z\\",\\n        \\"endedAt\\": \\"2021-06-24T22:05:45.321952Z\\",\\n        \\"durationMs\\": 186000,\\n        \\"args\\": {},\\n        \\"jobVersion\\": {\\n          \\"namespace\\": \\"food_delivery\\",\\n          \\"name\\": \\"example.delivery_times_7_days\\",\\n          \\"version\\": \\"e9eafa5b-e334-358d-a3b4-61c8d3de75f3\\"\\n        },\\n        \\"inputVersions\\": [\\n          {\\n            \\"namespace\\": \\"food_delivery\\",\\n            \\"name\\": \\"public.delivery_7_days\\",\\n            \\"version\\": \\"a40ec54f-b8e1-35f7-b868-58b27383b5ff\\"\\n          }\\n        ],\\n        \\"outputVersions\\": [],\\n        \\"context\\": {\\n          \\"sql\\": \\"INSERT INTO top_delivery_times (order_id, order_placed_on, order_dispatched_on, order_delivered_on, order_delivery_time,\\\\n    customer_email, restaurant_id, driver_id)\\\\n  SELECT order_id, order_placed_on, order_delivered_on, DATEDIFF(minute, order_placed_on, order_delivered_on) AS order_delivery_time,\\\\n    customer_email, restaurant_id, driver_id\\\\n    FROM delivery_7_days\\\\nGROUP BY restaurant_id\\\\nORDER BY order_delivery_time DESC\\\\n   LIMIT 1;\\"\\n        },\\n        \\"facets\\": {}\\n      },\\n```\\nHere, we see explicit version information in the `jobVersion`, the `inputVersions`, and the\\n`outputVersions` fields. This is included because every Run is tied to exactly one immutable\\nversion of a job and one immutable version of each input dataset and each output dataset (it\'s worth\\nnoting that a Run can be tied to one version of a dataset as its input and another version of the\\nsame dataset as its output- a SQL `MERGE` statement is one common use case supported by this).\\n\\nThe other important field to notice in the Run structure is the `state`\\n```json\\n        \\"state\\": \\"FAILED\\",\\n```\\nUh-oh. Looks like the last time this job ran, it failed.\\n\\n## Tracing Failures\\n\\nThe first question we have when diagnosing a failure is\\n\\n> Is this the first time it\'s failed? Or has it been broken a while?\\n\\nLet\'s use the API to find out. Checking previous runs is easily accomplished by hitting the job\'s `runs`\\nAPI. Job runs are returned in descending order by start time, so the latest runs should be at the top.\\nSince we only want to check whether (and which) previous runs failed, we can use the following command:\\n```bash\\ncurl \\"http://localhost:5000/api/v1/namespaces/food_delivery/jobs/example.delivery_times_7_days/runs\\" | \\\\\\n  jq \'.runs | map({\\"id\\": .id, \\"state\\": .state})\' | less\\n```\\n\\nI get the following output:\\n```json\\n[\\n  {\\n    \\"id\\": \\"cb436906-1c66-4ce4-b7ac-ceebfd1babf8\\",\\n    \\"state\\": \\"FAILED\\"\\n  },\\n  {\\n    \\"id\\": \\"34bd4d60-82a6-4cac-ad76-815e6d95a93c\\",\\n    \\"state\\": \\"COMPLETED\\"\\n  },\\n  {\\n    \\"id\\": \\"352c67c3-c8d7-4b3a-b7da-8532aa9b8335\\",\\n    \\"state\\": \\"COMPLETED\\"\\n  },\\n  {\\n    \\"id\\": \\"0c62b1cc-2e43-44d0-9443-0a1d9768fece\\",\\n    \\"state\\": \\"COMPLETED\\"\\n  },\\n  {\\n    \\"id\\": \\"5900de19-12f7-4a6e-8118-8e0792d98f65\\",\\n    \\"state\\": \\"COMPLETED\\"\\n  },\\n  ...\\n]\\n```\\nThis is an incomplete list of jobs, but it\'s obvious from this sampling that this is the first job failure\\nin the recent execution history. What we want to see now is what changed between the last successful\\nrun and this one. We\'ll need to grab the `id` fields of each of the runs we want to compare. The run\\nids in the seed data are randomly generated, so they\'ll be different if you\'re following along. Grab\\nthe run ids with the following shell commands:\\n```bash\\nFAILED_RUN_ID=$(curl \\"http://localhost:5000/api/v1/namespaces/food_delivery/jobs/example.delivery_times_7_days/runs\\" | jq -r \'.runs[0].id\')\\nSUCCESSFUL_RUN_ID=$(curl \\"http://localhost:5000/api/v1/namespaces/food_delivery/jobs/example.delivery_times_7_days/runs\\" | jq -r \'.runs[1].id\')\\n```\\nTo get a specific run, we call the `/jobs/runs` API. Since each Run ID is required to be unique, the\\nAPI doesn\'t require a namespace or a job name. We can get the failed job run with\\n```bash\\ncurl \\"http://localhost:5000/api/v1/jobs/runs/$FAILED_RUN_ID\\" | jq | less\\n```\\nThe output is the same as the `latestRun` field of the `JobVersions` API. Recall the output of that\\nAPI includes these three important fields: the `jobVersion`, the `inputVersions`\\nand the `outputVersions`.\\n```json\\n        \\"jobVersion\\": {\\n          \\"namespace\\": \\"food_delivery\\",\\n          \\"name\\": \\"example.delivery_times_7_days\\",\\n          \\"version\\": \\"e9eafa5b-e334-358d-a3b4-61c8d3de75f3\\"\\n        },\\n        \\"inputVersions\\": [\\n          {\\n            \\"namespace\\": \\"food_delivery\\",\\n            \\"name\\": \\"public.delivery_7_days\\",\\n            \\"version\\": \\"a40ec54f-b8e1-35f7-b868-58b27383b5ff\\"\\n          }\\n        ],\\n        \\"outputVersions\\": [],\\n```\\nThese fields give us what we need to trace the lineage of the specific job runs we want to compare.\\n\\n### Job Versions\\nThe first thing to look at is the `jobVersion`. Nearly 100% of the time, a job failure can be traced\\nto a code change. Let\'s compare the job version of the failed run with the job version of the successful\\none:\\n```bash\\ndiff <(curl -s \\"http://localhost:5000/api/v1/jobs/runs/$FAILED_RUN_ID\\" | jq -r \'.jobVersion.version\') \\\\\\n     <(curl -s \\"http://localhost:5000/api/v1/jobs/runs/$SUCCESSFUL_RUN_ID\\" | jq -r \'.jobVersion.version\')\\n1c1\\n< e9eafa5b-e334-358d-a3b4-61c8d3de75f3\\n---\\n> 92d801c0-021e-3c3d-ba18-c9e8504b143d\\n```\\nRight away, we see there is a difference. A number of factors contribute to the job versioning logic\\nin Marquez:\\n* The source code location\\n* The job context\\n* The list of input datasets\\n* The list of output datasets\\n\\nThe version generation code is a deterministic function of these four inputs, so if any of them change,\\nthe version will change. Let\'s find out what changed between the two job versions. To do the diff,\\nwe ought to get rid of anything we expect to differ ahead of time: the `version`, the `createdAt`\\nand `updatedAt` timestamps, and the `latestRun`. The `version` field is also nested within the job\\nversion\'s `id` field, so we\'ll omit that too.\\n```bash\\nFAILED_JOB_VERSION=$(curl -s \\"http://localhost:5000/api/v1/jobs/runs/$FAILED_RUN_ID\\" | jq -r \'.jobVersion.version\')\\nSUCCESSFUL_JOB_VERSION=$(curl -s \\"http://localhost:5000/api/v1/jobs/runs/$SUCCESSFUL_RUN_ID\\" | jq -r \'.jobVersion.version\')\\n\\ndiff <(curl -s \\"http://localhost:5000/api/v1/namespaces/food_delivery/jobs/example.delivery_times_7_days/versions/$FAILED_JOB_VERSION\\" | \\\\\\n      jq \'del(.[\\"id\\", \\"version\\", \\"createdAt\\", \\"updatedAt\\", \\"latestRun\\"])\') \\\\\\n     <(curl -s \\"http://localhost:5000/api/v1/namespaces/food_delivery/jobs/example.delivery_times_7_days/versions/$SUCCESSFUL_JOB_VERSION\\" | \\\\\\n      jq \'del(.[\\"id\\", \\"version\\", \\"createdAt\\", \\"updatedAt\\", \\"latestRun\\"])\')\\n14c14,23\\n<   \\"outputs\\": []\\n---\\n>   \\"outputs\\": [\\n>     {\\n>       \\"namespace\\": \\"food_delivery\\",\\n>       \\"name\\": \\"public.top_delivery_times\\"\\n>     },\\n>     {\\n>       \\"namespace\\": \\"food_delivery\\",\\n>       \\"name\\": \\"public.discounts\\"\\n>     }\\n>   ]\\n```\\nOh, interesting! The two job versions only differ because of the output datasets. This is an\\ninteresting point that should be addressed in the Marquez API- the version generation is constructed\\nwhen the run completes, _even if the job run failed_. Sometimes this has no impact on the versioning,\\nas the output datasets can be determined before the job run executes. But sometimes we see impacts\\nlike this where a job run failed before we had a chance to discover the output datasets.\\n\\n## Tracing Upstream Lineage\\n\\nSo what gives? The job code didn\'t actually change! So what caused the failure?\\n\\nHere\'s where the lineage tracking becomes useful. Recall again, the run output gave us\\n3 interesting fields: the `jobVersion`, the `inputVersions`, and the `outputVersions`.\\nWe already know that the `outputVersions` is empty because the latest failed run didn\'t have\\na chance to determine the outputs. But we can take a look at the input datasets.\\n\\n### Dataset Versions\\n```bash\\ndiff <(curl -s \\"http://localhost:5000/api/v1/jobs/runs/$FAILED_RUN_ID\\" | jq -r \'.inputVersions\') \\\\\\n     <(curl -s \\"http://localhost:5000/api/v1/jobs/runs/$SUCCESSFUL_RUN_ID\\" | jq -r \'.inputVersions\')\\n5c5\\n<     \\"version\\": \\"a40ec54f-b8e1-35f7-b868-58b27383b5ff\\"\\n---\\n>     \\"version\\": \\"5e439f1f-1a44-3700-961f-60c79c75a1ec\\"\\n```\\n\\nDataset versions work differently from job versions. They don\'t only change when the structure changes.\\nEvery time a job run _modifies or writes to_ a dataset, the dataset version changes. Unless a job schedule is more\\nfrequent than its upstream job\'s schedule (e.g., an hourly job consuming a daily generated dataset),\\nit is expected that each job run consumes a different version of a dataset. To find out if there is\\na significant difference, we have to compare the two versions with the dataset\'s `versions` API.\\n\\nWe know there\'s only a single input dataset, so we\'ll keep this simple, but you could also write a loop to\\ncheck multiple input datasets if needed.\\n\\nIn this post, we omit the structure of the `datasetVersion`, but you can explore it yourself with the following:\\n```bash\\nFAILED_DATASET_VERSION=$(curl -s \\"http://localhost:5000/api/v1/jobs/runs/$FAILED_RUN_ID\\" | jq -r \'.inputVersions[0].version\')\\ncurl -s \\"http://localhost:5000/api/v1/namespaces/food_delivery/datasets/public.delivery_7_days/versions/$FAILED_DATASET_VERSION\\" | jq | less\\n```\\n\\nAs with the job versions, we\'ll omit some of the data we expect to be different in order to produce\\na useful diff:\\n\\n```bash\\nFAILED_DATASET_VERSION=$(curl -s \\"http://localhost:5000/api/v1/jobs/runs/$FAILED_RUN_ID\\" | jq -r \'.inputVersions[0].version\')\\nSUCCESSFUL_DATASET_VERSION=$(curl -s \\"http://localhost:5000/api/v1/jobs/runs/$SUCCESSFUL_RUN_ID\\" | jq -r \'.inputVersions[0].version\')\\n\\ndiff <(curl -s \\"http://localhost:5000/api/v1/namespaces/food_delivery/datasets/public.delivery_7_days/versions/$FAILED_DATASET_VERSION\\" | \\\\\\n      jq \'del(.[\\"id\\", \\"version\\", \\"createdAt\\", \\"createdByRun\\"])\') \\\\\\n     <(curl -s \\"http://localhost:5000/api/v1/namespaces/food_delivery/datasets/public.delivery_7_days/versions/$SUCCESSFUL_DATASET_VERSION\\" | \\\\\\n      jq \'del(.[\\"id\\", \\"version\\", \\"createdAt\\", \\"createdByRun\\"])\')\\n58c58\\n<       \\"type\\": \\"VARCHAR\\",\\n---\\n>       \\"type\\": \\"INTEGER\\",\\n```\\n\\nHey! Somehow one of the fields was converted from a an `INT` to a `VARCHAR`! One of the helpful fields\\nin the `version` API is the `createdByRun`, which is similar to the `jobVersion`\'s `latestRun`.\\nIt provides the job run that last altered the dataset, creating the new version.\\n\\nWe can quickly compare the job versions of the runs that created these two dataset versions:\\n\\n```bash\\ndiff <(curl -s \\"http://localhost:5000/api/v1/namespaces/food_delivery/datasets/public.delivery_7_days/versions/$FAILED_DATASET_VERSION\\" | \\\\\\n      jq \'.createdByRun.jobVersion\') \\\\\\n    <(curl -s \\"http://localhost:5000/api/v1/namespaces/food_delivery/datasets/public.delivery_7_days/versions/$SUCCESSFUL_DATASET_VERSION\\" | \\\\\\n      jq \'.createdByRun.jobVersion\')\\n4c4\\n<   \\"version\\": \\"c222a72e-92cc-3bb6-b3b7-c174cbc76387\\"\\n---\\n>   \\"version\\": \\"76c375bf-58ac-3d19-b94f-424fe2784601\\"\\n```\\n\\nAnd we can do a quick comparison of the two job versions. Since the job name is different,\\nwe\'ll let jq generate the endpoints for us\\n\\n```bash\\ndiff <(curl -s $(curl -s \\"http://localhost:5000/api/v1/namespaces/food_delivery/datasets/public.delivery_7_days/versions/$FAILED_DATASET_VERSION\\" | \\\\\\n      jq -r \'.createdByRun.jobVersion | \\"http://localhost:5000/api/v1/namespaces/\\" + .namespace + \\"/jobs/\\" + .name + \\"/versions/\\" + .version\') | \\\\\\n      jq \'del(.[\\"id\\", \\"version\\", \\"createdAt\\", \\"updatedAt\\", \\"latestRun\\"])\') \\\\\\n    <(curl -s $(curl -s \\"http://localhost:5000/api/v1/namespaces/food_delivery/datasets/public.delivery_7_days/versions/$SUCCESSFUL_DATASET_VERSION\\" | \\\\\\n      jq -r \'.createdByRun.jobVersion | \\"http://localhost:5000/api/v1/namespaces/\\" + .namespace + \\"/jobs/\\" + .name + \\"/versions/\\" + .version\') | \\\\\\n      jq \'del(.[\\"id\\", \\"version\\", \\"createdAt\\", \\"updatedAt\\", \\"latestRun\\"])\')\\n4c4\\n<   \\"location\\": \\"https://github.com/example/jobs/blob/c87f2a40553cfa4ae7178083a068bf1d0c6ca3a8/etl_delivery_7_days.py\\",\\n---\\n>   \\"location\\": \\"https://github.com/example/jobs/blob/4d0b5d374261fdaf60a1fc588dd8f0d124b0e87f/etl_delivery_7_days.py\\",\\n```\\nAnd there it is. Because nearly 100% of the time, a job failure can be traced to a code change. In\\nthis example, the job immediately upstream decided to change the output schema of its dataset. In\\nreality, it\'s not always so straightforward. Sometimes the upstream job is just a passthrough- maybe\\nit applies some filters to a subset of the columns and writes out whatever schema it\'s given.\\nIn that case, the job immediately upstream would have succeeded without a change in the job version.\\nOr the code change in the upstream job could be innocuous. Maybe someone added a comment or fixed an\\nunrelated bug. We might do some follow up and discover we have to continue our search upstream.\\n\\nBut the Marquez API actually gives us that ability. Using the `/lineage` API, we can even explore the\\ndownsteam impact of changes. So if you owned the `etl_delivery_7_days` job and wanted to see what the\\nimpact of changing the varchar to an int was on running jobs, the following jq recursive script\\nwill let you walk the downstream jobs and show the state of the last run:\\n```bash\\n# For readability, the jq filter is in a file broken into multiple lines\\ncat recurse.jq\\n  .graph as $graph | .graph[]\\n  | select(.id == \\"job:food_delivery:example.etl_delivery_7_days\\")\\n  | recurse(.outEdges[] | .destination as $nodeId | $graph[] | select(.id == $nodeId))\\n  | select(.type == \\"JOB\\")\\n  | {\\"id\\": .id, \\"state\\": .data.latestRun.state}\\n\\ncurl -s \\"http://localhost:5000/api/v1-beta/lineage?nodeId=job:food_delivery:example.etl_delivery_7_days\\" | jq -f recurse.jq less\\n{\\n  \\"id\\": \\"job:food_delivery:example.etl_delivery_7_days\\",\\n  \\"state\\": \\"COMPLETED\\"\\n}\\n{\\n  \\"id\\": \\"job:food_delivery:example.delivery_times_7_days\\",\\n  \\"state\\": \\"FAILED\\"\\n}\\n```\\nIn this post, we did everything manually with bash (because the shell is your most powerful tool when\\ndebugging a live outage you\'ve never encountered before; and let\'s be honest- how many outages _aren\'t_\\nsomething you\'ve never encountered before), but this could easily have been done in Java or Go or Python.\\nThe [openapi spec](https://github.com/MarquezProject/marquez/blob/main/spec/openapi.yml)\\nin the Marquez repo can be used to generate a client in whatever language you want to write your ops\\ntool in. So build some tooling and help your next debugging session run a little more smoothly.\\n\\n> But wait! What about the times when the job isn\'t _failing_, but the data is wrong!\\n\\nAh, the data quality checks! This is where the extensibility of the OpenLineage model comes to our\\nrescue with a field in the responses that we completely glossed over\\n```json\\n      \\"facets\\": {}\\n```\\nBut I think that\'s a topic for another post."},{"id":"/backfilling-airflow-dags-using-marquez","metadata":{"permalink":"/blog/backfilling-airflow-dags-using-marquez","source":"@site/blog/backfilling-airflow-dags-using-marquez/index.mdx","title":"Backfilling Airflow DAGs using Marquez","description":"In this blog post, we\'ll discuss how lineage metadata can be used to automatically backfill DAGs with complex upstream and downstream dependencies.","date":"2021-06-30T00:00:00.000Z","formattedDate":"June 30, 2021","tags":[],"readingTime":7.785,"hasTruncateMarker":true,"authors":[{"name":"Willy Lulciuc","title":"Marquez Project Lead and OpenLineage Committer","url":"https://www.github.com/wslulciuc","imageURL":"https://www.github.com/wslulciuc.png","key":"Lulciuc"}],"frontMatter":{"title":"Backfilling Airflow DAGs using Marquez","date":"2021-06-30T00:00:00.000Z","authors":["Lulciuc"],"description":"In this blog post, we\'ll discuss how lineage metadata can be used to automatically backfill DAGs with complex upstream and downstream dependencies."},"prevItem":{"title":"Exploring Lineage History via the Marquez API","permalink":"/blog/explore-lineage-api"},"nextItem":{"title":"How OpenLineage takes inspiration from OpenTelemetry","permalink":"/blog/openlineage-takes-inspiration-from-opentelemetry"}},"content":"In this blog post, we\'ll discuss how lineage metadata can be used to automatically backfill DAGs with complex upstream and downstream dependencies.\\n\\n\x3c!--truncate--\x3e\\n\\nYou\'ve just deployed an Airflow [DAG](https://airflow.apache.org/docs/apache-airflow/stable/concepts/dags.html#dags) that calculates the total sum of weekly food orders. You were able to identify what input tables to query, the frequency in which your DAG would run, and made sure analysts knew the resulting output table to use in their weekly food order trends report. The DAG only needs to run once a week, and with the DAG managed and scheduled via [Airflow](https://airflow.apache.org), you feel confident that the aggregated food order data will be available every Sunday morning for the weekly report.\\n\\nAs a developer, you\'re monitoring your DAG for errors and after only a few DAG runs, you\'re alerted that your DAG suddenly started to fail! Before you begin troubleshooting the root cause of the DAG failure, you notify the analytics team that the food order data will be incorrect for the week. After viewing the DAG error logs and a few email exchanges, you eventually discover that an upstream DAG had failed to write food order data for certain daily table partitions. Now, backfilling the missing data can be a manual and tedious task. As you sip your morning coffee, you think to yourself, _there must be a better way_. Yes, there is and collecting DAG lineage metadata would be a great start.\\n\\nIn this blog post, we\'ll briefly introduce you to how backfills are handled in Airflow, then discuss how lineage metadata can be used to backfill DAGs with more complex upstream and downstream dependencies.\\n\\n## 1. Brief Intro to Backfilling Airflow DAGs\\n\\nAirflow supports [backfilling](https://airflow.apache.org/docs/apache-airflow/stable/dag-run.html#backfill) DAG runs for a historical time window given a _start_ and _end_ date. Let\'s say our `example.etl_orders_7_days` DAG started failing on `2021-06-06`, and we wanted to reprocess the daily table partitions for that week (assuming all partitions have been backfilled upstream). In order to run the backfill for `example.etl_orders_7_days`, using the Airflow [CLI](https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html), you open up a terminal and execute the following `backfill` [command](https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html#backfill):\\n\\n```\\n# Backfill weekly food orders\\n$ airflow dags backfill \\\\\\n    --start-date 2021-06-06 \\\\\\n    --end-date 2021-06-06 \\\\\\n    example.etl_orders_7_days\\n```\\n\\nNow, the backfill was fairly straightforward but they\'re not always trivial. That is, we still have the following open questions: \\n\\n* How quickly can data quality issues be identified and explored?\\n* What alerting rules should be in place to notify downstream DAGs of possible upstream processing issues or failures?\\n* What effects (if any) would upstream DAGs have on downstream DAGs if dataset consumption was delayed?\\n\\nNext, we\'ll demonstrate how lineage metadata managed with [Marquez](https://marquezproject.ai) can help answer some of these questions (and more!) by maintaining inter-DAG dependencies and cataloging historical runs of DAGs.\\n\\n## 2. Exploring Lineage Metadata using Marquez\\n\\n> **Note:** To seed the Marquez HTTP API server with the sample lineage metadata used in this blog post, see the [Write Sample Lineage Metadata to Marquez](https://marquezproject.github.io/marquez/quickstart.html#write-sample-lineage-metadata-to-marquez) section in Marquez\'s [quickstart](https://marquezproject.github.io/marquez/quickstart.html) guide.\\n\\n#### 2.1 COLLECT DAG LINEAGE METADATA\\n\\n![](./lineage-graph.png)\\n\\n> **Figure 1:** DAG lineage metadata.\\n\\n[Marquez](https://marquezproject.ai) is an open source metadata service for the collection, aggregation, and visualization of a data ecosystem\u2019s metadata. Marquez has integration support for Airflow with minimal configuration. Using the [`marquez-airflow`](https://github.com/MarquezProject/marquez/tree/main/integrations/airflow) library, DAG lineage metadata will be collected automatically during DAG execution using the [OpenLineage](https://openlineage.io) standard, then stored in Marquez\u2019s centralized data model. To learn more about how lineage metadata is stored and versioned in Marquez, see the [Data Model](https://marquezproject.github.io/marquez/quickstart.html#marquez-data-model) section in Marquez\'s [quickstart](https://marquezproject.github.io/marquez/quickstart.html) guide.\\n\\nThe Airflow integration gives us two important benefits:\\n\\n* **DAG Metadata:** Each DAG has a code version, inputs and outputs, run args, and run state transitions. Keeping a global historical log of DAG runs linked to code will quickly highlight upstream dependencies errors and minimize downstream impact.\\n* **Lineage Metadata:** Each DAG may have one or more upstream dependency. Keeping track of inter-DAG dependencies will allow for teams within an organization to safely depend on one another\u2019s datasets, while also understanding which DAGs will be impacted downstream of a DAG failure.\\n\\nIn this blog, we won\'t go into how to enable lineage metadata collection for Airflow DAGs. But, we encourage you to take a look at Marquez\'s Airflow [example](https://github.com/MarquezProject/marquez/tree/main/examples/airflow) to learn how to troubleshoot DAG failures using Marquez.\\n\\n#### 2.2 GET LINEAGE METADATA VIA REST API\\n\\nIn Marquez, each dataset and job has its own globally unique node ID that can be used to query the lineage graph. The [LineageAPI](https://marquezproject.github.io/marquez/openapi.html#tag/Lineage/paths/~1lineage/get) returns a set of **nodes** consisting of **edges**. An edge is **directed** and has a defined **origin** and **destination**. A lineage graph may contain the following node types: `dataset:<namespace>:<dataset>`, `job:<namespace>:<job>`.\\n\\nSo, let\'s start by querying the lineage graph for our `example.etl_orders_7_days` DAG using the node ID `job:food_delivery:example.etl_orders_7_days`. You\'ll notice in the returned lineage graph that the DAG _input_ datasets are `public.categories`, `public.orders`, and `public.menus` with `public.orders_7_days` as the _output_ dataset:\\n\\n##### REQUEST\\n\\n```\\n$ curl -X GET \\"http://localhost:5000/api/v1-beta/lineage?nodeId=job:food_delivery:example.etl_orders_7_days\\"\\n```\\n\\n##### RESPONSE\\n\\n`200 OK`\\n\\n```\\n{\\n  \\"graph\\": [{\\n    \\"id\\": \\"job:food_delivery:example.etl_orders_7_days\\",\\n    \\"type\\": \\"JOB\\",\\n    \\"data\\": {\\n      \\"type\\": \\"BATCH\\",\\n      \\"id\\": {\\n        \\"namespace\\": \\"food_delivery\\",\\n        \\"name\\": \\"example.etl_orders_7_days\\"\\n      },\\n      \\"name\\": \\"example.etl_orders_7_days\\",\\n      \\"createdAt\\": \\"2021-06-06T14:50:13.931946Z\\",\\n      \\"updatedAt\\": \\"2021-06-06T14:57:54.037399Z\\",\\n      \\"namespace\\": \\"food_delivery\\",\\n      \\"inputs\\": [\\n        {\\"namespace\\": \\"food_delivery\\", \\"name\\": \\"public.categories\\"},\\n        {\\"namespace\\": \\"food_delivery\\", \\"name\\": \\"public.menu_items\\"},\\n        {\\"namespace\\": \\"food_delivery\\", \\"name\\": \\"public.orders\\"},\\n        {\\"namespace\\": \\"food_delivery\\", \\"name\\": \\"public.menus\\"}\\n      ],\\n      \\"outputs\\": [\\n        {\\"namespace\\": \\"food_delivery\\", \\"name\\": \\"public.orders_7_days\\"}\\n      ],\\n      \\"location\\": \\"https://github.com/example/jobs/blob/2294bc15eb49071f38425dc927e48655530a2f2e/etl_orders_7_days.py\\",\\n        \\"context\\": {\\n          \\"sql\\": \\"INSERT INTO orders_7_days (order_id, placed_on, discount_id, menu_id, restaurant_id, menu_item_id, category_id)\\\\n  SELECT o.id AS order_id, o.placed_on, o.discount_id, m.id AS menu_id, m.restaurant_id, mi.id AS menu_item_id, c.id AS category_id\\\\n    FROM orders AS o\\\\n   INNER JOIN menu_items AS mi\\\\n      ON menu_items.id = o.menu_item_id\\\\n   INNER JOIN categories AS c\\\\n      ON c.id = mi.category_id\\\\n   INNER JOIN menu AS m\\\\n      ON m.id = c.menu_id\\\\n   WHERE o.placed_on >= NOW() - interval \'7 days\';\\"\\n        },\\n        \\"description\\": \\"Loads newly placed orders weekly.\\",\\n        \\"latestRun\\": {\\n          \\"id\\": \\"5c7f0dc4-d3c1-4f16-9ac3-dc86c5da37cc\\",\\n          \\"createdAt\\": \\"2021-06-06T14:50:36.853459Z\\",\\n          \\"updatedAt\\": \\"2021-06-06T14:57:54.037399Z\\",\\n          \\"nominalStartTime\\": \\"2021-06-06T14:54:00Z\\",\\n          \\"nominalEndTime\\": \\"2021-06-06T14:57:00Z\\",\\n          \\"state\\": \\"FAILED\\",\\n          \\"startedAt\\": \\"2021-06-06T14:54:14.037399Z\\",\\n          \\"endedAt\\": \\"2021-06-06T14:57:54.037399Z\\",\\n          \\"durationMs\\": 220000,\\n          \\"args\\": {},\\n          \\"location\\": \\"https://github.com/example/jobs/blob/2294bc15eb49071f38425dc927e48655530a2f2e/etl_orders_7_days.py\\",\\n          \\"context\\": {\\n            \\"sql\\": \\"INSERT INTO orders_7_days (order_id, placed_on, discount_id, menu_id, restaurant_id, menu_item_id, category_id)\\\\n  SELECT o.id AS order_id, o.placed_on, o.discount_id, m.id AS menu_id, m.restaurant_id, mi.id AS menu_item_id, c.id AS category_id\\\\n    FROM orders AS o\\\\n   INNER JOIN menu_items AS mi\\\\n      ON menu_items.id = o.menu_item_id\\\\n   INNER JOIN categories AS c\\\\n      ON c.id = mi.category_id\\\\n   INNER JOIN menu AS m\\\\n      ON m.id = c.menu_id\\\\n   WHERE o.placed_on >= NOW() - interval \'7 days\';\\"\\n          },\\n          \\"facets\\": {}\\n        }\\n      },\\n      \\"inEdges\\": [\\n        {\\"origin\\": \\"dataset:food_delivery:public.categories\\", \\"destination\\": \\"job:food_delivery:example.etl_orders_7_days\\"}, \\"destination\\": \\"job:food_delivery:example.etl_orders_7_days\\"},\\n        {\\"origin\\": \\"dataset:food_delivery:public.orders\\", \\"destination\\": \\"job:food_delivery:example.etl_orders_7_days\\"},\\n        {\\"origin\\": \\"dataset:food_delivery:public.menus\\", \\"destination\\": \\"job:food_delivery:example.etl_orders_7_days\\"}\\n      ],\\n      \\"outEdges\\": [\\n        {\\"origin\\": \\"job:food_delivery:example.etl_orders_7_days\\", \\"destination\\": \\"dataset:food_delivery:public.orders_7_days\\"}\\n      ]\\n    }\\n  }, ...]\\n}\\n```\\n\\n## 3. Using Lineage Metadata to Backfill Airflow DAGs\\n\\n#### 3.1 BACKFILLING\\n\\n![](./backfill.png)\\n\\n> **Figure 2:** Backfilled daily table partitions.\\n\\nTo run a backfill for `example.etl_orders_7_days` using the DAG lineage metadata stored in Marquez, we\'ll need to query the lineage graph for the upstream DAG where the error originated. Now, let\'s assume the `example.etl_orders` DAG upstream of `example.etl_orders_7_days` failed to write some of the daily table partitions needed for the weekly food order trends report (see **Figure 2**). To fix the weekly trends report, we\'ll first need to backfill the missing daily table partitions `public.orders_2021_06_04`, `public.orders_2021_06_05`, and `public.orders_2021_06_06`:\\n\\n```\\n# Backfill daily food orders\\n$ airflow dags backfill \\\\\\n    --start-date 2021-06-04 \\\\\\n    --end-date 2021-06-06 \\\\\\n    example.etl_orders\\n```\\n\\n![](./inter-dag-deps.png)\\n\\n> **Figure 3:** Airflow inter-DAG dependencies.\\n\\nThen, using the script `backfill.sh` defined below, we can easily backfill all DAGs downstream of `example.etl_orders`:\\n\\n##### `backfill.sh`\\n\\n> **Note:** Make sure you have [`jq`](https://stedolan.github.io/jq/download) installed before running `backfill.sh`. \\n\\n```bash\\n#!/bin/bash\\n#\\n# Backfill DAGs automatically using lineage metadata stored in Marquez.\\n#\\n# Usage: $ ./backfill.sh <start-date> <end-date> <dag-id>\\n\\nset -e\\n\\n# Backfills DAGs downstream of the given node ID, recursively.\\nbackfill_downstream_of() {\\n  node_id=\\"${1}\\"\\n  # Get out edges for node ID\\n  out_edges=($(echo $lineage_graph \\\\\\n    | jq -r --arg NODE_ID \\"${node_id}\\" \'.graph[] | select(.id==$NODE_ID) | .outEdges[].destination\'))\\n  for out_edge in \\"${out_edges[@]}\\"; do\\n    # Run backfill if out edge is a job node (i.e. <dataset> => <job>)\\n    if [[ \\"${out_edge}\\" = job:* ]]; then\\n      dag_id=\\"${out_edge##*:}\\"\\n      echo \\"backfilling ${dag_id}...\\"\\n      airflow backfill --start_date \\"${start_date}\\" --end_date \\"${start_date}\\" \\"${dag_id}\\"\\n    fi\\n    # Follow out edges downstream, recursively\\n    backfill_downstream_of \\"${out_edge}\\"\\n  done\\n}\\n\\nstart_date=\\"${1}\\"\\nend_date=\\"${2}\\"\\ndag_id=\\"${3}\\"\\n\\n# (1) Build job node ID (format: \'job:<namespace>:<job>\')\\nnode_id=\\"job:food_delivery:${dag_id}\\"\\n\\n# (2) Get lineage graph\\nlineage_graph=$(curl -s -X GET \\"http://localhost:5000/api/v1-beta/lineage?nodeId=${node_id}\\")\\n\\n# (3) Run backfill\\nbackfill_downstream_of \\"${node_id}\\"\\n```\\n\\nWhen you run the script `backfill.sh`, it will output all backfilled DAGs to the console:\\n\\n```\\n$ ./backfill.sh 2021-06-06 2021-06-06 example.etl_orders\\nbackfilling example.etl_orders_7_days...\\nbackfilling example.etl_delivery_7_days...\\nbackfilling example.delivery_times_7_days...\\n```\\n\\n## 4. Conclusion\\n\\nIn this blog post, we showed how easy it can be to automate backfilling DAGs downstream of a data quality issue using lineage metadata stored in Marquez. With only two steps, we were able to backfill missing daily table partitions, then automatically re-run failed DAGs downstream of the upstream DAG where the error originated. But, what measures can we put in place to detect low-quality data issues faster, therefore avoiding backfills altogether? Since Marquez collects DAG run metadata that can be viewed using the [Runs API](https://marquezproject.github.io/marquez/openapi.html#tag/Jobs/paths/~1jobs~1runs~1{id}/get), building automated processes that periodically check DAG run states and quickly notifying teams of upstream data quality issue (or missed SLAs) in a timely fashion is just one possible preventive measure.\\n\\nWe encourge you to explore Marquez\'s opinionated [Metadata API](https://marquezproject.github.io/marquez/openapi.html) and define your own automated process(es) for analyzing lineage metadata! If you need help or have any questions, you can always join our [Slack](http://bit.ly/MarquezSlack) channel or reach out to us on [Twitter](https://twitter.com/MarquezProject)."},{"id":"/openlineage-takes-inspiration-from-opentelemetry","metadata":{"permalink":"/blog/openlineage-takes-inspiration-from-opentelemetry","source":"@site/blog/openlineage-takes-inspiration-from-opentelemetry/index.mdx","title":"How OpenLineage takes inspiration from OpenTelemetry","description":"The data world and the service world have many similarities but also a few crucial differences.","date":"2021-06-20T00:00:00.000Z","formattedDate":"June 20, 2021","tags":[],"readingTime":7.26,"hasTruncateMarker":true,"authors":[{"name":"Julien Le Dem","title":"OpenLineage Project Lead","url":"https://www.github.com/julienledem/","imageURL":"https://avatars.githubusercontent.com/u/367841?v=4","key":"Le Dem"}],"frontMatter":{"title":"How OpenLineage takes inspiration from OpenTelemetry","date":"2021-06-20T00:00:00.000Z","authors":["Le Dem"],"description":"The data world and the service world have many similarities but also a few crucial differences."},"prevItem":{"title":"Backfilling Airflow DAGs using Marquez","permalink":"/blog/backfilling-airflow-dags-using-marquez"}},"content":"The data world and the service world have many similarities but also a few crucial differences. \\n\\n\x3c!--truncate--\x3e\\n\\n## Data vs Services\\n\\nThe data world and the service world have many similarities but also a few crucial differences. \\nLet\u2019s start by drawing the similarities:\\n- The contract for services is the API, in the data world the contract is the dataset schema. \\n- Properly tracked Data lineage is as powerful as distributed request tracing for services.\\n- Data Quality checks are the data pipelines equivalent of services\u2019 circuit breakers.\\n- The Data catalog is data\u2019s service discovery\\n- Data quality metrics are similar to service metrics and both can define SLOs.\\n\\n|      | **Data** | **Services** |\\n|:-----|:---------|:-------------|\\n| **Contract** | Dataset schema | Service API |\\n| **Tracking Dependencies** | Data lineage | Distributed traces |\\n| **Preventing cascading failures** | Data Quality checks | Circuit breakers |\\n| **Discovery** | Data catalog | Service Discovery |\\n| **SLOs** | Freshness, data quality | Availability, latency |\\n\\n## Observability in the Services world\\n\\nIn many ways, observability is a lot more mature in the services world than it is in the data world.\\nService telemetry data is usually described as traces, metrics and logs that allow us to observe how services behave and interact with each other. Recognizing how telemetry data is connected across service layers is key to understanding and mitigating complex distributed failures in today\u2019s environments.\\nOpenTelemetry is the standard that allows collection of telemetry data in a uniform, vendor-agnostic way across services and databases. For example, it enables the understanding of dependencies between microservices, facilitating investigation into how a single failure might impact services several layers removed.\\nThe creation of OpenTelemetry removed the need for every monitoring, tracing analysis and log indexing platform to create unique integrations to collect that information from the environment.\\n\\n## Observability in the Data world\\n\\nThe data world is organized in a slightly different manner. Services strive for high availability  and expose a contract to be requested from. Data pipelines consume datasets and produce datasets. They could be executed as batch processes or using streaming engines but their fundamental contract is to consume data from given inputs and produce data to given outputs. The contract is now the schema of the dataset and an expectation of a rate of update. \\n\\nIn this world observability cares about a few things:\\n- **Is the data being delivered?** We might be happy with data being delivered at an hourly or daily rate but we want to know if the job responsible for this is failing and won\u2019t be updating it at all. As a consequence all the datasets depending on this will also not be updated. Correlated SLA misses likes this must be identified to avoid many teams investigating the same problem independently.\\n- **Is the data being delivered on time?** Batch processing for example is relatively forgiving and can still deliver outputs according to a time SLA even when it failed and had to retry or was slower than usual because of disruptions in its environment. However critical data will need to be delivered according to pre-defined SLA. We want to be able to understand where in the data pipeline dependencies, a bottleneck caused a delay and resolve the issue.\\n- **Is the data correct?** Now the worst thing that can happen is not a data pipeline failing. This case is relatively easy to recover from. Once the issue is fixed and the pipeline restarted, it will typically catch up and get back to normal operation after the delay induced by the failure. The worst case scenario for a data engineer or data scientist, is the pipeline carrying through and producing bad data. This usually propagates to downstream consumers and all over the place. Recovering requires understanding that the data is incorrect (usually using a data quality library like Great Expectations or Deequ), identifying the upstream dataset where the problem originated, identifying the downstream datasets where the problem propagated, and restating all those datasets to the correct result.\\n- **Auditing what happened:** Another common need whether it\u2019s for compliance or governance reasons is being able to know if specific sensitive datasets are used according to a defined set of rules. This can be used to protect user privacy, comply with financial regulation, or ensure that key metrics are derived from trusted data sources.\\n\\nThe key common element in all those use cases is understanding dependencies through **data lineage**, just like services care about understanding dependencies through service traces.\\n\\n## Differences Between the Data world and the service world\\n\\nIf the data world has exactly the same needs for observability than the service world, there are key differences between the two that create the need for a different API to instrument data pipelines.\\n\\n### Overall dependency structure:\\n\\n- **Services** dependencies can vary at the request level. Different requests to the same service may trigger very different downstream requests to other services. Service logic may create very different dependency patterns depending on input, timing and context. Services depend on other services that can be persistent or stateless.\\n- **Data** pipelines tend to be expressed in terms of a transformation from a defined set of input datasets to one or several output datasets. Their input/output structure tends to be a lot more stable and not vary much from record to record in the dataset. It\u2019s effectively a bigraph: jobs consume datasets and datasets are produced by jobs.\\n\\n### Push vs Pull:\\n\\n - **Services** send or push requests to downstream services. Whether it\u2019s synchronous or asynchronous, they can add a traceid to their request that will be propagated downstream. An upstream request spawns one or more downstream requests in a tree structure.\\n- **Data** pipelines pull data from the datasets they consume from. They aggregate and join datasets together. The structure of dependencies is typically a Directed Acyclic Graph at the dataset level instead of a tree at the request level. This means that the granularity of updates does not match one to one in a lot of cases. The frequency of updates can be different from one pipeline to the next and does not neatly align with a trace flowing down the dependencies.\\n\\n### Granularity of data updates\\n\\n- **Services** treat one request at a time and tend to optimize for latency of the request being processed.\\n- **Data** pipelines consume entire datasets and tend to prioritize throughput over latency. The result output can be combining many records from various inputs. When a service request spawns multiple requests downstream a data pipeline tends to do the opposite at the record level while producing multiple derived datasets.\\n\\n## Parallels between OpenLineage and [OpenTelemetry](https://opentelemetry.io/docs/concepts/what-is-opentelemetry/)\\n\\n### An API first\\n\\nLike OpenTelemetry is an API to collect traces, logs and metrics, OpenLineage is first an API to collect lineage. It is agnostic to the backend collecting the data and aspires to be integrated in every data processing engine.\\nData lineage is the equivalent of traces for services. It keeps track of dependencies between datasets and data pipelines and how they change over time. \\n\\n### Broad language support\\n\\nLike OpenTelemetry, OpenLineage has broad language support through the definition of its API in the standard JSONSchema representation. It also has dedicated clients to simplify using its semantics in the languages most commonly used for data processing (Java and Python).\\n\\n### Backend agnostic\\n\\nLike OpenTelemetry, OpenLineage allows the implementation of multiple backends to consume lineage events for a variety of use cases. For example Marquez is an Open Source reference implementation that keeps track of all the changes in your environment and will help you understand what happened if something goes wrong.\\nOther metadata projects like Egeria, DataHub, Atlas or Amundsen can also benefit from OpenLineage. Egeria in particular is committed to support OpenLineage as a Metadata bus.\\nLike OpenTelemetry, anybody can consume and leverage OpenLineage events.\\n\\n### Integrates with popular frameworks and libraries\\n\\nLike OpenTelemetry, OpenLineage aspires to be integrated in every data processing tool in the ecosystem. It also provides integration with popular tools that are not integrated yet. For example today you can cover Apache Spark, BigQuery, Snowflake, Redshift, Apache Airflow and others.\\n\\n### OpenLineage specific capabilities\\n\\nIn addition to those, OpenLineage is extensible to cover various aspects of metadata that are specific to the data world. OpenLineage defines a notion of facet that lets attach well defined pieces of metadata to the OpenLineage entities (Jobs, Runs and Datasets). Facets can be either part of the core spec or be defined as custom facets by third parties. This flexible mechanism lets define independent specs for dataset schema, query profiles or data quality metrics for example. But this is a topic for another post."}]}')}}]);