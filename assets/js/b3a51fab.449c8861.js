"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[5882],{3905:(e,t,a)=>{a.d(t,{Zo:()=>c,kt:()=>m});var n=a(67294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var p=n.createContext({}),l=function(e){var t=n.useContext(p),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},c=function(e){var t=l(e.components);return n.createElement(p.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,r=e.originalType,p=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),u=l(a),m=i,h=u["".concat(p,".").concat(m)]||u[m]||d[m]||r;return a?n.createElement(h,o(o({ref:t},c),{},{components:a})):n.createElement(h,o({ref:t},c))}));function m(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=a.length,o=new Array(r);o[0]=u;var s={};for(var p in t)hasOwnProperty.call(t,p)&&(s[p]=t[p]);s.originalType=e,s.mdxType="string"==typeof e?e:i,o[1]=s;for(var l=2;l<r;l++)o[l]=a[l];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},50881:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>o,default:()=>d,frontMatter:()=>r,metadata:()=>s,toc:()=>l});var n=a(87462),i=(a(67294),a(3905));const r={sidebar_position:9,title:"Extending"},o=void 0,s={unversionedId:"integrations/spark/extending",id:"integrations/spark/extending",title:"Extending",description:"The Spark library is intended to support extension via custom implementations of a handful",source:"@site/docs/integrations/spark/extending.md",sourceDirName:"integrations/spark",slug:"/integrations/spark/extending",permalink:"/docs/integrations/spark/extending",draft:!1,editUrl:"https://github.com/OpenLineage/docs/tree/main/docs/integrations/spark/extending.md",tags:[],version:"current",sidebarPosition:9,frontMatter:{sidebar_position:9,title:"Extending"},sidebar:"tutorialSidebar",previous:{title:"Testing",permalink:"/docs/integrations/spark/testing"},next:{title:"Apache Airflow",permalink:"/docs/integrations/airflow/"}},p={},l=[{value:"API",id:"api",level:2},{value:"<code>OpenLineageEventHandlerFactory</code>",id:"openlineageeventhandlerfactory",level:3},{value:"<code>QueryPlanVisitor</code>",id:"queryplanvisitor",level:3},{value:"<code>InputDatasetBuilder</code> and <code>OutputDatasetBuilder</code>",id:"inputdatasetbuilder-and-outputdatasetbuilder",level:3},{value:"<code>CustomFacetBuilder</code>",id:"customfacetbuilder",level:3},{value:"Function Argument Types",id:"function-argument-types",level:3},{value:"Spark extensions&#39; built-in lineage extraction",id:"spark-extensions-built-in-lineage-extraction",level:2},{value:"Spark DataSource V2 API Extensions",id:"spark-datasource-v2-api-extensions",level:3}],c={toc:l};function d(e){let{components:t,...a}=e;return(0,i.kt)("wrapper",(0,n.Z)({},c,a,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"The Spark library is intended to support extension via custom implementations of a handful\nof interfaces. Nearly every extension interface extends or mimics Scala's ",(0,i.kt)("inlineCode",{parentName:"p"},"PartialFunction"),". The\n",(0,i.kt)("inlineCode",{parentName:"p"},"isDefinedAt(Object x)")," method determines whether a given input is a valid input to the function.\nA default implementation of ",(0,i.kt)("inlineCode",{parentName:"p"},"isDefinedAt(Object x)")," is provided, which checks the generic type\narguments of the concrete class, if concrete type arguments are given, and determines if the input\nargument matches the generic type. For example, the following class is automatically defined for an\ninput argument of type ",(0,i.kt)("inlineCode",{parentName:"p"},"MyDataset"),"."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"class MyDatasetDetector extends QueryPlanVisitor<MyDataset, OutputDataset> {\n}\n")),(0,i.kt)("h2",{id:"api"},"API"),(0,i.kt)("p",null,"The following APIs are still evolving and may change over time based on user feedback."),(0,i.kt)("h3",{id:"openlineageeventhandlerfactory"},(0,i.kt)("a",{parentName:"h3",href:"https://github.com/OpenLineage/OpenLineage/blob/main/integration/spark/shared/src/main/java/io/openlineage/spark/api/OpenLineageEventHandlerFactory.java"},(0,i.kt)("inlineCode",{parentName:"a"},"OpenLineageEventHandlerFactory"))),(0,i.kt)("p",null,"This interface defines the main entrypoint to the extension codebase. Custom implementations\nare registered by following Java's ",(0,i.kt)("a",{parentName:"p",href:"https://docs.oracle.com/javase/8/docs/api/java/util/ServiceLoader.html"},(0,i.kt)("inlineCode",{parentName:"a"},"ServiceLoader")," conventions"),".\nA file called ",(0,i.kt)("inlineCode",{parentName:"p"},"io.openlineage.spark.api.OpenLineageEventHandlerFactory")," must exist in the\napplication or jar's ",(0,i.kt)("inlineCode",{parentName:"p"},"META-INF/service")," directory. Each line of that file must be the fully\nqualified class name of a concrete implementation of ",(0,i.kt)("inlineCode",{parentName:"p"},"OpenLineageEventHandlerFactory"),". More than one\nimplementation can be present in a single file. This might be useful to separate extensions that\nare targeted toward different environments - e.g., one factory may contain Azure-specific extensions,\nwhile another factory may contain GCP extensions."),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"OpenLineageEventHandlerFactory")," interface makes heavy use of default methods. Implementations\nmay override any or all of the following methods"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-java"},"/**\n * Return a collection of QueryPlanVisitors that can generate InputDatasets from a LogicalPlan node\n */\nCollection<PartialFunction<LogicalPlan, List<InputDataset>>> createInputDatasetQueryPlanVisitors(OpenLineageContext context);\n\n/**\n * Return a collection of QueryPlanVisitors that can generate OutputDatasets from a LogicalPlan node\n */\nCollection<PartialFunction<LogicalPlan, List<OutputDataset>>> createOutputDatasetQueryPlanVisitors(OpenLineageContext context);\n\n/**\n * Return a collection of PartialFunctions that can generate InputDatasets from one of the\n * pre-defined Spark types accessible from SparkListenerEvents (see below)\n */\nCollection<PartialFunction<Object, List<InputDataset>>> createInputDatasetBuilder(OpenLineageContext context);\n\n/**\n * Return a collection of PartialFunctions that can generate OutputDatasets from one of the\n * pre-defined Spark types accessible from SparkListenerEvents (see below)\n */\nCollection<PartialFunction<Object, List<OutputDataset>>> createOutputDatasetBuilder(OpenLineageContext context);\n\n/**\n * Return a collection of CustomFacetBuilders that can generate InputDatasetFacets from one of the\n * pre-defined Spark types accessible from SparkListenerEvents (see below)\n */\nCollection<CustomFacetBuilder<?, ? extends InputDatasetFacet>> createInputDatasetFacetBuilders(OpenLineageContext context);\n\n/**\n * Return a collection of CustomFacetBuilders that can generate OutputDatasetFacets from one of the\n * pre-defined Spark types accessible from SparkListenerEvents (see below)\n */\nCollection<CustomFacetBuilder<?, ? extends OutputDatasetFacet>>createOutputDatasetFacetBuilders(OpenLineageContext context);\n\n/**\n * Return a collection of CustomFacetBuilders that can generate DatasetFacets from one of the\n * pre-defined Spark types accessible from SparkListenerEvents (see below)\n */\nCollection<CustomFacetBuilder<?, ? extends DatasetFacet>> createDatasetFacetBuilders(OpenLineageContext context);\n\n/**\n * Return a collection of CustomFacetBuilders that can generate RunFacets from one of the\n * pre-defined Spark types accessible from SparkListenerEvents (see below)\n */\nCollection<CustomFacetBuilder<?, ? extends RunFacet>> createRunFacetBuilders(OpenLineageContext context);\n\n/**\n * Return a collection of CustomFacetBuilders that can generate JobFacets from one of the\n * pre-defined Spark types accessible from SparkListenerEvents (see below)\n */\nCollection<CustomFacetBuilder<?, ? extends JobFacet>> createJobFacetBuilders(OpenLineageContext context);\n")),(0,i.kt)("p",null,"See the ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/OpenLineage/OpenLineage/blob/main/integration/spark/shared/src/main/java/io/openlineage/spark/api/OpenLineageEventHandlerFactory.java"},(0,i.kt)("inlineCode",{parentName:"a"},"OpenLineageEventHandlerFactory")," javadocs"),"\nfor specifics on each method."),(0,i.kt)("h3",{id:"queryplanvisitor"},(0,i.kt)("a",{parentName:"h3",href:"https://github.com/OpenLineage/OpenLineage/blob/main/integration/spark/shared/src/main/java/io/openlineage/spark/api/QueryPlanVisitor.java"},(0,i.kt)("inlineCode",{parentName:"a"},"QueryPlanVisitor"))),(0,i.kt)("p",null,"QueryPlanVisitors evaluate nodes of a Spark ",(0,i.kt)("inlineCode",{parentName:"p"},"LogicalPlan")," and attempt to generate ",(0,i.kt)("inlineCode",{parentName:"p"},"InputDataset"),"s or\n",(0,i.kt)("inlineCode",{parentName:"p"},"OutputDataset"),"s from the information found in the ",(0,i.kt)("inlineCode",{parentName:"p"},"LogicalPlan")," nodes. This is the most common\nabstraction present in the OpenLineage Spark library, and many examples can be found in the\n",(0,i.kt)("inlineCode",{parentName:"p"},"io.openlineage.spark.agent.lifecycle.plan")," package - examples include the\n",(0,i.kt)("a",{parentName:"p",href:"https://github.com/OpenLineage/OpenLineage/blob/main/integration/spark/shared/src/main/java/io/openlineage/spark/agent/lifecycle/plan/BigQueryNodeVisitor.java"},(0,i.kt)("inlineCode",{parentName:"a"},"BigQueryNodeVisitor")),",\nthe ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/OpenLineage/OpenLineage/blob/main/integration/spark/shared/src/main/java/io/openlineage/spark/agent/lifecycle/plan/KafkaRelationVisitor.java"},(0,i.kt)("inlineCode",{parentName:"a"},"KafkaRelationVisitor")),"\nand the ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/OpenLineage/OpenLineage/blob/main/integration/spark/shared/src/main/java/io/openlineage/spark/agent/lifecycle/plan/InsertIntoHiveTableVisitor.java"},(0,i.kt)("inlineCode",{parentName:"a"},"InsertIntoHiveTableVisitor")),"."),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"QueryPlanVisitor"),"s implement Scala's ",(0,i.kt)("inlineCode",{parentName:"p"},"PartialFunction")," interface and are tested against every node\nof a Spark query's optimized ",(0,i.kt)("inlineCode",{parentName:"p"},"LogicalPlan"),". Each invocation will expect either an ",(0,i.kt)("inlineCode",{parentName:"p"},"InputDataset"),"\nor an ",(0,i.kt)("inlineCode",{parentName:"p"},"OutputDataset"),". If a node can be either an ",(0,i.kt)("inlineCode",{parentName:"p"},"InputDataset")," or an ",(0,i.kt)("inlineCode",{parentName:"p"},"OutputDataset"),", the\nconstructor should accept a ",(0,i.kt)("inlineCode",{parentName:"p"},"DatasetFactory")," so that the correct dataset type is generated at\nruntime."),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"QueryPlanVisitor"),"s can attach facets to the Datasets created, e.g., ",(0,i.kt)("inlineCode",{parentName:"p"},"SchemaDatasetFacet")," and\n",(0,i.kt)("inlineCode",{parentName:"p"},"DatasourceDatasetFacet")," are typically attached to the dataset when it is created. Custom facets\ncan also be attached, though ",(0,i.kt)("inlineCode",{parentName:"p"},"CustomFacetBuilder"),"s ",(0,i.kt)("em",{parentName:"p"},"may")," override facets attached directly to the\ndataset."),(0,i.kt)("p",null,"Spark job's naming logic appends output dataset's identifier as job suffix. In order to provide a job suffix, a ",(0,i.kt)("inlineCode",{parentName:"p"},"QueryPlanVisitor"),"\nneeds to implement ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/OpenLineage/OpenLineage/blob/main/integration/spark/shared/src/main/java/io/openlineage/spark/api/JobNameSuffixProvider.java"},(0,i.kt)("inlineCode",{parentName:"a"},"JobNameSuffixProvider")),"\ninterface. Otherwise no suffix will be appended. Job suffix should contain human-readable name\nof the dataset so that consumers of OpenLineage events can correlate events with particular\nSpark actions within their code. The logic to extract dataset name should not depend on the existence\nof the dataset as in case of creating new dataset it may not exist at the moment of assigning job suffix.\nIn most cases, the suffix should contain spark catalog, database and table separated by ",(0,i.kt)("inlineCode",{parentName:"p"},".")," which shall be\nextracted from LogicalPlan nodes properties."),(0,i.kt)("h3",{id:"inputdatasetbuilder-and-outputdatasetbuilder"},(0,i.kt)("a",{parentName:"h3",href:"https://github.com/OpenLineage/OpenLineage/blob/main/integration/spark/shared/src/main/java/io/openlineage/spark/api/AbstractInputDatasetBuilder.java"},(0,i.kt)("inlineCode",{parentName:"a"},"InputDatasetBuilder"))," and ",(0,i.kt)("a",{parentName:"h3",href:"https://github.com/OpenLineage/OpenLineage/blob/main/integration/spark/shared/src/main/common/java/io/openlineage/spark/api/AbstractOutputDatasetBuilder.java"},(0,i.kt)("inlineCode",{parentName:"a"},"OutputDatasetBuilder"))),(0,i.kt)("p",null,"Similar to the ",(0,i.kt)("inlineCode",{parentName:"p"},"QueryPlanVisitor"),"s, ",(0,i.kt)("inlineCode",{parentName:"p"},"InputDatasetBuilder"),"s and ",(0,i.kt)("inlineCode",{parentName:"p"},"OutputDatasetBuilder"),"s are\n",(0,i.kt)("inlineCode",{parentName:"p"},"PartialFunction"),"s defined for a specific input (see below for the list of Spark listener events and\nscheduler objects that can be passed to a builder) that can generate either an ",(0,i.kt)("inlineCode",{parentName:"p"},"InputDataset")," or an\n",(0,i.kt)("inlineCode",{parentName:"p"},"OutputDataset"),". Though not strictly necessary, the abstract base classes\n",(0,i.kt)("a",{parentName:"p",href:"https://github.com/OpenLineage/OpenLineage/blob/main/integration/spark/shared/src/main/java/io/openlineage/spark/api/AbstractInputDatasetBuilder.java"},(0,i.kt)("inlineCode",{parentName:"a"},"AbstractInputDatasetBuilder")),"\nand ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/OpenLineage/OpenLineage/blob/main/integration/spark/shared/src/main/java/io/openlineage/spark/api/AbstractOutputDatasetBuilder.java"},(0,i.kt)("inlineCode",{parentName:"a"},"AbstractOutputDatasetBuilder")),"\nare available for builders to extend."),(0,i.kt)("p",null,"Spark job's naming logic appends output dataset's identifier as job suffix.\nIn order to provide a job suffix, a ",(0,i.kt)("inlineCode",{parentName:"p"},"OutputDatasetBuilder")," needs to implement ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/OpenLineage/OpenLineage/blob/main/integration/spark/shared/src/main/java/io/openlineage/spark/api/JobNameSuffixProvider.java"},(0,i.kt)("inlineCode",{parentName:"a"},"JobNameSuffixProvider")),"\ninterface. Otherwise no suffix will be appended. Job suffix should contain human-readable name\nof the dataset so that consumers of OpenLineage events can correlate events with particular\nSpark actions within their code. The logic to extract dataset name should not depend on the existence\nof the dataset as in case of creating new dataset it may not exist at the moment of assigning job suffix.\nIn most cases, the suffix should contain spark catalog, database and table separated by ",(0,i.kt)("inlineCode",{parentName:"p"},".")," which shall be\nextracted from LogicalPlan nodes properties."),(0,i.kt)("h3",{id:"customfacetbuilder"},(0,i.kt)("a",{parentName:"h3",href:"https://github.com/OpenLineage/OpenLineage/blob/main/integration/spark/shared/src/main/java/io/openlineage/spark/api/CustomFacetBuilder.java"},(0,i.kt)("inlineCode",{parentName:"a"},"CustomFacetBuilder"))),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"CustomFacetBuilders")," evaluate Spark event types and scheduler objects (see below) to construct custom\nfacets. ",(0,i.kt)("inlineCode",{parentName:"p"},"CustomFacetBuilders")," are used to create ",(0,i.kt)("inlineCode",{parentName:"p"},"InputDatsetFacet"),"s, ",(0,i.kt)("inlineCode",{parentName:"p"},"OutputDatsetFacet"),"s,\n",(0,i.kt)("inlineCode",{parentName:"p"},"DatsetFacet"),"s, ",(0,i.kt)("inlineCode",{parentName:"p"},"RunFacet"),"s, and ",(0,i.kt)("inlineCode",{parentName:"p"},"JobFacet"),"s. A few examples can be found in the\n",(0,i.kt)("a",{parentName:"p",href:"https://github.com/OpenLineage/OpenLineage/blob/main/integration/spark/shared/src/main/java/io/openlineage/spark/agent/facets/builder"},(0,i.kt)("inlineCode",{parentName:"a"},"io.openlineage.spark.agent.facets.builder")),"\npackage, including the ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/OpenLineage/OpenLineage/blob/main/integration/spark/shared/src/main/java/io/openlineage/spark/agent/facets/builder/ErrorFacetBuilder.java"},(0,i.kt)("inlineCode",{parentName:"a"},"ErrorFacetBuilder")),"\nand the ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/OpenLineage/OpenLineage/blob/main/integration/spark/shared/src/main/java/io/openlineage/spark/agent/facets/builder/LogicalPlanRunFacetBuilder.java"},(0,i.kt)("inlineCode",{parentName:"a"},"LogicalPlanRunFacetBuilder")),".\n",(0,i.kt)("inlineCode",{parentName:"p"},"CustomFacetBuilder"),"s are not ",(0,i.kt)("inlineCode",{parentName:"p"},"PartialFunction")," implementations, but do define the ",(0,i.kt)("inlineCode",{parentName:"p"},"isDefinedAt(Object)"),"\nmethod to determine whether a given input is valid for the function. They implement the ",(0,i.kt)("inlineCode",{parentName:"p"},"BiConsumer"),"\ninterface, accepting the valid input argument, and a ",(0,i.kt)("inlineCode",{parentName:"p"},"BiConsumer<String, Facet>")," consumer, which\naccepts the name and value of any custom facet that should be attached to the OpenLineage run.\nThere is no limit to the number of facets that can be reported by a given ",(0,i.kt)("inlineCode",{parentName:"p"},"CustomFacetBuilder"),".\nFacet names that conflict will overwrite previously reported facets if they are reported for the\nsame Spark event.\nThough not strictly necessary, the following abstract base classes are available for extension:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/OpenLineage/OpenLineage/blob/main/integration/spark/shared/src/main/java/io/openlineage/spark/api/AbstractJobFacetBuilder.java"},(0,i.kt)("inlineCode",{parentName:"a"},"AbstractJobFacetBuilder"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/OpenLineage/OpenLineage/blob/main/integration/spark/shared/src/main/java/io/openlineage/spark/api/AbstractRunFacetBuilder.java"},(0,i.kt)("inlineCode",{parentName:"a"},"AbstractRunFacetBuilder"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/OpenLineage/OpenLineage/blob/main/integration/spark/shared/src/main/java/io/openlineage/spark/api/AbstractInputDatasetFacetBuilder.java"},(0,i.kt)("inlineCode",{parentName:"a"},"AbstractInputDatasetFacetBuilder"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/OpenLineage/OpenLineage/blob/main/integration/spark/shared/src/main/java/io/openlineage/spark/api/AbstractOutputDatasetFacetBuilder.java"},(0,i.kt)("inlineCode",{parentName:"a"},"AbstractOutputDatasetFacetBuilder"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/OpenLineage/OpenLineage/blob/main/integration/spark/shared/src/main/java/io/openlineage/spark/api/AbstractDatasetFacetBuilder.java"},(0,i.kt)("inlineCode",{parentName:"a"},"AbstractDatasetFacetBuilder")))),(0,i.kt)("p",null,"Input/Output/Dataset facets returned are attached to ",(0,i.kt)("em",{parentName:"p"},"any")," Input/Output Dataset found for a given\nSpark event. Typically, a Spark job only has one ",(0,i.kt)("inlineCode",{parentName:"p"},"OutputDataset"),", so any ",(0,i.kt)("inlineCode",{parentName:"p"},"OutputDatasetFacet"),"\ngenerated will be attached to that ",(0,i.kt)("inlineCode",{parentName:"p"},"OutputDataset"),". However, Spark jobs often have multiple\n",(0,i.kt)("inlineCode",{parentName:"p"},"InputDataset"),"s. Typically, an ",(0,i.kt)("inlineCode",{parentName:"p"},"InputDataset")," is read within a single Spark ",(0,i.kt)("inlineCode",{parentName:"p"},"Stage"),", and any metrics\npertaining to that dataset may be present in the ",(0,i.kt)("inlineCode",{parentName:"p"},"StageInfo#taskMetrics()")," for that ",(0,i.kt)("inlineCode",{parentName:"p"},"Stage"),".\nAccumulators pertaining to a dataset should be reported in the task metrics for a stage so that the\n",(0,i.kt)("inlineCode",{parentName:"p"},"CustomFacetBuilder")," can match against the ",(0,i.kt)("inlineCode",{parentName:"p"},"StageInfo")," and retrieve the task metrics for that stage\nwhen generating the ",(0,i.kt)("inlineCode",{parentName:"p"},"InputDatasetFacet"),". Other facet information is often found by analyzing the\n",(0,i.kt)("inlineCode",{parentName:"p"},"RDD")," that reads the raw data for a dataset. ",(0,i.kt)("inlineCode",{parentName:"p"},"CustomFacetBuilder"),"s that generate these facets should\nbe defined for the specific subclass of ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD")," that is used to read the target dataset - e.g.,\n",(0,i.kt)("inlineCode",{parentName:"p"},"HadoopRDD"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"BigQueryRDD"),", or ",(0,i.kt)("inlineCode",{parentName:"p"},"JdbcRDD"),"."),(0,i.kt)("h3",{id:"function-argument-types"},"Function Argument Types"),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"CustomFacetBuilder"),"s and dataset builders can be defined for the following set of Spark listener\nevent types and scheduler types:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"org.apache.spark.scheduler.SparkListenerJobStart")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"org.apache.spark.scheduler.SparkListenerJobEnd")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"org.apache.spark.rdd.RDD")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"org.apache.spark.scheduler.Stage")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"org.apache.spark.scheduler.StageInfo")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"org.apache.spark.scheduler.ActiveJob"))),(0,i.kt)("p",null,"Note that ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD"),'s are "unwrapped" prior to being evaluated by builders, so there\'s no need to, e.g.,\ncheck a ',(0,i.kt)("inlineCode",{parentName:"p"},"MapPartitionsRDD"),"'s dependencies. The ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD")," for each ",(0,i.kt)("inlineCode",{parentName:"p"},"Stage")," can be evaluated when a\n",(0,i.kt)("inlineCode",{parentName:"p"},"org.apache.spark.scheduler.SparkListenerStageCompleted")," event occurs. When a\n",(0,i.kt)("inlineCode",{parentName:"p"},"org.apache.spark.scheduler.SparkListenerJobEnd")," event is encountered, the last ",(0,i.kt)("inlineCode",{parentName:"p"},"Stage")," for the\n",(0,i.kt)("inlineCode",{parentName:"p"},"ActiveJob")," can be evaluated."),(0,i.kt)("h2",{id:"spark-extensions-built-in-lineage-extraction"},"Spark extensions' built-in lineage extraction"),(0,i.kt)("p",null,"Spark ecosystem comes with a plenty of pluggable extensions like iceberg, delta or spark-bigquery-connector\nto name a few. Extensions modify logical plan of the job and inject its own classes from which lineage shall be\nextracted. This is adding extra complexity, as it makes ",(0,i.kt)("inlineCode",{parentName:"p"},"openlineage-spark")," codebase\ndependent on the extension packages. The complexity grows more when multiple versions\nof the same extension need to be supported."),(0,i.kt)("h3",{id:"spark-datasource-v2-api-extensions"},"Spark DataSource V2 API Extensions"),(0,i.kt)("p",null,"Some extensions rely on Spark DataSource V2 API and implement TableProvider, Table, ScanBuilder etc.\nthat are used within Spark to create ",(0,i.kt)("inlineCode",{parentName:"p"},"DataSourceV2Relation")," instances."),(0,i.kt)("p",null,"A logical plan node ",(0,i.kt)("inlineCode",{parentName:"p"},"DataSourceV2Relation")," contains ",(0,i.kt)("inlineCode",{parentName:"p"},"Table")," field with a properties map of type\n",(0,i.kt)("inlineCode",{parentName:"p"},"Map<String, String>"),". ",(0,i.kt)("inlineCode",{parentName:"p"},"openlineage-spark")," uses this map to extract dataset information for lineage\nevent from ",(0,i.kt)("inlineCode",{parentName:"p"},"DataSourceV2Relation"),". It is checking for the properties ",(0,i.kt)("inlineCode",{parentName:"p"},"openlineage.dataset.name")," and\n",(0,i.kt)("inlineCode",{parentName:"p"},"openlineage.dataset.namespace"),". If they are present, it uses them to identify a dataset. Please\nbe aware that namespace and name need to conform to ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/OpenLineage/OpenLineage/blob/main/spec/Naming.md"},"naming convention"),"."),(0,i.kt)("p",null,"Properties can be also used to pass any dataset facet. For example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'openlineage.dataset.facets.customFacet={"property1": "value1", "property2": "value2"}\n')),(0,i.kt)("p",null,"will enrich dataset with ",(0,i.kt)("inlineCode",{parentName:"p"},"customFacet"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-json"},'"inputs": [{\n    "name": "...",\n    "namespace": "...",\n    "facets": {\n        "customFacet": {\n            "property1": "value1",\n            "property2": "value2",\n            "_producer": "..."\n        },\n        "schema": { }\n}]\n')),(0,i.kt)("p",null,"The approach can be used for standard facets\nfrom OpenLineage spec as well. ",(0,i.kt)("inlineCode",{parentName:"p"},"schema")," does not need to be passed through the properties as\nit is derived within ",(0,i.kt)("inlineCode",{parentName:"p"},"openlineage-spark")," from ",(0,i.kt)("inlineCode",{parentName:"p"},"DataSourceV2Relation"),". Custom facets are automatically\nfilled with ",(0,i.kt)("inlineCode",{parentName:"p"},"_producer")," field."))}d.isMDXComponent=!0}}]);