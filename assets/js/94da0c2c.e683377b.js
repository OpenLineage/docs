"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8558],{3905:(e,a,t)=>{t.d(a,{Zo:()=>u,kt:()=>m});var n=t(67294);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function i(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function o(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?i(Object(t),!0).forEach((function(a){r(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function l(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var s=n.createContext({}),p=function(e){var a=n.useContext(s),t=a;return e&&(t="function"==typeof e?e(a):o(o({},a),e)),t},u=function(e){var a=p(e.components);return n.createElement(s.Provider,{value:a},e.children)},c={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},d=n.forwardRef((function(e,a){var t=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),d=p(t),m=r,k=d["".concat(s,".").concat(m)]||d[m]||c[m]||i;return t?n.createElement(k,o(o({ref:a},u),{},{components:t})):n.createElement(k,o({ref:a},u))}));function m(e,a){var t=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var i=t.length,o=new Array(i);o[0]=d;var l={};for(var s in a)hasOwnProperty.call(a,s)&&(l[s]=a[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,o[1]=l;for(var p=2;p<i;p++)o[p]=t[p];return n.createElement.apply(null,o)}return n.createElement.apply(null,t)}d.displayName="MDXCreateElement"},85162:(e,a,t)=>{t.d(a,{Z:()=>o});var n=t(67294),r=t(86010);const i="tabItem_Ymn6";function o(e){let{children:a,hidden:t,className:o}=e;return n.createElement("div",{role:"tabpanel",className:(0,r.Z)(i,o),hidden:t},a)}},74866:(e,a,t)=>{t.d(a,{Z:()=>_});var n=t(87462),r=t(67294),i=t(86010),o=t(76775),l=t(91980),s=t(67392),p=t(50012);function u(e){return function(e){return r.Children.map(e,(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:a}=e;return!!a&&"object"==typeof a&&"value"in a}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:a,label:t,attributes:n,default:r}}=e;return{value:a,label:t,attributes:n,default:r}}))}function c(e){const{values:a,children:t}=e;return(0,r.useMemo)((()=>{const e=a??u(t);return function(e){const a=(0,s.l)(e,((e,a)=>e.value===a.value));if(a.length>0)throw new Error(`Docusaurus error: Duplicate values "${a.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[a,t])}function d(e){let{value:a,tabValues:t}=e;return t.some((e=>e.value===a))}function m(e){let{queryString:a=!1,groupId:t}=e;const n=(0,o.k6)(),i=function(e){let{queryString:a=!1,groupId:t}=e;if("string"==typeof a)return a;if(!1===a)return null;if(!0===a&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:a,groupId:t});return[(0,l._X)(i),(0,r.useCallback)((e=>{if(!i)return;const a=new URLSearchParams(n.location.search);a.set(i,e),n.replace({...n.location,search:a.toString()})}),[i,n])]}function k(e){const{defaultValue:a,queryString:t=!1,groupId:n}=e,i=c(e),[o,l]=(0,r.useState)((()=>function(e){let{defaultValue:a,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(a){if(!d({value:a,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${a}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return a}const n=t.find((e=>e.default))??t[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:a,tabValues:i}))),[s,u]=m({queryString:t,groupId:n}),[k,h]=function(e){let{groupId:a}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(a),[n,i]=(0,p.Nk)(t);return[n,(0,r.useCallback)((e=>{t&&i.set(e)}),[t,i])]}({groupId:n}),E=(()=>{const e=s??k;return d({value:e,tabValues:i})?e:null})();(0,r.useLayoutEffect)((()=>{E&&l(E)}),[E]);return{selectedValue:o,selectValue:(0,r.useCallback)((e=>{if(!d({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);l(e),u(e),h(e)}),[u,h,i]),tabValues:i}}var h=t(12466),E=t(72389);const N="tabList__CuJ",f="tabItem_LNqP";function A(e){let{className:a,block:t,selectedValue:o,selectValue:l,tabValues:s}=e;const p=[],{blockElementScrollPositionUntilNextRender:u}=(0,h.o5)(),c=e=>{const a=e.currentTarget,t=p.indexOf(a),n=s[t].value;n!==o&&(u(a),l(n))},d=e=>{let a=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=p.indexOf(e.currentTarget)+1;a=p[t]??p[0];break}case"ArrowLeft":{const t=p.indexOf(e.currentTarget)-1;a=p[t]??p[p.length-1];break}}a?.focus()};return r.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.Z)("tabs",{"tabs--block":t},a)},s.map((e=>{let{value:a,label:t,attributes:l}=e;return r.createElement("li",(0,n.Z)({role:"tab",tabIndex:o===a?0:-1,"aria-selected":o===a,key:a,ref:e=>p.push(e),onKeyDown:d,onClick:c},l,{className:(0,i.Z)("tabs__item",f,l?.className,{"tabs__item--active":o===a})}),t??a)})))}function g(e){let{lazy:a,children:t,selectedValue:n}=e;const i=(Array.isArray(t)?t:[t]).filter(Boolean);if(a){const e=i.find((e=>e.props.value===n));return e?(0,r.cloneElement)(e,{className:"margin-top--md"}):null}return r.createElement("div",{className:"margin-top--md"},i.map(((e,a)=>(0,r.cloneElement)(e,{key:a,hidden:e.props.value!==n}))))}function b(e){const a=k(e);return r.createElement("div",{className:(0,i.Z)("tabs-container",N)},r.createElement(A,(0,n.Z)({},e,a)),r.createElement(g,(0,n.Z)({},e,a)))}function _(e){const a=(0,E.Z)();return r.createElement(b,(0,n.Z)({key:String(a)},e))}},78334:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>u,contentTitle:()=>s,default:()=>m,frontMatter:()=>l,metadata:()=>p,toc:()=>c});var n=t(87462),r=(t(67294),t(3905)),i=t(74866),o=t(85162);const l={sidebar_position:2,title:"Installation"},s=void 0,p={unversionedId:"integrations/spark/installation",id:"integrations/spark/installation",title:"Installation",description:"* Version 1.8.0 and earlier only supported Scala 2.12 variants of Apache Spark.",source:"@site/docs/integrations/spark/installation.md",sourceDirName:"integrations/spark",slug:"/integrations/spark/installation",permalink:"/docs/integrations/spark/installation",draft:!1,editUrl:"https://github.com/OpenLineage/docs/tree/main/docs/integrations/spark/installation.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,title:"Installation"},sidebar:"tutorialSidebar",previous:{title:"Scheduling from Airflow",permalink:"/docs/integrations/spark/configuration/airflow"},next:{title:"Quickstart with Jupyter",permalink:"/docs/integrations/spark/quickstart/quickstart_local"}},u={},c=[{value:"Bundle the package with your Apache Spark application project",id:"bundle-the-package-with-your-apache-spark-application-project",level:4},{value:"Place the JAR in your <code>${SPARK_HOME}/jars</code> directory",id:"place-the-jar-in-your-spark_homejars-directory",level:4},{value:"Use the <code>--jars</code> option with <code>spark-submit / spark-shell / pyspark</code>",id:"use-the---jars-option-with-spark-submit--spark-shell--pyspark",level:4},{value:"Use the <code>--packages</code> option with <code>spark-submit / spark-shell / pyspark</code>",id:"use-the---packages-option-with-spark-submit--spark-shell--pyspark",level:4}],d={toc:c};function m(e){let{components:a,...t}=e;return(0,r.kt)("wrapper",(0,n.Z)({},d,t,{components:a,mdxType:"MDXLayout"}),(0,r.kt)("admonition",{type:"warning"},(0,r.kt)("ul",{parentName:"admonition"},(0,r.kt)("li",{parentName:"ul"},"Version ",(0,r.kt)("inlineCode",{parentName:"li"},"1.8.0")," and earlier only supported Scala 2.12 variants of Apache Spark."),(0,r.kt)("li",{parentName:"ul"},"Version ",(0,r.kt)("inlineCode",{parentName:"li"},"1.9.0")," and later support both Scala 2.12 and 2.13 variants of Apache Spark.")),(0,r.kt)("p",{parentName:"admonition"},"The above necessitates a change in the artifact identifier for ",(0,r.kt)("inlineCode",{parentName:"p"},"io.openlineage:openlineage-spark"),".\nAfter version ",(0,r.kt)("inlineCode",{parentName:"p"},"1.8.0"),", the artifact identifier has been updated. For subsequent versions, utilize:\n",(0,r.kt)("inlineCode",{parentName:"p"},"io.openlineage:openlineage-spark_${SCALA_BINARY_VERSION}:${OPENLINEAGE_SPARK_VERSION}"),".")),(0,r.kt)("p",null,"To integrate OpenLineage Spark with your application, you can:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#bundle-the-package-with-your-apache-spark-application-project"},"Bundle the package with your Apache Spark application project"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#place-the-jar-in-your-spark_homejars-directory"},"Place the JAR in your ",(0,r.kt)("inlineCode",{parentName:"a"},"${SPARK_HOME}/jars")," directory")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#use-the---jars-option-with-spark-submit--spark-shell--pyspark"},"Use the ",(0,r.kt)("inlineCode",{parentName:"a"},"--jars")," option with ",(0,r.kt)("inlineCode",{parentName:"a"},"spark-submit / spark-shell / pyspark"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#use-the---packages-option-with-spark-submit--spark-shell--pyspark"},"Use the ",(0,r.kt)("inlineCode",{parentName:"a"},"--packages")," option with ",(0,r.kt)("inlineCode",{parentName:"a"},"spark-submit / spark-shell / pyspark")))),(0,r.kt)("h4",{id:"bundle-the-package-with-your-apache-spark-application-project"},"Bundle the package with your Apache Spark application project"),(0,r.kt)("admonition",{type:"info"},(0,r.kt)("p",{parentName:"admonition"},"This approach does not demonstrate how to configure the ",(0,r.kt)("inlineCode",{parentName:"p"},"OpenLineageSparkListener"),".\nPlease refer to the ",(0,r.kt)("a",{parentName:"p",href:"/docs/integrations/spark/configuration/usage"},"Configuration")," section.")),(0,r.kt)("p",null,"For Maven, add the following to your ",(0,r.kt)("inlineCode",{parentName:"p"},"pom.xml"),":"),(0,r.kt)(i.Z,{groupId:"spark",mdxType:"Tabs"},(0,r.kt)(o.Z,{value:"after-1.8.0",label:"After 1.8.0",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-xml"},"<dependency>\n  <groupId>io.openlineage</groupId>\n  <artifactId>openlineage-spark_${SCALA_BINARY_VERSION}</artifactId>\n  <version>${OPENLINEAGE_SPARK_VERSION}</version>\n</dependency>\n"))),(0,r.kt)(o.Z,{value:"1.8.0-and-earlier",label:"1.8.0 and earlier",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-xml"},"<dependency>\n  <groupId>io.openlineage</groupId>\n  <artifactId>openlineage-spark</artifactId>\n  <version>${OPENLINEAGE_SPARK_VERSION}</version>\n</dependency>\n")))),(0,r.kt)("p",null,"For Gradle, add this to your ",(0,r.kt)("inlineCode",{parentName:"p"},"build.gradle"),":"),(0,r.kt)(i.Z,{groupId:"spark",mdxType:"Tabs"},(0,r.kt)(o.Z,{value:"after-1.8.0",label:"After 1.8.0",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-groovy"},'implementation("io.openlineage:openlineage-spark_${SCALA_BINARY_VERSION}:${OPENLINEAGE_SPARK_VERSION}")\n'))),(0,r.kt)(o.Z,{value:"1.8.0-and-earlier",label:"1.8.0 and earlier",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-groovy"},'implementation("io.openlineage:openlineage-spark:${OPENLINEAGE_SPARK_VERSION}")\n')))),(0,r.kt)("h4",{id:"place-the-jar-in-your-spark_homejars-directory"},"Place the JAR in your ",(0,r.kt)("inlineCode",{parentName:"h4"},"${SPARK_HOME}/jars")," directory"),(0,r.kt)("admonition",{type:"info"},(0,r.kt)("p",{parentName:"admonition"},"This approach does not demonstrate how to configure the ",(0,r.kt)("inlineCode",{parentName:"p"},"OpenLineageSparkListener"),".\nPlease refer to the ",(0,r.kt)("a",{parentName:"p",href:"#configuration"},"Configuration")," section.")),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Download the JAR and its checksum from Maven Central."),(0,r.kt)("li",{parentName:"ol"},"Verify the JAR's integrity using the checksum."),(0,r.kt)("li",{parentName:"ol"},"Upon successful verification, move the JAR to ",(0,r.kt)("inlineCode",{parentName:"li"},"${SPARK_HOME}/jars"),".")),(0,r.kt)("p",null,"This script automates the download and verification process:"),(0,r.kt)(i.Z,{groupId:"spark",mdxType:"Tabs"},(0,r.kt)(o.Z,{value:"after-1.8.0",label:"After 1.8.0",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'#!/usr/bin/env bash\n\nif [ -z "$SPARK_HOME" ]; then\n    echo "SPARK_HOME is not set. Please define it as your Spark installation directory."\n    exit 1\nfi\n\nOPENLINEAGE_SPARK_VERSION=\'1.9.0\'  # Example version\nSCALA_BINARY_VERSION=\'2.13\'        # Example Scala version\nARTIFACT_ID="openlineage-spark_${SCALA_BINARY_VERSION}"\nJAR_NAME="${ARTIFACT_ID}-${OPENLINEAGE_SPARK_VERSION}.jar"\nCHECKSUM_NAME="${JAR_NAME}.sha512"\nBASE_URL="https://repo1.maven.org/maven2/io/openlineage/${ARTIFACT_ID}/${OPENLINEAGE_SPARK_VERSION}"\n\ncurl -O "${BASE_URL}/${JAR_NAME}"\ncurl -O "${BASE_URL}/${CHECKSUM_NAME}"\n\necho "$(cat ${CHECKSUM_NAME})  ${JAR_NAME}" | sha512sum -c\n\nif [ $? -eq 0 ]; then\n    mv "${JAR_NAME}" "${SPARK_HOME}/jars"\nelse\n    echo "Checksum verification failed."\n    exit 1\nfi\n'))),(0,r.kt)(o.Z,{value:"1.8.0-and-earlier",label:"1.8.0 and earlier",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'#!/usr/bin/env bash\n\nif [ -z "$SPARK_HOME" ]; then\n    echo "SPARK_HOME is not set. Please define it as your Spark installation directory."\n    exit 1\nfi\n\nOPENLINEAGE_SPARK_VERSION=\'1.8.0\'  # Example version\nARTIFACT_ID="openlineage-spark"\nJAR_NAME="${ARTIFACT_ID}-${OPENLINEAGE_SPARK_VERSION}.jar"\nCHECKSUM_NAME="${JAR_NAME}.sha512"\nBASE_URL="https://repo1.maven.org/maven2/io/openlineage/${ARTIFACT_ID}/${OPENLINEAGE_SPARK_VERSION}"\n\ncurl -O "${BASE_URL}/${JAR_NAME}"\ncurl -O "${BASE_URL}/${CHECKSUM_NAME}"\n\necho "$(cat ${CHECKSUM_NAME})  ${JAR_NAME}" | sha512sum -c\n\nif [ $? -eq 0 ]; then\n    mv "${JAR_NAME}" "${SPARK_HOME}/jars"\nelse\n    echo "Checksum verification failed."\n    exit 1\nfi\n')))),(0,r.kt)("h4",{id:"use-the---jars-option-with-spark-submit--spark-shell--pyspark"},"Use the ",(0,r.kt)("inlineCode",{parentName:"h4"},"--jars")," option with ",(0,r.kt)("inlineCode",{parentName:"h4"},"spark-submit / spark-shell / pyspark")),(0,r.kt)("admonition",{type:"info"},(0,r.kt)("p",{parentName:"admonition"},"This approach does not demonstrate how to configure the ",(0,r.kt)("inlineCode",{parentName:"p"},"OpenLineageSparkListener"),".\nPlease refer to the ",(0,r.kt)("a",{parentName:"p",href:"#configuration"},"Configuration")," section.")),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Download the JAR and its checksum from Maven Central."),(0,r.kt)("li",{parentName:"ol"},"Verify the JAR's integrity using the checksum."),(0,r.kt)("li",{parentName:"ol"},"Upon successful verification, submit a Spark application with the JAR using the ",(0,r.kt)("inlineCode",{parentName:"li"},"--jars")," option.")),(0,r.kt)("p",null,"This script demonstrate this process:"),(0,r.kt)(i.Z,{groupId:"spark",mdxType:"Tabs"},(0,r.kt)(o.Z,{value:"after-1.8.0",label:"After 1.8.0",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'#!/usr/bin/env bash\n\nOPENLINEAGE_SPARK_VERSION=\'1.9.0\'  # Example version\nSCALA_BINARY_VERSION=\'2.13\'        # Example Scala version\nARTIFACT_ID="openlineage-spark_${SCALA_BINARY_VERSION}"\nJAR_NAME="${ARTIFACT_ID}-${OPENLINEAGE_SPARK_VERSION}.jar"\nCHECKSUM_NAME="${JAR_NAME}.sha512"\nBASE_URL="https://repo1.maven.org/maven2/io/openlineage/${ARTIFACT_ID}/${OPENLINEAGE_SPARK_VERSION}"\n\ncurl -O "${BASE_URL}/${JAR_NAME}"\ncurl -O "${BASE_URL}/${CHECKSUM_NAME}"\n\necho "$(cat ${CHECKSUM_NAME})  ${JAR_NAME}" | sha512sum -c\n\nif [ $? -eq 0 ]; then\n    spark-submit --jars "path/to/${JAR_NAME}" \\\n      # ... other options\nelse\n    echo "Checksum verification failed."\n    exit 1\nfi\n'))),(0,r.kt)(o.Z,{value:"1.8.0-and-earlier",label:"1.8.0 and earlier",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'#!/usr/bin/env bash\n\nOPENLINEAGE_SPARK_VERSION=\'1.8.0\'  # Example version\nARTIFACT_ID="openlineage-spark"\nJAR_NAME="${ARTIFACT_ID}-${OPENLINEAGE_SPARK_VERSION}.jar"\nCHECKSUM_NAME="${JAR_NAME}.sha512"\nBASE_URL="https://repo1.maven.org/maven2/io/openlineage/${ARTIFACT_ID}/${OPENLINEAGE_SPARK_VERSION}"\n\ncurl -O "${BASE_URL}/${JAR_NAME}"\ncurl -O "${BASE_URL}/${CHECKSUM_NAME}"\n\necho "$(cat ${CHECKSUM_NAME})  ${JAR_NAME}" | sha512sum -c\n\nif [ $? -eq 0 ]; then\n    spark-submit --jars "path/to/${JAR_NAME}" \\\n      # ... other options\nelse\n    echo "Checksum verification failed."\n    exit 1\nfi\n')))),(0,r.kt)("h4",{id:"use-the---packages-option-with-spark-submit--spark-shell--pyspark"},"Use the ",(0,r.kt)("inlineCode",{parentName:"h4"},"--packages")," option with ",(0,r.kt)("inlineCode",{parentName:"h4"},"spark-submit / spark-shell / pyspark")),(0,r.kt)("admonition",{type:"info"},(0,r.kt)("p",{parentName:"admonition"},"This approach does not demonstrate how to configure the ",(0,r.kt)("inlineCode",{parentName:"p"},"OpenLineageSparkListener"),".\nPlease refer to the ",(0,r.kt)("a",{parentName:"p",href:"#configuration"},"Configuration")," section.")),(0,r.kt)("p",null,"Spark allows you to add packages at runtime using the ",(0,r.kt)("inlineCode",{parentName:"p"},"--packages")," option with ",(0,r.kt)("inlineCode",{parentName:"p"},"spark-submit"),". This\noption automatically downloads the package from Maven Central (or other configured repositories)\nduring runtime and adds it to the classpath of your Spark application."),(0,r.kt)(i.Z,{groupId:"spark",mdxType:"Tabs"},(0,r.kt)(o.Z,{value:"after-1.8.0",label:"After 1.8.0",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"OPENLINEAGE_SPARK_VERSION='1.9.0'  # Example version\nSCALA_BINARY_VERSION='2.13'        # Example Scala version\n\nspark-submit --packages \"io.openlineage:openlineage-spark_${SCALA_BINARY_VERSION}:${OPENLINEAGE_SPARK_VERSION}\" \\\n    # ... other options\n"))),(0,r.kt)(o.Z,{value:"1.8.0-and-earlier",label:"1.8.0 and earlier",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"OPENLINEAGE_SPARK_VERSION='1.8.0'  # Example version\n\nspark-submit --packages \"io.openlineage:openlineage-spark::${OPENLINEAGE_SPARK_VERSION}\" \\\n    # ... other options\n")))))}m.isMDXComponent=!0}}]);