"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8558],{3905:(e,a,n)=>{n.d(a,{Zo:()=>u,kt:()=>m});var t=n(67294);function r(e,a,n){return a in e?Object.defineProperty(e,a,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[a]=n,e}function i(e,a){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);a&&(t=t.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),n.push.apply(n,t)}return n}function o(e){for(var a=1;a<arguments.length;a++){var n=null!=arguments[a]?arguments[a]:{};a%2?i(Object(n),!0).forEach((function(a){r(e,a,n[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(n,a))}))}return e}function l(e,a){if(null==e)return{};var n,t,r=function(e,a){if(null==e)return{};var n,t,r={},i=Object.keys(e);for(t=0;t<i.length;t++)n=i[t],a.indexOf(n)>=0||(r[n]=e[n]);return r}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(t=0;t<i.length;t++)n=i[t],a.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=t.createContext({}),p=function(e){var a=t.useContext(s),n=a;return e&&(n="function"==typeof e?e(a):o(o({},a),e)),n},u=function(e){var a=p(e.components);return t.createElement(s.Provider,{value:a},e.children)},c={inlineCode:"code",wrapper:function(e){var a=e.children;return t.createElement(t.Fragment,{},a)}},d=t.forwardRef((function(e,a){var n=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),d=p(n),m=r,k=d["".concat(s,".").concat(m)]||d[m]||c[m]||i;return n?t.createElement(k,o(o({ref:a},u),{},{components:n})):t.createElement(k,o({ref:a},u))}));function m(e,a){var n=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var i=n.length,o=new Array(i);o[0]=d;var l={};for(var s in a)hasOwnProperty.call(a,s)&&(l[s]=a[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,o[1]=l;for(var p=2;p<i;p++)o[p]=n[p];return t.createElement.apply(null,o)}return t.createElement.apply(null,n)}d.displayName="MDXCreateElement"},85162:(e,a,n)=>{n.d(a,{Z:()=>o});var t=n(67294),r=n(86010);const i="tabItem_Ymn6";function o(e){let{children:a,hidden:n,className:o}=e;return t.createElement("div",{role:"tabpanel",className:(0,r.Z)(i,o),hidden:n},a)}},74866:(e,a,n)=>{n.d(a,{Z:()=>_});var t=n(87462),r=n(67294),i=n(86010),o=n(76775),l=n(91980),s=n(67392),p=n(50012);function u(e){return function(e){return r.Children.map(e,(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:a}=e;return!!a&&"object"==typeof a&&"value"in a}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:a,label:n,attributes:t,default:r}}=e;return{value:a,label:n,attributes:t,default:r}}))}function c(e){const{values:a,children:n}=e;return(0,r.useMemo)((()=>{const e=a??u(n);return function(e){const a=(0,s.l)(e,((e,a)=>e.value===a.value));if(a.length>0)throw new Error(`Docusaurus error: Duplicate values "${a.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[a,n])}function d(e){let{value:a,tabValues:n}=e;return n.some((e=>e.value===a))}function m(e){let{queryString:a=!1,groupId:n}=e;const t=(0,o.k6)(),i=function(e){let{queryString:a=!1,groupId:n}=e;if("string"==typeof a)return a;if(!1===a)return null;if(!0===a&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:a,groupId:n});return[(0,l._X)(i),(0,r.useCallback)((e=>{if(!i)return;const a=new URLSearchParams(t.location.search);a.set(i,e),t.replace({...t.location,search:a.toString()})}),[i,t])]}function k(e){const{defaultValue:a,queryString:n=!1,groupId:t}=e,i=c(e),[o,l]=(0,r.useState)((()=>function(e){let{defaultValue:a,tabValues:n}=e;if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(a){if(!d({value:a,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${a}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return a}const t=n.find((e=>e.default))??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:a,tabValues:i}))),[s,u]=m({queryString:n,groupId:t}),[k,h]=function(e){let{groupId:a}=e;const n=function(e){return e?`docusaurus.tab.${e}`:null}(a),[t,i]=(0,p.Nk)(n);return[t,(0,r.useCallback)((e=>{n&&i.set(e)}),[n,i])]}({groupId:t}),E=(()=>{const e=s??k;return d({value:e,tabValues:i})?e:null})();(0,r.useLayoutEffect)((()=>{E&&l(E)}),[E]);return{selectedValue:o,selectValue:(0,r.useCallback)((e=>{if(!d({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);l(e),u(e),h(e)}),[u,h,i]),tabValues:i}}var h=n(12466),E=n(72389);const N="tabList__CuJ",A="tabItem_LNqP";function f(e){let{className:a,block:n,selectedValue:o,selectValue:l,tabValues:s}=e;const p=[],{blockElementScrollPositionUntilNextRender:u}=(0,h.o5)(),c=e=>{const a=e.currentTarget,n=p.indexOf(a),t=s[n].value;t!==o&&(u(a),l(t))},d=e=>{let a=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const n=p.indexOf(e.currentTarget)+1;a=p[n]??p[0];break}case"ArrowLeft":{const n=p.indexOf(e.currentTarget)-1;a=p[n]??p[p.length-1];break}}a?.focus()};return r.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.Z)("tabs",{"tabs--block":n},a)},s.map((e=>{let{value:a,label:n,attributes:l}=e;return r.createElement("li",(0,t.Z)({role:"tab",tabIndex:o===a?0:-1,"aria-selected":o===a,key:a,ref:e=>p.push(e),onKeyDown:d,onClick:c},l,{className:(0,i.Z)("tabs__item",A,l?.className,{"tabs__item--active":o===a})}),n??a)})))}function g(e){let{lazy:a,children:n,selectedValue:t}=e;const i=(Array.isArray(n)?n:[n]).filter(Boolean);if(a){const e=i.find((e=>e.props.value===t));return e?(0,r.cloneElement)(e,{className:"margin-top--md"}):null}return r.createElement("div",{className:"margin-top--md"},i.map(((e,a)=>(0,r.cloneElement)(e,{key:a,hidden:e.props.value!==t}))))}function b(e){const a=k(e);return r.createElement("div",{className:(0,i.Z)("tabs-container",N)},r.createElement(f,(0,t.Z)({},e,a)),r.createElement(g,(0,t.Z)({},e,a)))}function _(e){const a=(0,E.Z)();return r.createElement(b,(0,t.Z)({key:String(a)},e))}},78334:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>u,contentTitle:()=>s,default:()=>m,frontMatter:()=>l,metadata:()=>p,toc:()=>c});var t=n(87462),r=(n(67294),n(3905)),i=n(74866),o=n(85162);const l={sidebar_position:2,title:"Installation"},s=void 0,p={unversionedId:"integrations/spark/installation",id:"integrations/spark/installation",title:"Installation",description:"* Version 1.8.0 and earlier only supported Scala 2.12 variants of Apache Spark.",source:"@site/docs/integrations/spark/installation.md",sourceDirName:"integrations/spark",slug:"/integrations/spark/installation",permalink:"/docs/integrations/spark/installation",draft:!1,editUrl:"https://github.com/OpenLineage/docs/tree/main/docs/integrations/spark/installation.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,title:"Installation"},sidebar:"tutorialSidebar",previous:{title:"Main Concepts",permalink:"/docs/integrations/spark/main_concept"},next:{title:"Usage",permalink:"/docs/integrations/spark/configuration/usage"}},u={},c=[{value:"Bundle the package with your Apache Spark application project",id:"bundle-the-package-with-your-apache-spark-application-project",level:4},{value:"Place the JAR in your <code>${SPARK_HOME}/jars</code> directory",id:"place-the-jar-in-your-spark_homejars-directory",level:4},{value:"Use the <code>--jars</code> option with <code>spark-submit / spark-shell / pyspark</code>",id:"use-the---jars-option-with-spark-submit--spark-shell--pyspark",level:4},{value:"Use the <code>--packages</code> option with <code>spark-submit / spark-shell / pyspark</code>",id:"use-the---packages-option-with-spark-submit--spark-shell--pyspark",level:4}],d={toc:c};function m(e){let{components:a,...n}=e;return(0,r.kt)("wrapper",(0,t.Z)({},d,n,{components:a,mdxType:"MDXLayout"}),(0,r.kt)("admonition",{type:"warning"},(0,r.kt)("ul",{parentName:"admonition"},(0,r.kt)("li",{parentName:"ul"},"Version ",(0,r.kt)("inlineCode",{parentName:"li"},"1.8.0")," and earlier only supported Scala 2.12 variants of Apache Spark."),(0,r.kt)("li",{parentName:"ul"},"Version ",(0,r.kt)("inlineCode",{parentName:"li"},"1.9.1")," and later support both Scala 2.12 and 2.13 variants of Apache Spark.")),(0,r.kt)("p",{parentName:"admonition"},"The above necessitates a change in the artifact identifier for ",(0,r.kt)("inlineCode",{parentName:"p"},"io.openlineage:openlineage-spark"),".\nAfter version ",(0,r.kt)("inlineCode",{parentName:"p"},"1.8.0"),", the artifact identifier has been updated. For subsequent versions, utilize:\n",(0,r.kt)("inlineCode",{parentName:"p"},"io.openlineage:openlineage-spark_${SCALA_BINARY_VERSION}:${OPENLINEAGE_SPARK_VERSION}"),".")),(0,r.kt)("p",null,"To integrate OpenLineage Spark with your application, you can:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#bundle-the-package-with-your-apache-spark-application-project"},"Bundle the package with your Apache Spark application project"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#place-the-jar-in-your-spark_homejars-directory"},"Place the JAR in your ",(0,r.kt)("inlineCode",{parentName:"a"},"${SPARK_HOME}/jars")," directory")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#use-the---jars-option-with-spark-submit--spark-shell--pyspark"},"Use the ",(0,r.kt)("inlineCode",{parentName:"a"},"--jars")," option with ",(0,r.kt)("inlineCode",{parentName:"a"},"spark-submit / spark-shell / pyspark"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"#use-the---packages-option-with-spark-submit--spark-shell--pyspark"},"Use the ",(0,r.kt)("inlineCode",{parentName:"a"},"--packages")," option with ",(0,r.kt)("inlineCode",{parentName:"a"},"spark-submit / spark-shell / pyspark")))),(0,r.kt)("h4",{id:"bundle-the-package-with-your-apache-spark-application-project"},"Bundle the package with your Apache Spark application project"),(0,r.kt)("admonition",{type:"info"},(0,r.kt)("p",{parentName:"admonition"},"This approach does not demonstrate how to configure the ",(0,r.kt)("inlineCode",{parentName:"p"},"OpenLineageSparkListener"),".\nPlease refer to the ",(0,r.kt)("a",{parentName:"p",href:"/docs/integrations/spark/configuration/usage"},"Configuration")," section.")),(0,r.kt)("p",null,"For Maven, add the following to your ",(0,r.kt)("inlineCode",{parentName:"p"},"pom.xml"),":"),(0,r.kt)(i.Z,{groupId:"spark",mdxType:"Tabs"},(0,r.kt)(o.Z,{value:"after-1.8.0",label:"After 1.8.0",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-xml"},"<dependency>\n  <groupId>io.openlineage</groupId>\n  <artifactId>openlineage-spark_${SCALA_BINARY_VERSION}</artifactId>\n  <version>${OPENLINEAGE_SPARK_VERSION}</version>\n</dependency>\n"))),(0,r.kt)(o.Z,{value:"1.8.0-and-earlier",label:"1.8.0 and earlier",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-xml"},"<dependency>\n  <groupId>io.openlineage</groupId>\n  <artifactId>openlineage-spark</artifactId>\n  <version>${OPENLINEAGE_SPARK_VERSION}</version>\n</dependency>\n")))),(0,r.kt)("p",null,"For Gradle, add this to your ",(0,r.kt)("inlineCode",{parentName:"p"},"build.gradle"),":"),(0,r.kt)(i.Z,{groupId:"spark",mdxType:"Tabs"},(0,r.kt)(o.Z,{value:"after-1.8.0",label:"After 1.8.0",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-groovy"},'implementation("io.openlineage:openlineage-spark_${SCALA_BINARY_VERSION}:${OPENLINEAGE_SPARK_VERSION}")\n'))),(0,r.kt)(o.Z,{value:"1.8.0-and-earlier",label:"1.8.0 and earlier",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-groovy"},'implementation("io.openlineage:openlineage-spark:${OPENLINEAGE_SPARK_VERSION}")\n')))),(0,r.kt)("h4",{id:"place-the-jar-in-your-spark_homejars-directory"},"Place the JAR in your ",(0,r.kt)("inlineCode",{parentName:"h4"},"${SPARK_HOME}/jars")," directory"),(0,r.kt)("admonition",{type:"info"},(0,r.kt)("p",{parentName:"admonition"},"This approach does not demonstrate how to configure the ",(0,r.kt)("inlineCode",{parentName:"p"},"OpenLineageSparkListener"),".\nPlease refer to the ",(0,r.kt)("a",{parentName:"p",href:"#configuration"},"Configuration")," section.")),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Download the JAR and its checksum from Maven Central."),(0,r.kt)("li",{parentName:"ol"},"Verify the JAR's integrity using the checksum."),(0,r.kt)("li",{parentName:"ol"},"Upon successful verification, move the JAR to ",(0,r.kt)("inlineCode",{parentName:"li"},"${SPARK_HOME}/jars"),".")),(0,r.kt)("p",null,"This script automates the download and verification process:"),(0,r.kt)(i.Z,{groupId:"spark",mdxType:"Tabs"},(0,r.kt)(o.Z,{value:"after-1.8.0",label:"After 1.8.0",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'#!/usr/bin/env bash\n\nif [ -z "$SPARK_HOME" ]; then\n    echo "SPARK_HOME is not set. Please define it as your Spark installation directory."\n    exit 1\nfi\n\nOPENLINEAGE_SPARK_VERSION=\'1.9.0\'  # Example version\nSCALA_BINARY_VERSION=\'2.13\'        # Example Scala version\nARTIFACT_ID="openlineage-spark_${SCALA_BINARY_VERSION}"\nJAR_NAME="${ARTIFACT_ID}-${OPENLINEAGE_SPARK_VERSION}.jar"\nCHECKSUM_NAME="${JAR_NAME}.sha512"\nBASE_URL="https://repo1.maven.org/maven2/io/openlineage/${ARTIFACT_ID}/${OPENLINEAGE_SPARK_VERSION}"\n\ncurl -O "${BASE_URL}/${JAR_NAME}"\ncurl -O "${BASE_URL}/${CHECKSUM_NAME}"\n\necho "$(cat ${CHECKSUM_NAME})  ${JAR_NAME}" | sha512sum -c\n\nif [ $? -eq 0 ]; then\n    mv "${JAR_NAME}" "${SPARK_HOME}/jars"\nelse\n    echo "Checksum verification failed."\n    exit 1\nfi\n'))),(0,r.kt)(o.Z,{value:"1.8.0-and-earlier",label:"1.8.0 and earlier",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'#!/usr/bin/env bash\n\nif [ -z "$SPARK_HOME" ]; then\n    echo "SPARK_HOME is not set. Please define it as your Spark installation directory."\n    exit 1\nfi\n\nOPENLINEAGE_SPARK_VERSION=\'1.8.0\'  # Example version\nARTIFACT_ID="openlineage-spark"\nJAR_NAME="${ARTIFACT_ID}-${OPENLINEAGE_SPARK_VERSION}.jar"\nCHECKSUM_NAME="${JAR_NAME}.sha512"\nBASE_URL="https://repo1.maven.org/maven2/io/openlineage/${ARTIFACT_ID}/${OPENLINEAGE_SPARK_VERSION}"\n\ncurl -O "${BASE_URL}/${JAR_NAME}"\ncurl -O "${BASE_URL}/${CHECKSUM_NAME}"\n\necho "$(cat ${CHECKSUM_NAME})  ${JAR_NAME}" | sha512sum -c\n\nif [ $? -eq 0 ]; then\n    mv "${JAR_NAME}" "${SPARK_HOME}/jars"\nelse\n    echo "Checksum verification failed."\n    exit 1\nfi\n')))),(0,r.kt)("h4",{id:"use-the---jars-option-with-spark-submit--spark-shell--pyspark"},"Use the ",(0,r.kt)("inlineCode",{parentName:"h4"},"--jars")," option with ",(0,r.kt)("inlineCode",{parentName:"h4"},"spark-submit / spark-shell / pyspark")),(0,r.kt)("admonition",{type:"info"},(0,r.kt)("p",{parentName:"admonition"},"This approach does not demonstrate how to configure the ",(0,r.kt)("inlineCode",{parentName:"p"},"OpenLineageSparkListener"),".\nPlease refer to the ",(0,r.kt)("a",{parentName:"p",href:"#configuration"},"Configuration")," section.")),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Download the JAR and its checksum from Maven Central."),(0,r.kt)("li",{parentName:"ol"},"Verify the JAR's integrity using the checksum."),(0,r.kt)("li",{parentName:"ol"},"Upon successful verification, submit a Spark application with the JAR using the ",(0,r.kt)("inlineCode",{parentName:"li"},"--jars")," option.")),(0,r.kt)("p",null,"This script demonstrate this process:"),(0,r.kt)(i.Z,{groupId:"spark",mdxType:"Tabs"},(0,r.kt)(o.Z,{value:"after-1.8.0",label:"After 1.8.0",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'#!/usr/bin/env bash\n\nOPENLINEAGE_SPARK_VERSION=\'1.9.0\'  # Example version\nSCALA_BINARY_VERSION=\'2.13\'        # Example Scala version\nARTIFACT_ID="openlineage-spark_${SCALA_BINARY_VERSION}"\nJAR_NAME="${ARTIFACT_ID}-${OPENLINEAGE_SPARK_VERSION}.jar"\nCHECKSUM_NAME="${JAR_NAME}.sha512"\nBASE_URL="https://repo1.maven.org/maven2/io/openlineage/${ARTIFACT_ID}/${OPENLINEAGE_SPARK_VERSION}"\n\ncurl -O "${BASE_URL}/${JAR_NAME}"\ncurl -O "${BASE_URL}/${CHECKSUM_NAME}"\n\necho "$(cat ${CHECKSUM_NAME})  ${JAR_NAME}" | sha512sum -c\n\nif [ $? -eq 0 ]; then\n    spark-submit --jars "path/to/${JAR_NAME}" \\\n      # ... other options\nelse\n    echo "Checksum verification failed."\n    exit 1\nfi\n'))),(0,r.kt)(o.Z,{value:"1.8.0-and-earlier",label:"1.8.0 and earlier",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'#!/usr/bin/env bash\n\nOPENLINEAGE_SPARK_VERSION=\'1.8.0\'  # Example version\nARTIFACT_ID="openlineage-spark"\nJAR_NAME="${ARTIFACT_ID}-${OPENLINEAGE_SPARK_VERSION}.jar"\nCHECKSUM_NAME="${JAR_NAME}.sha512"\nBASE_URL="https://repo1.maven.org/maven2/io/openlineage/${ARTIFACT_ID}/${OPENLINEAGE_SPARK_VERSION}"\n\ncurl -O "${BASE_URL}/${JAR_NAME}"\ncurl -O "${BASE_URL}/${CHECKSUM_NAME}"\n\necho "$(cat ${CHECKSUM_NAME})  ${JAR_NAME}" | sha512sum -c\n\nif [ $? -eq 0 ]; then\n    spark-submit --jars "path/to/${JAR_NAME}" \\\n      # ... other options\nelse\n    echo "Checksum verification failed."\n    exit 1\nfi\n')))),(0,r.kt)("h4",{id:"use-the---packages-option-with-spark-submit--spark-shell--pyspark"},"Use the ",(0,r.kt)("inlineCode",{parentName:"h4"},"--packages")," option with ",(0,r.kt)("inlineCode",{parentName:"h4"},"spark-submit / spark-shell / pyspark")),(0,r.kt)("admonition",{type:"info"},(0,r.kt)("p",{parentName:"admonition"},"This approach does not demonstrate how to configure the ",(0,r.kt)("inlineCode",{parentName:"p"},"OpenLineageSparkListener"),".\nPlease refer to the ",(0,r.kt)("a",{parentName:"p",href:"#configuration"},"Configuration")," section.")),(0,r.kt)("p",null,"Spark allows you to add packages at runtime using the ",(0,r.kt)("inlineCode",{parentName:"p"},"--packages")," option with ",(0,r.kt)("inlineCode",{parentName:"p"},"spark-submit"),". This\noption automatically downloads the package from Maven Central (or other configured repositories)\nduring runtime and adds it to the classpath of your Spark application."),(0,r.kt)(i.Z,{groupId:"spark",mdxType:"Tabs"},(0,r.kt)(o.Z,{value:"after-1.8.0",label:"After 1.8.0",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"OPENLINEAGE_SPARK_VERSION='1.9.0'  # Example version\nSCALA_BINARY_VERSION='2.13'        # Example Scala version\n\nspark-submit --packages \"io.openlineage:openlineage-spark_${SCALA_BINARY_VERSION}:${OPENLINEAGE_SPARK_VERSION}\" \\\n    # ... other options\n"))),(0,r.kt)(o.Z,{value:"1.8.0-and-earlier",label:"1.8.0 and earlier",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"OPENLINEAGE_SPARK_VERSION='1.8.0'  # Example version\n\nspark-submit --packages \"io.openlineage:openlineage-spark::${OPENLINEAGE_SPARK_VERSION}\" \\\n    # ... other options\n")))))}m.isMDXComponent=!0}}]);