"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[7473],{3905:(e,t,a)=>{a.d(t,{Zo:()=>c,kt:()=>h});var n=a(7294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var d=n.createContext({}),l=function(e){var t=n.useContext(d),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},c=function(e){var t=l(e.components);return n.createElement(d.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},p=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,i=e.originalType,d=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),p=l(a),h=o,m=p["".concat(d,".").concat(h)]||p[h]||u[h]||i;return a?n.createElement(m,r(r({ref:t},c),{},{components:a})):n.createElement(m,r({ref:t},c))}));function h(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=a.length,r=new Array(i);r[0]=p;var s={};for(var d in t)hasOwnProperty.call(t,d)&&(s[d]=t[d]);s.originalType=e,s.mdxType="string"==typeof e?e:o,r[1]=s;for(var l=2;l<i;l++)r[l]=a[l];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}p.displayName="MDXCreateElement"},1606:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>d,contentTitle:()=>r,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>l});var n=a(7462),o=(a(7294),a(3905));const i={title:"Expecting Great Quality with OpenLineage Facets",date:new Date("2021-08-12T00:00:00.000Z"),authors:["Collado"],description:"Good data is paramount to making good decisions- but how can you trust the quality of your data and its dependencies?"},r=void 0,s={permalink:"/blog/dataquality_expectations_facet",source:"@site/blog/dataquality_expectations_facet/index.mdx",title:"Expecting Great Quality with OpenLineage Facets",description:"Good data is paramount to making good decisions- but how can you trust the quality of your data and its dependencies?",date:"2021-08-12T00:00:00.000Z",formattedDate:"August 12, 2021",tags:[],readingTime:12.745,hasTruncateMarker:!0,authors:[{name:"Michael Collado",title:"OpenLineage Committer",url:"https://www.github.com/collado-mike",imageURL:"https://www.github.com/collado-mike.png",key:"Collado"}],frontMatter:{title:"Expecting Great Quality with OpenLineage Facets",date:"2021-08-12T00:00:00.000Z",authors:["Collado"],description:"Good data is paramount to making good decisions- but how can you trust the quality of your data and its dependencies?"},prevItem:{title:"Introducing OpenLineage 0.1.0",permalink:"/blog/0.1-release"},nextItem:{title:"Extending OpenLineage with Facets",permalink:"/blog/extending-with-facets"}},d={authorsImageUrls:[void 0]},l=[{value:"The Parable of Bad Data",id:"the-parable-of-bad-data",level:2},{value:"Data-Driven Depends On Data-Quality",id:"data-driven-depends-on-data-quality",level:2},{value:"A Sample Assertion Suite",id:"a-sample-assertion-suite",level:3},{value:"Adding Data Quality Checks to an Airflow pipeline",id:"adding-data-quality-checks-to-an-airflow-pipeline",level:3},{value:"Tracing Data Quality With OpenLineage Facets",id:"tracing-data-quality-with-openlineage-facets",level:2}],c={toc:l};function u(e){let{components:t,...i}=e;return(0,o.kt)("wrapper",(0,n.Z)({},c,i,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"Good data is paramount to making good decisions- but how can you trust the quality of your data and its dependencies?"),(0,o.kt)("h2",{id:"the-parable-of-bad-data"},"The Parable of Bad Data"),(0,o.kt)("p",null,"Several years ago, I worked as a developer on the experimentation team at Amazon, which owned the code libraries and\ndata processing systems that supported experimentation on the amazon.com website (among other systems). Developers used\nour libraries and a microservice we maintained to \u201ctrigger\u201d an experiment for a customer- that is, the customer was\nrandomized into either control or treatment and the resulting assignment was recorded in the logs, which my team\nconsumed in the analysis of the data later on. One of the interesting parts of my job was helping our users diagnose\nproblems with their experiment results."),(0,o.kt)("p",null,"A classic example was a Kindle developer who was prototyping a new feature for book pages that would make the site more\nengaging for Kindle owners- perhaps a \u201clook inside\u201d feature, or maybe some better recommendations. A customer would come\nto the website and the developer\u2019s code would determine if the customer belonged in ",(0,o.kt)("em",{parentName:"p"},"Control")," or ",(0,o.kt)("em",{parentName:"p"},"Treatment"),". If ",(0,o.kt)("em",{parentName:"p"},"Control"),",\nthe assignment was logged and no feature was shown- the site looked to the customer as it always had. But if the\nassignment was ",(0,o.kt)("em",{parentName:"p"},"Treatment"),", the code would check the customer\u2019s account to determine if they owned a Kindle device and,\nif yes, the assignment was logged and the customer saw the fancy new feature on the books page. "),(0,o.kt)("p",null,"The experiment showed the developer\u2019s feature would be wildly successful- an increase of over $10 in Kindle book\npurchases per customer on average over the course of the 2 weeks the experiment ran- projected to be tens of billions of\ndollars in annual revenue due to this one feature!"),(0,o.kt)("p",null,"With data in hand, the developer requested tons of resources to build the feature up to production standards. After\nthree months and two dozen people\u2019s labor, the feature was ready to ship. The developers deployed their new service and\nthe incredible feature was unleashed. For days afterward, everyone watched the metrics dashboards waiting for that\nhockey stick uptick in the revenue graphs \ud83d\udcc8. But it never materialized! The graph was flat. No change at all! Weeks\nwent by. Nothing. How could the experiment results be so far from reality?"),(0,o.kt)("p",null,"Of course, if you\u2019ve ever run A/B tests, you probably already recognized the developer\u2019s mistake. In their randomization\nlogic, customers who were assigned control were logged and forgotten, while customers who were assigned treatment were\nlogged only after validating that they owned a Kindle device. It turned out the total number of customers who came to\namazon.com was far greater than the number of customers who owned a Kindle device. And if you divide the total sum of\nKindle book sales by all of the amazon.com customers, regardless of whether they own a Kindle device, that average\nwill come out quite a lot lower than if you calculate the average Kindle book revenue from only customers who own Kindles."),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Yoda says, poorly run experiments lead to bad data. Bad data leads to bad insights. Bad insights lead to bad decisions. Bad decisions lead to suffering",src:a(2160).Z,width:"558",height:"447"})),(0,o.kt)("p",null,"In reality, this story never happened. Why? Because we knew the adage- Bad Data is Worse than No Data. In the story,\npeople took data of poor quality and used it to justify bad decisions. In our system, we checked the quality of the data\nand, if we detected assignment imbalances, we simply invalidated the experiment and hid the results. Over the years, I\ncan\u2019t count the number of times our users asked us to just give them partial results or just exclude certain segments or\nto let them know if things were \u201ctrending\u201d the right way. Our policy was firm- if we couldn\u2019t trust the quality of the\ndata, the results were meaningless and we would not surface them in our system."),(0,o.kt)("h2",{id:"data-driven-depends-on-data-quality"},"Data-Driven Depends On Data-Quality"),(0,o.kt)("p",null,"Today, most businesses consider themselves data-driven. The stereotype of the maverick CEO leading with his or her gut\nis mostly antiquated, with a handful of exceptions. And yet, even though people know intellectually that data is only\nuseful if it is correct, we tend to stop digging once we find some data that confirms our pre-existing assumptions. We\njustify bad decisions by claiming that they are \u201cdata-based\u201d without ever validating the quality of our sources. Where\ndid that data come from? How old is it? Is the logic that generated it correct? Is it derived from some other dataset?\nWhat is the quality of that dataset?\n",(0,o.kt)("img",{alt:"Data-driven: You keep using that word. I do not think it means what you think it means",src:a(6329).Z,width:"500",height:"432"})),(0,o.kt)("p",null,"Thankfully, data quality validation is becoming more and more common in data engineering organizations. In part, this is\ndue to the prevalence of new tools and their integration with common workflow engines which we already use to schedule\nthe jobs that generate and process our data. One such tool that has been gaining in popularity is called ",(0,o.kt)("a",{parentName:"p",href:"https://docs.greatexpectations.io"},(0,o.kt)("em",{parentName:"a"},"Great\nExpectations")),", a Python-based framework for defining assertions about data sets\nwhich easily integrates with existing workflow tools, such as Airflow. "),(0,o.kt)("p",null,"In software development, testing the behavior of our code with unit and integration tests has been common practice for\nyears. Similarly, using Great Expectations, a data engineer can assert that a dataset has a row count that falls within\nan expected range, that column values are not null, or that values match a specified regular expression. One can even\ncreate custom expectations, such as validating that the number of records in treatment is roughly the same as the number\nof records in control (this post is not intended to be an in-depth tutorial on setting up Great Expectations; if you\nwant to read more on its capabilities and to get started, I recommend the going through\nthe ",(0,o.kt)("a",{parentName:"p",href:"https://docs.greatexpectations.io/en/latest/guides/tutorials/quick_start.html"},"Quick Start tutorial"),")."),(0,o.kt)("h3",{id:"a-sample-assertion-suite"},"A Sample Assertion Suite"),(0,o.kt)("p",null,"As a simple example, imagine a table of new customers that you need to import into your Data Warehouse. Before\nimporting, we want to check the data quality of this new batch of customers. One example suite of assertions we could\ntest is below:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "data_asset_type": "Dataset",\n  "expectation_suite_name": "customers_suite",\n  "expectations": [\n    {\n      "expectation_type": "expect_table_row_count_to_be_between",\n      "kwargs": {\n        "max_value": 1000,\n        "min_value": 100\n      },\n      "meta": {}\n    },\n    {\n      "expectation_type": "expect_table_column_count_to_equal",\n      "kwargs": {\n        "value": 8\n      },\n      "meta": {}\n    },\n    {\n      "expectation_type": "expect_table_columns_to_match_ordered_list",\n      "kwargs": {\n        "column_list": [\n          "id",\n          "created_at",\n          "updated_at",\n          "name",\n          "email",\n          "address",\n          "phone",\n          "city_id"\n        ]\n      },\n      "meta": {}\n    },\n    {\n      "expectation_type": "expect_column_values_to_be_unique",\n      "kwargs": {\n        "column": "email"\n      },\n      "meta": {}\n    }\n  ],\n  "meta": {\n    // ...\n  }\n}\n\n')),(0,o.kt)("p",null,"This sample suite contains 4 data quality assertions- that the dataset contains between 100 and 1000 rows, that the\ntable contains exactly 8 columns, that they match the explicit list of column names we expect, and that the ",(0,o.kt)("inlineCode",{parentName:"p"},"email")," column\ncontains only distinct values."),(0,o.kt)("h3",{id:"adding-data-quality-checks-to-an-airflow-pipeline"},"Adding Data Quality Checks to an Airflow pipeline"),(0,o.kt)("p",null,"With a suite of assertions in hand, we can update our Airflow DAG to only import data into our Data Warehouse if it\nmatches our expectations. A simple DAG might look like this"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from airflow import DAG\nfrom airflow.contrib.operators.bigquery_operator import BigQueryOperator\nfrom airflow.utils.dates import days_ago\nfrom great_expectations_provider.operators.great_expectations import GreatExpectationsOperator\n\ndag = DAG(\n    'etl_customers',\n    schedule_interval='@daily',\n    catchup=False,\n    default_args=default_args,\n    description='Loads newly registered customers daily.'\n)\n\nt1 = BigQueryOperator(\n    task_id='if_not_exists',\n    sql='''\n  CREATE TABLE IF NOT EXISTS food_delivery.customers (\n  id         INT64,\n  created_at TIME,\n  updated_at TIME,\n  name       STRING,\n  email      STRING,\n  address    STRING,\n  phone      STRING,\n  city_id    INT64\n  )\n  ''',\n    use_legacy_sql=False,\n    dag=dag\n)\n\nt2 = GreatExpectationsOperator(\n    expectation_suite_name='customers_suite',\n    batch_kwargs={\n      'table': 'tmp_customers',\n      'datasource': 'food_delivery_db'\n    },\n    dag=dag\n    task_id='customers_expectation',\n)\n\nt3 = BigQueryOperator(\n    task_id='etl',\n    sql='''\n    SELECT id, created_at, updated_at, name, email, address, phone, city_id\n    FROM food_delivery.tmp_customers\n  ''',\n    destination_dataset_table='airflow_marquez.food_delivery.customers',\n    use_legacy_sql=False,\n    dag=dag\n)\n\nt1 >> t2 >> t3\n")),(0,o.kt)("p",null,"This is great! Our DAG creates our target table in BigQuery (if it doesn\u2019t already exist), checks the quality of the\n",(0,o.kt)("inlineCode",{parentName:"p"},"tmp_customers")," table by running the ",(0,o.kt)("inlineCode",{parentName:"p"},"customers_suite")," defined earlier, then imports ",(0,o.kt)("em",{parentName:"p"},"only if all")," of the data quality\nchecks pass."),(0,o.kt)("p",null,"And thus ended all data quality problems forever."),(0,o.kt)("p",null,"Just kidding."),(0,o.kt)("p",null,"Because reality is never so straightforward. In reality, the recommendations team wanted to start generating\nrecommendations for new customers without waiting until the next day, so they built a data pipeline to start consuming\nfrom the ",(0,o.kt)("inlineCode",{parentName:"p"},"tmp_customers")," table directly. And the supply chain folks wanted to start detecting what recipes are gaining\npopularity so they can predict what supplies will need to be restocked sooner, so they started reading from the both\n",(0,o.kt)("inlineCode",{parentName:"p"},"tmp_orders")," table and the ",(0,o.kt)("inlineCode",{parentName:"p"},"tmp_customers")," table before they\u2019re available in the DW. Oh, and the scheduling team needs\nan idea of the geography of the various customers that are ordering and what the distances are between restaurants and\ncustomers so they can get the right number of drivers in the right neighborhoods and of course the marketing team wants\nto use all of this data to make predictions about how much to spend on the right search engine and social media ads and\nthey absolutely cannot wait until tomorrow at 8AM to update their models.\n",(0,o.kt)("img",{alt:"I am the gatekeeper",src:a(5216).Z,width:"640",height:"360"})),(0,o.kt)("h2",{id:"tracing-data-quality-with-openlineage-facets"},"Tracing Data Quality With OpenLineage Facets"),(0,o.kt)("p",null,"Users are never satisfied with the way things are supposed to work. There\u2019s always a reason to work around gatekeepers-\noftentimes, very good reasons that have real business impact- and data engineering is full of creative and resourceful\npeople who absolutely will find a way to get at that data. Even at Amazon, the experiment data was available in the\nclick stream logs, so resourceful users could (and sometimes did) calculate their own experiment results if they really\nwanted to. So it\u2019s important not just to have data quality checks, but to trace the impact of that data throughout an\norganization."),(0,o.kt)("p",null,"The OpenLineage standard uses ",(0,o.kt)("a",{parentName:"p",href:"https://openlineage.io/blog/extending-with-facets/"},"Facets")," to augment the core data\nmodel with useful information about the jobs, runs, and datasets reported on. One interesting detail about facets is\nthat they can be attached to an entity after the fact. In the Marquez reference implementation, a dataset version is\ncreated every time a job run writes to or otherwise modifies a dataset. ",(0,o.kt)("em",{parentName:"p"},"Output")," facets, such as the new record count or\nthe number of bytes written, are attached directly to the dataset version when the job run completes. But consuming\njobs can also attach facets to the version of the dataset that exists at the start time of the job\u2019s execution."),(0,o.kt)("p",null,"In the OpenLineage Airflow integration, Great Expectations tasks, such as the one in our example DAG above, are\nevaluated after they run and the expectation results (as well as some other data quality metrics) are collected\ninto a\n",(0,o.kt)("a",{parentName:"p",href:"https://github.com/OpenLineage/OpenLineage/blob/main/spec/OpenLineage.json#L446-L499"},"DataQuality Metrics Input Dataset Facet"),"\n, which is reported to the server along with the rest of the lineage metadata. In Marquez, we recognize the version of\nthe dataset that was read by the job run and the data quality metadata is permanently associated with that dataset\nversion. The impact of this is that any job that reads that data, whether it happens before or after the dataset quality\nassertion, can be linked to the data quality facet recorded (provided that the dataset version doesn\u2019t change between\nthe data quality check and the read job)."),(0,o.kt)("p",null,"This integration is extremely straightforward to get working. If you already have the Marquez Airflow DAG running in\nyour Airflow workflows, there\u2019s nothing to do! Great Expectations tasks are already being detected and the metrics and\nassertion statuses are already being reported to your configured instance of Marquez."),(0,o.kt)("p",null,"If you\u2019ve never integrated Marquez with your Airflow setup, add a couple\nof ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/OpenLineage/OpenLineage/tree/main/integration/airflow#configuration"},"environment variables"),"\nand ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/OpenLineage/OpenLineage/tree/main/integration/airflow#usage"},"change one line of code"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-diff"},"- from airflow import DAG\n+ from marquez_airflow import DAG\nfrom airflow.contrib.operators.bigquery_operator import BigQueryOperator\nfrom airflow.utils.dates import days_ago\nfrom great_expectations_provider.operators.great_expectations import GreatExpectationsOperator\n")),(0,o.kt)("p",null,"I\u2019ve previously written\nabout ",(0,o.kt)("a",{parentName:"p",href:"https://openlineage.io/blog/explore-lineage-api/"},"how to determine the version of the dataset that was read by a particular job run"),"\n. With Great Expectations now integrated into my Airflow DAG, I want to see what the data quality metrics are for the\nlatest version of the ",(0,o.kt)("inlineCode",{parentName:"p"},"customers")," dataset that was processed by my ETL job. I\u2019ll hit my datakin demo instance:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'$ curl "https://demo.datakin.com/api/v1/namespaces/food_delivery/jobs/etl.etl_delivery_7_days" | jq | less\n{\n  "id": {\n    "namespace": "food_delivery",\n    "name": "etl.etl_delivery_7_days"\n  },\n  "type": "BATCH",\n  "name": "etl.etl_delivery_7_days",\n  "createdAt": "2021-07-23T19:32:03.401782Z",\n  "updatedAt": "2021-08-06T05:11:03.604573Z",\n  "namespace": "food_delivery",\n  "inputs": [\n    {\n      "namespace": "food_delivery",\n      "name": "public.customers"\n    },\n    //...\n  ],\n  "latestRun": {\n    "id": "1043e596-ccb8-4bfb-8fc2-7ee066253248",\n    "jobVersion": {\n      "namespace": "food_delivery",\n      "name": "etl.etl_delivery_7_days",\n      "version": "bc6c294b-b0eb-3160-a06d-1ff9ba3a4e1c"\n    },\n    "inputVersions": [\n      {\n        "namespace": "food_delivery",\n        "name": "public.customers",\n        "version": "4c33f292-40a9-304d-b43f-c7ffb2256e7f"\n      },\n      // ...\n    ],\n    // ...\n  }\n}\n')),(0,o.kt)("p",null,"With the input version of the ",(0,o.kt)("inlineCode",{parentName:"p"},"public.customers")," dataset, I can query Marquez for all the metadata we have about that\nspecific version of the dataset."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'$ curl "https://demo.datakin.com/api/v1/namespaces/food_delivery/datasets/public.customers/versions/4c33f292-40a9-304d-b43f-c7ffb2256e7f" | jq | less\n{\n  "id": {\n    "namespace": "food_delivery",\n    "name": "public.customers"\n  },\n  "type": "DB_TABLE",\n  "name": "public.customers",\n  "physicalName": "public.customers",\n  "createdAt": "2021-08-06T05:02:59.189118Z",\n  "version": "4c33f292-40a9-304d-b43f-c7ffb2256e7f",\n  "namespace": "food_delivery",\n  "sourceName": "analytics_db",\n  "fields": [\n    {\n      "name": "id",\n      "type": "INTEGER",\n      "tags": [],\n      "description": "The unique ID of the customer."\n    },\n    // ...\n  ],\n  "facets": {\n    "stats": {\n      "size": 53362712,\n      "rowCount": 4969\n    },\n    "dataSource": {\n      "uri": "jdbc:postgresql://localhost:3306/deliveries",\n      "name": "analytics_db"\n    },\n    "description": "A table for customers.",\n    "dataQuality": {\n      "bytes": 53362712,\n      "rowCount": 4969,\n      "columnMetrics": {\n        "id": {\n          "nullCount": 0,\n          "distinctCount": 4969\n        },\n        "name": {\n          "nullCount": 0,\n          "distinctCount": 4969\n        },\n        "email": {\n          "nullCount": 0,\n          "distinctCount": 4969\n        }\n      }\n    },\n    "greatExpectations_assertions": {\n      "assertions": [\n        {\n          "success": true,\n          "expectationType": "expect_table_row_count_to_be_between"\n        },\n        {\n          "success": true,\n          "expectationType": "expect_column_to_exist"\n        },\n        {\n          "success": true,\n          "columnId": "id",\n          "expectationType": "expect_column_values_to_be_unique"\n        },\n        {\n          "success": true,\n          "columnId": "id",\n          "expectationType": "expect_column_values_to_not_be_null"\n        },\n        {\n          "success": true,\n          "columnId": "created_at",\n          "expectationType": "expect_column_values_to_not_be_null"\n        },\n        //....\n      ]\n    }\n  }\n}\n')),(0,o.kt)("p",null,"Note the ",(0,o.kt)("inlineCode",{parentName:"p"},"facets")," field contains several properties- ",(0,o.kt)("inlineCode",{parentName:"p"},"stats"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"dataSource"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"description"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"dataQuality"),"\nand ",(0,o.kt)("inlineCode",{parentName:"p"},"greatExpectations_assertions"),". Each of those describes some attribute about the dataset version. Some of the facets\nare attached at write-time, some are attached later- when the dataset is read."),(0,o.kt)("p",null,"In our ",(0,o.kt)("a",{parentName:"p",href:"https://demo.datakin.com"},"Datakin demo"),", we have a lot more assertions than what I included in the sample suite\nabove and that can be seen in this response. In addition to counting rows and columns, we also validate that id columns\nare unique and non-null, timestamps fall within specified ranges (did you know that if you accidentally write a\ntimestamp too far in the future, certain JDBC drivers will overflow the Calendar instance they use for converting\ntimezones?), and emails match expected regular expressions."),(0,o.kt)("p",null,"With the ability to attach data quality facets to dataset versions and the ability to trace the specific versions of\ndatasets read by and written to by specific job runs, I can trust whether the data I\u2019m looking at is good data or bad\ndata. And if my data quality checks fail, I can find out whether I need to contact somebody over in marketing or\nrecommendations to ",(0,o.kt)("a",{parentName:"p",href:"https://openlineage.io/blog/backfilling-airflow-dags-using-marquez/"},"backfill their pipelines")," once\nthe issue has been corrected."),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Good data, bad data",src:a(8239).Z,width:"600",height:"315"})),(0,o.kt)("p",null,"Whether your business is an e-commerce shop that wants to improve its customer experience or a music streaming service\nthat wants to make better listening recommendations or an autonomous vehicle company trying to improve the car\u2019s ability\nto detect double parked vehicles, the quality of your data is paramount to making good decisions. Quality testing tools\nare out there and, chances are, they already work with the pipeline workflow tool you\u2019re using today. And with\nOpenLineage support, you can be confident in the quality of the data at every stage in your pipeline."))}u.isMDXComponent=!0},8239:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/bad_data-2dfc03116878cb8acc920569c17bed30.png"},2160:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/bad_experiment-8384ed6a25865b9e0c8142c741da3f42.png"},6329:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/data_driven-5f55fde8883a9dc5147edff1d6ebe117.png"},5216:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/gatekeeper-619fdddfc62d56485e393cd9adb059f4.png"}}]);