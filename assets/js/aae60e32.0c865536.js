"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[6678],{3905:(e,t,a)=>{a.d(t,{Zo:()=>c,kt:()=>d});var n=a(7294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var l=n.createContext({}),p=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},c=function(e){var t=p(e.components);return n.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,r=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),m=p(a),d=o,f=m["".concat(l,".").concat(d)]||m[d]||u[d]||r;return a?n.createElement(f,i(i({ref:t},c),{},{components:a})):n.createElement(f,i({ref:t},c))}));function d(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=a.length,i=new Array(r);i[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:o,i[1]=s;for(var p=2;p<r;p++)i[p]=a[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},5232:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>u,frontMatter:()=>r,metadata:()=>s,toc:()=>p});var n=a(7462),o=(a(7294),a(3905));const r={sidebar_position:2},i="Default extractors",s={unversionedId:"integrations/airflow/extractors/default-extractors",id:"integrations/airflow/extractors/default-extractors",title:"Default extractors",description:"Default extractors are a new way in OpenLineage 0.17.0+ to easily add lineage to your data pipelines by modifying your Airflow operators directly. This means custom operators\u2014built in house or forked from another project\u2014can provide you and your team with lineage data without having to modify the OpenLineage project directly, with data sent to your lineage backend of choice, most commonly using the OPENLINEAGE_URL environment variable.",source:"@site/docs/integrations/airflow/extractors/default-extractors.md",sourceDirName:"integrations/airflow/extractors",slug:"/integrations/airflow/extractors/default-extractors",permalink:"/docs/integrations/airflow/extractors/default-extractors",draft:!1,editUrl:"https://github.com/OpenLineage/docs/tree/main/docs/integrations/airflow/extractors/default-extractors.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Custom extractors",permalink:"/docs/integrations/airflow/extractors/custom-extractors"},next:{title:"Testing custom extractors",permalink:"/docs/integrations/airflow/extractors/extractor-testing"}},l={},p=[{value:"Implementing Default Extractors",id:"implementing-default-extractors",level:2},{value:"1. <code>DataSourceDatasetFacet</code>",id:"1-datasourcedatasetfacet",level:3},{value:"2. Inputs",id:"2-inputs",level:3},{value:"3. Outputs",id:"3-outputs",level:3},{value:"4. Job Facets",id:"4-job-facets",level:3},{value:"5. Run facets",id:"5-run-facets",level:3},{value:"6. On Complete",id:"6-on-complete",level:3},{value:"Custom Facets",id:"custom-facets",level:3},{value:"Testing",id:"testing",level:3}],c={toc:p};function u(e){let{components:t,...a}=e;return(0,o.kt)("wrapper",(0,n.Z)({},c,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"default-extractors"},"Default extractors"),(0,o.kt)("p",null,"Default extractors are a new way in OpenLineage 0.17.0+ to easily add lineage to your data pipelines by modifying your Airflow operators directly. This means custom operators\u2014built in house or forked from another project\u2014can provide you and your team with lineage data without having to modify the OpenLineage project directly, with data sent to your lineage backend of choice, most commonly using the ",(0,o.kt)("inlineCode",{parentName:"p"},"OPENLINEAGE_URL")," environment variable."),(0,o.kt)("p",null,"The default extractor works a bit differently under the hood than other extractors. While extractors in the OpenLineage project have a getter method for operator names that they\u2019re associated with, the default extractor looks for two specific methods in the operator itself and will call them directly if found. This means that the implementation for the extractor is just two methods in your operator."),(0,o.kt)("p",null,"Those methods are ",(0,o.kt)("inlineCode",{parentName:"p"},"get_openlineage_facets_on_start()")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"get_openlineage_facets_on_complete()"),", called when the operator is first scheduled to run and when the operator has finished execution respectively. Either, or both, of the methods may be implemented by the operator."),(0,o.kt)("p",null,"In the rest of this post, we\u2019ll see how to write these methods within an operator class called ",(0,o.kt)("inlineCode",{parentName:"p"},"DfToGcsOperator"),". This operator moves a Dataframe from an arbitrary source table using a supplied python callable to a specified path in GCS. Most of the ",(0,o.kt)("inlineCode",{parentName:"p"},"__init__()")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"execute()")," methods of the operator aren\u2019t necessary to see to put together the extractor, but an abbreviated version of each method is given below for context. The final two methods in the class are ",(0,o.kt)("inlineCode",{parentName:"p"},"get_openlineage_facets_on_start()")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"get_openlineage_facets_on_complete()"),", which we will be implementing piece-by-piece in the rest of the guide, but are provided here in their entirety for completeness."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from openlineage.airflow.extractors.base import OperatorLineage\nfrom openlineage.client.facet import (\n    DataSourceDatasetFacet,\n    DocumentationJobFacet,\n    OwnershipJobFacet,\n    OwnershipJobFacetOwners,\n    SchemaDatasetFacet,\n    SchemaField,\n)\nfrom openlineage.client.run import Dataset\n\n\nclass DfToGcsOperator():\n    def __init__(\n        self,\n        task_id,\n        python_callable,\n        data_source,\n        bucket=None,\n        table=None,\n        security_group,\n        pipeline_phase,\n        col_types=None,\n        check_cols=True,\n        **kwargs,\n    ):\n        """Initialize a DfToGcsOperator."""\n        super().__init__(task_id=task_id, **kwargs)\n        self.python_callable = python_callable\n        self.data_source = data_source\n        self.table = table if table is not None else task_id\n        self.bucket = bucket\n        self.security_group = security_group\n        self.pipeline_phase = pipeline_phase\n        # col_types is a dict that stores expected column names and types, \n        self.col_types = col_types\n        self.check_cols = check_cols\n\n        self.base_path = "/".join(\n            [self.security_group, self.pipeline_phase, self.data_source, self.table]\n        )\n        # Holds meta information about the dataframe, col names and col types,\n        # that are used in the extractor.\n        self.df_meta = None\n\n    def execute(self, context):\n        """\n        Run a DfToGcs task.\n\n        The task will run the python_callable and save\n        the resulting dataframe to GCS under the proper object path\n        <security_group>/<pipeline_phase>/<data_source>/<table>/.\n        """\n        ...\n        \n        df = get_python_callable_result(self.python_callable, context)\n        if len(df) > 0:\n            df.columns = [clean_column_name(c) for c in df.columns]\n            if self.col_types and self.check_cols:\n                check_cols = [c.lower().strip() for c in self.col_types.keys()]\n                missing = [m for m in check_cols if m not in df.columns]\n                assert (\n                    len(missing) == 0\n                ), "Columns present in col_types but not in DataFrame: " + ",".join(\n                    missing\n                )\n\n            # ----------- #\n            # Save to GCS #\n            # ----------- #\n\n            # Note: this is an imported helper function.\n            df_to_gcs(df, self.bucket, save_to_path)\n\n            # ----------- #\n            # Return Data #\n            # ----------- #\n\n            # Allow us to extract additional lineage information\n            # about all of the fields available in the dataframe\n            self.df_meta = extract_df_fields(df)\n        else:\n            print("Empty dataframe, no artifact saved to GCS.")\n\n    def extract_df_fields(df):\n        from openlineage.common.dataset import SchemaField\n        """Extract a list of SchemaFields from a DataFrame."""\n        fields = []\n        for (col, dtype) in zip(df.columns, df.dtypes):\n            fields.append(SchemaField(name=col, type=str(dtype)))\n        return fields\n\n    def get_openlineage_facets_on_start(self):\n        """Add lineage to DfToGcsOperator on task start."""\n        if not self.bucket:\n            ol_bucket = get_env_bucket()\n        else:\n            ol_bucket = self.bucket\n\n        input_uri = "://".join([self.data_source, self.table])\n        input_source = DataSourceDatasetFacet(\n            name=self.table,\n            uri=input_uri,\n        )\n\n        input_facet = {\n            "datasource": input_source,\n            "schema": SchemaDatasetFacet(\n                fields=[\n                    SchemaField(name=col_name, type=col_type)\n                    for col_name, col_type in self.col_types.items()\n                ]\n            ),\n        }\n\n        input = Dataset(namespace=self.data_source, name=self.table, facets=input_facet)\n\n        output_namespace = "gs://" + ol_bucket\n        output_name = self.base_path\n        output_uri = "/".join(\n            [\n                output_namespace,\n                output_name,\n            ]\n        )\n\n        output_source = DataSourceDatasetFacet(\n            name=output_name,\n            uri=output_uri,\n        )\n\n        output_facet = {\n            "datasource": output_source,\n            "schema": SchemaDatasetFacet(\n                fields=[\n                    SchemaField(name=col_name, type=col_type)\n                    for col_name, col_type in self.col_types.items()\n                ]\n            ),\n        }\n\n        output = Dataset(\n            namespace=output_namespace,\n            name=output_name,\n            facets=output_facet,\n        )\n\n        return OperatorLineage(\n            inputs=[input],\n            outputs=[output],\n            run_facets={},\n            job_facets={\n                "documentation": DocumentationJobFacet(\n                    description=f"""\n                    Takes data from the data source {input_uri}\n                    and puts it in GCS at the path: {output_uri}\n                    """\n                ),\n                "ownership": OwnershipJobFacet(\n                    owners=[OwnershipJobFacetOwners(name=self.owner, type=self.email)]\n                ),\n            }\n        )\n\n    def get_openlineage_facets_on_complete(self, task_instance):\n        """Add lineage to DfToGcsOperator on task completion."""\n        starting_facets = self.get_openlineage_facets_on_start()\n        if task_instance.task.df_meta is not None:\n            for i in starting_facets.inputs:\n                i.facets["SchemaDatasetFacet"].fields = task_instance.task.df_meta\n        else:\n            starting_facets.run_facets = {\n                "errorMessage": ErrorMessageRunFacet(\n                    message="Empty dataframe, no artifact saved to GCS.",\n                    programmingLanguage="python"\n                )\n            }\n        return starting_facets\n')),(0,o.kt)("h2",{id:"implementing-default-extractors"},"Implementing Default Extractors"),(0,o.kt)("p",null,"To implement a default extractor, first you need an operator class. In this example, we\u2019ll use the ",(0,o.kt)("inlineCode",{parentName:"p"},"DfToGcsOperator"),", a custom operator created by the Astronomer Data team to load arbitrary dataframes to our GCS bucket. We\u2019ll implement both ",(0,o.kt)("inlineCode",{parentName:"p"},"get_openlineage_facets_on_start()")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"get_openlineage_facets_on_complete()")," for our custom operator. The specific details of the implementation will vary from operator to operator, but there will always be five basic steps that these functions will share."),(0,o.kt)("p",null,"Both the methods return an ",(0,o.kt)("inlineCode",{parentName:"p"},"OperatorLineage")," object, which itself is a collection of facets. Four of the five steps mentioned above are creating these facets where necessary, and the fifth is creating the ",(0,o.kt)("inlineCode",{parentName:"p"},"DataSourceDatasetFacet"),". First, though, we\u2019ll need to import some OpenLineage objects:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from openlineage.airflow.extractors.base import OperatorLineage\nfrom openlineage.client.facet import (\n    DataSourceDatasetFacet,\n    SchemaDatasetFacet,\n    SchemaField,\n)\nfrom openlineage.client.run import Dataset\n")),(0,o.kt)("p",null,"Now, we\u2019ll start building the facets for the ",(0,o.kt)("inlineCode",{parentName:"p"},"OperatorLineage")," object in the ",(0,o.kt)("inlineCode",{parentName:"p"},"get_openlineage_facets_on_start()")," method."),(0,o.kt)("h3",{id:"1-datasourcedatasetfacet"},"1. ",(0,o.kt)("inlineCode",{parentName:"h3"},"DataSourceDatasetFacet")),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"DataSourceDatasestFacet")," is a simple object, containing two fields, ",(0,o.kt)("inlineCode",{parentName:"p"},"name")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"uri"),", which should be populated with the unique name of the data source and the URI. We\u2019ll make two of these objects, an ",(0,o.kt)("inlineCode",{parentName:"p"},"input_source")," to specify where the data came from, and an ",(0,o.kt)("inlineCode",{parentName:"p"},"output_source")," to specify where the data is going."),(0,o.kt)("p",null,"A quick note about the philosophy behind the ",(0,o.kt)("inlineCode",{parentName:"p"},"name")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"uri"),": the ",(0,o.kt)("inlineCode",{parentName:"p"},"uri")," is built from the ",(0,o.kt)("inlineCode",{parentName:"p"},"namespace")," and the ",(0,o.kt)("inlineCode",{parentName:"p"},"name"),", and each is expected to be unique with respect to its environment. This means a ",(0,o.kt)("inlineCode",{parentName:"p"},"namespace")," should be globally unique in the OpenLineage universe, and the ",(0,o.kt)("inlineCode",{parentName:"p"},"name")," unique within the ",(0,o.kt)("inlineCode",{parentName:"p"},"namespace"),". The two are then concatenated to form the ",(0,o.kt)("inlineCode",{parentName:"p"},"uri"),", so that ",(0,o.kt)("inlineCode",{parentName:"p"},"uri = namespace + name"),". The full OpenLineage naming spec can be found ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/OpenLineage/OpenLineage/blob/main/spec/Naming.md"},"here"),"."),(0,o.kt)("p",null,"In our case, the input ",(0,o.kt)("inlineCode",{parentName:"p"},"name")," will be the table we are pulling data from, ",(0,o.kt)("inlineCode",{parentName:"p"},"self.table"),", and the ",(0,o.kt)("inlineCode",{parentName:"p"},"namespace")," will be our ",(0,o.kt)("inlineCode",{parentName:"p"},"self.data_source"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'input_source = DataSourceDatasetFacet(\n    name=self.table,\n    uri="://".join([self.data_source, self.table]),\n)\n')),(0,o.kt)("p",null,"The output data source object\u2019s ",(0,o.kt)("inlineCode",{parentName:"p"},"name")," will always be the base path given to the operator, ",(0,o.kt)("inlineCode",{parentName:"p"},"self.base_path"),". The ",(0,o.kt)("inlineCode",{parentName:"p"},"namespace")," is always in GCS, so we use the OpenLineage spec\u2019s ",(0,o.kt)("inlineCode",{parentName:"p"},"gs://")," as the scheme and our bucket as the authority, giving us ",(0,o.kt)("inlineCode",{parentName:"p"},"gs://{ol_bucket}"),". The ",(0,o.kt)("inlineCode",{parentName:"p"},"uri")," is simply the concatenation of the two."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'if not self.bucket:\n    ol_bucket = get_env_bucket()\nelse:\n    ol_bucket = self.bucket\n\noutput_namespace = "gs://" + ol_bucket\noutput_name = self.base_path\noutput_uri = "/".join(\n    [\n        output_namespace,\n        output_name,\n    ]\n)\n\noutput_source = DataSourceDatasetFacet(\n    name=output_name,\n    uri=output_uri,\n)\n')),(0,o.kt)("h3",{id:"2-inputs"},"2. Inputs"),(0,o.kt)("p",null,"Next we\u2019ll create the input dataset object. As we are moving data from a dataframe to GCS in this operator, we\u2019ll make sure that we are capturing all the info in the dataframe being extracted in a ",(0,o.kt)("inlineCode",{parentName:"p"},"Dataset"),". To create the ",(0,o.kt)("inlineCode",{parentName:"p"},"Dataset")," object, we\u2019ll need ",(0,o.kt)("inlineCode",{parentName:"p"},"namespace"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"name"),", and ",(0,o.kt)("inlineCode",{parentName:"p"},"facets")," objects. The first two are strings, and ",(0,o.kt)("inlineCode",{parentName:"p"},"facets")," is a dictionary."),(0,o.kt)("p",null,"Our ",(0,o.kt)("inlineCode",{parentName:"p"},"namespace")," will come from the operator, where we use ",(0,o.kt)("inlineCode",{parentName:"p"},"self.data_source")," again. The ",(0,o.kt)("inlineCode",{parentName:"p"},"name")," parameter for this facet will be the table, again coming from the operator\u2019s parameter list. The ",(0,o.kt)("inlineCode",{parentName:"p"},"facets")," will contain two entries, the first is our ",(0,o.kt)("inlineCode",{parentName:"p"},"DataSourceDatasetFacet"),' with the key "datasource" coming from the previous step and ',(0,o.kt)("inlineCode",{parentName:"p"},"input_source"),' as the value. The second has the key "schema", with the value being a ',(0,o.kt)("inlineCode",{parentName:"p"},"SchemaDatasetFacet"),", which itself is a collection of ",(0,o.kt)("inlineCode",{parentName:"p"},"SchemaField")," objects, one for each column, created via a list comprehension over the operator's ",(0,o.kt)("inlineCode",{parentName:"p"},"self.col_types")," parameter."),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"inputs")," parameter to ",(0,o.kt)("inlineCode",{parentName:"p"},"OperatorLineage")," is a list of ",(0,o.kt)("inlineCode",{parentName:"p"},"Dataset")," objects, so we\u2019ll end up adding a single ",(0,o.kt)("inlineCode",{parentName:"p"},"Dataset")," object to the list later. The creation of the ",(0,o.kt)("inlineCode",{parentName:"p"},"Dataset")," object looks as follows:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'input_facet = {\n    "datasource": input_source,\n    "schema": SchemaDatasetFacet(\n        fields=[\n            SchemaField(name=col_name, type=col_type)\n            for col_name, col_type in self.col_types.items()\n        ]\n    ),\n}\n\ninput = Dataset(namespace=self.data_source, name=self.table, facets=input_facet)\n')),(0,o.kt)("h3",{id:"3-outputs"},"3. Outputs"),(0,o.kt)("p",null,"Our output facet will look almost identical to the input facet, except it will use the ",(0,o.kt)("inlineCode",{parentName:"p"},"output_source")," we previously created, and will also have a different ",(0,o.kt)("inlineCode",{parentName:"p"},"namespace"),". Our output facet object will be built as follows:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'output_facet = {\n    "datasource": output_source,\n    "schema": SchemaDatasetFacet(\n        fields=[\n            SchemaField(name=col_name, type=col_type)\n            for col_name, col_type in self.col_types.items()\n        ]\n    ),\n}\n\noutput = Dataset(\n    namespace=output_namespace,\n    name=output_name,\n    facets=output_facet,\n)\n')),(0,o.kt)("h3",{id:"4-job-facets"},"4. Job Facets"),(0,o.kt)("p",null,"A Job in OpenLineage is a process definition that consumes and produces datasets. The Job evolves over time, and that change is captured when the job runs. This means the facets we would want to capture in at the Job level are independent of the state the Job is in. Custom facets can be created to capture this job data. For our operator, we stuck with pre-existing job facets, the ",(0,o.kt)("inlineCode",{parentName:"p"},"DocumentationJobFacet")," and the ",(0,o.kt)("inlineCode",{parentName:"p"},"OwnershipJobFacet"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'job_facets = {\n    "documentation": DocumentationJobFacet(\n        description=f"""\n            Takes data from the data source {input_uri}\n            and puts it in GCS at the path: {output_uri}\n            """\n    ),\n    "ownership": OwnershipJobFacet(\n        owners=[OwnershipJobFacetOwners(name=self.owner, type=self.email)]\n    )\n}\n')),(0,o.kt)("h3",{id:"5-run-facets"},"5. Run facets"),(0,o.kt)("p",null,"A Run is and instance of a Job execution. For example, when an Airflow Operator begins execution, the Run state of the OpenLineage Job transitions to Start, then to Running. When writing an Extractor, this means a Run facet should contain information pertinent to the specific instance of the job, something that could change every Run."),(0,o.kt)("p",null,"In this example, we will emit an error message when there is an empty dataframe, using the existing ",(0,o.kt)("inlineCode",{parentName:"p"},"ErrorMessageRunFacet"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'starting_facets.run_facets = {\n    "errorMessage": ErrorMessageRunFacet(\n        message="Empty dataframe, no artifact saved to GCS.",\n        programmingLanguage="python"\n    )\n}\n')),(0,o.kt)("h3",{id:"6-on-complete"},"6. On Complete"),(0,o.kt)("p",null,"Finally, we\u2019ll implement the ",(0,o.kt)("inlineCode",{parentName:"p"},"get_openlineage_metadata_on_complete()")," method. Most our work is already done for us, so we will start by calling ",(0,o.kt)("inlineCode",{parentName:"p"},"get_openlineage_metadata_on_start()")," and then modify the returned object slightly before returning it again. The two main additions here are replacing the original ",(0,o.kt)("inlineCode",{parentName:"p"},"SchemaDatasetFacet")," fields and adding a potential error message to the ",(0,o.kt)("inlineCode",{parentName:"p"},"run_facets"),"."),(0,o.kt)("p",null,"For the ",(0,o.kt)("inlineCode",{parentName:"p"},"SchemaDatasetFacet")," update, we replace the old fields facet with updated ones based on the now-filled-out ",(0,o.kt)("inlineCode",{parentName:"p"},"df_meta")," dict, which is populated during the operator\u2019s ",(0,o.kt)("inlineCode",{parentName:"p"},"execute()")," method and is therefore unavailable to ",(0,o.kt)("inlineCode",{parentName:"p"},"get_openlineage_metadata_on_start()"),". Because ",(0,o.kt)("inlineCode",{parentName:"p"},"df_meta")," is already a list of ",(0,o.kt)("inlineCode",{parentName:"p"},"SchemaField")," objects, we can set the property directly. Although we use a for-loop here, the operator ensures only one dataframe will ever be extracted per execute, so the for loop will only ever run once and we therefore do not have to worry about multiple input dataframes updating."),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"run_facets")," update is done only if there is an error, which is a mutually exclusive event to updating the fields facets. We pass the same message to this facet that is printed in the ",(0,o.kt)("inlineCode",{parentName:"p"},"execute()")," method when an empty dataframe is found. This error message does not halt operator execution, as it gets added ",(0,o.kt)("strong",{parentName:"p"},"*"),"after",(0,o.kt)("strong",{parentName:"p"},"*")," execution, but it does create an alert in the OpenLineage UI."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'def get_openlineage_facets_on_complete(self, task_instance):\n    """Add lineage to DfToGcsOperator on task completion."""\n    starting_facets = self.get_openlineage_facets_on_start()\n    if task_instance.task.df_meta is not None:\n        for i in starting_facets.inputs:\n            i.facets["SchemaDatasetFacet"].fields = task_instance.task.df_meta\n    else:\n        starting_facets.run_facets = {\n            "errorMessage": ErrorMessageRunFacet(\n                message="Empty dataframe, no artifact saved to GCS.",\n                programmingLanguage="python"\n            )\n        }\n    return starting_facets\n')),(0,o.kt)("p",null,"And with that final piece of the puzzle, we have a working extractor for our custom operator!"),(0,o.kt)("h3",{id:"custom-facets"},"Custom Facets"),(0,o.kt)("p",null,"The OpenLineage spec may not contain all the facets you need to write your extractor, in which case you will have to make your own ",(0,o.kt)("a",{parentName:"p",href:"https://openlineage.io/docs/spec/facets/custom-facets"},"custom facets"),". More on creating custom facets can be found ",(0,o.kt)("a",{parentName:"p",href:"https://openlineage.io/blog/extending-with-facets/"},"here"),"."),(0,o.kt)("h3",{id:"testing"},"Testing"),(0,o.kt)("p",null,"See the doc on ",(0,o.kt)("a",{parentName:"p",href:"https://openlineage.io/docs/integrations/airflow/extractors/extractor-testing"},"testing custom extractors"),"."))}u.isMDXComponent=!0}}]);