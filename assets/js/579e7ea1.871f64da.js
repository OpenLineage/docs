"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[5377],{3905:(e,n,t)=>{t.d(n,{Zo:()=>d,kt:()=>m});var i=t(67294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);n&&(i=i.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,i)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,i,a=function(e,n){if(null==e)return{};var t,i,a={},o=Object.keys(e);for(i=0;i<o.length;i++)t=o[i],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(i=0;i<o.length;i++)t=o[i],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var s=i.createContext({}),p=function(e){var n=i.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},d=function(e){var n=p(e.components);return i.createElement(s.Provider,{value:n},e.children)},c={inlineCode:"code",wrapper:function(e){var n=e.children;return i.createElement(i.Fragment,{},n)}},u=i.forwardRef((function(e,n){var t=e.components,a=e.mdxType,o=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),u=p(t),m=a,h=u["".concat(s,".").concat(m)]||u[m]||c[m]||o;return t?i.createElement(h,r(r({ref:n},d),{},{components:t})):i.createElement(h,r({ref:n},d))}));function m(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var o=t.length,r=new Array(o);r[0]=u;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l.mdxType="string"==typeof e?e:a,r[1]=l;for(var p=2;p<o;p++)r[p]=t[p];return i.createElement.apply(null,r)}return i.createElement.apply(null,t)}u.displayName="MDXCreateElement"},36952:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>r,default:()=>c,frontMatter:()=>o,metadata:()=>l,toc:()=>p});var i=t(87462),a=(t(67294),t(3905));const o={sidebar_position:7,title:"Column-Level Lineage"},r=void 0,l={unversionedId:"integrations/spark/spark_column_lineage",id:"integrations/spark/spark_column_lineage",title:"Column-Level Lineage",description:"Column-level lineage works only with Spark 3.",source:"@site/docs/integrations/spark/spark_column_lineage.md",sourceDirName:"integrations/spark",slug:"/integrations/spark/spark_column_lineage",permalink:"/docs/integrations/spark/spark_column_lineage",draft:!1,editUrl:"https://github.com/OpenLineage/docs/tree/main/docs/integrations/spark/spark_column_lineage.md",tags:[],version:"current",sidebarPosition:7,frontMatter:{sidebar_position:7,title:"Column-Level Lineage"},sidebar:"tutorialSidebar",previous:{title:"Job Hierarchy",permalink:"/docs/integrations/spark/job-hierarchy"},next:{title:"Spark Integration Metrics",permalink:"/docs/integrations/spark/metrics"}},s={},p=[{value:"Standard specification",id:"standard-specification",level:2},{value:"Code architecture and its mechanics",id:"code-architecture-and-its-mechanics",level:2},{value:"Expression dependency collection process",id:"expression-dependency-collection-process",level:3},{value:"Facet building process",id:"facet-building-process",level:3},{value:"Writing custom extensions",id:"writing-custom-extensions",level:2}],d={toc:p};function c(e){let{components:n,...t}=e;return(0,a.kt)("wrapper",(0,i.Z)({},d,t,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("admonition",{type:"warning"},(0,a.kt)("p",{parentName:"admonition"},"Column-level lineage works only with Spark 3.")),(0,a.kt)("admonition",{type:"info"},(0,a.kt)("p",{parentName:"admonition"},"Column-level lineage for Spark is turned on by default and requires no additional work to be done. The following documentation describes its internals. ")),(0,a.kt)("admonition",{type:"info"},(0,a.kt)("p",{parentName:"admonition"},"Lineage contains information about what fields were used to create of influence the field but also how, see ",(0,a.kt)("a",{parentName:"p",href:"/docs/spec/facets/dataset-facets/column_lineage_facet#transformation-type"},"Transformation Types"))),(0,a.kt)("p",null,"Column-level lineage provides fine-grained information on datasets dependencies. Not only do we know the dependency exists, but we are also able to understand which input columns are used to produce output columns. This allows for answering questions like ",(0,a.kt)("em",{parentName:"p"},"Which root input columns are used to construct column x?")," "),(0,a.kt)("h2",{id:"standard-specification"},"Standard specification"),(0,a.kt)("p",null,"Collected information is sent in OpenLineage event within ",(0,a.kt)("inlineCode",{parentName:"p"},"columnLineage")," dataset facet described ",(0,a.kt)("a",{parentName:"p",href:"/docs/spec/facets/dataset-facets/column_lineage_facet"},"here"),". "),(0,a.kt)("h2",{id:"code-architecture-and-its-mechanics"},"Code architecture and its mechanics"),(0,a.kt)("p",null,"Column-level lineage has been implemented separately from the rest of builders and visitors extracting lineage information from Spark logical plans. As a result the codebase is stored in ",(0,a.kt)("inlineCode",{parentName:"p"},"io.openlineage.spark3.agent.lifecycle.plan.columnLineage")," package within classes responsible only for this feature."),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Class ",(0,a.kt)("inlineCode",{parentName:"p"},"ColumnLevelLineageUtils.java")," is an entry point to run the mechanism and is used within ",(0,a.kt)("inlineCode",{parentName:"p"},"OpenLineageRunEventBuilder"),".")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Classes ",(0,a.kt)("inlineCode",{parentName:"p"},"ColumnLevelLineageUtilsNonV2CatalogTest")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"ColumnLevelLineageUtilsV2CatalogTest")," contain real-life test cases which run Spark jobs and get an access to the last query plan executed.\nThey evaluate column-level lineage based on the plan and expected output schema.\nThen, they verify if this meets the requirements.\nThis allows testing column-level lineage behavior in real scenarios. The more tests and scenarios put here, the better.")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Class ",(0,a.kt)("inlineCode",{parentName:"p"},"ColumnLevelLineageBuilder")," contains both the logic of building output facet (",(0,a.kt)("inlineCode",{parentName:"p"},"ColumnLineageDatasetFacetFields"),")\nand datastructures containing necessary information:"),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"schema - ",(0,a.kt)("inlineCode",{parentName:"li"},"SchemaDatasetFacet")," contains information about output schema "),(0,a.kt)("li",{parentName:"ul"},"inputs - map pointing from ",(0,a.kt)("inlineCode",{parentName:"li"},"ExprId")," to column name and ",(0,a.kt)("inlineCode",{parentName:"li"},"DatasetIdentifier")," identifying the datasource "),(0,a.kt)("li",{parentName:"ul"},"outputs - map pointing from output field name to its ",(0,a.kt)("inlineCode",{parentName:"li"},"ExprId")),(0,a.kt)("li",{parentName:"ul"},"exprDependencies - map pointing from ",(0,a.kt)("inlineCode",{parentName:"li"},"ExprId")," to set of its ",(0,a.kt)("inlineCode",{parentName:"li"},"Dependency")," objects containing ",(0,a.kt)("inlineCode",{parentName:"li"},"ExprId")," and information about type of the dependency."),(0,a.kt)("li",{parentName:"ul"},"datasetDependencies - list of ",(0,a.kt)("inlineCode",{parentName:"li"},"ExprId")," representing pseudo-expressions representing operations like ",(0,a.kt)("inlineCode",{parentName:"li"},"filter"),", ",(0,a.kt)("inlineCode",{parentName:"li"},"join")," etc."),(0,a.kt)("li",{parentName:"ul"},"externalExpressionMappings - map poiting from ",(0,a.kt)("inlineCode",{parentName:"li"},"ColumnMeta")," object to ",(0,a.kt)("inlineCode",{parentName:"li"},"ExprId")," used for dependencies extracted by ",(0,a.kt)("inlineCode",{parentName:"li"},"sql-parser"))))),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Class ",(0,a.kt)("inlineCode",{parentName:"p"},"ColumnLevelLineageBuilder")," is used when traversing logical plans to store all the information required to produce column-level lineage.\nIt allows storing input/output columns. It also stores dependencies between the expressions contained in query plan.\nOnce inputs, outputs and dependencies are filled, build method is used to produce output facet (",(0,a.kt)("inlineCode",{parentName:"p"},"ColumnLineageDatasetFacetFields"),").")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("inlineCode",{parentName:"p"},"OutputFieldsCollector")," class is used to traverse the plan to gather the ",(0,a.kt)("inlineCode",{parentName:"p"},"outputs"),",\neven though the information about output dataset is already in ",(0,a.kt)("inlineCode",{parentName:"p"},"schema"),", it's not coupled information about the outputs ",(0,a.kt)("inlineCode",{parentName:"p"},"ExprId"),".\nThe collector traverses the plan and matches the outputs existing there, inside ",(0,a.kt)("inlineCode",{parentName:"p"},"Aggregate")," or ",(0,a.kt)("inlineCode",{parentName:"p"},"Project")," objects, with the ones in ",(0,a.kt)("inlineCode",{parentName:"p"},"schema")," by their name.")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("inlineCode",{parentName:"p"},"InputFieldsCollector")," class is used to collect the inputs which can be extracted from ",(0,a.kt)("inlineCode",{parentName:"p"},"DataSourceV2Relation"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"DataSourceV2ScanRelation"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"HiveTableRelation")," or ",(0,a.kt)("inlineCode",{parentName:"p"},"LogicalRelation"),".\nEach input field has its ",(0,a.kt)("inlineCode",{parentName:"p"},"ExprId")," within the plan. Each input is identified by ",(0,a.kt)("inlineCode",{parentName:"p"},"DatasetIdentifier"),", which means it contains name and namespace, of a dataset and an input field.")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("inlineCode",{parentName:"p"},"ExpressionDependenciesCollector")," traverses the plan to identify dependencies between different expressions using their ",(0,a.kt)("inlineCode",{parentName:"p"},"ExprId"),". Dependencies map parent expressions to its dependencies with additional information about the transformation type.\nThis is used evaluate which inputs influenced certain output and what kind of influence was it."))),(0,a.kt)("h3",{id:"expression-dependency-collection-process"},"Expression dependency collection process"),(0,a.kt)("p",null,"For each node in ",(0,a.kt)("inlineCode",{parentName:"p"},"LogicalPlan")," the ",(0,a.kt)("inlineCode",{parentName:"p"},"ExpressionDependencyCollector")," attempts to extract the column lineage information based on its type.\nFirst it goes through ",(0,a.kt)("inlineCode",{parentName:"p"},"ColumnLineageVisitors")," to check if any applies to current node, if so then it extract dependencies from them.\nNext if the node is ",(0,a.kt)("inlineCode",{parentName:"p"},"LogicalRelation")," and relation type is ",(0,a.kt)("inlineCode",{parentName:"p"},"JDBCRelation"),", the sql-parser extracts lineage data from query string itself."),(0,a.kt)("admonition",{type:"warning"},(0,a.kt)("p",{parentName:"admonition"},"Because Sql parser only parses the query string in ",(0,a.kt)("inlineCode",{parentName:"p"},"JDBCRelation")," it does not collect information about input field types or transformation types.\nThe only info collected is the name of the table/view and field, as it is mentioned in the query.")),(0,a.kt)("p",null,"After that all that's left are following types of nodes: ",(0,a.kt)("inlineCode",{parentName:"p"},"Project"),",",(0,a.kt)("inlineCode",{parentName:"p"},"Aggregate"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"Join"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"Filter"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"Sort"),".\nEach of them contains dependency expressions that can be added to one of the lists ",(0,a.kt)("inlineCode",{parentName:"p"},"expressions")," or ",(0,a.kt)("inlineCode",{parentName:"p"},"datasetDependencies"),"."),(0,a.kt)("p",null,"When node is ",(0,a.kt)("inlineCode",{parentName:"p"},"Aggregate"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"Join"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"Filter")," or ",(0,a.kt)("inlineCode",{parentName:"p"},"Sort")," it contains dependencies that don't affect one single output but all the outputs,\nso they need to be treated differently than normal dependencies.\nFor each of those nodes the new ",(0,a.kt)("inlineCode",{parentName:"p"},"ExprId"),' is created to represent "all outputs", all its dependencies will be of ',(0,a.kt)("inlineCode",{parentName:"p"},"INDIRECT")," type."),(0,a.kt)("p",null,"For each of the ",(0,a.kt)("inlineCode",{parentName:"p"},"expressions")," the collector tries to go through it and possible children expressions and add them to ",(0,a.kt)("inlineCode",{parentName:"p"},"exprDependencies")," map with appropriate transformation type and ",(0,a.kt)("inlineCode",{parentName:"p"},"masking")," flag.\nMost of the expressions represent ",(0,a.kt)("inlineCode",{parentName:"p"},"DIRECT")," transformation, only exceptions are ",(0,a.kt)("inlineCode",{parentName:"p"},"If")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"CaseWhen")," which contain condition expressions."),(0,a.kt)("h3",{id:"facet-building-process"},"Facet building process"),(0,a.kt)("p",null,"For each of the outputs ",(0,a.kt)("inlineCode",{parentName:"p"},"ColumnLevelLineageBuilder")," goes through the ",(0,a.kt)("inlineCode",{parentName:"p"},"exprDependencies")," to build the list final dependencies, then using ",(0,a.kt)("inlineCode",{parentName:"p"},"inputs")," maps them to fields in datasets.\nDuring the process it also unravels the transformation type between the input and output.\nTo unravel two dependencies implement following logic:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"if current type is ",(0,a.kt)("inlineCode",{parentName:"li"},"INDIRECT")," the result takes the type and subtype from current"),(0,a.kt)("li",{parentName:"ul"},"if current type is ",(0,a.kt)("inlineCode",{parentName:"li"},"DIRECT")," and other one is null, result is null"),(0,a.kt)("li",{parentName:"ul"},"if current type is ",(0,a.kt)("inlineCode",{parentName:"li"},"DIRECT")," and other is ",(0,a.kt)("inlineCode",{parentName:"li"},"INDIRECT")," the result takes type and subtype from other"),(0,a.kt)("li",{parentName:"ul"},"if both are ",(0,a.kt)("inlineCode",{parentName:"li"},"DIRECT")," the result is type ",(0,a.kt)("inlineCode",{parentName:"li"},"DIRECT"),", subtype is the first existing from the order ",(0,a.kt)("inlineCode",{parentName:"li"},"AGGREGATION"),", ",(0,a.kt)("inlineCode",{parentName:"li"},"TRANSFORMATION"),", ",(0,a.kt)("inlineCode",{parentName:"li"},"IDENTITY")),(0,a.kt)("li",{parentName:"ul"},"if any of the transformations is masking, the result is masking")),(0,a.kt)("p",null,"The inputs are also mapped for all dataset dependencies. The result is added to each output.\nFinally, the list of outputs with all their inputs is mapped to ",(0,a.kt)("inlineCode",{parentName:"p"},"ColumnLineageDatasetFacetFields")," object."),(0,a.kt)("h2",{id:"writing-custom-extensions"},"Writing custom extensions"),(0,a.kt)("p",null,"Spark framework is known for its great ability to be extended by custom libraries capable of reading or writing to anything. In case of having a custom implementation, we prepared an ability to extend column-level lineage implementation to be able to retrieve information from other input or output LogicalPlan nodes. "),(0,a.kt)("p",null,"Creating such an extension requires implementing a following interface: "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"/** Interface for implementing custom collectors of column-level lineage. */\ninterface CustomColumnLineageVisitor {\n\n  /**\n   * Collect inputs for a given {@link LogicalPlan}. Column-level lineage mechanism traverses\n   * LogicalPlan on its node. This method will be called for each traversed node. Input information\n   * should be put into builder.\n   *\n   * @param node\n   * @param builder\n   */\n  void collectInputs(LogicalPlan node, ColumnLevelLineageBuilder builder);\n\n  /**\n   * Collect outputs for a given {@link LogicalPlan}. Column-level lineage mechanism traverses\n   * LogicalPlan on its node. This method will be called for each traversed node. Output information\n   * should be put into builder.\n   *\n   * @param node\n   * @param builder\n   */\n  void collectOutputs(LogicalPlan node, ColumnLevelLineageBuilder builder);\n\n  /**\n   * Collect expressions for a given {@link LogicalPlan}. Column-level lineage mechanism traverses\n   * LogicalPlan on its node. This method will be called for each traversed node. Expression\n   * dependency information should be put into builder.\n   *\n   * @param node\n   * @param builder\n   */\n  void collectExpressionDependencies(LogicalPlan node, ColumnLevelLineageBuilder builder);\n}\n")),(0,a.kt)("p",null,"and making it available for Service Loader (implementation class name has to be put in a resource file ",(0,a.kt)("inlineCode",{parentName:"p"},"META-INF/services/io.openlineage.spark.agent.lifecycle.plan.column.CustomColumnLineageVisitor"),")."))}c.isMDXComponent=!0}}]);