"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8410],{3905:(e,t,a)=>{a.d(t,{Zo:()=>c,kt:()=>m});var n=a(67294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var l=n.createContext({}),p=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},c=function(e){var t=p(e.components);return n.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,r=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),d=p(a),m=o,h=d["".concat(l,".").concat(m)]||d[m]||u[m]||r;return a?n.createElement(h,i(i({ref:t},c),{},{components:a})):n.createElement(h,i({ref:t},c))}));function m(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=a.length,i=new Array(r);i[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:o,i[1]=s;for(var p=2;p<r;p++)i[p]=a[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},7751:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>u,frontMatter:()=>r,metadata:()=>s,toc:()=>p});var n=a(87462),o=(a(67294),a(3905));const r={title:"Pursuing Lineage from Airflow using Custom Extractors",date:new Date("2022-09-08T00:00:00.000Z"),authors:["Obuchowski","Robinson"],description:"Built-in support for custom extractors makes OpenLineage a highly adaptable solution for pipelines that use Airflow."},i=void 0,s={permalink:"/blog/extractors",source:"@site/blog/extractors/index.mdx",title:"Pursuing Lineage from Airflow using Custom Extractors",description:"Built-in support for custom extractors makes OpenLineage a highly adaptable solution for pipelines that use Airflow.",date:"2022-09-08T00:00:00.000Z",formattedDate:"September 8, 2022",tags:[],readingTime:4.16,hasTruncateMarker:!0,authors:[{name:"Maciej Obuchowski",title:"OpenLineage Committer",url:"https://github.com/mobuchowski",imageURL:"https://github.com/mobuchowski.png",key:"Obuchowski"},{name:"Michael Robinson",title:"OpenLineage Community Manager",url:"https://github.com/merobi-hub",imageURL:"https://github.com/merobi-hub.png",key:"Robinson"}],frontMatter:{title:"Pursuing Lineage from Airflow using Custom Extractors",date:"2022-09-08T00:00:00.000Z",authors:["Obuchowski","Robinson"],description:"Built-in support for custom extractors makes OpenLineage a highly adaptable solution for pipelines that use Airflow."},prevItem:{title:"At Manta, OpenLineage Opens Doors to New Insights",permalink:"/blog/manta-integration"},nextItem:{title:"How Operators and Extractors Work Under-the-Hook",permalink:"/blog/operators-and-extractors-technical-deep-dive"}},l={authorsImageUrls:[void 0,void 0]},p=[{value:"Overview",id:"overview",level:3},{value:"How it works",id:"how-it-works",level:3},{value:"Example: the RedshiftDataExtractor",id:"example-the-redshiftdataextractor",level:3},{value:"Common issues",id:"common-issues",level:3},{value:"How to get started",id:"how-to-get-started",level:3},{value:"How to contribute",id:"how-to-contribute",level:3}],c={toc:p};function u(e){let{components:t,...a}=e;return(0,o.kt)("wrapper",(0,n.Z)({},c,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"Built-in support for custom extractors makes OpenLineage a highly adaptable solution for pipelines that use Airflow."),(0,o.kt)("h3",{id:"overview"},"Overview"),(0,o.kt)("p",null,"Airflow is built around operators, each having a different function and requiring a different approach to lineage. The OpenLineage Airflow integration detects which Airflow operators your DAG is using and extracts lineage data from them using extractors."),(0,o.kt)("p",null,"The community has already authored a number of extractors to support Airflow\u2019s Great Expectations, BigQuery, Python, Postgres, SQL and Bash operators (and more \u2013 you can find all the extractors ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/OpenLineage/OpenLineage/tree/main/integration/airflow/openlineage/airflow/extractors"},"here"),".) Nevertheless, in the course of pursuing lineage, you may find yourself needing to write custom extractors. "),(0,o.kt)("p",null,"Some teams use custom extractors to automate repeatable work \u2013 using the same code from ",(0,o.kt)("inlineCode",{parentName:"p"},"PythonOperator")," across a project, for example. Another common use case is that a team needs to use an operator for which a pre-built extractor does not yet exist. Airflow has literally hundreds of operators. "),(0,o.kt)("p",null,"Built-in support for custom extractors makes OpenLineage a flexible, highly adaptable solution for pipelines that use Airflow for orchestration. "),(0,o.kt)("h3",{id:"how-it-works"},"How it works"),(0,o.kt)("p",null,"As we explain in the OpenLineage ",(0,o.kt)("a",{parentName:"p",href:"https://openlineage.io/docs/integrations/airflow/extractor"},"docs"),", custom extractors must be derived from the ",(0,o.kt)("inlineCode",{parentName:"p"},"BaseExtractor")," class (import it from ",(0,o.kt)("inlineCode",{parentName:"p"},"openlineage.airflow.extractors.base"),")."),(0,o.kt)("p",null,"Extractors have methods they can implement: ",(0,o.kt)("inlineCode",{parentName:"p"},"extract"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"extract_on_complete")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"get_operator_classnames"),". Either ",(0,o.kt)("inlineCode",{parentName:"p"},"extract")," or ",(0,o.kt)("inlineCode",{parentName:"p"},"extract_on_complete")," is required. The ",(0,o.kt)("inlineCode",{parentName:"p"},"get_operator_classnames")," method, also required, is for providing a list of operators from which your extractor can get lineage."),(0,o.kt)("p",null,"For example:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"@classmethod\ndef get_operator_classnames(cls) -> List[str]:\n  return ['PostgresOperator']\n")),(0,o.kt)("p",null,"If the name of the operator matches one of the names on the list, the extractor will be instantiated \u2013 using the operator passed to the extractor as a parameter and stored in the ",(0,o.kt)("inlineCode",{parentName:"p"},"self.operator")," property \u2013 and both the ",(0,o.kt)("inlineCode",{parentName:"p"},"extract")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"extract_on_complete")," methods will be called. They both return information used by the OpenLineage integration to emit OpenLineage events. The difference is that ",(0,o.kt)("inlineCode",{parentName:"p"},"extract")," is called before the operator's ",(0,o.kt)("inlineCode",{parentName:"p"},"execute")," method to generate a ",(0,o.kt)("inlineCode",{parentName:"p"},"START")," event, while ",(0,o.kt)("inlineCode",{parentName:"p"},"extract_on_complete")," is called afterward to generate a ",(0,o.kt)("inlineCode",{parentName:"p"},"COMPLETE")," event. The latter has access to any additional information that the operator leaves behind following execution. A good example of this is the ",(0,o.kt)("inlineCode",{parentName:"p"},"SnowflakeOperator"),", which sets ",(0,o.kt)("inlineCode",{parentName:"p"},"query_id"),"s after execution."),(0,o.kt)("p",null,"Both methods return a ",(0,o.kt)("inlineCode",{parentName:"p"},"TaskMetadata")," structure:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"@attr.s\nclass TaskMetadata:\n    name: str = attr.ib()  # deprecated\n    inputs: List[Dataset] = attr.ib(factory=list)\n    outputs: List[Dataset] = attr.ib(factory=list)\n    run_facets: Dict[str, BaseFacet] = attr.ib(factory=dict)\n    job_facets: Dict[str, BaseFacet] = attr.ib(factory=dict)\n")),(0,o.kt)("p",null,"The inputs and outputs are lists of plain ",(0,o.kt)("a",{parentName:"p",href:"https://openlineage.io/docs/client/python"},"OpenLineage datasets"),"."),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"run_facets")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"job_facets")," are dictionaries of optional ",(0,o.kt)("a",{parentName:"p",href:"https://openlineage.io/docs/client/python"},(0,o.kt)("inlineCode",{parentName:"a"},"JobFacets"))," and ",(0,o.kt)("a",{parentName:"p",href:"https://openlineage.io/docs/client/python"},(0,o.kt)("inlineCode",{parentName:"a"},"RunFacets"))," that accompany a job. For example, you might want to attach a ",(0,o.kt)("inlineCode",{parentName:"p"},"SqlJobFacet")," if your operator is executing SQL."),(0,o.kt)("p",null,"Note: in order for a custom extractor to work, it must be registered first, so the OpenLineage integration can import it. You can read about how to use environment variables to do this ",(0,o.kt)("a",{parentName:"p",href:"https://openlineage.io/docs/integrations/airflow/extractor#registering-custom-extractor"},"here"),"."),(0,o.kt)("h3",{id:"example-the-redshiftdataextractor"},"Example: the RedshiftDataExtractor"),(0,o.kt)("p",null,"In the ",(0,o.kt)("inlineCode",{parentName:"p"},"RedshiftDataExtractor"),", the ",(0,o.kt)("inlineCode",{parentName:"p"},"extract_on_complete")," method parses SQL, obtains task ",(0,o.kt)("inlineCode",{parentName:"p"},"stats")," using the ",(0,o.kt)("inlineCode",{parentName:"p"},"get_facets")," method of the ",(0,o.kt)("inlineCode",{parentName:"p"},"RedshiftDataDatasetsProvider")," class, and returns a ",(0,o.kt)("inlineCode",{parentName:"p"},"TaskMetadata")," instance. We can see usage of a SQL statement, and the connection is provided by an actual operator. "),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'def extract_on_complete(self, task_instance) -> Optional[TaskMetadata]:\n        log.debug(f"extract_on_complete({task_instance})")\n        job_facets = {"sql": SqlJobFacet(self.operator.sql)}\n\n        log.debug(f"Sending SQL to parser: {self.operator.sql}")\n        sql_meta: Optional[SqlMeta] = parse(self.operator.sql, self.default_schema)\n        log.debug(f"Got meta {sql_meta}")\n        try:\n            redshift_job_id = self._get_xcom_redshift_job_id(task_instance)\n            if redshift_job_id is None:\n                raise Exception(\n                    "Xcom could not resolve Redshift job id. Job may have failed."\n                )\n        except Exception as e:\n            log.error(f"Cannot retrieve job details from {e}", exc_info=True)\n            return TaskMetadata(\n                name=get_job_name(task=self.operator),\n                run_facets={},\n                job_facets=job_facets,\n            )\n\n        client = self.operator.hook.conn\n\n        redshift_details = [\n            "database",\n            "cluster_identifier",\n            "db_user",\n            "secret_arn",\n            "region",\n        ]\n\n        connection_details = {\n            detail: getattr(self.operator, detail) for detail in redshift_details\n        }\n\n        stats = RedshiftDataDatasetsProvider(\n            client=client, connection_details=connection_details\n        ).get_facets(\n            job_id=redshift_job_id,\n            inputs=sql_meta.in_tables if sql_meta else [],\n            outputs=sql_meta.out_tables if sql_meta else [],\n        )\n\n        return TaskMetadata(\n            name=get_job_name(task=self.operator),\n            inputs=[ds.to_openlineage_dataset() for ds in stats.inputs],\n            outputs=[ds.to_openlineage_dataset() for ds in stats.output],\n            run_facets=stats.run_facets,\n            job_facets={"sql": SqlJobFacet(self.operator.sql)},\n        )\n')),(0,o.kt)("h3",{id:"common-issues"},"Common issues"),(0,o.kt)("p",null,"There are two common issues associated with custom extractors. "),(0,o.kt)("p",null,"First, when the wrong path is provided to ",(0,o.kt)("inlineCode",{parentName:"p"},"OPENLINEAGE_EXTRACTORS"),", the extractor isn\u2019t imported and OpenLineage events aren\u2019t emitted. The path needs to be exactly the same as the one you are using in your code. Also, make sure that the extractor code is available to import from Airflow\u2019s Python interpreter."),(0,o.kt)("p",null,"Second, imports from Airflow can be unnoticeably cyclical. This is due to the fact that OpenLineage code gets instantiated when the Airflow worker itself starts, in contrast to DAG code. OpenLineage extraction can fail as a result. To avoid this issue, make sure that all imports from Airflow are local \u2013 in the ",(0,o.kt)("inlineCode",{parentName:"p"},"extract")," or ",(0,o.kt)("inlineCode",{parentName:"p"},"extract_on_complete")," methods. If you need imports for type checking, guard them behind ",(0,o.kt)("inlineCode",{parentName:"p"},"typing.TYPE_CHECKING"),"."),(0,o.kt)("h3",{id:"how-to-get-started"},"How to get started"),(0,o.kt)("p",null,"Check out the existing extractors ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/OpenLineage/OpenLineage/tree/main/integration/airflow/openlineage/airflow/extractors"},"here"),"."),(0,o.kt)("p",null,"Read the docs about the Airflow integration, including tips on registering and debugging your custom extractor, ",(0,o.kt)("a",{parentName:"p",href:"https://openlineage.io/docs/integrations/airflow/"},"here"),". "),(0,o.kt)("h3",{id:"how-to-contribute"},"How to contribute"),(0,o.kt)("p",null,"We welcome your contributions! One of our existing ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/OpenLineage/OpenLineage/tree/main/integration"},"integrations")," might be a good place to start. OpenLineage\u2019s growing list of partners includes Airflow, dbt, Dagster and Flink."),(0,o.kt)("p",null,"Sounds fun? Check out our ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/OpenLineage/OpenLineage/blob/main/CONTRIBUTING.md"},"new contributor guide")," to get started."))}u.isMDXComponent=!0}}]);