"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9939],{3905:(e,a,t)=>{t.d(a,{Zo:()=>p,kt:()=>m});var n=t(67294);function i(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function r(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function o(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?r(Object(t),!0).forEach((function(a){i(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function l(e,a){if(null==e)return{};var t,n,i=function(e,a){if(null==e)return{};var t,n,i={},r=Object.keys(e);for(n=0;n<r.length;n++)t=r[n],a.indexOf(t)>=0||(i[t]=e[t]);return i}(e,a);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)t=r[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var s=n.createContext({}),d=function(e){var a=n.useContext(s),t=a;return e&&(t="function"==typeof e?e(a):o(o({},a),e)),t},p=function(e){var a=d(e.components);return n.createElement(s.Provider,{value:a},e.children)},u={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},c=n.forwardRef((function(e,a){var t=e.components,i=e.mdxType,r=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),c=d(t),m=i,g=c["".concat(s,".").concat(m)]||c[m]||u[m]||r;return t?n.createElement(g,o(o({ref:a},p),{},{components:t})):n.createElement(g,o({ref:a},p))}));function m(e,a){var t=arguments,i=a&&a.mdxType;if("string"==typeof e||i){var r=t.length,o=new Array(r);o[0]=c;var l={};for(var s in a)hasOwnProperty.call(a,s)&&(l[s]=a[s]);l.originalType=e,l.mdxType="string"==typeof e?e:i,o[1]=l;for(var d=2;d<r;d++)o[d]=t[d];return n.createElement.apply(null,o)}return n.createElement.apply(null,t)}c.displayName="MDXCreateElement"},82100:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>s,contentTitle:()=>o,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>d});var n=t(87462),i=(t(67294),t(3905));const r={title:"Backfilling Airflow DAGs using Marquez",date:new Date("2021-06-30T00:00:00.000Z"),authors:["Lulciuc"],description:"In this blog post, we'll discuss how lineage metadata can be used to automatically backfill DAGs with complex upstream and downstream dependencies."},o=void 0,l={permalink:"/blog/backfilling-airflow-dags-using-marquez",source:"@site/blog/backfilling-airflow-dags-using-marquez/index.mdx",title:"Backfilling Airflow DAGs using Marquez",description:"In this blog post, we'll discuss how lineage metadata can be used to automatically backfill DAGs with complex upstream and downstream dependencies.",date:"2021-06-30T00:00:00.000Z",formattedDate:"June 30, 2021",tags:[],readingTime:7.785,hasTruncateMarker:!0,authors:[{name:"Willy Lulciuc",title:"Marquez Project Lead and OpenLineage Committer",url:"https://www.github.com/wslulciuc",imageURL:"https://www.github.com/wslulciuc.png",key:"Lulciuc"}],frontMatter:{title:"Backfilling Airflow DAGs using Marquez",date:"2021-06-30T00:00:00.000Z",authors:["Lulciuc"],description:"In this blog post, we'll discuss how lineage metadata can be used to automatically backfill DAGs with complex upstream and downstream dependencies."},prevItem:{title:"Exploring Lineage History via the Marquez API",permalink:"/blog/explore-lineage-api"},nextItem:{title:"How OpenLineage takes inspiration from OpenTelemetry",permalink:"/blog/openlineage-takes-inspiration-from-opentelemetry"}},s={authorsImageUrls:[void 0]},d=[{value:"1. Brief Intro to Backfilling Airflow DAGs",id:"1-brief-intro-to-backfilling-airflow-dags",level:2},{value:"2. Exploring Lineage Metadata using Marquez",id:"2-exploring-lineage-metadata-using-marquez",level:2},{value:"2.1 COLLECT DAG LINEAGE METADATA",id:"21-collect-dag-lineage-metadata",level:4},{value:"2.2 GET LINEAGE METADATA VIA REST API",id:"22-get-lineage-metadata-via-rest-api",level:4},{value:"REQUEST",id:"request",level:5},{value:"RESPONSE",id:"response",level:5},{value:"3. Using Lineage Metadata to Backfill Airflow DAGs",id:"3-using-lineage-metadata-to-backfill-airflow-dags",level:2},{value:"3.1 BACKFILLING",id:"31-backfilling",level:4},{value:"<code>backfill.sh</code>",id:"backfillsh",level:5},{value:"4. Conclusion",id:"4-conclusion",level:2}],p={toc:d};function u(e){let{components:a,...r}=e;return(0,i.kt)("wrapper",(0,n.Z)({},p,r,{components:a,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"In this blog post, we'll discuss how lineage metadata can be used to automatically backfill DAGs with complex upstream and downstream dependencies."),(0,i.kt)("p",null,"You've just deployed an Airflow ",(0,i.kt)("a",{parentName:"p",href:"https://airflow.apache.org/docs/apache-airflow/stable/concepts/dags.html#dags"},"DAG")," that calculates the total sum of weekly food orders. You were able to identify what input tables to query, the frequency in which your DAG would run, and made sure analysts knew the resulting output table to use in their weekly food order trends report. The DAG only needs to run once a week, and with the DAG managed and scheduled via ",(0,i.kt)("a",{parentName:"p",href:"https://airflow.apache.org"},"Airflow"),", you feel confident that the aggregated food order data will be available every Sunday morning for the weekly report."),(0,i.kt)("p",null,"As a developer, you're monitoring your DAG for errors and after only a few DAG runs, you're alerted that your DAG suddenly started to fail! Before you begin troubleshooting the root cause of the DAG failure, you notify the analytics team that the food order data will be incorrect for the week. After viewing the DAG error logs and a few email exchanges, you eventually discover that an upstream DAG had failed to write food order data for certain daily table partitions. Now, backfilling the missing data can be a manual and tedious task. As you sip your morning coffee, you think to yourself, ",(0,i.kt)("em",{parentName:"p"},"there must be a better way"),". Yes, there is and collecting DAG lineage metadata would be a great start."),(0,i.kt)("p",null,"In this blog post, we'll briefly introduce you to how backfills are handled in Airflow, then discuss how lineage metadata can be used to backfill DAGs with more complex upstream and downstream dependencies."),(0,i.kt)("h2",{id:"1-brief-intro-to-backfilling-airflow-dags"},"1. Brief Intro to Backfilling Airflow DAGs"),(0,i.kt)("p",null,"Airflow supports ",(0,i.kt)("a",{parentName:"p",href:"https://airflow.apache.org/docs/apache-airflow/stable/dag-run.html#backfill"},"backfilling")," DAG runs for a historical time window given a ",(0,i.kt)("em",{parentName:"p"},"start")," and ",(0,i.kt)("em",{parentName:"p"},"end")," date. Let's say our ",(0,i.kt)("inlineCode",{parentName:"p"},"example.etl_orders_7_days")," DAG started failing on ",(0,i.kt)("inlineCode",{parentName:"p"},"2021-06-06"),", and we wanted to reprocess the daily table partitions for that week (assuming all partitions have been backfilled upstream). In order to run the backfill for ",(0,i.kt)("inlineCode",{parentName:"p"},"example.etl_orders_7_days"),", using the Airflow ",(0,i.kt)("a",{parentName:"p",href:"https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html"},"CLI"),", you open up a terminal and execute the following ",(0,i.kt)("inlineCode",{parentName:"p"},"backfill")," ",(0,i.kt)("a",{parentName:"p",href:"https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html#backfill"},"command"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"# Backfill weekly food orders\n$ airflow dags backfill \\\n    --start-date 2021-06-06 \\\n    --end-date 2021-06-06 \\\n    example.etl_orders_7_days\n")),(0,i.kt)("p",null,"Now, the backfill was fairly straightforward but they're not always trivial. That is, we still have the following open questions: "),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"How quickly can data quality issues be identified and explored?"),(0,i.kt)("li",{parentName:"ul"},"What alerting rules should be in place to notify downstream DAGs of possible upstream processing issues or failures?"),(0,i.kt)("li",{parentName:"ul"},"What effects (if any) would upstream DAGs have on downstream DAGs if dataset consumption was delayed?")),(0,i.kt)("p",null,"Next, we'll demonstrate how lineage metadata managed with ",(0,i.kt)("a",{parentName:"p",href:"https://marquezproject.ai"},"Marquez")," can help answer some of these questions (and more!) by maintaining inter-DAG dependencies and cataloging historical runs of DAGs."),(0,i.kt)("h2",{id:"2-exploring-lineage-metadata-using-marquez"},"2. Exploring Lineage Metadata using Marquez"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},(0,i.kt)("strong",{parentName:"p"},"Note:")," To seed the Marquez HTTP API server with the sample lineage metadata used in this blog post, see the ",(0,i.kt)("a",{parentName:"p",href:"https://marquezproject.github.io/marquez/quickstart.html#write-sample-lineage-metadata-to-marquez"},"Write Sample Lineage Metadata to Marquez")," section in Marquez's ",(0,i.kt)("a",{parentName:"p",href:"https://marquezproject.github.io/marquez/quickstart.html"},"quickstart")," guide.")),(0,i.kt)("h4",{id:"21-collect-dag-lineage-metadata"},"2.1 COLLECT DAG LINEAGE METADATA"),(0,i.kt)("p",null,(0,i.kt)("img",{src:t(13476).Z,width:"639",height:"501"})),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},(0,i.kt)("strong",{parentName:"p"},"Figure 1:")," DAG lineage metadata.")),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://marquezproject.ai"},"Marquez")," is an open source metadata service for the collection, aggregation, and visualization of a data ecosystem\u2019s metadata. Marquez has integration support for Airflow with minimal configuration. Using the ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/MarquezProject/marquez/tree/main/integrations/airflow"},(0,i.kt)("inlineCode",{parentName:"a"},"marquez-airflow"))," library, DAG lineage metadata will be collected automatically during DAG execution using the ",(0,i.kt)("a",{parentName:"p",href:"https://openlineage.io"},"OpenLineage")," standard, then stored in Marquez\u2019s centralized data model. To learn more about how lineage metadata is stored and versioned in Marquez, see the ",(0,i.kt)("a",{parentName:"p",href:"https://marquezproject.github.io/marquez/quickstart.html#marquez-data-model"},"Data Model")," section in Marquez's ",(0,i.kt)("a",{parentName:"p",href:"https://marquezproject.github.io/marquez/quickstart.html"},"quickstart")," guide."),(0,i.kt)("p",null,"The Airflow integration gives us two important benefits:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"DAG Metadata:")," Each DAG has a code version, inputs and outputs, run args, and run state transitions. Keeping a global historical log of DAG runs linked to code will quickly highlight upstream dependencies errors and minimize downstream impact."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Lineage Metadata:")," Each DAG may have one or more upstream dependency. Keeping track of inter-DAG dependencies will allow for teams within an organization to safely depend on one another\u2019s datasets, while also understanding which DAGs will be impacted downstream of a DAG failure.")),(0,i.kt)("p",null,"In this blog, we won't go into how to enable lineage metadata collection for Airflow DAGs. But, we encourage you to take a look at Marquez's Airflow ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/MarquezProject/marquez/tree/main/examples/airflow"},"example")," to learn how to troubleshoot DAG failures using Marquez."),(0,i.kt)("h4",{id:"22-get-lineage-metadata-via-rest-api"},"2.2 GET LINEAGE METADATA VIA REST API"),(0,i.kt)("p",null,"In Marquez, each dataset and job has its own globally unique node ID that can be used to query the lineage graph. The ",(0,i.kt)("a",{parentName:"p",href:"https://marquezproject.github.io/marquez/openapi.html#tag/Lineage/paths/~1lineage/get"},"LineageAPI")," returns a set of ",(0,i.kt)("strong",{parentName:"p"},"nodes")," consisting of ",(0,i.kt)("strong",{parentName:"p"},"edges"),". An edge is ",(0,i.kt)("strong",{parentName:"p"},"directed")," and has a defined ",(0,i.kt)("strong",{parentName:"p"},"origin")," and ",(0,i.kt)("strong",{parentName:"p"},"destination"),". A lineage graph may contain the following node types: ",(0,i.kt)("inlineCode",{parentName:"p"},"dataset:<namespace>:<dataset>"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"job:<namespace>:<job>"),"."),(0,i.kt)("p",null,"So, let's start by querying the lineage graph for our ",(0,i.kt)("inlineCode",{parentName:"p"},"example.etl_orders_7_days")," DAG using the node ID ",(0,i.kt)("inlineCode",{parentName:"p"},"job:food_delivery:example.etl_orders_7_days"),". You'll notice in the returned lineage graph that the DAG ",(0,i.kt)("em",{parentName:"p"},"input")," datasets are ",(0,i.kt)("inlineCode",{parentName:"p"},"public.categories"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"public.orders"),", and ",(0,i.kt)("inlineCode",{parentName:"p"},"public.menus")," with ",(0,i.kt)("inlineCode",{parentName:"p"},"public.orders_7_days")," as the ",(0,i.kt)("em",{parentName:"p"},"output")," dataset:"),(0,i.kt)("h5",{id:"request"},"REQUEST"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'$ curl -X GET "http://localhost:5000/api/v1-beta/lineage?nodeId=job:food_delivery:example.etl_orders_7_days"\n')),(0,i.kt)("h5",{id:"response"},"RESPONSE"),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"200 OK")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'{\n  "graph": [{\n    "id": "job:food_delivery:example.etl_orders_7_days",\n    "type": "JOB",\n    "data": {\n      "type": "BATCH",\n      "id": {\n        "namespace": "food_delivery",\n        "name": "example.etl_orders_7_days"\n      },\n      "name": "example.etl_orders_7_days",\n      "createdAt": "2021-06-06T14:50:13.931946Z",\n      "updatedAt": "2021-06-06T14:57:54.037399Z",\n      "namespace": "food_delivery",\n      "inputs": [\n        {"namespace": "food_delivery", "name": "public.categories"},\n        {"namespace": "food_delivery", "name": "public.menu_items"},\n        {"namespace": "food_delivery", "name": "public.orders"},\n        {"namespace": "food_delivery", "name": "public.menus"}\n      ],\n      "outputs": [\n        {"namespace": "food_delivery", "name": "public.orders_7_days"}\n      ],\n      "location": "https://github.com/example/jobs/blob/2294bc15eb49071f38425dc927e48655530a2f2e/etl_orders_7_days.py",\n        "context": {\n          "sql": "INSERT INTO orders_7_days (order_id, placed_on, discount_id, menu_id, restaurant_id, menu_item_id, category_id)\\n  SELECT o.id AS order_id, o.placed_on, o.discount_id, m.id AS menu_id, m.restaurant_id, mi.id AS menu_item_id, c.id AS category_id\\n    FROM orders AS o\\n   INNER JOIN menu_items AS mi\\n      ON menu_items.id = o.menu_item_id\\n   INNER JOIN categories AS c\\n      ON c.id = mi.category_id\\n   INNER JOIN menu AS m\\n      ON m.id = c.menu_id\\n   WHERE o.placed_on >= NOW() - interval \'7 days\';"\n        },\n        "description": "Loads newly placed orders weekly.",\n        "latestRun": {\n          "id": "5c7f0dc4-d3c1-4f16-9ac3-dc86c5da37cc",\n          "createdAt": "2021-06-06T14:50:36.853459Z",\n          "updatedAt": "2021-06-06T14:57:54.037399Z",\n          "nominalStartTime": "2021-06-06T14:54:00Z",\n          "nominalEndTime": "2021-06-06T14:57:00Z",\n          "state": "FAILED",\n          "startedAt": "2021-06-06T14:54:14.037399Z",\n          "endedAt": "2021-06-06T14:57:54.037399Z",\n          "durationMs": 220000,\n          "args": {},\n          "location": "https://github.com/example/jobs/blob/2294bc15eb49071f38425dc927e48655530a2f2e/etl_orders_7_days.py",\n          "context": {\n            "sql": "INSERT INTO orders_7_days (order_id, placed_on, discount_id, menu_id, restaurant_id, menu_item_id, category_id)\\n  SELECT o.id AS order_id, o.placed_on, o.discount_id, m.id AS menu_id, m.restaurant_id, mi.id AS menu_item_id, c.id AS category_id\\n    FROM orders AS o\\n   INNER JOIN menu_items AS mi\\n      ON menu_items.id = o.menu_item_id\\n   INNER JOIN categories AS c\\n      ON c.id = mi.category_id\\n   INNER JOIN menu AS m\\n      ON m.id = c.menu_id\\n   WHERE o.placed_on >= NOW() - interval \'7 days\';"\n          },\n          "facets": {}\n        }\n      },\n      "inEdges": [\n        {"origin": "dataset:food_delivery:public.categories", "destination": "job:food_delivery:example.etl_orders_7_days"}, "destination": "job:food_delivery:example.etl_orders_7_days"},\n        {"origin": "dataset:food_delivery:public.orders", "destination": "job:food_delivery:example.etl_orders_7_days"},\n        {"origin": "dataset:food_delivery:public.menus", "destination": "job:food_delivery:example.etl_orders_7_days"}\n      ],\n      "outEdges": [\n        {"origin": "job:food_delivery:example.etl_orders_7_days", "destination": "dataset:food_delivery:public.orders_7_days"}\n      ]\n    }\n  }, ...]\n}\n')),(0,i.kt)("h2",{id:"3-using-lineage-metadata-to-backfill-airflow-dags"},"3. Using Lineage Metadata to Backfill Airflow DAGs"),(0,i.kt)("h4",{id:"31-backfilling"},"3.1 BACKFILLING"),(0,i.kt)("p",null,(0,i.kt)("img",{src:t(98353).Z,width:"843",height:"532"})),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},(0,i.kt)("strong",{parentName:"p"},"Figure 2:")," Backfilled daily table partitions.")),(0,i.kt)("p",null,"To run a backfill for ",(0,i.kt)("inlineCode",{parentName:"p"},"example.etl_orders_7_days")," using the DAG lineage metadata stored in Marquez, we'll need to query the lineage graph for the upstream DAG where the error originated. Now, let's assume the ",(0,i.kt)("inlineCode",{parentName:"p"},"example.etl_orders")," DAG upstream of ",(0,i.kt)("inlineCode",{parentName:"p"},"example.etl_orders_7_days")," failed to write some of the daily table partitions needed for the weekly food order trends report (see ",(0,i.kt)("strong",{parentName:"p"},"Figure 2"),"). To fix the weekly trends report, we'll first need to backfill the missing daily table partitions ",(0,i.kt)("inlineCode",{parentName:"p"},"public.orders_2021_06_04"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"public.orders_2021_06_05"),", and ",(0,i.kt)("inlineCode",{parentName:"p"},"public.orders_2021_06_06"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"# Backfill daily food orders\n$ airflow dags backfill \\\n    --start-date 2021-06-04 \\\n    --end-date 2021-06-06 \\\n    example.etl_orders\n")),(0,i.kt)("p",null,(0,i.kt)("img",{src:t(52262).Z,width:"958",height:"373"})),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},(0,i.kt)("strong",{parentName:"p"},"Figure 3:")," Airflow inter-DAG dependencies.")),(0,i.kt)("p",null,"Then, using the script ",(0,i.kt)("inlineCode",{parentName:"p"},"backfill.sh")," defined below, we can easily backfill all DAGs downstream of ",(0,i.kt)("inlineCode",{parentName:"p"},"example.etl_orders"),":"),(0,i.kt)("h5",{id:"backfillsh"},(0,i.kt)("inlineCode",{parentName:"h5"},"backfill.sh")),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},(0,i.kt)("strong",{parentName:"p"},"Note:")," Make sure you have ",(0,i.kt)("a",{parentName:"p",href:"https://stedolan.github.io/jq/download"},(0,i.kt)("inlineCode",{parentName:"a"},"jq"))," installed before running ",(0,i.kt)("inlineCode",{parentName:"p"},"backfill.sh"),". ")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},'#!/bin/bash\n#\n# Backfill DAGs automatically using lineage metadata stored in Marquez.\n#\n# Usage: $ ./backfill.sh <start-date> <end-date> <dag-id>\n\nset -e\n\n# Backfills DAGs downstream of the given node ID, recursively.\nbackfill_downstream_of() {\n  node_id="${1}"\n  # Get out edges for node ID\n  out_edges=($(echo $lineage_graph \\\n    | jq -r --arg NODE_ID "${node_id}" \'.graph[] | select(.id==$NODE_ID) | .outEdges[].destination\'))\n  for out_edge in "${out_edges[@]}"; do\n    # Run backfill if out edge is a job node (i.e. <dataset> => <job>)\n    if [[ "${out_edge}" = job:* ]]; then\n      dag_id="${out_edge##*:}"\n      echo "backfilling ${dag_id}..."\n      airflow backfill --start_date "${start_date}" --end_date "${start_date}" "${dag_id}"\n    fi\n    # Follow out edges downstream, recursively\n    backfill_downstream_of "${out_edge}"\n  done\n}\n\nstart_date="${1}"\nend_date="${2}"\ndag_id="${3}"\n\n# (1) Build job node ID (format: \'job:<namespace>:<job>\')\nnode_id="job:food_delivery:${dag_id}"\n\n# (2) Get lineage graph\nlineage_graph=$(curl -s -X GET "http://localhost:5000/api/v1-beta/lineage?nodeId=${node_id}")\n\n# (3) Run backfill\nbackfill_downstream_of "${node_id}"\n')),(0,i.kt)("p",null,"When you run the script ",(0,i.kt)("inlineCode",{parentName:"p"},"backfill.sh"),", it will output all backfilled DAGs to the console:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"$ ./backfill.sh 2021-06-06 2021-06-06 example.etl_orders\nbackfilling example.etl_orders_7_days...\nbackfilling example.etl_delivery_7_days...\nbackfilling example.delivery_times_7_days...\n")),(0,i.kt)("h2",{id:"4-conclusion"},"4. Conclusion"),(0,i.kt)("p",null,"In this blog post, we showed how easy it can be to automate backfilling DAGs downstream of a data quality issue using lineage metadata stored in Marquez. With only two steps, we were able to backfill missing daily table partitions, then automatically re-run failed DAGs downstream of the upstream DAG where the error originated. But, what measures can we put in place to detect low-quality data issues faster, therefore avoiding backfills altogether? Since Marquez collects DAG run metadata that can be viewed using the ",(0,i.kt)("a",{parentName:"p",href:"https://marquezproject.github.io/marquez/openapi.html#tag/Jobs/paths/~1jobs~1runs~1%7Bid%7D/get"},"Runs API"),", building automated processes that periodically check DAG run states and quickly notifying teams of upstream data quality issue (or missed SLAs) in a timely fashion is just one possible preventive measure."),(0,i.kt)("p",null,"We encourge you to explore Marquez's opinionated ",(0,i.kt)("a",{parentName:"p",href:"https://marquezproject.github.io/marquez/openapi.html"},"Metadata API")," and define your own automated process(es) for analyzing lineage metadata! If you need help or have any questions, you can always join our ",(0,i.kt)("a",{parentName:"p",href:"http://bit.ly/MarquezSlack"},"Slack")," channel or reach out to us on ",(0,i.kt)("a",{parentName:"p",href:"https://twitter.com/MarquezProject"},"Twitter"),"."))}u.isMDXComponent=!0},98353:(e,a,t)=>{t.d(a,{Z:()=>n});const n=t.p+"assets/images/backfill-d6459ce4e9a65760a7b441987c74b772.png"},52262:(e,a,t)=>{t.d(a,{Z:()=>n});const n=t.p+"assets/images/inter-dag-deps-c39c00c0fa8120b6aabdd7fa849bb15d.png"},13476:(e,a,t)=>{t.d(a,{Z:()=>n});const n=t.p+"assets/images/lineage-graph-47b55cf38f6030ddd58cf8b5079506cf.png"}}]);