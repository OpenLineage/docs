"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[427],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>h});var n=a(7294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function s(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var l=n.createContext({}),u=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):s(s({},t),e)),a},d=function(e){var t=u(e.components);return n.createElement(l.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},p=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,r=e.originalType,l=e.parentName,d=i(e,["components","mdxType","originalType","parentName"]),p=u(a),h=o,m=p["".concat(l,".").concat(h)]||p[h]||c[h]||r;return a?n.createElement(m,s(s({ref:t},d),{},{components:a})):n.createElement(m,s({ref:t},d))}));function h(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=a.length,s=new Array(r);s[0]=p;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i.mdxType="string"==typeof e?e:o,s[1]=i;for(var u=2;u<r;u++)s[u]=a[u];return n.createElement.apply(null,s)}return n.createElement.apply(null,a)}p.displayName="MDXCreateElement"},1162:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>c,frontMatter:()=>r,metadata:()=>i,toc:()=>u});var n=a(7462),o=(a(7294),a(3905));const r={title:"Using Marquez to Visualize dbt Models",date:new Date("2021-09-21T00:00:00.000Z"),authors:["Turk"],description:"Each time dbt runs, it generates a trove of metadata about datasets and the work it performs with them. In this post, I\u2019d like to show you how to harvest this metadata and put it to good use."},s=void 0,i={permalink:"/blog/dbt-with-marquez",source:"@site/blog/dbt-with-marquez/index.mdx",title:"Using Marquez to Visualize dbt Models",description:"Each time dbt runs, it generates a trove of metadata about datasets and the work it performs with them. In this post, I\u2019d like to show you how to harvest this metadata and put it to good use.",date:"2021-09-21T00:00:00.000Z",formattedDate:"September 21, 2021",tags:[],readingTime:10.095,hasTruncateMarker:!0,authors:[{name:"Ross Turk",title:"OpenLineage Committer",url:"https://www.github.com/rossturk",imageURL:"https://www.github.com/rossturk.png",key:"Turk"}],frontMatter:{title:"Using Marquez to Visualize dbt Models",date:"2021-09-21T00:00:00.000Z",authors:["Turk"],description:"Each time dbt runs, it generates a trove of metadata about datasets and the work it performs with them. In this post, I\u2019d like to show you how to harvest this metadata and put it to good use."},prevItem:{title:"How Northwestern Mutual Simplified Data Observability with OpenLineage & Marquez",permalink:"/blog/openlineage-at-northwestern-mutual"},nextItem:{title:"Introducing OpenLineage 0.1.0",permalink:"/blog/0.1-release"}},l={authorsImageUrls:[void 0]},u=[],d={toc:u};function c(e){let{components:t,...r}=e;return(0,o.kt)("wrapper",(0,n.Z)({},d,r,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"Each time dbt runs, it generates a trove of metadata about datasets and the work it performs with them. In this post, I\u2019d like to show you how to harvest this metadata and put it to good use."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-toc"},"")),(0,o.kt)("p",null,"The first time I built a data warehouse was in a completely different era, even though it wasn\u2019t all that long ago. It was a few dozen tables + a collection of loader scripts and an ETL tool. If I\u2019m honest, calling the whole thing a \u201cdata warehouse\u201d is a bit grandiose, but it worked."),(0,o.kt)("p",null,"At the time, my defining question was \u201chow can I make all of my most important data available for study without spending more than it\u2019s worth?\u201d Because my database capacity wasn\u2019t infinite, I couldn\u2019t keep all of my data forever. The jobs I wrote would pull data from operational data stores, perform a bunch of slicing and aggregation, and load summary data into the warehouse. They shoveled bits every night from one server to another, performing calculations in between - and that meant they had to run on a beefy server with close proximity to my data."),(0,o.kt)("p",null,"Skip forward to the current day and here I am, building and running models from a cafe over pretty shaky wifi. ",(0,o.kt)("strong",{parentName:"p"},"My, how things have changed.")),(0,o.kt)("p",null,"Cloud data warehouses like ",(0,o.kt)("a",{parentName:"p",href:"https://cloud.google.com/bigquery/"},"Google BigQuery"),", ",(0,o.kt)("a",{parentName:"p",href:"https://aws.amazon.com/redshift/"},"Amazon Redshift"),", and ",(0,o.kt)("a",{parentName:"p",href:"https://www.snowflake.com"},"Snowflake")," have created a new economic and technological possibility: we can now pretty much just load everything - including our entire operational data stores - into a single warehouse. Once everything is in one place, data can be sliced up and analyzed much more quickly. This is where ",(0,o.kt)("a",{parentName:"p",href:"https://www.getdbt.com"},"dbt")," shines, at making transformations within a cloud data warehouse easy. And we all know what happens when you make something easy: it finds a way to happen a lot. People are doing more complex transformations than ever before, and the need for lineage context is becoming greater than ever."),(0,o.kt)("p",null,"Fortunately, each time dbt runs it generates a trove of metadata about datasets and the work it performs with them. In this post, I\u2019d like to show you how to harvest this metadata and put it to good use."),(0,o.kt)("h1",{id:"our-example"},"Our Example"),(0,o.kt)("p",null,"For our example, let\u2019s choose the kind of experiment that I might run in my day-to-day life. I\u2019m the head of marketing at ",(0,o.kt)("a",{parentName:"p",href:"https://datakin.com"},"Datakin"),", which means the metrics I\u2019m most interested in are usually about some sort of human behavior."),(0,o.kt)("p",null,"I ask questions like:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Does ","[x]"," technology space matter, and to whom? Is it waxing or waning?"),(0,o.kt)("li",{parentName:"ul"},"Are there adjacent ecosystems we should be collaborating with?"),(0,o.kt)("li",{parentName:"ul"},"Who are the influencers in this space? Who are the major contributors?"),(0,o.kt)("li",{parentName:"ul"},"What challenges are users facing? What does successful adoption look like?")),(0,o.kt)("p",null,"There are a lot of ways to try to answer these questions. None of them are any more reliable than human behavior itself, and every resulting metric requires analysis and judgment. But there are still some pretty fun things to discover. And what better data source to mine to understand technical audiences than ",(0,o.kt)("a",{parentName:"p",href:"https://www.stackoverflow.com"},"Stack Overflow"),"?"),(0,o.kt)("p",null,"So let\u2019s see what we can learn from the Stack Overflow ",(0,o.kt)("a",{parentName:"p",href:"https://cloud.google.com/blog/topics/public-datasets/google-bigquery-public-datasets-now-include-stack-overflow-q-a"},"public data set in BigQuery"),". But not the whole thing; it is very large, so let\u2019s study just a part of it. I created a ",(0,o.kt)("a",{parentName:"p",href:"http://github.com/rossturk/stackostudy/"},"sample dbt project")," that contains a handful of models to study all of the questions and answers we can find about the topic of ELT. These models:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Create slices of the key Stack Overflow tables, pulling them into a separate BigQuery project. These slices only contain the rows that are related to questions tagged with \u201celt\u201d. That way, we can query them tortuously all day long without scanning through gobs of partitions and running up our bill."),(0,o.kt)("li",{parentName:"ul"},"Augment these slices by performing some helpful calculations - in this case, the number of upvotes/downvotes per question."),(0,o.kt)("li",{parentName:"ul"},"Populate two summary tables for consumption by a BI system of some sort: a daily summary table that can be used to study trends and a user summary table that can be used to learn about the most influential contributors.")),(0,o.kt)("p",null,"This is exactly the kind of experiment I have run multiple times over the years, across numerous stacks. It\u2019s usually pretty messy. But this time, after running all of these models, we will be rewarded with a gorgeous ",(0,o.kt)("a",{parentName:"p",href:"https://marquezproject.ai/"},"Marquez")," lineage graph. We\u2019ll be able to see how everything fits together."),(0,o.kt)("h1",{id:"setting-everything-up"},"Setting Everything Up"),(0,o.kt)("p",null,"First, if you haven\u2019t already, run through the excellent ",(0,o.kt)("a",{parentName:"p",href:"https://docs.getdbt.com/tutorial/setting-up"},"dbt tutorial"),". It will show you how to create a BigQuery project, provision a service account, download a JSON key, and set up your local dbt environment. The rest of this example assumes that you have created a BigQuery project where our models can be run, and you know how to properly configure dbt to connect to it."),(0,o.kt)("p",null,"Next, let\u2019s start a local Marquez instance to store our lineage metadata. Make sure you have Docker running, and then:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"git clone https://github.com/MarquezProject/marquez.git && cd marquez\n./docker/up.sh\n")),(0,o.kt)("p",null,"Check to make sure Marquez is up by visiting ",(0,o.kt)("a",{parentName:"p",href:"http://localhost:3000"},"http://localhost:3000"),". You should see an empty Marquez instance with a message saying there isn\u2019t any data. Also, you should be able to see the server output from your requests in the terminal window where Marquez is running. Keep this window open until we\u2019re done."),(0,o.kt)("p",null,"Now, let\u2019s open a new terminal window/pane and clone the GitHub project containing our models:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"git clone https://github.com/rossturk/stackostudy.git && cd stackostudy\n")),(0,o.kt)("p",null,"Next we need to install dbt and its integration with OpenLineage. I like to do this in a Python virtual environment because I make mistakes - as we all do - and I enjoy knowing that I can burn everything down and start over quickly if I need to. Virtual environments make this easy. To create one and install everything we need, run the following commands:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"python -m venv virtualenv\nsource virtualenv/bin/activate\npip install dbt dbt-openlineage\n")),(0,o.kt)("p",null,"dbt learns how to connect to your BigQuery project by looking for a matching profile in ",(0,o.kt)("inlineCode",{parentName:"p"},"~/.dbt/profiles.yml"),". Create or edit this file so it contains a section with your BigQuery connection details. You will need to point to the location of a file containing the JSON key for your service account. If you aren\u2019t sure, you can follow ",(0,o.kt)("a",{parentName:"p",href:"https://docs.getdbt.com/tutorial/create-a-project-dbt-cli#connect-to-bigquery"},"this section")," in the dbt documentation. My ",(0,o.kt)("inlineCode",{parentName:"p"},"profiles.yml")," looked like this when I was done:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"stackostudy:\n  target: dev\n  outputs:\n    dev:\n      type: bigquery\n      method: service-account\n      keyfile: /Users/rturk/.dbt/dbt-example.json\n      project: dbt-example\n      dataset: stackostudy\n      threads: 1\n      timeout_seconds: 300\n      location: US\n      priority: interactive\n")),(0,o.kt)("p",null,"Run ",(0,o.kt)("inlineCode",{parentName:"p"},"dbt debug")," to make sure that you have everything configured correctly."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"% dbt debug\nRunning with dbt=0.20.1\ndbt version: 0.20.1\npython version: 3.8.12\npython path: /opt/homebrew/Cellar/dbt/0.20.1_1/libexec/bin/python3\nos info: macOS-11.5.2-arm64-arm-64bit\nUsing profiles.yml file at /Users/rturk/.dbt/profiles.yml\nUsing dbt_project.yml file at /Users/rturk/projects/stackostudy/dbt_project.yml\n\nConfiguration:\n  profiles.yml file [OK found and valid]\n  dbt_project.yml file [OK found and valid]\n\nRequired dependencies:\n - git [OK found]\n\nConnection:\n  method: service-account\n  database: stacko-study\n  schema: stackostudy\n  location: US\n  priority: interactive\n  timeout_seconds: 300\n  maximum_bytes_billed: None\n  Connection test: OK connection ok\n")),(0,o.kt)("h1",{id:"a-few-important-details"},"A Few Important Details"),(0,o.kt)("p",null,"There are a couple of considerations to make when designing dbt models for use with OpenLineage. By following these conventions, you can help OpenLineage collect the most complete metadata possible."),(0,o.kt)("p",null,"First, when working with datasets outside of your dbt project, define them in a schema YAML file inside the ",(0,o.kt)("inlineCode",{parentName:"p"},"models/")," directory:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"version: 2\n\nsources:\n  - name: stackoverflow\n    database: bigquery-public-data\n    schema: stackoverflow\n    tables:\n      - name: posts_questions\n      - name: posts_answers\n      - name: users\n      - name: votes\n")),(0,o.kt)("p",null,"This contains the name of the external dataset - in this case, ",(0,o.kt)("inlineCode",{parentName:"p"},"bigquery-public-datasets"),", and lists the tables that are used by the models in this project. It doesn\u2019t matter what the file is named, as long as it ends with ",(0,o.kt)("inlineCode",{parentName:"p"},".yml")," and is inside the ",(0,o.kt)("inlineCode",{parentName:"p"},"models/")," directory, so I called mine ",(0,o.kt)("inlineCode",{parentName:"p"},"schema.yml")," \ud83e\udd37\u200d\u2642\ufe0f If you hardcode dataset and table names into your queries instead, dbt will likely run successfully but dataset metadata will be incompletely collected."),(0,o.kt)("p",null,"When writing queries, be sure to use the ",(0,o.kt)("inlineCode",{parentName:"p"},"{{ ref() }}")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"{{ source() }}")," jinja functions when referring to data sources. The ",(0,o.kt)("inlineCode",{parentName:"p"},"{{ ref() }}")," function can be used to refer to tables within the same model, and the ",(0,o.kt)("inlineCode",{parentName:"p"},"{{ source() }}")," function refers to tables we have defined in ",(0,o.kt)("inlineCode",{parentName:"p"},"schema.yml"),". That way, dbt will properly keep track of the relationships between datasets. For example, to select from both an external dataset and one in this model:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sql"},"select * from {{ source('stackoverflow', 'posts_answers') }}\nwhere parent_id in (select id from {{ ref('filtered_questions') }} )\n")),(0,o.kt)("h1",{id:"performing-a-run"},"Performing a Run"),(0,o.kt)("p",null,"Okay! We are ready to perform a run. Before we do, though, there\u2019s one last step we need to take."),(0,o.kt)("p",null,"Run ",(0,o.kt)("inlineCode",{parentName:"p"},"dbt docs generate"),". This will cause dbt to create a ",(0,o.kt)("inlineCode",{parentName:"p"},"target/catalog.json")," file containing the schemas of each dataset referred to in the models. This file will be parsed by the dbt OpenLineage integration and sent to our Marquez server. If it doesn\u2019t exist, a lineage graph will still be generated but schema details won\u2019t be available in Marquez."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"dbt docs generate\nRunning with dbt=0.20.1\nFound 8 models, 0 tests, 0 snapshots, 0 analyses, 164 macros, 0 operations, 0 seed files, 4 sources, 0 exposures\n\n12:15:10 | Concurrency: 1 threads (target='dev')\n12:15:10 |\n12:15:10 | Done.\n12:15:10 | Building catalog\n12:15:26 | Catalog written to /Users/rturk/projects/stackostudy/target/catalog.json\n")),(0,o.kt)("p",null,"The OpenLineage integration for dbt is implemented as a wrapper, ",(0,o.kt)("inlineCode",{parentName:"p"},"dbt-ol"),". This wrapper runs dbt and, after it completes, analyzes the ",(0,o.kt)("inlineCode",{parentName:"p"},"target/catalog.json"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"target/run_results.json")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"target/manifest.json")," files. It sends corresponding OpenLineage events to the endpoint specified in the ",(0,o.kt)("inlineCode",{parentName:"p"},"OPENLINEAGE_URL")," environment variable."),(0,o.kt)("p",null,"To run the models: "),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"% OPENLINEAGE_URL=http://localhost:5000 dbt-ol run\nRunning with dbt=0.20.1\nFound 8 models, 0 tests, 0 snapshots, 0 analyses, 164 macros, 0 operations, 0 seed files, 4 sources, 0 exposures\n\n12:35:41 | Concurrency: 1 threads (target='dev')\n12:35:41 |\n12:35:41 | 1 of 8 START incremental model stackostudy.filtered_questions........ [RUN]\n12:35:46 | 1 of 8 OK created incremental model stackostudy.filtered_questions... [MERGE (0.0 rows, 34.6 GB processed) in 4.52s]\n12:35:46 | 2 of 8 START incremental model stackostudy.filtered_answers.......... [RUN]\n12:35:51 | 2 of 8 OK created incremental model stackostudy.filtered_answers..... [MERGE (0.0 rows, 26.8 GB processed) in 5.22s]\n12:35:51 | 3 of 8 START incremental model stackostudy.filtered_votes............ [RUN]\n12:36:05 | 3 of 8 OK created incremental model stackostudy.filtered_votes....... [MERGE (0.0 rows, 6.5 GB processed) in 14.58s]\n12:36:05 | 4 of 8 START incremental model stackostudy.filtered_users............ [RUN]\n12:36:21 | 4 of 8 OK created incremental model stackostudy.filtered_users....... [MERGE (0.0 rows, 2.5 GB processed) in 16.09s]\n12:36:21 | 5 of 8 START view model stackostudy.summary_daily.................... [RUN]\n12:36:23 | 5 of 8 OK created view model stackostudy.summary_daily............... [OK in 1.01s]\n12:36:23 | 6 of 8 START view model stackostudy.answer_stats..................... [RUN]\n12:36:23 | 6 of 8 OK created view model stackostudy.answer_stats................ [OK in 0.96s]\n12:36:23 | 7 of 8 START view model stackostudy.question_stats................... [RUN]\n12:36:24 | 7 of 8 OK created view model stackostudy.question_stats.............. [OK in 0.88s]\n12:36:24 | 8 of 8 START view model stackostudy.user_stats....................... [RUN]\n12:36:26 | 8 of 8 OK created view model stackostudy.user_stats.................. [OK in 1.21s]\n12:36:26 |\n12:36:26 | Finished running 4 incremental models, 4 view models in 45.39s.\n\nCompleted successfully\n\nDone. PASS=8 WARN=0 ERROR=0 SKIP=0 TOTAL=8\nEmitted 16 openlineage events\n")),(0,o.kt)("p",null,"Note the output showing the number of OpenLineage events emitted to Marquez."),(0,o.kt)("h1",{id:"reviewing-the-output"},"Reviewing the Output"),(0,o.kt)("p",null,"If everything ran successfully you should be able to see a list of jobs when you navigate to http://localhost:3000. Upon clicking a job, you will see a lineage graph that looks similar to this:"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"The stackostudy Marquez lineage graph",src:a(7317).Z,width:"2274",height:"1204"})),(0,o.kt)("p",null,"Our set of models, previously represented by SQL inside text files, has become more easily digestible. The dependencies between datasets are now completely obvious. Data engineers can throw away their remaining whiteboards, hooray!"),(0,o.kt)("p",null,"There\u2019s something satisfying about seeing models represented in two-dimensional space. But more importantly, this integration allows us to capture the state of a dbt pipeline as it runs. Using a long-running instance of Marquez (or another OpenLineage-compatible metadata repository) this information can be studied as it changes over time."),(0,o.kt)("p",null,"To see how the OpenLineage dbt integration works, visit its ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/OpenLineage/OpenLineage/tree/main/integration/dbt"},"GitHub repository"),"."))}c.isMDXComponent=!0},7317:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/graph-3c5ae5c92d39d6f5756806a69afd2809.png"}}]);