"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4715],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>m});var a=n(67294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},d=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),u=p(n),m=i,h=u["".concat(l,".").concat(m)]||u[m]||c[m]||o;return n?a.createElement(h,r(r({ref:t},d),{},{components:n})):a.createElement(h,r({ref:t},d))}));function m(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=n.length,r=new Array(o);r[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:i,r[1]=s;for(var p=2;p<o;p++)r[p]=n[p];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}u.displayName="MDXCreateElement"},28906:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>c,frontMatter:()=>o,metadata:()=>s,toc:()=>p});var a=n(87462),i=(n(67294),n(3905));const o={sidebar_position:2,title:"Integrating with Spark extensions"},r=void 0,s={unversionedId:"development/developing/spark/built_in_lineage",id:"development/developing/spark/built_in_lineage",title:"Integrating with Spark extensions",description:"Feature available since 1.11.",source:"@site/docs/development/developing/spark/built_in_lineage.md",sourceDirName:"development/developing/spark",slug:"/development/developing/spark/built_in_lineage",permalink:"/docs/development/developing/spark/built_in_lineage",draft:!1,editUrl:"https://github.com/OpenLineage/docs/tree/main/docs/development/developing/spark/built_in_lineage.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,title:"Integrating with Spark extensions"},sidebar:"tutorialSidebar",previous:{title:"Build",permalink:"/docs/development/developing/spark/setup"},next:{title:"Example Lineage Events",permalink:"/docs/development/examples"}},l={},p=[{value:"Problem definition",id:"problem-definition",level:2},{value:"Solution",id:"solution",level:2},{value:"Extracting lineage from plan nodes",id:"extracting-lineage-from-plan-nodes",level:2},{value:"The easy way - return all the metadata about dataset",id:"the-easy-way---return-all-the-metadata-about-dataset",level:3},{value:"When extracting dataset name and namespace is non-trivial",id:"when-extracting-dataset-name-and-namespace-is-non-trivial",level:3},{value:"When extension implements a relation within standard LogicalRelation",id:"when-extension-implements-a-relation-within-standard-logicalrelation",level:3},{value:"When extension implements a provider to create relations",id:"when-extension-implements-a-provider-to-create-relations",level:3},{value:"When extension uses Spark DataSource v2 API",id:"when-extension-uses-spark-datasource-v2-api",level:3},{value:"Column Level Lineage",id:"column-level-lineage",level:2}],d={toc:p};function c(e){let{components:t,...n}=e;return(0,i.kt)("wrapper",(0,a.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("admonition",{type:"info"},(0,i.kt)("p",{parentName:"admonition"},"Feature available since 1.11.")),(0,i.kt)("admonition",{type:"info"},(0,i.kt)("p",{parentName:"admonition"},"To get even better lineage coverage for Spark extensions, we recommend implementing lineage extraction\ndirectly within the extensions' code and this page contains documentation on that.")),(0,i.kt)("p",null,"Spark ecosystem comes with a plenty of extensions that affect lineage extraction logic.\n",(0,i.kt)("inlineCode",{parentName:"p"},"spark-interfaces-scala")," package contains Scala traits which can be implemented on the extension's side to\ngenerate high quality metadata for OpenLineage events."),(0,i.kt)("p",null,"In general, a mechanism works in a following way: "),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Package ",(0,i.kt)("inlineCode",{parentName:"li"},"spark-interfaces-scala")," is a simple and lightweight.\nIts only purpose is to contain methods to generate OpenLineage model objects (like facets, datasets) programmatically\nand interfaces' definitions (Scala traits) to expose lineage information from nodes of Spark logical plan."),(0,i.kt)("li",{parentName:"ul"},"Any extension that adds custom node to Spark logical plan can implement the interfaces."),(0,i.kt)("li",{parentName:"ul"},"Spark OpenLineage integration, when traversing logical plan tree, checks if its nodes implement\nthose interfaces and uses their methods to extract lineage metadata from those nodes.")),(0,i.kt)("h2",{id:"problem-definition"},"Problem definition"),(0,i.kt)("p",null,"OpenLineage Spark integration is based on ",(0,i.kt)("inlineCode",{parentName:"p"},"openlineage-spark-*.jar")," library attached\nto a running Spark job. The library traverses Spark logical plan on run state updates to generate\nOpenLineage events. While traversing plan's tree, the library extracts input and output datasets\nas well as other interesting aspects of this particular job, run or datasets involved in the processing.\nExtraction code for each node is contained within ",(0,i.kt)("inlineCode",{parentName:"p"},"openlineage-spark.jar"),"."),(0,i.kt)("p",null,"Two main issues with this approach are:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"Spark ecosystem comes with plenty of extensions and many of them add\ncustom nodes into the logical plan of the query executed.\nThese nodes need to be traversed and understood by ",(0,i.kt)("inlineCode",{parentName:"p"},"openlineage-spark")," to\nextract lineage out of them. This brings serious complexity to the code base. Not only OpenLineage\nhas to cover multiple Spark versions, but also each Spark version supports multiple versions of\nmultiple extensions.")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"Spark extensions know a lot of valuable metadata that can be published within OpenLineage events.\nIt makes sense to allow extensions publish facets on their own. This ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/OpenLineage/OpenLineage/issues/167"},"issue"),"\ncontains great example of useful aspects that can be retrieved from Iceberg extension."))),(0,i.kt)("h2",{id:"solution"},"Solution"),(0,i.kt)("p",null,"A remedy to the problems above is to migrate lineage extraction logic directly to\nSpark ",(0,i.kt)("inlineCode",{parentName:"p"},"LogicalPlan")," nodes. The advantages of this approach are:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"One-to-one version matching")," - there is no further need for a single integration code to support\nmultiple versions of a Spark extension."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Avoid breaking changes")," - this approach limits amount of upgrades that break integration between\n",(0,i.kt)("inlineCode",{parentName:"li"},"openlineage-spark")," and other extensions, as lineage extraction code is directly put into extensions\ncodebase which assures that changes on the Spark extension side are not breaking it.")),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"spark-interfaces-scala")," package contains traits that shall be implemented as well as extra utility\nclasses to let integrate OpenLineage within any Spark extension."),(0,i.kt)("p",null,"Package code should not be shipped with extension that implements traits. Dependency should be marked\nas compile-only. Implementation of the code calling the methods should be responsible for providing\n",(0,i.kt)("inlineCode",{parentName:"p"},"spark-interfaces-scala")," on the classpath."),(0,i.kt)("p",null,"Please note that this package as well as the traits should be considered experimental and may evolve\nin the future. All the current logic has been put into ",(0,i.kt)("inlineCode",{parentName:"p"},"*.scala.v1")," package. First, it is possible\nwe develop the same interfaces in Java. Secondly, in case of non-compatible changes,\nwe are going to release ",(0,i.kt)("inlineCode",{parentName:"p"},"v2")," interfaces. We're aiming to support different versions within spark\nintegration."),(0,i.kt)("h2",{id:"extracting-lineage-from-plan-nodes"},"Extracting lineage from plan nodes"),(0,i.kt)("h3",{id:"the-easy-way---return-all-the-metadata-about-dataset"},"The easy way - return all the metadata about dataset"),(0,i.kt)("p",null,"Spark optimized logical plan is a tree created of ",(0,i.kt)("inlineCode",{parentName:"p"},"LogicalPlan")," nodes. Oftentimes, it is a Spark extension\ninternal class that implements ",(0,i.kt)("inlineCode",{parentName:"p"},"LogicalPlan")," and becomes node within a tree. In this case, it is\nreasonable to implement lineage extraction logic directly within that class."),(0,i.kt)("p",null,"Two interfaces have been prepared:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"io.openlineage.spark.builtin.scala.v1.InputLineageNode")," with ",(0,i.kt)("inlineCode",{parentName:"li"},"getInputs")," method,"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"io.openlineage.spark.builtin.scala.v1.OutputLineageNode")," with ",(0,i.kt)("inlineCode",{parentName:"li"},"getOutputs")," method.")),(0,i.kt)("p",null,"They return list of ",(0,i.kt)("inlineCode",{parentName:"p"},"InputDatasetWithFacets")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"OutputDatasetWithFacets")," respectively. Each trait has methods\nto expose dataset facets as well facets that relate to particular dataset only in the context of\ncurrent run, like amount of bytes read from a certain dataset."),(0,i.kt)("h3",{id:"when-extracting-dataset-name-and-namespace-is-non-trivial"},"When extracting dataset name and namespace is non-trivial"),(0,i.kt)("p",null,"The simple approach is to let the extension provide dataset identifier containing ",(0,i.kt)("inlineCode",{parentName:"p"},"namespace")," as ",(0,i.kt)("inlineCode",{parentName:"p"},"name"),".\nHowever, in some cases this can be cumbersome.\nFor example, within Spark's codebase there are several nodes whose output dataset is\n",(0,i.kt)("inlineCode",{parentName:"p"},"DatasourceV2Relation")," and extracting dataset's ",(0,i.kt)("inlineCode",{parentName:"p"},"name")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"namespace")," from such nodes includes\nnon-trivial logic. In such scenarios, it does not make sense to require an extension to re-implement\nthe logic already present within ",(0,i.kt)("inlineCode",{parentName:"p"},"spark-openlineage")," code. To solve this, the traits introduce datasets\nwith delegates which don't contain exact dataset identifier with name and namespace. Instead, they contain\npointer to other member of the plan where ",(0,i.kt)("inlineCode",{parentName:"p"},"spark-openlineage")," should extract identifier from. "),(0,i.kt)("p",null,"For this scenario, case classes ",(0,i.kt)("inlineCode",{parentName:"p"},"InputDatasetWithDelegate")," and\n",(0,i.kt)("inlineCode",{parentName:"p"},"OutputDatasetWithDelegate")," have been created. They allow assigning facets to a dataset, while\nstill letting other code to extract metadata for the same dataset. The classes contain ",(0,i.kt)("inlineCode",{parentName:"p"},"node")," object\nproperty which defines node within logical plan to contain more metadata about the dataset.\nIn other words, returning a delegate will make OpenLineage Spark integration extract lineage from\nthe delegate and enrich it with facets attached to a delegate."),(0,i.kt)("p",null,"An example implementation for ",(0,i.kt)("inlineCode",{parentName:"p"},"ReplaceIcebergData")," node:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"override def getOutputs(context: OpenLineageContext): List[OutputDatasetWithFacets] = {\n    if (!table.isInstanceOf[DataSourceV2Relation]) {\n      List()\n    } else {\n      val relation = table.asInstanceOf[DataSourceV2Relation]\n      val datasetFacetsBuilder: DatasetFacetsBuilder = {\n        new OpenLineage.DatasetFacetsBuilder()\n          .lifecycleStateChange(\n          context\n            .openLineage\n            .newLifecycleStateChangeDatasetFacet(\n              OpenLineage.LifecycleStateChangeDatasetFacet.LifecycleStateChange.OVERWRITE,\n              null\n            )\n        )\n      }\n      \n      // enrich dataset with additional facets like a dataset version\n      DatasetVersionUtils.getVersionOf(relation) match {\n        case Some(version) => datasetFacetsBuilder.version(\n          context\n            .openLineage\n            .newDatasetVersionDatasetFacet(version)\n        )\n        case None =>\n      }\n\n      // return output dataset while pointing that more dataset details shall be extracted from\n      // `relation` object.\n      List(\n        OutputDatasetWithDelegate(\n          relation,\n          datasetFacetsBuilder,\n          new OpenLineage.OutputDatasetOutputFacetsBuilder()\n        )\n      )\n    }\n  }\n")),(0,i.kt)("h3",{id:"when-extension-implements-a-relation-within-standard-logicalrelation"},"When extension implements a relation within standard LogicalRelation"),(0,i.kt)("p",null,"In this scenario, Spark extension is using standard ",(0,i.kt)("inlineCode",{parentName:"p"},"LogicalRelation")," node within the logical plan.\nHowever, the node may contain extension's specific ",(0,i.kt)("inlineCode",{parentName:"p"},"relation")," property which extends\n",(0,i.kt)("inlineCode",{parentName:"p"},"org.apache.spark.sql.sources.BaseRelation"),". In this case, we allow ",(0,i.kt)("inlineCode",{parentName:"p"},"BaseRelation")," implementation\nto implement ",(0,i.kt)("inlineCode",{parentName:"p"},"io.openlineage.spark.builtin.scala.v1.LineageRelation")," interface."),(0,i.kt)("h3",{id:"when-extension-implements-a-provider-to-create-relations"},"When extension implements a provider to create relations"),(0,i.kt)("p",null,"An extension can contain implementation of ",(0,i.kt)("inlineCode",{parentName:"p"},"org.apache.spark.sql.sources.RelationProvider"),"\nwhich again does not use any custom nodes within the logical plan, but provides classes to\ncreate relations. To support this scenarion, ",(0,i.kt)("inlineCode",{parentName:"p"},"io.openlineage.spark.builtin.scala.v1.LineageDatasetProvider"),"\ncan be implemented. "),(0,i.kt)("h3",{id:"when-extension-uses-spark-datasource-v2-api"},"When extension uses Spark DataSource v2 API"),(0,i.kt)("p",null,"Some extensions rely on Spark DataSource V2 API and implement TableProvider, Table, ScanBuilder etc.\nthat are used within Spark to create ",(0,i.kt)("inlineCode",{parentName:"p"},"DataSourceV2Relation")," instances."),(0,i.kt)("p",null,"A logical plan node ",(0,i.kt)("inlineCode",{parentName:"p"},"DataSourceV2Relation")," contains ",(0,i.kt)("inlineCode",{parentName:"p"},"Table")," field with a properties map of type\n",(0,i.kt)("inlineCode",{parentName:"p"},"Map<String, String>"),". ",(0,i.kt)("inlineCode",{parentName:"p"},"openlineage-spark")," uses this map to extract dataset information for lineage\nevent from ",(0,i.kt)("inlineCode",{parentName:"p"},"DataSourceV2Relation"),". It is checking for the properties ",(0,i.kt)("inlineCode",{parentName:"p"},"openlineage.dataset.name")," and\n",(0,i.kt)("inlineCode",{parentName:"p"},"openlineage.dataset.namespace"),". If they are present, it uses them to identify a dataset. Please\nbe aware that namespace and name need to conform to ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/OpenLineage/OpenLineage/blob/main/spec/Naming.md"},"naming convention"),"."),(0,i.kt)("p",null,"Properties can be also used to pass any dataset facet. For example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'openlineage.dataset.facets.customFacet={"property1": "value1", "property2": "value2"}\n')),(0,i.kt)("p",null,"will enrich dataset with ",(0,i.kt)("inlineCode",{parentName:"p"},"customFacet"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-json"},'"inputs": [{\n"name": "...",\n"namespace": "...",\n"facets": {\n    "customFacet": {\n        "property1": "value1",\n        "property2": "value2",\n        "_producer": "..."\n    },\n    "schema": { }\n}]\n')),(0,i.kt)("h2",{id:"column-level-lineage"},"Column Level Lineage"),(0,i.kt)("p",null,"Lineage is extracted from the optimized logical plan. The plan is a tree with the root being the output\ndataset and leaves the input datasets. In order to collect column level lineage we need to track dependencies between input and output fields."),(0,i.kt)("p",null,"Each node within plan has to understand which input attributes it consumes and how they affect output attributes produced by the node.\nAttribute fields within plan are identified by ",(0,i.kt)("inlineCode",{parentName:"p"},"ExprId"),". In order to build column level lineage,\ndependencies between input and output attributes for each plan's node need to be identified."),(0,i.kt)("p",null,"In order to emit column level lineage from a given spark node, ",(0,i.kt)("inlineCode",{parentName:"p"},"io.openlineage.spark.builtin.scala.v1.ColumnLevelLineageNode"),"\ntrait has to be implemented. The trait should implement following methods"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"def columnLevelLineageInputs(context: OpenLineageContext): List[DatasetFieldLineage]")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"def columnLevelLineageOutputs(context: OpenLineageContext): List[DatasetFieldLineage]")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"columnLevelLineageDependencies(context: OpenLineageContext): List[ExpressionDependency]"))),(0,i.kt)("p",null,"First two methods are used to identify input and output fields as well as matching the fields\nto expressions which use the fields. Returned field lineage can contain identifier, which is mostly\nfield name, but can also be represented by a delegate object pointing to expression where\nthe identifier shall be extracted from."),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"ExpressionDependency")," allows matching, for each Spark plan node, input expressions onto output\nexpressions. Having all the inputs and outputs identified, as well as intermediate dependencies between\nthe expressions used, allow building column level lineage facet."),(0,i.kt)("p",null,"Code below contains an example of ",(0,i.kt)("inlineCode",{parentName:"p"},"ColumnLevelLineageNode")," within Iceberg's ",(0,i.kt)("inlineCode",{parentName:"p"},"MergeRows")," class\nthat implements ",(0,i.kt)("inlineCode",{parentName:"p"},"MERGE INTO")," for Spark 3.4:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"case class MergeRows(\n    ...,\n    matchedOutputs: Seq[Seq[Seq[Expression]]],\n    notMatchedOutputs: Seq[Seq[Expression]],\n    output: Seq[Attribute],\n    child: LogicalPlan\n) extends UnaryNode with ColumnLevelLineageNode {\n  \n    override def columnLevelLineageDependencies(context: OpenLineageContext): List[ExpressionDependency] = {\n      val deps: ListBuffer[ExpressionDependency] = ListBuffer()\n\n      // For each matched and not-matched outputs `ExpressionDependencyWithDelegate` is created\n      // This means for output expression id `attr.exprId.id`, `expr` node needs to be examined to \n      // detect input expression ids. \n      output.zipWithIndex.foreach {\n        case (attr: Attribute, index: Int) =>\n          notMatchedOutputs\n            .toStream\n            .filter(exprs => exprs.size > index)\n            .map(exprs => exprs(index))\n            .foreach(expr => deps += ExpressionDependencyWithDelegate(OlExprId(attr.exprId.id), expr))\n          matchedOutputs\n            .foreach {\n              matched =>\n                matched\n                  .toStream\n                  .filter(exprs => exprs.size > index)\n                  .map(exprs => exprs(index))\n                  .foreach(expr => deps += ExpressionDependencyWithDelegate(OlExprId(attr.exprId.id), expr))\n            }\n      }\n\n      deps.toList\n    }\n\n    override def columnLevelLineageInputs(context: OpenLineageContext): List[DatasetFieldLineage] = {\n      // Delegates input field extraction to other logical plan node\n      List(InputDatasetFieldFromDelegate(child))\n    }\n\n    override def columnLevelLineageOutputs(context: OpenLineageContext): List[DatasetFieldLineage] = {\n      // For each output attribute return its name and ExprId assigned to it.\n      // We're aiming for lineage traits to stay Spark version agnostic and don't want to rely \n      // on Spark classes. That's why `OlExprId` is used to pass `ExprId`\n      output.map(a => OutputDatasetField(a.name, OlExprId(a.exprId.id))).toList\n    }\n  }\n")),(0,i.kt)("p",null,"Please note that ",(0,i.kt)("inlineCode",{parentName:"p"},"ExpressionDependency")," can be extended in the future to contain more information\non how inputs were used to produce a certain output attribute."))}c.isMDXComponent=!0}}]);