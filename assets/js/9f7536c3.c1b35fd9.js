"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1872],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>m});var i=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,i,a=function(e,t){if(null==e)return{};var n,i,a={},r=Object.keys(e);for(i=0;i<r.length;i++)n=r[i],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(i=0;i<r.length;i++)n=r[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=i.createContext({}),p=function(e){var t=i.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=p(e.components);return i.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},u=i.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),u=p(n),m=a,g=u["".concat(l,".").concat(m)]||u[m]||d[m]||r;return n?i.createElement(g,o(o({ref:t},c),{},{components:n})):i.createElement(g,o({ref:t},c))}));function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,o=new Array(r);o[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:a,o[1]=s;for(var p=2;p<r;p++)o[p]=n[p];return i.createElement.apply(null,o)}return i.createElement.apply(null,n)}u.displayName="MDXCreateElement"},66027:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>r,metadata:()=>s,toc:()=>p});var i=n(87462),a=(n(67294),n(3905));const r={title:"Testing",sidebar_position:8},o="Testing",s={unversionedId:"integrations/spark/testing",id:"integrations/spark/testing",title:"Testing",description:"Configurable Integration Test",source:"@site/docs/integrations/spark/testing.md",sourceDirName:"integrations/spark",slug:"/integrations/spark/testing",permalink:"/docs/integrations/spark/testing",draft:!1,editUrl:"https://github.com/OpenLineage/docs/tree/main/docs/integrations/spark/testing.md",tags:[],version:"current",sidebarPosition:8,frontMatter:{title:"Testing",sidebar_position:8},sidebar:"tutorialSidebar",previous:{title:"Spark Integration Metrics",permalink:"/docs/integrations/spark/metrics"},next:{title:"Extending",permalink:"/docs/integrations/spark/extending"}},l={},p=[{value:"Configurable Integration Test",id:"configurable-integration-test",level:2},{value:"Command details",id:"command-details",level:3},{value:"Spark configuration file",id:"spark-configuration-file",level:3},{value:"Tests definition directories",id:"tests-definition-directories",level:3}],c={toc:p};function d(e){let{components:t,...n}=e;return(0,a.kt)("wrapper",(0,i.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"testing"},"Testing"),(0,a.kt)("h2",{id:"configurable-integration-test"},"Configurable Integration Test"),(0,a.kt)("p",null,"Starting of version 1.17, OpenLineage Spark integration provides a command line tooling to help\ncreating custom integration tests. ",(0,a.kt)("inlineCode",{parentName:"p"},"configurable-test.sh")," script can be used to build\n",(0,a.kt)("inlineCode",{parentName:"p"},"openlineage-spark")," from the current directory, script arguments are used to pass Spark\njob. Then, emitted OpenLineage events are validated against JSON files with expected events' fields. Build process and\nintegration test run itself is performed within Docker environment which makes the command",(0,a.kt)("br",{parentName:"p"}),"\n","Java environment agnostic. "),(0,a.kt)("admonition",{type:"info"},(0,a.kt)("p",{parentName:"admonition"},"Quickstart: try running following command from OpenLineage project root directory: "),(0,a.kt)("pre",{parentName:"admonition"},(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"./integration/spark/cli/configurable-test.sh --spark ./integration/spark/cli/spark-conf.yml --test ./integration/spark/cli/tests\n")),(0,a.kt)("p",{parentName:"admonition"},"This should run four integration tests ",(0,a.kt)("inlineCode",{parentName:"p"},"./integration/spark/cli/tests")," and store their output into\n",(0,a.kt)("inlineCode",{parentName:"p"},"./integration/spark/cli/runs"),". Feel free to add extra test directories with custom tests.  ")),(0,a.kt)("p",null,"What's happening when running  ",(0,a.kt)("inlineCode",{parentName:"p"},"configurable-test.sh")," command? "),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"At first, a docker container with Java 11 is created. It builds a docker image ",(0,a.kt)("inlineCode",{parentName:"li"},"openlineage-test:$OPENLINEAGE_VERSION"),". During the build process, all the internal dependencies (like ",(0,a.kt)("inlineCode",{parentName:"li"},"openlineage-java"),") are added to the image. It's because we don't want to build it in each run as it speeds up single command run. In case of subproject changes, a new image has to be built."),(0,a.kt)("li",{parentName:"ul"},"Once the docker image is built, docker container is started and starts gradle ",(0,a.kt)("inlineCode",{parentName:"li"},"configurableIntegrationTest")," task. Task depends on ",(0,a.kt)("inlineCode",{parentName:"li"},"shadowJar")," to build ",(0,a.kt)("inlineCode",{parentName:"li"},"openlineage-spark")," jar. The built jar should be also available on host machine. "),(0,a.kt)("li",{parentName:"ul"},"Gradle test task spawns additional Spark containers which run the Spark job and emit OpenLineage events to local file. A gradle test code has access to mounted event file location, fetches the events emitted and verifies them against expected JSON events. Matching is done through MockServer Json body matching with ",(0,a.kt)("inlineCode",{parentName:"li"},"ONLY_MATCHING_FIELDS")," flag set, as it's happening within other integration tests."),(0,a.kt)("li",{parentName:"ul"},"Test output is written into ",(0,a.kt)("inlineCode",{parentName:"li"},"./integration/spark/cli/runs")," directories with subdirectories containing test definition and file with events that was emitted. ")),(0,a.kt)("admonition",{type:"info"},(0,a.kt)("p",{parentName:"admonition"},"Please be aware that first run of the command will download several gigabytes of docker images being used\nas well as gradle dependencies required to build JAR from the source code. All of them are stored\nwithin Docker volumes, which makes consecutive runs a way faster. ")),(0,a.kt)("h3",{id:"command-details"},"Command details"),(0,a.kt)("p",null,"It is important to run command from the project root directory. This is the only way to let\ncreated Docker containers get mounted volumes containing spark integration code, java client code,\nsql integration code. Command has extra check to verify if work directory is correct."),(0,a.kt)("p",null,"Try running:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"./integration/spark/cli/configurable-test.sh --help\n")),(0,a.kt)("p",null,"to see all the options available within your version. These should include:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"--spark")," - to define spark environment configuration file,"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"--test")," - location for the directory containing tests,"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"--clean")," - flague marking docker image to be re-build from scratch.")),(0,a.kt)("h3",{id:"spark-configuration-file"},"Spark configuration file"),(0,a.kt)("p",null,"This an example Spark environment configuration file:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},'appName: "CLI test application"\nsparkVersion: 3.3.4\nscalaBinaryVersion: 2.12\nenableHiveSupport: true\npackages:\n  - org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.5.2\nsparkConf:\n  spark.openlineage.debugFacet: enabled\n')),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"sparkVersion")," and ",(0,a.kt)("inlineCode",{parentName:"li"},"scalaBinaryVersion")," are used to determine Spark and Scala version to be tested. Spark is run on docker from the images available in\n",(0,a.kt)("a",{parentName:"li",href:"https://quay.io/repository/openlineage/spark?tab=tags"},"https://quay.io/repository/openlineage/spark?tab=tags"),". A combination of Spark and Scala version provided within\nthe config has to match images available."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"appName")," and ",(0,a.kt)("inlineCode",{parentName:"li"},"enableHiveSupport")," parameters are used when starting Spark session."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"sparkConf")," can be used to pass any spark configuration entries. OpenLineage transport defined is file based with a specified file location and is set within the test being run. Those settings should not be overrider. "),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"packages")," lets define custom jar packages to be installed with ",(0,a.kt)("inlineCode",{parentName:"li"},"spark-submit")," command. ")),(0,a.kt)("h3",{id:"tests-definition-directories"},"Tests definition directories"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Specified test directory should contain one ore more directories and each of the subdirectories contains separate test definition. "),(0,a.kt)("li",{parentName:"ul"},"Each test directory should contain a single ",(0,a.kt)("inlineCode",{parentName:"li"},".sql")," or ",(0,a.kt)("inlineCode",{parentName:"li"},".py")," pySpark code file containing a job definition. For ",(0,a.kt)("inlineCode",{parentName:"li"},".sql")," file each line of the file is decorated with ",(0,a.kt)("inlineCode",{parentName:"li"},"spark.sql()")," and transformed into pySpark script.\nFor pySpark scripts, a user should instantiate SparkSession with OpenLineage parameters configured properly. Please refer to existing tests for usage examples. "),(0,a.kt)("li",{parentName:"ul"},"Each test directory should contain on or more event definition file with ",(0,a.kt)("inlineCode",{parentName:"li"},".json")," extensions defining an expected content of any of the events emitted by the job run.")))}d.isMDXComponent=!0}}]);