"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"About OpenLineage","href":"/docs/","docId":"index"},{"type":"link","label":"Getting Started","href":"/docs/getting-started","docId":"getting-started"},{"type":"category","label":"Core Specification","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Object Model","href":"/docs/spec/object-model","docId":"spec/object-model"},{"type":"link","label":"Naming Conventions","href":"/docs/spec/naming","docId":"spec/naming"},{"type":"link","label":"The Run Cycle","href":"/docs/spec/run-cycle","docId":"spec/run-cycle"},{"type":"category","label":"Facets & Extensibility","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Run Facets","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Error Message Facet","href":"/docs/spec/facets/run-facets/error_message","docId":"spec/facets/run-facets/error_message"},{"type":"link","label":"External Query Facet","href":"/docs/spec/facets/run-facets/external_query","docId":"spec/facets/run-facets/external_query"},{"type":"link","label":"Nominal Time Facet","href":"/docs/spec/facets/run-facets/nominal_time","docId":"spec/facets/run-facets/nominal_time"},{"type":"link","label":"Parent Run Facet","href":"/docs/spec/facets/run-facets/parent_run","docId":"spec/facets/run-facets/parent_run"}],"href":"/docs/spec/facets/run-facets/"},{"type":"category","label":"Job Facets","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Documentation Facet","href":"/docs/spec/facets/job-facets/documentation","docId":"spec/facets/job-facets/documentation"},{"type":"link","label":"Ownership Job Facet","href":"/docs/spec/facets/job-facets/ownership","docId":"spec/facets/job-facets/ownership"},{"type":"link","label":"Source Code Facet","href":"/docs/spec/facets/job-facets/source-code","docId":"spec/facets/job-facets/source-code"},{"type":"link","label":"Source Code Location Facet","href":"/docs/spec/facets/job-facets/source-code-location","docId":"spec/facets/job-facets/source-code-location"},{"type":"link","label":"SQL Job Facet","href":"/docs/spec/facets/job-facets/sql","docId":"spec/facets/job-facets/sql"}],"href":"/docs/spec/facets/job-facets/"},{"type":"category","label":"Dataset Facets","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Column Level Lineage Dataset Facet","href":"/docs/spec/facets/dataset-facets/column_lineage_facet","docId":"spec/facets/dataset-facets/column_lineage_facet"},{"type":"link","label":"Datasource Facet","href":"/docs/spec/facets/dataset-facets/data_source","docId":"spec/facets/dataset-facets/data_source"},{"type":"link","label":"Data Quality Assertions Facet","href":"/docs/spec/facets/dataset-facets/data_quality_assertions","docId":"spec/facets/dataset-facets/data_quality_assertions"},{"type":"link","label":"Lifecycle State Change Facet","href":"/docs/spec/facets/dataset-facets/lifecycle_state_change","docId":"spec/facets/dataset-facets/lifecycle_state_change"},{"type":"link","label":"Ownership Dataset Facet","href":"/docs/spec/facets/dataset-facets/ownership","docId":"spec/facets/dataset-facets/ownership"},{"type":"link","label":"Schema Dataset Facet","href":"/docs/spec/facets/dataset-facets/schema","docId":"spec/facets/dataset-facets/schema"},{"type":"link","label":"Storage Facet","href":"/docs/spec/facets/dataset-facets/storage","docId":"spec/facets/dataset-facets/storage"},{"type":"link","label":"Symlinks Facet","href":"/docs/spec/facets/dataset-facets/symlinks","docId":"spec/facets/dataset-facets/symlinks"},{"type":"link","label":"Version Facet","href":"/docs/spec/facets/dataset-facets/version_facet","docId":"spec/facets/dataset-facets/version_facet"},{"type":"category","label":"Input Dataset Facets","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Data Quality Metrics Facet","href":"/docs/spec/facets/dataset-facets/input-dataset-facets/data_quality_metrics","docId":"spec/facets/dataset-facets/input-dataset-facets/data_quality_metrics"}]},{"type":"category","label":"Output Dataset Facets","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Output Statistics Facet","href":"/docs/spec/facets/dataset-facets/output-dataset-facets/output_statistics","docId":"spec/facets/dataset-facets/output-dataset-facets/output_statistics"}]}],"href":"/docs/spec/facets/dataset-facets/"},{"type":"link","label":"Custom Facets","href":"/docs/spec/facets/custom-facets","docId":"spec/facets/custom-facets"}],"href":"/docs/spec/facets/"},{"type":"link","label":"Producers","href":"/docs/spec/producers","docId":"spec/producers"},{"type":"link","label":"Working with Schemas","href":"/docs/spec/schemas","docId":"spec/schemas"}]},{"type":"category","label":"Client Libraries","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Java","href":"/docs/client/java","docId":"client/java"},{"type":"link","label":"Python","href":"/docs/client/python","docId":"client/python"}]},{"type":"category","label":"Integrations","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"OpenLineage Integrations","href":"/docs/integrations/about","docId":"integrations/about"},{"type":"category","label":"Apache Spark","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Quickstart with Jupyter","href":"/docs/integrations/spark/quickstart_local","docId":"integrations/spark/quickstart_local"},{"type":"link","label":"Quickstart with Databricks","href":"/docs/integrations/spark/quickstart_databricks","docId":"integrations/spark/quickstart_databricks"},{"type":"link","label":"Column Level Lineage","href":"/docs/integrations/spark/spark_column_lineage","docId":"integrations/spark/spark_column_lineage"}],"href":"/docs/integrations/spark/"},{"type":"category","label":"Apache Airflow","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Using the Airflow integration","href":"/docs/integrations/airflow/usage","docId":"integrations/airflow/usage"},{"type":"link","label":"Exposing lineage in Airflow operators","href":"/docs/integrations/airflow/operator","docId":"integrations/airflow/operator"},{"type":"link","label":"Manually annotated lineage","href":"/docs/integrations/airflow/manual","docId":"integrations/airflow/manual"},{"type":"link","label":"Using OpenLineage with older versions of Airflow","href":"/docs/integrations/airflow/older","docId":"integrations/airflow/older"},{"type":"category","label":"Extractors","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Custom extractors","href":"/docs/integrations/airflow/extractors/custom-extractors","docId":"integrations/airflow/extractors/custom-extractors"},{"type":"link","label":"Default extractors","href":"/docs/integrations/airflow/extractors/default-extractors","docId":"integrations/airflow/extractors/default-extractors"},{"type":"link","label":"Testing custom extractors","href":"/docs/integrations/airflow/extractors/extractor-testing","docId":"integrations/airflow/extractors/extractor-testing"}]}],"href":"/docs/integrations/airflow/"},{"type":"link","label":"Apache Flink","href":"/docs/integrations/flink","docId":"integrations/flink"},{"type":"link","label":"dbt","href":"/docs/integrations/dbt","docId":"integrations/dbt"},{"type":"link","label":"Great Expectations","href":"/docs/integrations/great-expectations","docId":"integrations/great-expectations"}]},{"type":"category","label":"Guides","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"About These Guides","href":"/docs/guides/about","docId":"guides/about"},{"type":"link","label":"Using OpenLineage with Spark","href":"/docs/guides/spark","docId":"guides/spark"},{"type":"link","label":"Using OpenLineage with Airflow","href":"/docs/guides/airflow","docId":"guides/airflow"},{"type":"link","label":"Backfilling Airflow DAGs Using Marquez","href":"/docs/guides/airflow-backfill-dags","docId":"guides/airflow-backfill-dags"},{"type":"link","label":"Using Marquez with dbt","href":"/docs/guides/dbt","docId":"guides/dbt"},{"type":"link","label":"Understanding and Using Facets","href":"/docs/guides/facets","docId":"guides/facets"}]},{"type":"link","label":"Frequently Asked Questions","href":"/docs/faq","docId":"faq"},{"type":"category","label":"Development","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Developing With OpenLineage","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Python","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Setup a development environment","href":"/docs/development/developing/python/setup","docId":"development/developing/python/setup"},{"type":"category","label":"Tests","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Client","href":"/docs/development/developing/python/tests/client","docId":"development/developing/python/tests/client"},{"type":"link","label":"Airflow","href":"/docs/development/developing/python/tests/airflow","docId":"development/developing/python/tests/airflow"},{"type":"link","label":"Common","href":"/docs/development/developing/python/tests/common","docId":"development/developing/python/tests/common"},{"type":"link","label":"Dagster","href":"/docs/development/developing/python/tests/dagster","docId":"development/developing/python/tests/dagster"}]},{"type":"category","label":"Troubleshooting","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Logging","href":"/docs/development/developing/python/troubleshooting/logging","docId":"development/developing/python/troubleshooting/logging"}]}]},{"type":"category","label":"Java","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Setup a development environment","href":"/docs/development/developing/java/setup","docId":"development/developing/java/setup"},{"type":"category","label":"Troubleshooting","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Logging","href":"/docs/development/developing/java/troubleshooting/logging","docId":"development/developing/java/troubleshooting/logging"}]}]}],"href":"/docs/development/developing/"},{"type":"link","label":"Example Lineage Events","href":"/docs/development/examples","docId":"development/examples"},{"type":"link","label":"OpenLineage Proxy","href":"/docs/development/ol-proxy","docId":"development/ol-proxy"},{"type":"link","label":"SQL parser","href":"/docs/development/sql","docId":"development/sql"}]},{"type":"category","label":"Releases","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"0.20.6","href":"/docs/releases/0_20_6","docId":"releases/0_20_6"},{"type":"link","label":"0.20.4","href":"/docs/releases/0_20_4","docId":"releases/0_20_4"},{"type":"link","label":"0.19.2","href":"/docs/releases/0_19_2","docId":"releases/0_19_2"},{"type":"link","label":"0.18.0","href":"/docs/releases/0_18_0","docId":"releases/0_18_0"},{"type":"link","label":"0.17.0","href":"/docs/releases/0_17_0","docId":"releases/0_17_0"},{"type":"link","label":"0.16.1","href":"/docs/releases/0_16_1","docId":"releases/0_16_1"},{"type":"link","label":"0.15.1","href":"/docs/releases/0_15_1","docId":"releases/0_15_1"},{"type":"link","label":"0.14.1","href":"/docs/releases/0_14_1","docId":"releases/0_14_1"},{"type":"link","label":"0.14.0","href":"/docs/releases/0_14_0","docId":"releases/0_14_0"},{"type":"link","label":"0.13.1","href":"/docs/releases/0_13_1","docId":"releases/0_13_1"},{"type":"link","label":"0.13.0","href":"/docs/releases/0_13_0","docId":"releases/0_13_0"},{"type":"link","label":"0.12.0","href":"/docs/releases/0_12_0","docId":"releases/0_12_0"},{"type":"link","label":"0.11.0","href":"/docs/releases/0_11_0","docId":"releases/0_11_0"},{"type":"link","label":"0.10.0","href":"/docs/releases/0_10_0","docId":"releases/0_10_0"},{"type":"link","label":"0.9.0","href":"/docs/releases/0_9_0","docId":"releases/0_9_0"},{"type":"link","label":"0.8.2","href":"/docs/releases/0_8_2","docId":"releases/0_8_2"},{"type":"link","label":"0.8.1","href":"/docs/releases/0_8_1","docId":"releases/0_8_1"},{"type":"link","label":"0.7.1","href":"/docs/releases/0_7_1","docId":"releases/0_7_1"},{"type":"link","label":"0.6.2","href":"/docs/releases/0_6_2","docId":"releases/0_6_2"},{"type":"link","label":"0.6.1","href":"/docs/releases/0_6_1","docId":"releases/0_6_1"},{"type":"link","label":"0.6.0","href":"/docs/releases/0_6_0","docId":"releases/0_6_0"},{"type":"link","label":"0.5.2","href":"/docs/releases/0_5_2","docId":"releases/0_5_2"},{"type":"link","label":"0.5.1","href":"/docs/releases/0_5_1","docId":"releases/0_5_1"},{"type":"link","label":"0.4.0","href":"/docs/releases/0_4_0","docId":"releases/0_4_0"},{"type":"link","label":"0.3.1","href":"/docs/releases/0_3_1","docId":"releases/0_3_1"},{"type":"link","label":"0.3.0","href":"/docs/releases/0_3_0","docId":"releases/0_3_0"},{"type":"link","label":"0.2.3","href":"/docs/releases/0_2_3","docId":"releases/0_2_3"},{"type":"link","label":"0.2.2","href":"/docs/releases/0_2_2","docId":"releases/0_2_2"},{"type":"link","label":"0.2.1","href":"/docs/releases/0_2_1","docId":"releases/0_2_1"},{"type":"link","label":"0.2.0","href":"/docs/releases/0_2_0","docId":"releases/0_2_0"},{"type":"link","label":"0.1.0","href":"/docs/releases/0_1_0","docId":"releases/0_1_0"}]}]},"docs":{"client/java":{"id":"client/java","title":"Java","description":"The Java client is a SDK for Java programming language that users can use to generate and emit OpenLineage events to OpenLineage backends.","sidebar":"tutorialSidebar"},"client/python":{"id":"client/python","title":"Python","description":"Overview","sidebar":"tutorialSidebar"},"development/developing/developing":{"id":"development/developing/developing","title":"Developing With OpenLineage","description":"As there are hundreds and possibly thousands databases, query engines and other tools you could use to process, create and move data, there\'s great chance that existing OpenLineage integration won\'t cover your needs.","sidebar":"tutorialSidebar"},"development/developing/java/setup":{"id":"development/developing/java/setup","title":"Setup a development environment","description":"This page needs your contribution! Please contribute new examples using the edit link at the bottom.","sidebar":"tutorialSidebar"},"development/developing/java/troubleshooting/logging":{"id":"development/developing/java/troubleshooting/logging","title":"Logging","description":"OpenLineage Java library is based on slf4j when generating logs. Being able to emit logs for various purposes is very helpful when troubleshooting OpenLineage.","sidebar":"tutorialSidebar"},"development/developing/python/setup":{"id":"development/developing/python/setup","title":"Setup a development environment","description":"There are four Python OpenLineage packages that you can install locally when setting up a development environment.","sidebar":"tutorialSidebar"},"development/developing/python/tests/airflow":{"id":"development/developing/python/tests/airflow","title":"Airflow","description":"OpenLineage provides an integration with Apache Airflow. As Airflow is actively developed and major changes happen quite often it is advised to test OpenLineage integration against multiple Airflow versions. In the current CI process OpenLineage is tested against following versions:","sidebar":"tutorialSidebar"},"development/developing/python/tests/client":{"id":"development/developing/python/tests/client","title":"Client","description":"This page needs your contribution! Please contribute new examples using the edit link at the bottom.","sidebar":"tutorialSidebar"},"development/developing/python/tests/common":{"id":"development/developing/python/tests/common","title":"Common","description":"This page needs your contribution! Please contribute new examples using the edit link at the bottom.","sidebar":"tutorialSidebar"},"development/developing/python/tests/dagster":{"id":"development/developing/python/tests/dagster","title":"Dagster","description":"This page needs your contribution! Please contribute new examples using the edit link at the bottom.","sidebar":"tutorialSidebar"},"development/developing/python/troubleshooting/logging":{"id":"development/developing/python/troubleshooting/logging","title":"Logging","description":"OpenLineage uses python\'s logging facility when generating logs. Being able to emit logs for various purposes is very helpful when troubleshooting OpenLineage.","sidebar":"tutorialSidebar"},"development/examples":{"id":"development/examples","title":"Example Lineage Events","description":"Simple Examples","sidebar":"tutorialSidebar"},"development/ol-proxy":{"id":"development/ol-proxy","title":"OpenLineage Proxy","description":"OpenLineage Proxy is a simple Java server that can be used to monitor the JSON events that OpenLineage client emits, as well as tunnel the transmission to the OpenLineage backend such as Marquez.","sidebar":"tutorialSidebar"},"development/sql":{"id":"development/sql","title":"SQL parser","description":"SQL is the most widely used data processing language and for a lot of use cases, getting lineage from SQL-based tasks is solving majority of the problem.","sidebar":"tutorialSidebar"},"faq":{"id":"faq","title":"Frequently Asked Questions","description":"This page needs your contribution! Please contribute new questions (or answers) using the edit link at the bottom.","sidebar":"tutorialSidebar"},"getting-started":{"id":"getting-started","title":"Getting Started","description":"This guide covers how you can quickly get started collecting dataset, job, and run metadata using OpenLineage. We\'ll show how to collect run-level metadata as OpenLineage events using Marquez as the HTTP backend, then explore lineage metadata via the Marquez UI.","sidebar":"tutorialSidebar"},"guides/about":{"id":"guides/about","title":"About These Guides","description":"The following tutorials take you through the process of exploiting the lineage metadata provided by Marquez and OpenLineage to solve common data engineering problems and make new analytical and historical insights into your pipelines.","sidebar":"tutorialSidebar"},"guides/airflow":{"id":"guides/airflow","title":"Using OpenLineage with Airflow","description":"This tutorial introduces you to using OpenLineage with Airflow. OpenLineage has various integrations that will enable airflow to emit OpenLineage events when using Airflow Integrations. In this tutorial, you will be running a local instance of Airflow using docker compose, and learn how to enable and setup OpenLineage to emit data lineage events. The tutorial will use two backends to check the data lineage, 1) using OpenLineage Proxy, and 2) using Marquez.","sidebar":"tutorialSidebar"},"guides/airflow-backfill-dags":{"id":"guides/airflow-backfill-dags","title":"Backfilling Airflow DAGs Using Marquez","description":"Adapted from a blog post by Willy Lulciuc","sidebar":"tutorialSidebar"},"guides/dbt":{"id":"guides/dbt","title":"Using Marquez with dbt","description":"Adapted from a blog post by Ross Turk","sidebar":"tutorialSidebar"},"guides/facets":{"id":"guides/facets","title":"Understanding and Using Facets","description":"Adapted from the OpenLineage spec.","sidebar":"tutorialSidebar"},"guides/spark":{"id":"guides/spark","title":"Using OpenLineage with Spark","description":"Adapted from a blog post by Michael Collado","sidebar":"tutorialSidebar"},"index":{"id":"index","title":"About OpenLineage","description":"OpenLineage is an open framework for data lineage collection and analysis. At its core is an extensible specification that systems can use to interoperate with lineage metadata.","sidebar":"tutorialSidebar"},"integrations/about":{"id":"integrations/about","title":"OpenLineage Integrations","description":"Capability Matrix","sidebar":"tutorialSidebar"},"integrations/airflow/airflow":{"id":"integrations/airflow/airflow","title":"Apache Airflow","description":"Airflow is a widely-used workflow automation and scheduling platform that can be used to author and manage data pipelines. Airflow uses workflows made of directed acyclic graphs (DAGs) of tasks. To learn more about Airflow, check out the Airflow documentation.","sidebar":"tutorialSidebar"},"integrations/airflow/extractors/custom-extractors":{"id":"integrations/airflow/extractors/custom-extractors","title":"Custom extractors","description":"This integration works by detecting which Airflow operators your DAG is using, and extracting lineage data from them using corresponding extractors.","sidebar":"tutorialSidebar"},"integrations/airflow/extractors/default-extractors":{"id":"integrations/airflow/extractors/default-extractors","title":"Default extractors","description":"Default extractors are a new way in Airflow 2.3+ and OpenLineage 0.17.0+ to easily add lineage to your data pipelines by modifying your Airflow operators directly. This means custom operators\u2014built in house or forked from another project\u2014can provide you and your team with lineage data without having to modify the OpenLineage project directly, with data sent to your lineage backend of choice, most commonly using the OPENLINEAGE_URL environment variable.","sidebar":"tutorialSidebar"},"integrations/airflow/extractors/extractor-testing":{"id":"integrations/airflow/extractors/extractor-testing","title":"Testing custom extractors","description":"OpenLineage comes with a variety of extractors for Airflow operators out of the box, but not every operator is covered. And if you are using a custom operator you or your team wrote, you\'ll certainly need to write a custom extractor. This guide will walk you through how to set up testing in a local dev environment, the most important data structures to write tests for, unit testing private functions, and some notes on troubleshooting.","sidebar":"tutorialSidebar"},"integrations/airflow/manual":{"id":"integrations/airflow/manual","title":"Manually annotated lineage","description":"This feature is only supported with Airflow versions greater than 2.1.0)","sidebar":"tutorialSidebar"},"integrations/airflow/older":{"id":"integrations/airflow/older","title":"Using OpenLineage with older versions of Airflow","description":"For Airflow 2.3+ OpenLineage integration automatically registers itself. Nothing else is required besides specifying where OpenLineage events should end up. However, some additional configuration is required for older OpenLineage versions.","sidebar":"tutorialSidebar"},"integrations/airflow/operator":{"id":"integrations/airflow/operator","title":"Exposing lineage in Airflow operators","description":"Since: 0.18.0","sidebar":"tutorialSidebar"},"integrations/airflow/usage":{"id":"integrations/airflow/usage","title":"Using the Airflow integration","description":"PREREQUISITES","sidebar":"tutorialSidebar"},"integrations/dbt":{"id":"integrations/dbt","title":"dbt","description":"dbt (data build tool) is a powerful transformation engine. It operates on data already within a warehouse, making it easy for data engineers to build complex pipelines from the comfort of their laptops. While it doesn\u2019t perform extraction and loading of data, it\u2019s extremely powerful at transformations.","sidebar":"tutorialSidebar"},"integrations/flink":{"id":"integrations/flink","title":"Apache Flink","description":"This integration is considered experimental: only specific workflows and use cases are supported.","sidebar":"tutorialSidebar"},"integrations/great-expectations":{"id":"integrations/great-expectations","title":"Great Expectations","description":"Great Expectations is a robust data quality tool. It runs suites of checks, called expectations, over a defined dataset. This dataset can be a table in a database, or a Spark or Pandas dataframe. Expectations are run by checkpoints, which are configuration files that describe not just the expectations to use, but also any batching, runtime configurations, and, importantly, the action list of actions run after the expectation suite completes.","sidebar":"tutorialSidebar"},"integrations/spark/quickstart_databricks":{"id":"integrations/spark/quickstart_databricks","title":"Quickstart with Databricks","description":"OpenLineage\'s Spark Integration can be installed on Databricks leveraging init scripts. Please note, Databricks on Google Cloud does not currently support the DBFS CLI, so the proposed solution will not work on Google Cloud until that feature is enabled.","sidebar":"tutorialSidebar"},"integrations/spark/quickstart_local":{"id":"integrations/spark/quickstart_local","title":"Quickstart with Jupyter","description":"Trying out the Spark integration is super easy if you already have Docker Desktop and git installed.","sidebar":"tutorialSidebar"},"integrations/spark/spark":{"id":"integrations/spark/spark","title":"Apache Spark","description":"Spark jobs typically run on clusters of machines. A single machine hosts the \\"driver\\" application,","sidebar":"tutorialSidebar"},"integrations/spark/spark_column_lineage":{"id":"integrations/spark/spark_column_lineage","title":"Column Level Lineage","description":"Column level lineage for Spark is turned on by default and requires no additional work to be done. The following documentation describes its internals.","sidebar":"tutorialSidebar"},"releases/0_1_0":{"id":"releases/0_1_0","title":"0.1.0","description":"OpenLineage is an Open Standard for lineage metadata collection designed to record metadata for a job in execution. The initial public release includes:","sidebar":"tutorialSidebar"},"releases/0_10_0":{"id":"releases/0_10_0","title":"0.10.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_11_0":{"id":"releases/0_11_0","title":"0.11.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_12_0":{"id":"releases/0_12_0","title":"0.12.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_13_0":{"id":"releases/0_13_0","title":"0.13.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_13_1":{"id":"releases/0_13_1","title":"0.13.1","description":"Fixed","sidebar":"tutorialSidebar"},"releases/0_14_0":{"id":"releases/0_14_0","title":"0.14.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_14_1":{"id":"releases/0_14_1","title":"0.14.1","description":"Fixed","sidebar":"tutorialSidebar"},"releases/0_15_1":{"id":"releases/0_15_1","title":"0.15.1","description":"Added","sidebar":"tutorialSidebar"},"releases/0_16_1":{"id":"releases/0_16_1","title":"0.16.1","description":"Added","sidebar":"tutorialSidebar"},"releases/0_17_0":{"id":"releases/0_17_0","title":"0.17.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_18_0":{"id":"releases/0_18_0","title":"0.18.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_19_2":{"id":"releases/0_19_2","title":"0.19.2","description":"Added","sidebar":"tutorialSidebar"},"releases/0_2_0":{"id":"releases/0_2_0","title":"0.2.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_2_1":{"id":"releases/0_2_1","title":"0.2.1","description":"Fixed","sidebar":"tutorialSidebar"},"releases/0_2_2":{"id":"releases/0_2_2","title":"0.2.2","description":"Added","sidebar":"tutorialSidebar"},"releases/0_2_3":{"id":"releases/0_2_3","title":"0.2.3","description":"Fixed","sidebar":"tutorialSidebar"},"releases/0_20_4":{"id":"releases/0_20_4","title":"0.20.4","description":"Added","sidebar":"tutorialSidebar"},"releases/0_20_6":{"id":"releases/0_20_6","title":"0.20.6","description":"Added","sidebar":"tutorialSidebar"},"releases/0_3_0":{"id":"releases/0_3_0","title":"0.3.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_3_1":{"id":"releases/0_3_1","title":"0.3.1","description":"Fixed","sidebar":"tutorialSidebar"},"releases/0_4_0":{"id":"releases/0_4_0","title":"0.4.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_5_1":{"id":"releases/0_5_1","title":"0.5.1","description":"Added","sidebar":"tutorialSidebar"},"releases/0_5_2":{"id":"releases/0_5_2","title":"0.5.2","description":"Added","sidebar":"tutorialSidebar"},"releases/0_6_0":{"id":"releases/0_6_0","title":"0.6.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_6_1":{"id":"releases/0_6_1","title":"0.6.1","description":"Fixed","sidebar":"tutorialSidebar"},"releases/0_6_2":{"id":"releases/0_6_2","title":"0.6.2","description":"Added","sidebar":"tutorialSidebar"},"releases/0_7_1":{"id":"releases/0_7_1","title":"0.7.1","description":"Added","sidebar":"tutorialSidebar"},"releases/0_8_1":{"id":"releases/0_8_1","title":"0.8.1","description":"Added","sidebar":"tutorialSidebar"},"releases/0_8_2":{"id":"releases/0_8_2","title":"0.8.2","description":"Added","sidebar":"tutorialSidebar"},"releases/0_9_0":{"id":"releases/0_9_0","title":"0.9.0","description":"Added","sidebar":"tutorialSidebar"},"spec/facets/custom-facets":{"id":"spec/facets/custom-facets","title":"Custom Facets","description":"In addition to the existing facets mentioned in this documentation, users can extend the base facets and provide their own facet definition as part of the payload in OpenLineage event. For example, when OpenLineage event is emitted from the Apache Airflow using OpenLineage\'s Airflow integration, the following facets can be observed:","sidebar":"tutorialSidebar"},"spec/facets/dataset-facets/column_lineage_facet":{"id":"spec/facets/dataset-facets/column_lineage_facet","title":"Column Level Lineage Dataset Facet","description":"Column level lineage provides fine grained information on datasets\' dependencies. Not only we know the dependency exist, but we are also able to understand which input columns are used to produce output columns. This allows answering questions like Which root input columns are used to construct column x?","sidebar":"tutorialSidebar"},"spec/facets/dataset-facets/data_quality_assertions":{"id":"spec/facets/dataset-facets/data_quality_assertions","title":"Data Quality Assertions Facet","description":"Example:","sidebar":"tutorialSidebar"},"spec/facets/dataset-facets/data_source":{"id":"spec/facets/dataset-facets/data_source","title":"Datasource Facet","description":"Example:","sidebar":"tutorialSidebar"},"spec/facets/dataset-facets/dataset-facets":{"id":"spec/facets/dataset-facets/dataset-facets","title":"Dataset Facets","description":"Dataset Facets are generally consisted of common facet that is used both in inputs and outputs of the OpenLineage event. There are facets that exist specifically for input or output datasets.","sidebar":"tutorialSidebar"},"spec/facets/dataset-facets/input-dataset-facets/data_quality_metrics":{"id":"spec/facets/dataset-facets/input-dataset-facets/data_quality_metrics","title":"Data Quality Metrics Facet","description":"Example:","sidebar":"tutorialSidebar"},"spec/facets/dataset-facets/lifecycle_state_change":{"id":"spec/facets/dataset-facets/lifecycle_state_change","title":"Lifecycle State Change Facet","description":"Example:","sidebar":"tutorialSidebar"},"spec/facets/dataset-facets/output-dataset-facets/output_statistics":{"id":"spec/facets/dataset-facets/output-dataset-facets/output_statistics","title":"Output Statistics Facet","description":"Example:","sidebar":"tutorialSidebar"},"spec/facets/dataset-facets/ownership":{"id":"spec/facets/dataset-facets/ownership","title":"Ownership Dataset Facet","description":"Example:","sidebar":"tutorialSidebar"},"spec/facets/dataset-facets/schema":{"id":"spec/facets/dataset-facets/schema","title":"Schema Dataset Facet","description":"The schema dataset facet contains the schema of a particular dataset.","sidebar":"tutorialSidebar"},"spec/facets/dataset-facets/storage":{"id":"spec/facets/dataset-facets/storage","title":"Storage Facet","description":"Example:","sidebar":"tutorialSidebar"},"spec/facets/dataset-facets/symlinks":{"id":"spec/facets/dataset-facets/symlinks","title":"Symlinks Facet","description":"Example:","sidebar":"tutorialSidebar"},"spec/facets/dataset-facets/version_facet":{"id":"spec/facets/dataset-facets/version_facet","title":"Version Facet","description":"Example:","sidebar":"tutorialSidebar"},"spec/facets/facets":{"id":"spec/facets/facets","title":"Facets & Extensibility","description":"Facets provide context to the OpenLineage events. Generally, an OpenLineage event contains the type of the event, who created it, and when the event happened. In addition to the basic information related to the event, it provides facets for more details in four general categories:","sidebar":"tutorialSidebar"},"spec/facets/job-facets/documentation":{"id":"spec/facets/job-facets/documentation","title":"Documentation Facet","description":"Contains the documentation or description of the job.","sidebar":"tutorialSidebar"},"spec/facets/job-facets/job-facets":{"id":"spec/facets/job-facets/job-facets","title":"Job Facets","description":"Job Facets apply to a distinct instance of a job: an abstract process that consumes, executes, and produces datasets (defined as its inputs and outputs). It is identified by a unique name within a namespace. The Job evolves over time and this change is captured during the job runs.","sidebar":"tutorialSidebar"},"spec/facets/job-facets/ownership":{"id":"spec/facets/job-facets/ownership","title":"Ownership Job Facet","description":"The facet that contains the information regarding users or group who owns this particular job.","sidebar":"tutorialSidebar"},"spec/facets/job-facets/source-code":{"id":"spec/facets/job-facets/source-code","title":"Source Code Facet","description":"The source code of a particular job (e.g. Python script)","sidebar":"tutorialSidebar"},"spec/facets/job-facets/source-code-location":{"id":"spec/facets/job-facets/source-code-location","title":"Source Code Location Facet","description":"The facet that indicates where the source code is located.","sidebar":"tutorialSidebar"},"spec/facets/job-facets/sql":{"id":"spec/facets/job-facets/sql","title":"SQL Job Facet","description":"The SQL Job Facet contains a SQL query that was used in a particular job.","sidebar":"tutorialSidebar"},"spec/facets/run-facets/error_message":{"id":"spec/facets/run-facets/error_message","title":"Error Message Facet","description":"The facet to contain information about the failures during the run of the job. A typical payload would be the message, stack trace, etc.","sidebar":"tutorialSidebar"},"spec/facets/run-facets/external_query":{"id":"spec/facets/run-facets/external_query","title":"External Query Facet","description":"The facet that describes the identification of the query that the run is related to which was executed by external systems. Even though the query itself is not contained, using this facet, the user should be able to access the query and its details.","sidebar":"tutorialSidebar"},"spec/facets/run-facets/nominal_time":{"id":"spec/facets/run-facets/nominal_time","title":"Nominal Time Facet","description":"The facet to describe the nominal start and end time of the run. The nominal usually means the time the job run was expected to run (like a scheduled time), and the actual time can be different.","sidebar":"tutorialSidebar"},"spec/facets/run-facets/parent_run":{"id":"spec/facets/run-facets/parent_run","title":"Parent Run Facet","description":"Commonly, scheduler systems like Apache Airflow will trigger processes on remote systems, such as on Apache Spark or Apache Beam jobs.","sidebar":"tutorialSidebar"},"spec/facets/run-facets/run-facets":{"id":"spec/facets/run-facets/run-facets","title":"Run Facets","description":"Run Facets apply to a specifies instance of a particular running job. Every run will have a uniquely identifiable run ID that is usually in UUID format, that can later be tracked.","sidebar":"tutorialSidebar"},"spec/naming":{"id":"spec/naming","title":"Naming Conventions","description":"Employing a unique naming strategy per resource ensures that the spec is followed uniformly regardless of metadata producer.","sidebar":"tutorialSidebar"},"spec/object-model":{"id":"spec/object-model","title":"Object Model","description":"OpenLineage was designed to enable large-scale observation of datasets as they move through a complex pipeline.","sidebar":"tutorialSidebar"},"spec/producers":{"id":"spec/producers","title":"Producers","description":"This page could use some extra detail! You\'re welcome to contribute using the Edit link at the bottom.","sidebar":"tutorialSidebar"},"spec/run-cycle":{"id":"spec/run-cycle","title":"The Run Cycle","description":"The OpenLineage object model is event-based, and each event is called a Run State Update. These updates provide an OpenLineage backend with details about the activities of a Job.","sidebar":"tutorialSidebar"},"spec/schemas":{"id":"spec/schemas","title":"Working with Schemas","description":"OpenLineage is a rapidly growing open source project, and therefore, will face many new changes in its SPEC. The spec file is based on JSON schema specification and defines how the OpenLineage\'s event message would be structured. More details on what are defined in its object model can be found here.","sidebar":"tutorialSidebar"}}}')}}]);