"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9360],{3905:(e,t,a)=>{a.d(t,{Zo:()=>s,kt:()=>d});var n=a(7294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var p=n.createContext({}),m=function(e){var t=n.useContext(p),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},s=function(e){var t=m(e.components);return n.createElement(p.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},k=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,r=e.originalType,p=e.parentName,s=o(e,["components","mdxType","originalType","parentName"]),k=m(a),d=i,u=k["".concat(p,".").concat(d)]||k[d]||c[d]||r;return a?n.createElement(u,l(l({ref:t},s),{},{components:a})):n.createElement(u,l({ref:t},s))}));function d(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=a.length,l=new Array(r);l[0]=k;var o={};for(var p in t)hasOwnProperty.call(t,p)&&(o[p]=t[p]);o.originalType=e,o.mdxType="string"==typeof e?e:i,l[1]=o;for(var m=2;m<r;m++)l[m]=a[m];return n.createElement.apply(null,l)}return n.createElement.apply(null,a)}k.displayName="MDXCreateElement"},8986:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>l,default:()=>c,frontMatter:()=>r,metadata:()=>o,toc:()=>m});var n=a(7462),i=(a(7294),a(3905));const r={title:"1.1.0",sidebar_position:9956},l="1.1.0 - 2023-08-23",o={unversionedId:"releases/1_1_0",id:"releases/1_1_0",title:"1.1.0",description:"Added",source:"@site/docs/releases/1_1_0.md",sourceDirName:"releases",slug:"/releases/1_1_0",permalink:"/docs/releases/1_1_0",draft:!1,editUrl:"https://github.com/OpenLineage/docs/tree/main/docs/releases/1_1_0.md",tags:[],version:"current",sidebarPosition:9956,frontMatter:{title:"1.1.0",sidebar_position:9956},sidebar:"tutorialSidebar",previous:{title:"1.2.2",permalink:"/docs/releases/1_2_2"},next:{title:"1.0.0",permalink:"/docs/releases/1_0_0"}},p={},m=[{value:"Added",id:"added",level:3},{value:"Fixed",id:"fixed",level:3}],s={toc:m};function c(e){let{components:t,...a}=e;return(0,i.kt)("wrapper",(0,n.Z)({},s,a,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"110---2023-08-23"},"1.1.0 - 2023-08-23"),(0,i.kt)("h3",{id:"added"},"Added"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Flink: create Openlineage configuration based on Flink configuration")," ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/OpenLineage/OpenLineage/pull/2033"},(0,i.kt)("inlineCode",{parentName:"a"},"#2033"))," ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/pawel-big-lebowski"},"@pawel-big-lebowski"),(0,i.kt)("br",{parentName:"li"}),"",(0,i.kt)("em",{parentName:"li"},"Flink configuration entries starting with")," ",(0,i.kt)("inlineCode",{parentName:"li"},"openlineage.*")," ",(0,i.kt)("em",{parentName:"li"},"are passed to the Openlineage client.")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Java: add Javadocs to the Java client")," ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/OpenLineage/OpenLineage/pull/2004"},(0,i.kt)("inlineCode",{parentName:"a"},"#2004"))," ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/julienledem"},"@julienledem"),(0,i.kt)("br",{parentName:"li"}),"",(0,i.kt)("em",{parentName:"li"},"The client was missing some Javadocs.")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Spark: append output dataset name to a job name")," ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/OpenLineage/OpenLineage/pull/2036"},(0,i.kt)("inlineCode",{parentName:"a"},"#2036"))," ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/pawel-big-lebowski"},"@pawel-big-lebowski"),(0,i.kt)("br",{parentName:"li"}),"",(0,i.kt)("em",{parentName:"li"},"Solves the problem of multiple jobs writing to different datasets while having the same job name. The feature is enabled by default and results in different job names. It can be disabled by setting ",(0,i.kt)("inlineCode",{parentName:"em"},"spark.openlineage.jobName.appendDatasetName")," to ",(0,i.kt)("inlineCode",{parentName:"em"},"false"),"."),(0,i.kt)("br",{parentName:"li"}),"",(0,i.kt)("em",{parentName:"li"},"Unifies job names generated on the Databricks platform (using a dot job part separator instead of an underscore). The default behaviour can be altered with ",(0,i.kt)("inlineCode",{parentName:"em"},"spark.openlineage.jobName.replaceDotWithUnderscore"),".")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Spark: support Spark 3.4.1")," ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/OpenLineage/OpenLineage/pull/2057"},(0,i.kt)("inlineCode",{parentName:"a"},"#2057"))," ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/pawel-big-lebowski"},"@pawel-big-lebowski"),(0,i.kt)("br",{parentName:"li"}),"",(0,i.kt)("em",{parentName:"li"},"Bumps the latest Spark version to be covered in integration tests."))),(0,i.kt)("h3",{id:"fixed"},"Fixed"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Airflow: do not use database as fallback when no schema parsed")," ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/OpenLineage/OpenLineage/pull/2023"},(0,i.kt)("inlineCode",{parentName:"a"},"#2023"))," ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/mobuchowski"},"@mobuchowski"),(0,i.kt)("br",{parentName:"li"}),"",(0,i.kt)("em",{parentName:"li"},"Sets the schema to ",(0,i.kt)("inlineCode",{parentName:"em"},"None")," in ",(0,i.kt)("inlineCode",{parentName:"em"},"TablesHierarchy")," to skip filtering on the schema level in the information schema query.")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Flink: fix a bug when getting schema for ",(0,i.kt)("inlineCode",{parentName:"strong"},"KafkaSink"))," ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/OpenLineage/OpenLineage/pull/2042"},(0,i.kt)("inlineCode",{parentName:"a"},"#2042"))," ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/pentium3"},"@pentium3"),(0,i.kt)("br",{parentName:"li"}),"",(0,i.kt)("em",{parentName:"li"},"Fixes the incomplete schema from ",(0,i.kt)("inlineCode",{parentName:"em"},"KafkaSinkVisitor")," by changing the ",(0,i.kt)("inlineCode",{parentName:"em"},"KafkaSinkWrapper")," to catch schemas of type ",(0,i.kt)("inlineCode",{parentName:"em"},"AvroSerializationSchema"),".")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Spark: filter ",(0,i.kt)("inlineCode",{parentName:"strong"},"CreateView")," events")," ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/OpenLineage/OpenLineage/pull/1968"},(0,i.kt)("inlineCode",{parentName:"a"},"#1968")),(0,i.kt)("a",{parentName:"li",href:"https://github.com/OpenLineage/OpenLineage/pull/1987"},(0,i.kt)("inlineCode",{parentName:"a"},"#1987"))," ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/pawel-big-lebowski"},"@pawel-big-lebowski"),(0,i.kt)("br",{parentName:"li"}),"",(0,i.kt)("em",{parentName:"li"},"Clears events generated by logical plans having ",(0,i.kt)("inlineCode",{parentName:"em"},"CreateView")," nodes as root.")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Spark: fix ",(0,i.kt)("inlineCode",{parentName:"strong"},"MERGE INTO")," for delta tables identified by physical locations")," ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/OpenLineage/OpenLineage/pull/2026"},(0,i.kt)("inlineCode",{parentName:"a"},"#2026"))," ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/pawel-big-lebowski"},"@pawel-big-lebowski"),(0,i.kt)("br",{parentName:"li"}),"",(0,i.kt)("em",{parentName:"li"},"Delta tables identified by physical locations were not properly recognized.")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Spark: fix incorrect naming of JDBC datasets")," ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/OpenLineage/OpenLineage/pull/2035"},(0,i.kt)("inlineCode",{parentName:"a"},"#2035"))," ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/mobuchowski"},"@mobuchowski"),(0,i.kt)("br",{parentName:"li"}),"",(0,i.kt)("em",{parentName:"li"},"Makes the namespace generated by the JDBC/Spark connector conform to the naming schema in the spec."),"  "),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Spark: fix ignored event ",(0,i.kt)("inlineCode",{parentName:"strong"},"adaptive_spark_plan")," in Databricks")," ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/OpenLineage/OpenLineage/pull/2061"},(0,i.kt)("inlineCode",{parentName:"a"},"#2061"))," ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/algoithmy1"},"@algorithmy1"),(0,i.kt)("br",{parentName:"li"}),"",(0,i.kt)("em",{parentName:"li"},"Removes ",(0,i.kt)("inlineCode",{parentName:"em"},"adaptive_spark_plan")," from the ",(0,i.kt)("inlineCode",{parentName:"em"},"excludedNodes")," in ",(0,i.kt)("inlineCode",{parentName:"em"},"DatabricksEventFilter"),"."))))}c.isMDXComponent=!0}}]);