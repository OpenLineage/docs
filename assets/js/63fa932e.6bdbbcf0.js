"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[6740],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>d});var a=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),m=p(n),d=o,f=m["".concat(l,".").concat(d)]||m[d]||u[d]||i;return n?a.createElement(f,r(r({ref:t},c),{},{components:n})):a.createElement(f,r({ref:t},c))}));function d(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,r=new Array(i);r[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:o,r[1]=s;for(var p=2;p<i;p++)r[p]=n[p];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},17382:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>p});var a=n(87462),o=(n(67294),n(3905));const i={sidebar_position:3,title:"Exposing Lineage in Airflow Operators"},r=void 0,s={unversionedId:"integrations/airflow/default-extractors",id:"integrations/airflow/default-extractors",title:"Exposing Lineage in Airflow Operators",description:"This page is about Airflow's external integration that works mainly for Airflow versions <2.7.",source:"@site/docs/integrations/airflow/default-extractors.md",sourceDirName:"integrations/airflow",slug:"/integrations/airflow/default-extractors",permalink:"/docs/integrations/airflow/default-extractors",draft:!1,editUrl:"https://github.com/OpenLineage/docs/tree/main/docs/integrations/airflow/default-extractors.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3,title:"Exposing Lineage in Airflow Operators"},sidebar:"tutorialSidebar",previous:{title:"Supported Airflow versions",permalink:"/docs/integrations/airflow/older"},next:{title:"Manually Annotated Lineage",permalink:"/docs/integrations/airflow/manual"}},l={},p=[{value:"Implementing lineage in an operator",id:"implementing-lineage-in-an-operator",level:2},{value:"1. <code>DataSourceDatasetFacet</code>",id:"1-datasourcedatasetfacet",level:3},{value:"2. Inputs",id:"2-inputs",level:3},{value:"3. Outputs",id:"3-outputs",level:3},{value:"4. Job facets",id:"4-job-facets",level:3},{value:"5. Run facets",id:"5-run-facets",level:3},{value:"6. On complete",id:"6-on-complete",level:3},{value:"Custom Facets",id:"custom-facets",level:3},{value:"Testing",id:"testing",level:3}],c={toc:p};function u(e){let{components:t,...n}=e;return(0,o.kt)("wrapper",(0,a.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("admonition",{type:"caution"},(0,o.kt)("p",{parentName:"admonition"},"This page is about Airflow's external integration that works mainly for Airflow versions <2.7.\n",(0,o.kt)("a",{parentName:"p",href:"https://airflow.apache.org/docs/apache-airflow-providers-openlineage/stable/index.html"},"If you're using Airflow 2.7+, look at native Airflow OpenLineage provider documentation."),"  ",(0,o.kt)("br",null),(0,o.kt)("br",null)," "),(0,o.kt)("p",{parentName:"admonition"},"The ongoing development and enhancements will be focused on the ",(0,o.kt)("inlineCode",{parentName:"p"},"apache-airflow-providers-openlineage")," package,\nwhile the ",(0,o.kt)("inlineCode",{parentName:"p"},"openlineage-airflow")," will primarily be updated for bug fixes. See ",(0,o.kt)("a",{parentName:"p",href:"/docs/integrations/airflow/older#supported-airflow-versions"},"all Airflow versions supported by this integration"))),(0,o.kt)("p",null,"OpenLineage 0.17.0+ makes adding lineage to your data pipelines easy through support of direct modification of Airflow operators. This means that custom operators\u2014built in-house or forked from another project\u2014can provide you and your team with lineage data without requiring modification of the OpenLineage project. The data will still go to your lineage backend of choice, most commonly using the ",(0,o.kt)("inlineCode",{parentName:"p"},"OPENLINEAGE_URL")," environment variable."),(0,o.kt)("p",null,"Lineage extraction works a bit differently under the hood starting with OpenLineage 0.17.0. While extractors in the OpenLineage project have a getter method for operator names that they\u2019re associated with, the default extractor looks for two specific methods in the operator itself and calls them directly if found. This means that implementation now consists of just two methods in your operator."),(0,o.kt)("p",null,"Those methods are ",(0,o.kt)("inlineCode",{parentName:"p"},"get_openlineage_facets_on_start()")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"get_openlineage_facets_on_complete()"),", called when the operator is first scheduled to run and when the operator has finished execution respectively. Either, or both, of the methods may be implemented by the operator."),(0,o.kt)("p",null,"In the rest of this doc, you will see how to write these methods within an operator class called ",(0,o.kt)("inlineCode",{parentName:"p"},"DfToGcsOperator"),". This operator moves a Dataframe from an arbitrary source table using a supplied Python callable to a specified path in GCS. Thorough understanding of the ",(0,o.kt)("inlineCode",{parentName:"p"},"__init__()")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"execute()")," methods of the operator is not required, but an abbreviated version of each method is given below for context. The final two methods in the class are ",(0,o.kt)("inlineCode",{parentName:"p"},"get_openlineage_facets_on_start()")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"get_openlineage_facets_on_complete()"),", which we will be implementing piece-by-piece in the rest of the doc. They are provided here in their entirety for completeness."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from openlineage.airflow.extractors.base import OperatorLineage\nfrom openlineage.client.facet import (\n    DataSourceDatasetFacet,\n    DocumentationJobFacet,\n    OwnershipJobFacet,\n    OwnershipJobFacetOwners,\n    SchemaDatasetFacet,\n    SchemaField,\n)\nfrom openlineage.client.run import Dataset\n\n\nclass DfToGcsOperator():\n    def __init__(\n        self,\n        task_id,\n        python_callable,\n        data_source,\n        bucket=None,\n        table=None,\n        security_group,\n        pipeline_phase,\n        col_types=None,\n        check_cols=True,\n        **kwargs,\n    ):\n        """Initialize a DfToGcsOperator."""\n        super().__init__(task_id=task_id, **kwargs)\n        self.python_callable = python_callable\n        self.data_source = data_source\n        self.table = table if table is not None else task_id\n        self.bucket = bucket\n        self.security_group = security_group\n        self.pipeline_phase = pipeline_phase\n        # col_types is a dict that stores expected column names and types, \n        self.col_types = col_types\n        self.check_cols = check_cols\n\n        self.base_path = "/".join(\n            [self.security_group, self.pipeline_phase, self.data_source, self.table]\n        )\n        # Holds meta information about the dataframe, col names and col types,\n        # that are used in the extractor.\n        self.df_meta = None\n\n    def execute(self, context):\n        """\n        Run a DfToGcs task.\n\n        The task will run the python_callable and save\n        the resulting dataframe to GCS under the proper object path\n        <security_group>/<pipeline_phase>/<data_source>/<table>/.\n        """\n        ...\n        \n        df = get_python_callable_result(self.python_callable, context)\n        if len(df) > 0:\n            df.columns = [clean_column_name(c) for c in df.columns]\n            if self.col_types and self.check_cols:\n                check_cols = [c.lower().strip() for c in self.col_types.keys()]\n                missing = [m for m in check_cols if m not in df.columns]\n                assert (\n                    len(missing) == 0\n                ), "Columns present in col_types but not in DataFrame: " + ",".join(\n                    missing\n                )\n\n            # ----------- #\n            # Save to GCS #\n            # ----------- #\n\n            # Note: this is an imported helper function.\n            df_to_gcs(df, self.bucket, save_to_path)\n\n            # ----------- #\n            # Return Data #\n            # ----------- #\n\n            # Allow us to extract additional lineage information\n            # about all of the fields available in the dataframe\n            self.df_meta = extract_df_fields(df)\n        else:\n            print("Empty dataframe, no artifact saved to GCS.")\n\n    def extract_df_fields(df):\n        from openlineage.common.dataset import SchemaField\n        """Extract a list of SchemaFields from a DataFrame."""\n        fields = []\n        for (col, dtype) in zip(df.columns, df.dtypes):\n            fields.append(SchemaField(name=col, type=str(dtype)))\n        return fields\n\n    def get_openlineage_facets_on_start(self):\n        """Add lineage to DfToGcsOperator on task start."""\n        if not self.bucket:\n            ol_bucket = get_env_bucket()\n        else:\n            ol_bucket = self.bucket\n\n        input_uri = "://".join([self.data_source, self.table])\n        input_source = DataSourceDatasetFacet(\n            name=self.table,\n            uri=input_uri,\n        )\n\n        input_facet = {\n            "datasource": input_source,\n            "schema": SchemaDatasetFacet(\n                fields=[\n                    SchemaField(name=col_name, type=col_type)\n                    for col_name, col_type in self.col_types.items()\n                ]\n            ),\n        }\n\n        input = Dataset(namespace=self.data_source, name=self.table, facets=input_facet)\n\n        output_namespace = "gs://" + ol_bucket\n        output_name = self.base_path\n        output_uri = "/".join(\n            [\n                output_namespace,\n                output_name,\n            ]\n        )\n\n        output_source = DataSourceDatasetFacet(\n            name=output_name,\n            uri=output_uri,\n        )\n\n        output_facet = {\n            "datasource": output_source,\n            "schema": SchemaDatasetFacet(\n                fields=[\n                    SchemaField(name=col_name, type=col_type)\n                    for col_name, col_type in self.col_types.items()\n                ]\n            ),\n        }\n\n        output = Dataset(\n            namespace=output_namespace,\n            name=output_name,\n            facets=output_facet,\n        )\n\n        return OperatorLineage(\n            inputs=[input],\n            outputs=[output],\n            run_facets={},\n            job_facets={\n                "documentation": DocumentationJobFacet(\n                    description=f"""\n                    Takes data from the data source {input_uri}\n                    and puts it in GCS at the path: {output_uri}\n                    """\n                ),\n                "ownership": OwnershipJobFacet(\n                    owners=[OwnershipJobFacetOwners(name=self.owner, type=self.email)]\n                ),\n            }\n        )\n\n    def get_openlineage_facets_on_complete(self, task_instance):\n        """Add lineage to DfToGcsOperator on task completion."""\n        starting_facets = self.get_openlineage_facets_on_start()\n        if task_instance.task.df_meta is not None:\n            for i in starting_facets.inputs:\n                i.facets["SchemaDatasetFacet"].fields = task_instance.task.df_meta\n        else:\n            starting_facets.run_facets = {\n                "errorMessage": ErrorMessageRunFacet(\n                    message="Empty dataframe, no artifact saved to GCS.",\n                    programmingLanguage="python"\n                )\n            }\n        return starting_facets\n')),(0,o.kt)("h2",{id:"implementing-lineage-in-an-operator"},"Implementing lineage in an operator"),(0,o.kt)("p",null,"Not surprisingly, you will need an operator class to implement lineage collection in an operator. Here, we\u2019ll use the ",(0,o.kt)("inlineCode",{parentName:"p"},"DfToGcsOperator"),", a custom operator created by the Astronomer Data team to load arbitrary dataframes to our GCS bucket. We\u2019ll implement both ",(0,o.kt)("inlineCode",{parentName:"p"},"get_openlineage_facets_on_start()")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"get_openlineage_facets_on_complete()")," for our custom operator. The specific details of the implementation will vary from operator to operator, but there will always be five basic steps that these functions will share."),(0,o.kt)("p",null,"Both the methods return an ",(0,o.kt)("inlineCode",{parentName:"p"},"OperatorLineage")," object, which itself is a collection of facets. Four of the five steps mentioned above are creating these facets where necessary, and the fifth is creating the ",(0,o.kt)("inlineCode",{parentName:"p"},"DataSourceDatasetFacet"),". First, though, we\u2019ll need to import some OpenLineage objects:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from openlineage.airflow.extractors.base import OperatorLineage\nfrom openlineage.client.facet import (\n    DataSourceDatasetFacet,\n    SchemaDatasetFacet,\n    SchemaField,\n)\nfrom openlineage.client.run import Dataset\n")),(0,o.kt)("p",null,"Now, we\u2019ll start building the facets for the ",(0,o.kt)("inlineCode",{parentName:"p"},"OperatorLineage")," object in the ",(0,o.kt)("inlineCode",{parentName:"p"},"get_openlineage_facets_on_start()")," method."),(0,o.kt)("h3",{id:"1-datasourcedatasetfacet"},"1. ",(0,o.kt)("inlineCode",{parentName:"h3"},"DataSourceDatasetFacet")),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"DataSourceDatasestFacet")," is a simple object, containing two fields, ",(0,o.kt)("inlineCode",{parentName:"p"},"name")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"uri"),", which should be populated with the unique name of the data source and the URI. We\u2019ll make two of these objects, an ",(0,o.kt)("inlineCode",{parentName:"p"},"input_source")," to specify where the data came from and an ",(0,o.kt)("inlineCode",{parentName:"p"},"output_source")," to specify where the data is going."),(0,o.kt)("p",null,"A quick note about the philosophy behind the ",(0,o.kt)("inlineCode",{parentName:"p"},"name")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"uri")," in the OpenLineage spec: the ",(0,o.kt)("inlineCode",{parentName:"p"},"uri")," is built from the ",(0,o.kt)("inlineCode",{parentName:"p"},"namespace")," and the ",(0,o.kt)("inlineCode",{parentName:"p"},"name"),", and each is expected to be unique with respect to its environment. This means a ",(0,o.kt)("inlineCode",{parentName:"p"},"namespace")," should be globally unique in the OpenLineage universe, and the ",(0,o.kt)("inlineCode",{parentName:"p"},"name")," unique within the ",(0,o.kt)("inlineCode",{parentName:"p"},"namespace"),". The two are then concatenated to form the ",(0,o.kt)("inlineCode",{parentName:"p"},"uri"),", so that ",(0,o.kt)("inlineCode",{parentName:"p"},"uri = namespace + name"),". The full naming spec can be found ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/OpenLineage/OpenLineage/blob/main/spec/Naming.md"},"here"),"."),(0,o.kt)("p",null,"In our case, the input ",(0,o.kt)("inlineCode",{parentName:"p"},"name")," will be the table we are pulling data from, ",(0,o.kt)("inlineCode",{parentName:"p"},"self.table"),", and the ",(0,o.kt)("inlineCode",{parentName:"p"},"namespace")," will be our ",(0,o.kt)("inlineCode",{parentName:"p"},"self.data_source"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'input_source = DataSourceDatasetFacet(\n    name=self.table,\n    uri="://".join([self.data_source, self.table]),\n)\n')),(0,o.kt)("p",null,"The output data source object\u2019s ",(0,o.kt)("inlineCode",{parentName:"p"},"name")," will always be the base path given to the operator, ",(0,o.kt)("inlineCode",{parentName:"p"},"self.base_path"),". The ",(0,o.kt)("inlineCode",{parentName:"p"},"namespace")," is always in GCS, so we use the OpenLineage spec\u2019s ",(0,o.kt)("inlineCode",{parentName:"p"},"gs://")," as the scheme and our bucket as the authority, giving us ",(0,o.kt)("inlineCode",{parentName:"p"},"gs://{ol_bucket}"),". The ",(0,o.kt)("inlineCode",{parentName:"p"},"uri")," is simply the concatenation of the two."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'if not self.bucket:\n    ol_bucket = get_env_bucket()\nelse:\n    ol_bucket = self.bucket\n\noutput_namespace = "gs://" + ol_bucket\noutput_name = self.base_path\noutput_uri = "/".join(\n    [\n        output_namespace,\n        output_name,\n    ]\n)\n\noutput_source = DataSourceDatasetFacet(\n    name=output_name,\n    uri=output_uri,\n)\n')),(0,o.kt)("h3",{id:"2-inputs"},"2. Inputs"),(0,o.kt)("p",null,"Next we\u2019ll create the input dataset object. As we are moving data from a dataframe to GCS in this operator, we\u2019ll make sure that we are capturing all the info in the dataframe being extracted in a ",(0,o.kt)("inlineCode",{parentName:"p"},"Dataset"),". To create the ",(0,o.kt)("inlineCode",{parentName:"p"},"Dataset")," object, we\u2019ll need ",(0,o.kt)("inlineCode",{parentName:"p"},"namespace"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"name"),", and ",(0,o.kt)("inlineCode",{parentName:"p"},"facets")," objects. The first two are strings, and ",(0,o.kt)("inlineCode",{parentName:"p"},"facets")," is a dictionary."),(0,o.kt)("p",null,"Our ",(0,o.kt)("inlineCode",{parentName:"p"},"namespace")," will come from the operator, where we use ",(0,o.kt)("inlineCode",{parentName:"p"},"self.data_source")," again. The ",(0,o.kt)("inlineCode",{parentName:"p"},"name")," parameter for this facet will be the table, again coming from the operator\u2019s parameter list. The ",(0,o.kt)("inlineCode",{parentName:"p"},"facets")," will contain two entries, the first being our ",(0,o.kt)("inlineCode",{parentName:"p"},"DataSourceDatasetFacet"),' with the key "datasource" coming from the previous step and ',(0,o.kt)("inlineCode",{parentName:"p"},"input_source"),' being the value. The second has the key "schema", with the value being a ',(0,o.kt)("inlineCode",{parentName:"p"},"SchemaDatasetFacet"),", which itself is a collection of ",(0,o.kt)("inlineCode",{parentName:"p"},"SchemaField")," objects, one for each column, created via a list comprehension over the operator's ",(0,o.kt)("inlineCode",{parentName:"p"},"self.col_types")," parameter."),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"inputs")," parameter to ",(0,o.kt)("inlineCode",{parentName:"p"},"OperatorLineage")," is a list of ",(0,o.kt)("inlineCode",{parentName:"p"},"Dataset")," objects, so we\u2019ll end up adding a single ",(0,o.kt)("inlineCode",{parentName:"p"},"Dataset")," object to the list later. The creation of the ",(0,o.kt)("inlineCode",{parentName:"p"},"Dataset")," object looks like the following:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'input_facet = {\n    "datasource": input_source,\n    "schema": SchemaDatasetFacet(\n        fields=[\n            SchemaField(name=col_name, type=col_type)\n            for col_name, col_type in self.col_types.items()\n        ]\n    ),\n}\n\ninput = Dataset(namespace=self.data_source, name=self.table, facets=input_facet)\n')),(0,o.kt)("h3",{id:"3-outputs"},"3. Outputs"),(0,o.kt)("p",null,"Our output facet will closely resemble the input facet, except it will use the ",(0,o.kt)("inlineCode",{parentName:"p"},"output_source")," we previously created, and will also have a different ",(0,o.kt)("inlineCode",{parentName:"p"},"namespace"),". Our output facet object will be built as follows:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'output_facet = {\n    "datasource": output_source,\n    "schema": SchemaDatasetFacet(\n        fields=[\n            SchemaField(name=col_name, type=col_type)\n            for col_name, col_type in self.col_types.items()\n        ]\n    ),\n}\n\noutput = Dataset(\n    namespace=output_namespace,\n    name=output_name,\n    facets=output_facet,\n)\n')),(0,o.kt)("h3",{id:"4-job-facets"},"4. Job facets"),(0,o.kt)("p",null,"A Job in OpenLineage is a process definition that consumes and produces datasets. The Job evolves over time, and this change is captured when the Job runs. This means the facets we would want to capture in the Job level are independent of the state of the Job. Custom facets can be created to capture this Job data. For our operator, we went with pre-existing job facets, the ",(0,o.kt)("inlineCode",{parentName:"p"},"DocumentationJobFacet")," and the ",(0,o.kt)("inlineCode",{parentName:"p"},"OwnershipJobFacet"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'job_facets = {\n    "documentation": DocumentationJobFacet(\n        description=f"""\n            Takes data from the data source {input_uri}\n            and puts it in GCS at the path: {output_uri}\n            """\n    ),\n    "ownership": OwnershipJobFacet(\n        owners=[OwnershipJobFacetOwners(name=self.owner, type=self.email)]\n    )\n}\n')),(0,o.kt)("h3",{id:"5-run-facets"},"5. Run facets"),(0,o.kt)("p",null,"A Run is an instance of a Job execution. For example, when an Airflow Operator begins execution, the Run state of the OpenLineage Job transitions to Start, then to Running. When writing an emitter, this means a Run facet should contain information pertinent to the specific instance of the Job, something that could change every Run."),(0,o.kt)("p",null,"In this example, we will output an error message when there is an empty dataframe, using the existing ",(0,o.kt)("inlineCode",{parentName:"p"},"ErrorMessageRunFacet"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'starting_facets.run_facets = {\n    "errorMessage": ErrorMessageRunFacet(\n        message="Empty dataframe, no artifact saved to GCS.",\n        programmingLanguage="python"\n    )\n}\n')),(0,o.kt)("h3",{id:"6-on-complete"},"6. On complete"),(0,o.kt)("p",null,"Finally, we\u2019ll implement the ",(0,o.kt)("inlineCode",{parentName:"p"},"get_openlineage_metadata_on_complete()")," method. Most of our work has already been done for us, so we will start by calling ",(0,o.kt)("inlineCode",{parentName:"p"},"get_openlineage_metadata_on_start()")," and then modifying the returned object slightly before returning it again. The two main additions here are replacing the original ",(0,o.kt)("inlineCode",{parentName:"p"},"SchemaDatasetFacet")," fields and adding a potential error message to the ",(0,o.kt)("inlineCode",{parentName:"p"},"run_facets"),"."),(0,o.kt)("p",null,"For the ",(0,o.kt)("inlineCode",{parentName:"p"},"SchemaDatasetFacet")," update, we replace the old fields facet with updated ones based on the now-filled-out ",(0,o.kt)("inlineCode",{parentName:"p"},"df_meta")," dict, which is populated during the operator\u2019s ",(0,o.kt)("inlineCode",{parentName:"p"},"execute()")," method and is therefore unavailable to ",(0,o.kt)("inlineCode",{parentName:"p"},"get_openlineage_metadata_on_start()"),". Because ",(0,o.kt)("inlineCode",{parentName:"p"},"df_meta")," is already a list of ",(0,o.kt)("inlineCode",{parentName:"p"},"SchemaField")," objects, we can set the property directly. Although we use a for loop here, the operator ensures only one dataframe will ever be extracted per execution, so the for loop will only ever run once and we therefore do not have to worry about multiple input dataframes updating."),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"run_facets")," update is performed only if there is an error, which is a mutually exclusive event to updating the fields facets. We pass the same message to this facet that is printed in the ",(0,o.kt)("inlineCode",{parentName:"p"},"execute()")," method when an empty dataframe is found. This error message does not halt operator execution, as it gets added ",(0,o.kt)("strong",{parentName:"p"},"*"),"after",(0,o.kt)("strong",{parentName:"p"},"*")," execution, but it does create an alert in the Marquez UI."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'def get_openlineage_facets_on_complete(self, task_instance):\n    """Add lineage to DfToGcsOperator on task completion."""\n    starting_facets = self.get_openlineage_facets_on_start()\n    if task_instance.task.df_meta is not None:\n        for i in starting_facets.inputs:\n            i.facets["SchemaDatasetFacet"].fields = task_instance.task.df_meta\n    else:\n        starting_facets.run_facets = {\n            "errorMessage": ErrorMessageRunFacet(\n                message="Empty dataframe, no artifact saved to GCS.",\n                programmingLanguage="python"\n            )\n        }\n    return starting_facets\n')),(0,o.kt)("p",null,"And with that final piece of the puzzle, we have a working implementation of lineage extraction from our custom operator!"),(0,o.kt)("h3",{id:"custom-facets"},"Custom Facets"),(0,o.kt)("p",null,"The OpenLineage spec might not contain all the facets you need to write your extractor, in which case you will have to make your own ",(0,o.kt)("a",{parentName:"p",href:"https://openlineage.io/docs/spec/facets/custom-facets"},"custom facets"),". More on creating custom facets can be found ",(0,o.kt)("a",{parentName:"p",href:"https://openlineage.io/blog/extending-with-facets/"},"here"),"."),(0,o.kt)("h3",{id:"testing"},"Testing"),(0,o.kt)("p",null,"For information about testing your implementation, see the doc on ",(0,o.kt)("a",{parentName:"p",href:"https://openlineage.io/docs/integrations/airflow/extractors/extractor-testing"},"testing custom extractors"),"."))}u.isMDXComponent=!0}}]);