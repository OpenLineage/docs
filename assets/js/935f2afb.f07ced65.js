"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"About OpenLineage","href":"/docs/","docId":"index"},{"type":"category","label":"Core Specification","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Object Model","href":"/docs/spec/object-model","docId":"spec/object-model"},{"type":"link","label":"Naming Conventions","href":"/docs/spec/naming","docId":"spec/naming"},{"type":"link","label":"The Run Cycle","href":"/docs/spec/run-cycle","docId":"spec/run-cycle"},{"type":"category","label":"Facets & Extensibility","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Run Facets","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Error Message Facet","href":"/docs/spec/facets/run-facets/error_message","docId":"spec/facets/run-facets/error_message"},{"type":"link","label":"External Query Facet","href":"/docs/spec/facets/run-facets/external_query","docId":"spec/facets/run-facets/external_query"},{"type":"link","label":"Nominal Time Facet","href":"/docs/spec/facets/run-facets/nominal_time","docId":"spec/facets/run-facets/nominal_time"},{"type":"link","label":"Parent Run Facet","href":"/docs/spec/facets/run-facets/parent_run","docId":"spec/facets/run-facets/parent_run"}],"href":"/docs/spec/facets/run-facets/"},{"type":"category","label":"Job Facets","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Documentation Facet","href":"/docs/spec/facets/job-facets/documentation","docId":"spec/facets/job-facets/documentation"},{"type":"link","label":"Ownership Job Facet","href":"/docs/spec/facets/job-facets/ownership","docId":"spec/facets/job-facets/ownership"},{"type":"link","label":"Source Code Facet","href":"/docs/spec/facets/job-facets/source-code","docId":"spec/facets/job-facets/source-code"},{"type":"link","label":"Source Code Location Facet","href":"/docs/spec/facets/job-facets/source-code-location","docId":"spec/facets/job-facets/source-code-location"},{"type":"link","label":"SQL Job Facet","href":"/docs/spec/facets/job-facets/sql","docId":"spec/facets/job-facets/sql"},{"type":"link","label":"Job type Job Facet","href":"/docs/spec/facets/job-facets/job-type","docId":"spec/facets/job-facets/job-type"}],"href":"/docs/spec/facets/job-facets/"},{"type":"category","label":"Dataset Facets","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Column Level Lineage Dataset Facet","href":"/docs/spec/facets/dataset-facets/column_lineage_facet","docId":"spec/facets/dataset-facets/column_lineage_facet"},{"type":"link","label":"Datasource Facet","href":"/docs/spec/facets/dataset-facets/data_source","docId":"spec/facets/dataset-facets/data_source"},{"type":"link","label":"Data Quality Assertions Facet","href":"/docs/spec/facets/dataset-facets/data_quality_assertions","docId":"spec/facets/dataset-facets/data_quality_assertions"},{"type":"link","label":"Lifecycle State Change Facet","href":"/docs/spec/facets/dataset-facets/lifecycle_state_change","docId":"spec/facets/dataset-facets/lifecycle_state_change"},{"type":"link","label":"Ownership Dataset Facet","href":"/docs/spec/facets/dataset-facets/ownership","docId":"spec/facets/dataset-facets/ownership"},{"type":"link","label":"Schema Dataset Facet","href":"/docs/spec/facets/dataset-facets/schema","docId":"spec/facets/dataset-facets/schema"},{"type":"link","label":"Storage Facet","href":"/docs/spec/facets/dataset-facets/storage","docId":"spec/facets/dataset-facets/storage"},{"type":"link","label":"Symlinks Facet","href":"/docs/spec/facets/dataset-facets/symlinks","docId":"spec/facets/dataset-facets/symlinks"},{"type":"link","label":"Version Facet","href":"/docs/spec/facets/dataset-facets/version_facet","docId":"spec/facets/dataset-facets/version_facet"},{"type":"category","label":"Input Dataset Facets","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Data Quality Metrics Facet","href":"/docs/spec/facets/dataset-facets/input-dataset-facets/data_quality_metrics","docId":"spec/facets/dataset-facets/input-dataset-facets/data_quality_metrics"}]},{"type":"category","label":"Output Dataset Facets","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Output Statistics Facet","href":"/docs/spec/facets/dataset-facets/output-dataset-facets/output_statistics","docId":"spec/facets/dataset-facets/output-dataset-facets/output_statistics"}]}],"href":"/docs/spec/facets/dataset-facets/"},{"type":"link","label":"Custom Facets","href":"/docs/spec/facets/custom-facets","docId":"spec/facets/custom-facets"}],"href":"/docs/spec/facets/"},{"type":"link","label":"Producers","href":"/docs/spec/producers","docId":"spec/producers"},{"type":"link","label":"Working with Schemas","href":"/docs/spec/schemas","docId":"spec/schemas"},{"type":"link","label":"Job Hierarchy","href":"/docs/spec/job-hierarchy","docId":"spec/job-hierarchy"}]},{"type":"category","label":"Client Libraries","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Java","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Configuration","href":"/docs/client/java/configuration","docId":"client/java/configuration"},{"type":"link","label":"Usage Example","href":"/docs/client/java/usage","docId":"client/java/usage"}],"href":"/docs/client/java/"},{"type":"link","label":"Python","href":"/docs/client/python","docId":"client/python"}]},{"type":"category","label":"Integrations","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"OpenLineage Integrations","href":"/docs/integrations/about","docId":"integrations/about"},{"type":"category","label":"Apache Spark","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Main Concepts","href":"/docs/integrations/spark/main_concept","docId":"integrations/spark/main_concept"},{"type":"category","label":"Configuration","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Usage","href":"/docs/integrations/spark/configuration/usage","docId":"integrations/spark/configuration/usage"},{"type":"link","label":"Spark Config Parameters","href":"/docs/integrations/spark/configuration/spark_conf","docId":"integrations/spark/configuration/spark_conf"},{"type":"link","label":"Transport","href":"/docs/integrations/spark/configuration/transport","docId":"integrations/spark/configuration/transport"},{"type":"link","label":"Circuit Breaker","href":"/docs/integrations/spark/configuration/circuit_breaker","docId":"integrations/spark/configuration/circuit_breaker"},{"type":"link","label":"Scheduling from Airflow","href":"/docs/integrations/spark/configuration/airflow","docId":"integrations/spark/configuration/airflow"}]},{"type":"link","label":"Installation","href":"/docs/integrations/spark/installation","docId":"integrations/spark/installation"},{"type":"category","label":"Quickstart","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Quickstart with Jupyter","href":"/docs/integrations/spark/quickstart/quickstart_local","docId":"integrations/spark/quickstart/quickstart_local"},{"type":"link","label":"Quickstart with Databricks","href":"/docs/integrations/spark/quickstart/quickstart_databricks","docId":"integrations/spark/quickstart/quickstart_databricks"}]},{"type":"link","label":"Column-Level Lineage","href":"/docs/integrations/spark/spark_column_lineage","docId":"integrations/spark/spark_column_lineage"},{"type":"link","label":"Extending","href":"/docs/integrations/spark/extending","docId":"integrations/spark/extending"},{"type":"link","label":"Job Hierarchy","href":"/docs/integrations/spark/job-hierarchy","docId":"integrations/spark/job-hierarchy"},{"type":"link","label":"Spark Integration Metrics","href":"/docs/integrations/spark/metrics","docId":"integrations/spark/metrics"},{"type":"link","label":"Testing","href":"/docs/integrations/spark/testing","docId":"integrations/spark/testing"}],"href":"/docs/integrations/spark/"},{"type":"category","label":"Apache Airflow","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Using the Airflow Integration","href":"/docs/integrations/airflow/usage","docId":"integrations/airflow/usage"},{"type":"link","label":"Supported Airflow versions","href":"/docs/integrations/airflow/older","docId":"integrations/airflow/older"},{"type":"link","label":"Preflight check DAG","href":"/docs/integrations/airflow/preflight-check-dag","docId":"integrations/airflow/preflight-check-dag"},{"type":"link","label":"Exposing Lineage in Airflow Operators","href":"/docs/integrations/airflow/default-extractors","docId":"integrations/airflow/default-extractors"},{"type":"link","label":"Manually Annotated Lineage","href":"/docs/integrations/airflow/manual","docId":"integrations/airflow/manual"},{"type":"category","label":"Extractors","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Custom Extractors","href":"/docs/integrations/airflow/extractors/custom-extractors","docId":"integrations/airflow/extractors/custom-extractors"},{"type":"link","label":"Testing Custom Extractors","href":"/docs/integrations/airflow/extractors/extractor-testing","docId":"integrations/airflow/extractors/extractor-testing"}]},{"type":"link","label":"Job Hierarchy","href":"/docs/integrations/airflow/job-hierarchy","docId":"integrations/airflow/job-hierarchy"}],"href":"/docs/integrations/airflow/"},{"type":"link","label":"Apache Flink","href":"/docs/integrations/flink","docId":"integrations/flink"},{"type":"link","label":"dbt","href":"/docs/integrations/dbt","docId":"integrations/dbt"},{"type":"link","label":"Great Expectations","href":"/docs/integrations/great-expectations","docId":"integrations/great-expectations"},{"type":"link","label":"Trino","href":"/docs/integrations/trino","docId":"integrations/trino"}]},{"type":"category","label":"Guides","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"About These Guides","href":"/docs/guides/about","docId":"guides/about"},{"type":"link","label":"Getting Started with Airflow and OpenLineage+Marquez","href":"/docs/guides/airflow-quickstart","docId":"guides/airflow-quickstart"},{"type":"link","label":"Using OpenLineage with Spark","href":"/docs/guides/spark","docId":"guides/spark"},{"type":"link","label":"Backfilling Airflow DAGs Using Marquez","href":"/docs/guides/airflow-backfill-dags","docId":"guides/airflow-backfill-dags"},{"type":"link","label":"Using Marquez with dbt","href":"/docs/guides/dbt","docId":"guides/dbt"},{"type":"link","label":"Understanding and Using Facets","href":"/docs/guides/facets","docId":"guides/facets"},{"type":"link","label":"Using the OpenLineage Proxy with Airflow","href":"/docs/guides/airflow_proxy","docId":"guides/airflow_proxy"}]},{"type":"link","label":"Frequently Asked Questions","href":"/docs/faq","docId":"faq"},{"type":"category","label":"Development","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Developing With OpenLineage","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Python","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Setup a development environment","href":"/docs/development/developing/python/setup","docId":"development/developing/python/setup"},{"type":"category","label":"Tests","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Client","href":"/docs/development/developing/python/tests/client","docId":"development/developing/python/tests/client"},{"type":"link","label":"Airflow","href":"/docs/development/developing/python/tests/airflow","docId":"development/developing/python/tests/airflow"},{"type":"link","label":"Common","href":"/docs/development/developing/python/tests/common","docId":"development/developing/python/tests/common"},{"type":"link","label":"Dagster","href":"/docs/development/developing/python/tests/dagster","docId":"development/developing/python/tests/dagster"}]},{"type":"category","label":"Troubleshooting","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Logging","href":"/docs/development/developing/python/troubleshooting/logging","docId":"development/developing/python/troubleshooting/logging"}]},{"type":"category","label":"API Reference","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Python Client","href":"/docs/development/developing/python/api-reference/openlineage.client","docId":"development/developing/python/api-reference/openlineage.client"}]}]},{"type":"category","label":"Java","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Setup a development environment","href":"/docs/development/developing/java/setup","docId":"development/developing/java/setup"},{"type":"category","label":"Troubleshooting","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Logging","href":"/docs/development/developing/java/troubleshooting/logging","docId":"development/developing/java/troubleshooting/logging"}]},{"type":"link","label":"Metrics Backends","href":"/docs/development/developing/java/adding_metrics","docId":"development/developing/java/adding_metrics"}]},{"type":"category","label":"Spark","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Build","href":"/docs/development/developing/spark/setup","docId":"development/developing/spark/setup"},{"type":"link","label":"Integrating with Spark extensions","href":"/docs/development/developing/spark/built_in_lineage","docId":"development/developing/spark/built_in_lineage"}]}],"href":"/docs/development/developing/"},{"type":"link","label":"Example Lineage Events","href":"/docs/development/examples","docId":"development/examples"},{"type":"link","label":"OpenLineage Proxy","href":"/docs/development/ol-proxy","docId":"development/ol-proxy"}]},{"type":"category","label":"Releases","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"1.16.0","href":"/docs/releases/1_16_0","docId":"releases/1_16_0"},{"type":"link","label":"1.15.0","href":"/docs/releases/1_15_0","docId":"releases/1_15_0"},{"type":"link","label":"1.14.0","href":"/docs/releases/1_14_0","docId":"releases/1_14_0"},{"type":"link","label":"1.13.1","href":"/docs/releases/1_13_1","docId":"releases/1_13_1"},{"type":"link","label":"1.12.0","href":"/docs/releases/1_12_0","docId":"releases/1_12_0"},{"type":"link","label":"1.11.3","href":"/docs/releases/1_11_3","docId":"releases/1_11_3"},{"type":"link","label":"1.10.2","href":"/docs/releases/1_10_2","docId":"releases/1_10_2"},{"type":"link","label":"1.9.1","href":"/docs/releases/1_9_1","docId":"releases/1_9_1"},{"type":"link","label":"1.8.0","href":"/docs/releases/1_8_0","docId":"releases/1_8_0"},{"type":"link","label":"1.7.0","href":"/docs/releases/1_7_0","docId":"releases/1_7_0"},{"type":"link","label":"1.6.2","href":"/docs/releases/1_6_2","docId":"releases/1_6_2"},{"type":"link","label":"1.5.0","href":"/docs/releases/1_5_0","docId":"releases/1_5_0"},{"type":"link","label":"1.4.1","href":"/docs/releases/1_4_1","docId":"releases/1_4_1"},{"type":"link","label":"1.3.1","href":"/docs/releases/1_3_1","docId":"releases/1_3_1"},{"type":"link","label":"1.2.2","href":"/docs/releases/1_2_2","docId":"releases/1_2_2"},{"type":"link","label":"1.1.0","href":"/docs/releases/1_1_0","docId":"releases/1_1_0"},{"type":"link","label":"1.0.0","href":"/docs/releases/1_0_0","docId":"releases/1_0_0"},{"type":"link","label":"0.30.1","href":"/docs/releases/0_30_1","docId":"releases/0_30_1"},{"type":"link","label":"0.29.2","href":"/docs/releases/0_29_2","docId":"releases/0_29_2"},{"type":"link","label":"0.28.0","href":"/docs/releases/0_28_0","docId":"releases/0_28_0"},{"type":"link","label":"0.27.2","href":"/docs/releases/0_27_2","docId":"releases/0_27_2"},{"type":"link","label":"0.27.1","href":"/docs/releases/0_27_1","docId":"releases/0_27_1"},{"type":"link","label":"0.26.0","href":"/docs/releases/0_26_0","docId":"releases/0_26_0"},{"type":"link","label":"0.25.0","href":"/docs/releases/0_25_0","docId":"releases/0_25_0"},{"type":"link","label":"0.24.0","href":"/docs/releases/0_24_0","docId":"releases/0_24_0"},{"type":"link","label":"0.23.0","href":"/docs/releases/0_23_0","docId":"releases/0_23_0"},{"type":"link","label":"0.22.0","href":"/docs/releases/0_22_0","docId":"releases/0_22_0"},{"type":"link","label":"0.21.1","href":"/docs/releases/0_21_1","docId":"releases/0_21_1"},{"type":"link","label":"0.20.6","href":"/docs/releases/0_20_6","docId":"releases/0_20_6"},{"type":"link","label":"0.20.4","href":"/docs/releases/0_20_4","docId":"releases/0_20_4"},{"type":"link","label":"0.19.2","href":"/docs/releases/0_19_2","docId":"releases/0_19_2"},{"type":"link","label":"0.18.0","href":"/docs/releases/0_18_0","docId":"releases/0_18_0"},{"type":"link","label":"0.17.0","href":"/docs/releases/0_17_0","docId":"releases/0_17_0"},{"type":"link","label":"0.16.1","href":"/docs/releases/0_16_1","docId":"releases/0_16_1"},{"type":"link","label":"0.15.1","href":"/docs/releases/0_15_1","docId":"releases/0_15_1"},{"type":"link","label":"0.14.1","href":"/docs/releases/0_14_1","docId":"releases/0_14_1"},{"type":"link","label":"0.14.0","href":"/docs/releases/0_14_0","docId":"releases/0_14_0"},{"type":"link","label":"0.13.1","href":"/docs/releases/0_13_1","docId":"releases/0_13_1"},{"type":"link","label":"0.13.0","href":"/docs/releases/0_13_0","docId":"releases/0_13_0"},{"type":"link","label":"0.12.0","href":"/docs/releases/0_12_0","docId":"releases/0_12_0"},{"type":"link","label":"0.11.0","href":"/docs/releases/0_11_0","docId":"releases/0_11_0"},{"type":"link","label":"0.10.0","href":"/docs/releases/0_10_0","docId":"releases/0_10_0"},{"type":"link","label":"0.9.0","href":"/docs/releases/0_9_0","docId":"releases/0_9_0"},{"type":"link","label":"0.8.2","href":"/docs/releases/0_8_2","docId":"releases/0_8_2"},{"type":"link","label":"0.8.1","href":"/docs/releases/0_8_1","docId":"releases/0_8_1"},{"type":"link","label":"0.7.1","href":"/docs/releases/0_7_1","docId":"releases/0_7_1"},{"type":"link","label":"0.6.2","href":"/docs/releases/0_6_2","docId":"releases/0_6_2"},{"type":"link","label":"0.6.1","href":"/docs/releases/0_6_1","docId":"releases/0_6_1"},{"type":"link","label":"0.6.0","href":"/docs/releases/0_6_0","docId":"releases/0_6_0"},{"type":"link","label":"0.5.2","href":"/docs/releases/0_5_2","docId":"releases/0_5_2"},{"type":"link","label":"0.5.1","href":"/docs/releases/0_5_1","docId":"releases/0_5_1"},{"type":"link","label":"0.4.0","href":"/docs/releases/0_4_0","docId":"releases/0_4_0"},{"type":"link","label":"0.3.1","href":"/docs/releases/0_3_1","docId":"releases/0_3_1"},{"type":"link","label":"0.3.0","href":"/docs/releases/0_3_0","docId":"releases/0_3_0"},{"type":"link","label":"0.2.3","href":"/docs/releases/0_2_3","docId":"releases/0_2_3"},{"type":"link","label":"0.2.2","href":"/docs/releases/0_2_2","docId":"releases/0_2_2"},{"type":"link","label":"0.2.1","href":"/docs/releases/0_2_1","docId":"releases/0_2_1"},{"type":"link","label":"0.2.0","href":"/docs/releases/0_2_0","docId":"releases/0_2_0"},{"type":"link","label":"0.1.0","href":"/docs/releases/0_1_0","docId":"releases/0_1_0"}]}]},"docs":{"client/java/configuration":{"id":"client/java/configuration","title":"Configuration","description":"We recommend configuring the client with an openlineage.yml file that contains all the","sidebar":"tutorialSidebar"},"client/java/java":{"id":"client/java/java","title":"Java","description":"Overview","sidebar":"tutorialSidebar"},"client/java/usage":{"id":"client/java/usage","title":"Usage Example","description":"1. Simple OpenLineage Client Test for Console Transport","sidebar":"tutorialSidebar"},"client/python":{"id":"client/python","title":"Python","description":"Overview","sidebar":"tutorialSidebar"},"development/developing/developing":{"id":"development/developing/developing","title":"Developing With OpenLineage","description":"As there are hundreds and possibly thousands databases, query engines and other tools you could use to process, create and move data, there\'s great chance that existing OpenLineage integration won\'t cover your needs.","sidebar":"tutorialSidebar"},"development/developing/java/adding_metrics":{"id":"development/developing/java/adding_metrics","title":"Metrics Backends","description":"To integrate additional metrics backend into the OpenLineage client, implement the MeterRegistryFactory interface and ensure it is utilized by the MicrometerProvider\'s getMetricsBuilders method.","sidebar":"tutorialSidebar"},"development/developing/java/setup":{"id":"development/developing/java/setup","title":"Setup a development environment","description":"This page needs your contribution! Please contribute new examples using the edit link at the bottom.","sidebar":"tutorialSidebar"},"development/developing/java/troubleshooting/logging":{"id":"development/developing/java/troubleshooting/logging","title":"Logging","description":"OpenLineage Java library is based on slf4j when generating logs. Being able to emit logs for various purposes is very helpful when troubleshooting OpenLineage.","sidebar":"tutorialSidebar"},"development/developing/python/api-reference/openlineage.client":{"id":"development/developing/python/api-reference/openlineage.client","title":"Python Client","description":"\\\\n\\\\n\\\\n On this page\\\\n  \\\\n\\\\n\\\\nopenlineage.client.client module\\\\nOpenLineageClientOptions\\\\nOpenLineageClientOptions.timeout\\\\nOpenLineageClientOptions.verify\\\\nOpenLineageClientOptions.apikey\\\\nOpenLineageClientOptions.adapter\\\\n\\\\n\\\\nOpenLineageClient\\\\nOpenLineageClient.fromenvironment()\\\\nOpenLineageClient.fromdict()\\\\nOpenLineageClient.filterevent()\\\\nOpenLineageClient.emit()\\\\nOpenLineageClient.config\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.eventv2 module\\\\nBaseEvent\\\\nBaseEvent.eventTime\\\\nBaseEvent.producer\\\\nBaseEvent.schemaURL\\\\nBaseEvent.skipredact\\\\nBaseEvent.eventtimecheck()\\\\nBaseEvent.producercheck()\\\\nBaseEvent.schemaurlcheck()\\\\n\\\\n\\\\nRunEvent\\\\nRunEvent.run\\\\nRunEvent.job\\\\nRunEvent.eventType\\\\nRunEvent.inputs\\\\nRunEvent.outputs\\\\n\\\\n\\\\nJobEvent\\\\nJobEvent.job\\\\nJobEvent.inputs\\\\nJobEvent.outputs\\\\n\\\\n\\\\nDatasetEvent\\\\nDatasetEvent.dataset\\\\n\\\\n\\\\nRunState\\\\nDataset\\\\nDataset.namespace\\\\nDataset.name\\\\nDataset.facets\\\\n\\\\n\\\\nInputDataset\\\\nInputDataset.inputFacets\\\\n\\\\n\\\\nOutputDataset\\\\nOutputDataset.outputFacets\\\\n\\\\n\\\\nRun\\\\nRun.runId\\\\nRun.facets\\\\nRun.runidcheck()\\\\n\\\\n\\\\nJob\\\\nJob.namespace\\\\nJob.name\\\\nJob.facets\\\\n\\\\n\\\\nsetproducer()\\\\n\\\\n\\\\nopenlineage.client.facet module\\\\nsetproducer()\\\\nBaseFacet\\\\nBaseFacet.skipredact\\\\n\\\\n\\\\nNominalTimeRunFacet\\\\nNominalTimeRunFacet.nominalStartTime\\\\nNominalTimeRunFacet.nominalEndTime\\\\n\\\\n\\\\nParentRunFacet\\\\nParentRunFacet.run\\\\nParentRunFacet.job\\\\nParentRunFacet.create()\\\\n\\\\n\\\\nDocumentationJobFacet\\\\nDocumentationJobFacet.description\\\\n\\\\n\\\\nSourceCodeLocationJobFacet\\\\nSourceCodeLocationJobFacet.type\\\\nSourceCodeLocationJobFacet.url\\\\n\\\\n\\\\nSqlJobFacet\\\\nSqlJobFacet.query\\\\n\\\\n\\\\nDocumentationDatasetFacet\\\\nDocumentationDatasetFacet.description\\\\n\\\\n\\\\nSchemaField\\\\nSchemaField.name\\\\nSchemaField.type\\\\nSchemaField.description\\\\n\\\\n\\\\nSchemaDatasetFacet\\\\nSchemaDatasetFacet.fields\\\\n\\\\n\\\\nDataSourceDatasetFacet\\\\nDataSourceDatasetFacet.name\\\\nDataSourceDatasetFacet.uri\\\\n\\\\n\\\\nOutputStatisticsOutputDatasetFacet\\\\nOutputStatisticsOutputDatasetFacet.rowCount\\\\nOutputStatisticsOutputDatasetFacet.size\\\\nOutputStatisticsOutputDatasetFacet.fileCount\\\\n\\\\n\\\\nColumnMetric\\\\nColumnMetric.nullCount\\\\nColumnMetric.distinctCount\\\\nColumnMetric.sum\\\\nColumnMetric.count\\\\nColumnMetric.min\\\\nColumnMetric.max\\\\nColumnMetric.quantiles\\\\n\\\\n\\\\nDataQualityMetricsInputDatasetFacet\\\\nDataQualityMetricsInputDatasetFacet.rowCount\\\\nDataQualityMetricsInputDatasetFacet.bytes\\\\nDataQualityMetricsInputDatasetFacet.fileCount\\\\nDataQualityMetricsInputDatasetFacet.columnMetrics\\\\n\\\\n\\\\nAssertion\\\\nAssertion.assertion\\\\nAssertion.success\\\\nAssertion.column\\\\n\\\\n\\\\nDataQualityAssertionsDatasetFacet\\\\nDataQualityAssertionsDatasetFacet.assertions\\\\n\\\\n\\\\nSourceCodeJobFacet\\\\nSourceCodeJobFacet.language\\\\nSourceCodeJobFacet.source\\\\n\\\\n\\\\nExternalQueryRunFacet\\\\nExternalQueryRunFacet.externalQueryId\\\\nExternalQueryRunFacet.source\\\\n\\\\n\\\\nErrorMessageRunFacet\\\\nErrorMessageRunFacet.message\\\\nErrorMessageRunFacet.programmingLanguage\\\\nErrorMessageRunFacet.stackTrace\\\\n\\\\n\\\\nSymlinksDatasetFacetIdentifiers\\\\nSymlinksDatasetFacetIdentifiers.namespace\\\\nSymlinksDatasetFacetIdentifiers.name\\\\nSymlinksDatasetFacetIdentifiers.type\\\\n\\\\n\\\\nSymlinksDatasetFacet\\\\nSymlinksDatasetFacet.identifiers\\\\n\\\\n\\\\nStorageDatasetFacet\\\\nStorageDatasetFacet.storageLayer\\\\nStorageDatasetFacet.fileFormat\\\\n\\\\n\\\\nOwnershipJobFacetOwners\\\\nOwnershipJobFacetOwners.name\\\\nOwnershipJobFacetOwners.type\\\\n\\\\n\\\\nOwnershipJobFacet\\\\nOwnershipJobFacet.owners\\\\n\\\\n\\\\nJobTypeJobFacet\\\\nJobTypeJobFacet.processingType\\\\nJobTypeJobFacet.integration\\\\nJobTypeJobFacet.jobType\\\\n\\\\n\\\\nDatasetVersionDatasetFacet\\\\nDatasetVersionDatasetFacet.datasetVersion\\\\n\\\\n\\\\nLifecycleStateChange\\\\nLifecycleStateChange.ALTER\\\\nLifecycleStateChange.CREATE\\\\nLifecycleStateChange.DROP\\\\nLifecycleStateChange.OVERWRITE\\\\nLifecycleStateChange.RENAME\\\\nLifecycleStateChange.TRUNCATE\\\\n\\\\n\\\\nLifecycleStateChangeDatasetFacetPreviousIdentifier\\\\nLifecycleStateChangeDatasetFacetPreviousIdentifier.name\\\\nLifecycleStateChangeDatasetFacetPreviousIdentifier.namespace\\\\n\\\\n\\\\nLifecycleStateChangeDatasetFacet\\\\nLifecycleStateChangeDatasetFacet.lifecycleStateChange\\\\nLifecycleStateChangeDatasetFacet.previousIdentifier\\\\n\\\\n\\\\nOwnershipDatasetFacetOwners\\\\nOwnershipDatasetFacetOwners.name\\\\nOwnershipDatasetFacetOwners.type\\\\n\\\\n\\\\nOwnershipDatasetFacet\\\\nOwnershipDatasetFacet.owners\\\\n\\\\n\\\\nColumnLineageDatasetFacetFieldsAdditionalInputFields\\\\nColumnLineageDatasetFacetFieldsAdditionalInputFields.namespace\\\\nColumnLineageDatasetFacetFieldsAdditionalInputFields.name\\\\nColumnLineageDatasetFacetFieldsAdditionalInputFields.field\\\\n\\\\n\\\\nColumnLineageDatasetFacetFieldsAdditional\\\\nColumnLineageDatasetFacetFieldsAdditional.inputFields\\\\nColumnLineageDatasetFacetFieldsAdditional.transformationDescription\\\\nColumnLineageDatasetFacetFieldsAdditional.transformationType\\\\n\\\\n\\\\nColumnLineageDatasetFacet\\\\nColumnLineageDatasetFacet.fields\\\\n\\\\n\\\\nProcessingEngineRunFacet\\\\nProcessingEngineRunFacet.version\\\\nProcessingEngineRunFacet.name\\\\nProcessingEngineRunFacet.openlineageAdapterVersion\\\\n\\\\n\\\\nExtractionError\\\\nExtractionError.errorMessage\\\\nExtractionError.stackTrace\\\\nExtractionError.task\\\\nExtractionError.taskNumber\\\\n\\\\n\\\\nExtractionErrorRunFacet\\\\nExtractionErrorRunFacet.totalTasks\\\\nExtractionErrorRunFacet.failedTasks\\\\nExtractionErrorRunFacet.errors\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.facetv2 module\\\\nBaseFacet\\\\nBaseFacet.skipredact\\\\n\\\\n\\\\nDatasetFacet\\\\nInputDatasetFacet\\\\nJobFacet\\\\nOutputDatasetFacet\\\\nRunFacet\\\\nsetproducer()\\\\n\\\\n\\\\nopenlineage.client.filter module\\\\nFilter\\\\nFilter.filterevent()\\\\n\\\\n\\\\nExactMatchFilter\\\\nExactMatchFilter.filterevent()\\\\n\\\\n\\\\nRegexFilter\\\\nRegexFilter.filterevent()\\\\n\\\\n\\\\ncreatefilter()\\\\n\\\\n\\\\nopenlineage.client.run module\\\\nRunState\\\\nRunState.START\\\\nRunState.RUNNING\\\\nRunState.COMPLETE\\\\nRunState.ABORT\\\\nRunState.FAIL\\\\nRunState.OTHER\\\\n\\\\n\\\\nDataset\\\\nDataset.namespace\\\\nDataset.name\\\\nDataset.facets\\\\n\\\\n\\\\nInputDataset\\\\nInputDataset.inputFacets\\\\n\\\\n\\\\nOutputDataset\\\\nOutputDataset.outputFacets\\\\n\\\\n\\\\nDatasetEvent\\\\nDatasetEvent.eventTime\\\\nDatasetEvent.producer\\\\nDatasetEvent.schemaURL\\\\nDatasetEvent.dataset\\\\n\\\\n\\\\nJob\\\\nJob.namespace\\\\nJob.name\\\\nJob.facets\\\\n\\\\n\\\\nJobEvent\\\\nJobEvent.eventTime\\\\nJobEvent.producer\\\\nJobEvent.schemaURL\\\\nJobEvent.job\\\\nJobEvent.inputs\\\\nJobEvent.outputs\\\\n\\\\n\\\\nRun\\\\nRun.runId\\\\nRun.facets\\\\nRun.check()\\\\n\\\\n\\\\nRunEvent\\\\nRunEvent.eventType\\\\nRunEvent.eventTime\\\\nRunEvent.run\\\\nRunEvent.job\\\\nRunEvent.producer\\\\nRunEvent.inputs\\\\nRunEvent.outputs\\\\nRunEvent.schemaURL\\\\nRunEvent.check()\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.serde module\\\\nSerde\\\\nSerde.removenullsandenums()\\\\nSerde.todict()\\\\nSerde.tojson()\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.utils module\\\\nimportfromstring()\\\\ntryimportfromstring()\\\\ngetonlyspecifiedfields()\\\\nRedactMixin\\\\nRedactMixin.skipredact\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.uuid module\\\\ngeneratenewuuid()\\\\ngeneratestaticuuid()\\\\n\\\\n\\\\nopenlineage.client.generated.base module\\\\nsetproducer()\\\\nBaseEvent\\\\nBaseEvent.eventTime\\\\nBaseEvent.producer\\\\nBaseEvent.schemaURL\\\\nBaseEvent.skipredact\\\\nBaseEvent.eventtimecheck()\\\\nBaseEvent.producercheck()\\\\nBaseEvent.schemaurlcheck()\\\\n\\\\n\\\\nBaseFacet\\\\nBaseFacet.skipredact\\\\n\\\\n\\\\nDataset\\\\nDataset.namespace\\\\nDataset.name\\\\nDataset.facets\\\\n\\\\n\\\\nDatasetEvent\\\\nDatasetEvent.dataset\\\\n\\\\n\\\\nDatasetFacet\\\\nEventType\\\\nEventType.START\\\\nEventType.RUNNING\\\\nEventType.COMPLETE\\\\nEventType.ABORT\\\\nEventType.FAIL\\\\nEventType.OTHER\\\\n\\\\n\\\\nInputDataset\\\\nInputDataset.inputFacets\\\\n\\\\n\\\\nInputDatasetFacet\\\\nJob\\\\nJob.namespace\\\\nJob.name\\\\nJob.facets\\\\n\\\\n\\\\nJobEvent\\\\nJobEvent.job\\\\nJobEvent.inputs\\\\nJobEvent.outputs\\\\n\\\\n\\\\nJobFacet\\\\nOutputDataset\\\\nOutputDataset.outputFacets\\\\n\\\\n\\\\nOutputDatasetFacet\\\\nRun\\\\nRun.runId\\\\nRun.facets\\\\nRun.runidcheck()\\\\n\\\\n\\\\nRunEvent\\\\nRunEvent.run\\\\nRunEvent.job\\\\nRunEvent.eventType\\\\nRunEvent.inputs\\\\nRunEvent.outputs\\\\n\\\\n\\\\nRunFacet\\\\nStaticDataset\\\\n\\\\n\\\\nopenlineage.client.generated.columnlineagedataset module\\\\nColumnLineageDatasetFacet\\\\nColumnLineageDatasetFacet.fields\\\\n\\\\n\\\\nFields\\\\nFields.inputFields\\\\nFields.transformationDescription\\\\nFields.transformationType\\\\n\\\\n\\\\nInputField\\\\nInputField.namespace\\\\nInputField.name\\\\nInputField.field\\\\nInputField.transformations\\\\n\\\\n\\\\nTransformation\\\\nTransformation.type\\\\nTransformation.subtype\\\\nTransformation.description\\\\nTransformation.masking\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.dataqualityassertionsdataset module\\\\nAssertion\\\\nAssertion.assertion\\\\nAssertion.success\\\\nAssertion.column\\\\n\\\\n\\\\nDataQualityAssertionsDatasetFacet\\\\nDataQualityAssertionsDatasetFacet.assertions\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.dataqualitymetricsinputdataset module\\\\nColumnMetrics\\\\nColumnMetrics.nullCount\\\\nColumnMetrics.distinctCount\\\\nColumnMetrics.sum\\\\nColumnMetrics.count\\\\nColumnMetrics.min\\\\nColumnMetrics.max\\\\nColumnMetrics.quantiles\\\\n\\\\n\\\\nDataQualityMetricsInputDatasetFacet\\\\nDataQualityMetricsInputDatasetFacet.columnMetrics\\\\nDataQualityMetricsInputDatasetFacet.rowCount\\\\nDataQualityMetricsInputDatasetFacet.bytes\\\\nDataQualityMetricsInputDatasetFacet.fileCount\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.datasetversiondataset module\\\\nDatasetVersionDatasetFacet\\\\nDatasetVersionDatasetFacet.datasetVersion\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.datasourcedataset module\\\\nDatasourceDatasetFacet\\\\nDatasourceDatasetFacet.name\\\\nDatasourceDatasetFacet.uri\\\\nDatasourceDatasetFacet.uricheck()\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.documentationdataset module\\\\nDocumentationDatasetFacet\\\\nDocumentationDatasetFacet.description\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.documentationjob module\\\\nDocumentationJobFacet\\\\nDocumentationJobFacet.description\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.errormessagerun module\\\\nErrorMessageRunFacet\\\\nErrorMessageRunFacet.message\\\\nErrorMessageRunFacet.programmingLanguage\\\\nErrorMessageRunFacet.stackTrace\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.externalqueryrun module\\\\nExternalQueryRunFacet\\\\nExternalQueryRunFacet.externalQueryId\\\\nExternalQueryRunFacet.source\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.extractionerrorrun module\\\\nError\\\\nError.errorMessage\\\\nError.stackTrace\\\\nError.task\\\\nError.taskNumber\\\\n\\\\n\\\\nExtractionErrorRunFacet\\\\nExtractionErrorRunFacet.totalTasks\\\\nExtractionErrorRunFacet.failedTasks\\\\nExtractionErrorRunFacet.errors\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.jobtypejob module\\\\nJobTypeJobFacet\\\\nJobTypeJobFacet.processingType\\\\nJobTypeJobFacet.integration\\\\nJobTypeJobFacet.jobType\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.lifecyclestatechangedataset module\\\\nLifecycleStateChange\\\\nLifecycleStateChange.ALTER\\\\nLifecycleStateChange.CREATE\\\\nLifecycleStateChange.DROP\\\\nLifecycleStateChange.OVERWRITE\\\\nLifecycleStateChange.RENAME\\\\nLifecycleStateChange.TRUNCATE\\\\n\\\\n\\\\nLifecycleStateChangeDatasetFacet\\\\nLifecycleStateChangeDatasetFacet.lifecycleStateChange\\\\nLifecycleStateChangeDatasetFacet.previousIdentifier\\\\n\\\\n\\\\nPreviousIdentifier\\\\nPreviousIdentifier.name\\\\nPreviousIdentifier.namespace\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.nominaltimerun module\\\\nNominalTimeRunFacet\\\\nNominalTimeRunFacet.nominalStartTime\\\\nNominalTimeRunFacet.nominalEndTime\\\\nNominalTimeRunFacet.nominalstarttimecheck()\\\\nNominalTimeRunFacet.nominalendtimecheck()\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.outputstatisticsoutputdataset module\\\\nOutputStatisticsOutputDatasetFacet\\\\nOutputStatisticsOutputDatasetFacet.rowCount\\\\nOutputStatisticsOutputDatasetFacet.size\\\\nOutputStatisticsOutputDatasetFacet.fileCount\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.ownershipdataset module\\\\nOwner\\\\nOwner.name\\\\nOwner.type\\\\n\\\\n\\\\nOwnershipDatasetFacet\\\\nOwnershipDatasetFacet.owners\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.ownershipjob module\\\\nOwner\\\\nOwner.name\\\\nOwner.type\\\\n\\\\n\\\\nOwnershipJobFacet\\\\nOwnershipJobFacet.owners\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.parentrun module\\\\nJob\\\\nJob.namespace\\\\nJob.name\\\\n\\\\n\\\\nParentRunFacet\\\\nParentRunFacet.run\\\\nParentRunFacet.job\\\\nParentRunFacet.create()\\\\n\\\\n\\\\nRun\\\\nRun.runId\\\\nRun.runidcheck()\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.processingenginerun module\\\\nProcessingEngineRunFacet\\\\nProcessingEngineRunFacet.version\\\\nProcessingEngineRunFacet.name\\\\nProcessingEngineRunFacet.openlineageAdapterVersion\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.schemadataset module\\\\nSchemaDatasetFacet\\\\nSchemaDatasetFacet.fields\\\\n\\\\n\\\\nSchemaDatasetFacetFields\\\\nSchemaDatasetFacetFields.name\\\\nSchemaDatasetFacetFields.type\\\\nSchemaDatasetFacetFields.description\\\\nSchemaDatasetFacetFields.fields\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.sourcecodejob module\\\\nSourceCodeJobFacet\\\\nSourceCodeJobFacet.language\\\\nSourceCodeJobFacet.sourceCode\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.sourcecodelocationjob module\\\\nSourceCodeLocationJobFacet\\\\nSourceCodeLocationJobFacet.type\\\\nSourceCodeLocationJobFacet.url\\\\nSourceCodeLocationJobFacet.repoUrl\\\\nSourceCodeLocationJobFacet.path\\\\nSourceCodeLocationJobFacet.version\\\\nSourceCodeLocationJobFacet.tag\\\\nSourceCodeLocationJobFacet.branch\\\\nSourceCodeLocationJobFacet.urlcheck()\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.sqljob module\\\\nSQLJobFacet\\\\nSQLJobFacet.query\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.storagedataset module\\\\nStorageDatasetFacet\\\\nStorageDatasetFacet.storageLayer\\\\nStorageDatasetFacet.fileFormat\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.symlinksdataset module\\\\nIdentifier\\\\nIdentifier.namespace\\\\nIdentifier.name\\\\nIdentifier.type\\\\n\\\\n\\\\nSymlinksDatasetFacet\\\\nSymlinksDatasetFacet.identifiers\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.transport.console module\\\\nConsoleConfig\\\\nConsoleTransport\\\\nConsoleTransport.kind\\\\nConsoleTransport.configclass\\\\nConsoleTransport.emit()\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.transport.factory module\\\\nDefaultTransportFactory\\\\nDefaultTransportFactory.registertransport()\\\\nDefaultTransportFactory.create()\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.transport.file module\\\\nFileConfig\\\\nFileConfig.logfilepath\\\\nFileConfig.append\\\\nFileConfig.fromdict()\\\\n\\\\n\\\\nFileTransport\\\\nFileTransport.kind\\\\nFileTransport.configclass\\\\nFileTransport.emit()\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.transport.http module\\\\nTokenProvider\\\\nTokenProvider.getbearer()\\\\n\\\\n\\\\nHttpCompression\\\\nHttpCompression.GZIP\\\\n\\\\n\\\\nApiKeyTokenProvider\\\\nApiKeyTokenProvider.getbearer()\\\\n\\\\n\\\\ncreatetokenprovider()\\\\ngetsession()\\\\nHttpConfig\\\\nHttpConfig.url\\\\nHttpConfig.endpoint\\\\nHttpConfig.timeout\\\\nHttpConfig.verify\\\\nHttpConfig.auth\\\\nHttpConfig.compression\\\\nHttpConfig.session\\\\nHttpConfig.adapter\\\\nHttpConfig.fromdict()\\\\nHttpConfig.fromoptions()\\\\n\\\\n\\\\nHttpTransport\\\\nHttpTransport.kind\\\\nHttpTransport.configclass\\\\nHttpTransport.setadapter()\\\\nHttpTransport.emit()\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.transport.kafka module\\\\nKafkaConfig\\\\nKafkaConfig.config\\\\nKafkaConfig.topic\\\\nKafkaConfig.messageKey\\\\nKafkaConfig.flush\\\\nKafkaConfig.fromdict()\\\\n\\\\n\\\\nondelivery()\\\\nKafkaTransport\\\\nKafkaTransport.kind\\\\nKafkaTransport.configclass\\\\nKafkaTransport.emit()\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.transport.mskiam module\\\\nMSKIAMConfig\\\\nMSKIAMConfig.region\\\\nMSKIAMConfig.awsprofile\\\\nMSKIAMConfig.rolearn\\\\nMSKIAMConfig.awsdebugcreds\\\\n\\\\n\\\\nMSKIAMTransport\\\\nMSKIAMTransport.kind\\\\nMSKIAMTransport.configclass\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.transport.noop module\\\\nNoopConfig\\\\nNoopTransport\\\\nNoopTransport.kind\\\\nNoopTransport.configclass\\\\nNoopTransport.emit()\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.transport.transport module\\\\nConfig\\\\nConfig.fromdict()\\\\n\\\\n\\\\nTransport\\\\nTransport.kind\\\\nTransport.configclass\\\\nTransport.emit()\\\\n\\\\n\\\\nTransportFactory\\\\nTransportFactory.create()\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n Show Source\\\\n    \\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.client module\\\\n\\\\n\\\\nclass openlineage.client.client.OpenLineageClientOptions(timeout=5.0, verify=True, apikey=None, adapter=None)\\\\nBases\\\\n\\\\ntimeout (float)\\\\nverify (bool)\\\\napikey (Optionalstr])\\\\nadapter (Optional[HTTPAdapter])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ntimeout bool\\\\n\\\\n\\\\n\\\\napikey HTTPAdapter\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.client.OpenLineageClient(url=None, options=None, session=None, transport=None, factory=None)\\\\nBases\\\\n\\\\nurl (str | None)\\\\noptions (OpenLineageClientOptions | None)\\\\nsession (Session | None)\\\\ntransport (Transport | None)\\\\nfactory (TransportFactory | None)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclassmethod fromenvironment()\\\\n\\\\nReturn type\\\\nconfig (dict[str, str])\\\\n\\\\nReturn type\\\\nevent (Event)\\\\n\\\\nReturn type\\\\nevent (Union[RunEvent, DatasetEvent, JobEvent, RunEvent, DatasetEvent, JobEvent])\\\\n\\\\nReturn type dict[str, Any]\\\\nContent of OpenLineage YAML config file.\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.eventv2 module\\\\n\\\\n\\\\nclass openlineage.client.eventv2.BaseEvent(*, eventTime, producer=\'\')\\\\nBases\\\\n\\\\neventTime (str)\\\\nproducer (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\neventTime str\\\\n\\\\n\\\\n\\\\nschemaURL list[str]\\\\n\\\\n\\\\n\\\\neventtimecheck(attribute, value)\\\\n\\\\nParameters\\\\nNone\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nproducercheck(attribute, value)\\\\n\\\\nParameters\\\\nNone\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nschemaurlcheck(attribute, value)\\\\n\\\\nParameters\\\\nNone\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.eventv2.RunEvent(, eventTime, producer=\'\', run, job, eventType=None, inputs=_Nothing.NOTHING, outputs=_Nothing.NOTHING)\\\\nBases\\\\n\\\\neventTime (str)\\\\nproducer (str)\\\\nrun (Run)\\\\njob (Job)\\\\neventType (EventType | None)\\\\ninputs (list[InputDataset] | None)\\\\noutputs (list[OutputDataset] | None)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nrun Job\\\\n\\\\n\\\\n\\\\neventType list[InputDataset] | None\\\\nThe set of input datasets.\\\\n\\\\n\\\\n\\\\noutputs BaseEvent\\\\n\\\\nParameters Job\\\\n\\\\n\\\\n\\\\ninputs list[OutputDataset] | None\\\\nThe set of output datasets.\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.eventv2.DatasetEvent(*, eventTime, producer=\'\', dataset)\\\\nBases\\\\n\\\\neventTime (str)\\\\nproducer (str)\\\\ndataset (StaticDataset)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ndataset RedactMixin\\\\n\\\\nParameters str\\\\nThe namespace containing that dataset\\\\n\\\\n\\\\n\\\\nname dict[str, DatasetFacet] | None\\\\nThe facets for this dataset\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.eventv2.InputDataset(namespace, name, inputFacets=Nothing.NOTHING, , facets=_Nothing.NOTHING)\\\\nBases\\\\n\\\\nnamespace (str)\\\\nname (str)\\\\ninputFacets (dict[str, InputDatasetFacet] | None)\\\\nfacets (dict[str, DatasetFacet] | None)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ninputFacets Dataset\\\\nAn output dataset\\\\n\\\\nParameters dict[str, OutputDatasetFacet] | None\\\\nThe output facets for this dataset\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.eventv2.Run(runId, facets=Nothing.NOTHING)\\\\nBases\\\\n\\\\nrunId (str)\\\\nfacets (dict[str, RunFacet] | None)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nrunId dict[str, RunFacet] | None\\\\nThe run facets.\\\\n\\\\n\\\\n\\\\nrunidcheck(attribute, value)\\\\n\\\\nParameters\\\\nNone\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.eventv2.Job(namespace, name, facets=Nothing.NOTHING)\\\\nBases\\\\n\\\\nnamespace (str)\\\\nname (str)\\\\nfacets (dict[str, JobFacet] | None)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nnamespace str\\\\nThe unique name for that job within that namespace\\\\n\\\\n\\\\n\\\\nfacets\\\\nproducer (str)\\\\n\\\\nReturn type\\\\nproducer (str)\\\\n\\\\nReturn type RedactMixin\\\\n\\\\n\\\\n\\\\n\\\\nproperty skipredact BaseFacet\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\nnominalEndTime BaseFacet\\\\n\\\\nParameters Dict[Any, Any]\\\\n\\\\n\\\\n\\\\njob\\\\n\\\\nrunId (str)\\\\nnamespace (str)\\\\nname (str)\\\\n\\\\n\\\\nReturn type BaseFacet\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.facet.SourceCodeLocationJobFacet(type, url)\\\\nBases\\\\n\\\\ntype (str)\\\\nurl (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ntype str\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.facet.SqlJobFacet(query)\\\\nBases\\\\nquery (str)\\\\n\\\\n\\\\n\\\\n\\\\nquery BaseFacet\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.facet.SchemaField(name, type, description=None)\\\\nBases\\\\n\\\\nname (str)\\\\ntype (str)\\\\ndescription (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nname str\\\\n\\\\n\\\\n\\\\ndescription BaseFacet\\\\n\\\\nParameters List[SchemaField]\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.facet.DataSourceDatasetFacet(name, uri)\\\\nBases\\\\n\\\\nname (str)\\\\nuri (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nname str\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.facet.OutputStatisticsOutputDatasetFacet(rowCount=None, size=None, fileCount=None)\\\\nBases\\\\n\\\\nrowCount (Optional[int])\\\\nsize (Optional[int])\\\\nfileCount (Optional[int])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nrowCount Optional[int]\\\\n\\\\n\\\\n\\\\nfileCount object\\\\n\\\\nParameters Optional[int]\\\\n\\\\n\\\\n\\\\ndistinctCount Optional[int]\\\\n\\\\n\\\\n\\\\ncount Optional[float]\\\\n\\\\n\\\\n\\\\nmax Optional[Dict[str, float]]\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.facet.DataQualityMetricsInputDatasetFacet(rowCount=None, bytes=None, fileCount=None, columnMetrics=Nothing.NOTHING)\\\\nBases\\\\n\\\\nrowCount (Optional[int])\\\\nbytes (Optional[int])\\\\nfileCount (Optional[int])\\\\ncolumnMetrics (Dict[str, ColumnMetric])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nrowCount Optional[int]\\\\n\\\\n\\\\n\\\\nfileCount Dict[str, ColumnMetric]\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.facet.Assertion(assertion, success, column=None)\\\\nBases\\\\n\\\\nassertion (str)\\\\nsuccess (bool)\\\\ncolumn (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nassertion bool\\\\n\\\\n\\\\n\\\\ncolumn BaseFacet\\\\nThis facet represents asserted expectations on dataset or it\\\\u2019s column.\\\\n\\\\nParameters List[Assertion]\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.facet.SourceCodeJobFacet(language, source)\\\\nBases\\\\n\\\\nlanguage (str)\\\\nsource (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nlanguage str\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.facet.ExternalQueryRunFacet(externalQueryId, source)\\\\nBases\\\\n\\\\nexternalQueryId (str)\\\\nsource (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nexternalQueryId str\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.facet.ErrorMessageRunFacet(message, programmingLanguage, stackTrace=None)\\\\nBases\\\\n\\\\nmessage (str)\\\\nprogrammingLanguage (str)\\\\nstackTrace (Optional[str])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nmessage str\\\\n\\\\n\\\\n\\\\nstackTrace object\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\nname str\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.facet.SymlinksDatasetFacet(identifiers=Nothing.NOTHING)\\\\nBases\\\\nidentifiers (List[SymlinksDatasetFacetIdentifiers])\\\\n\\\\n\\\\n\\\\n\\\\nidentifiers BaseFacet\\\\nThis facet represents dataset symlink names.\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\nfileFormat object\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\ntype BaseFacet\\\\nThis facet represents ownership of a job.\\\\n\\\\nParameters List[OwnershipJobFacetOwners]\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.facet.JobTypeJobFacet(processingType, integration, jobType)\\\\nBases\\\\n\\\\nprocessingType (str)\\\\nintegration (str)\\\\njobType (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nprocessingType str\\\\n\\\\n\\\\n\\\\njobType BaseFacet\\\\nThis facet represents version of a dataset.\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.facet.LifecycleStateChange(value)\\\\nBases object\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\nnamespace BaseFacet\\\\nThis facet represents information of lifecycle changes of a dataset.\\\\n\\\\nParameters LifecycleStateChange\\\\n\\\\n\\\\n\\\\npreviousIdentifier object\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\ntype BaseFacet\\\\nThis facet represents ownership of a dataset.\\\\n\\\\nParameters List[OwnershipDatasetFacetOwners]\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.facet.ColumnLineageDatasetFacetFieldsAdditionalInputFields(namespace, name, field)\\\\nBases\\\\n\\\\nnamespace (str)\\\\nname (str)\\\\nfield (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nnamespace str\\\\n\\\\n\\\\n\\\\nfield object\\\\n\\\\nParameters ClassVar[List[ColumnLineageDatasetFacetFieldsAdditionalInputFields]]\\\\n\\\\n\\\\n\\\\ntransformationDescription str\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.facet.ColumnLineageDatasetFacet(fields=Nothing.NOTHING)\\\\nBases\\\\nfields (Dict[str, ColumnLineageDatasetFacetFieldsAdditional])\\\\n\\\\n\\\\n\\\\n\\\\nfields BaseFacet\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\nname str\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.facet.ExtractionError(errorMessage, stackTrace, task, taskNumber)\\\\nBases\\\\n\\\\nerrorMessage (str)\\\\nstackTrace (Optional[str])\\\\ntask (Optional[str])\\\\ntaskNumber (Optional[int])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nerrorMessage Optional[str]\\\\n\\\\n\\\\n\\\\ntask Optional[int]\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.facet.ExtractionErrorRunFacet(totalTasks, failedTasks, errors)\\\\nBases\\\\n\\\\ntotalTasks (int)\\\\nfailedTasks (int)\\\\nerrors (List[ExtractionError])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ntotalTasks int\\\\n\\\\n\\\\n\\\\nerrors RedactMixin\\\\nall fields of the base facet are prefixed with  to avoid name conflicts in facets\\\\n\\\\nParameters list[str]\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.facetv2.DatasetFacet(, producer=\'\', deleted=None)\\\\nBases\\\\n\\\\nproducer (str)\\\\ndeleted (bool | None)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.facet_v2.InputDatasetFacet(, producer=\'\')\\\\nBases\\\\nproducer (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.facetv2.JobFacet(*, producer=\'\', deleted=None)\\\\nBases\\\\n\\\\nproducer (str)\\\\ndeleted (bool | None)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.facetv2.OutputDatasetFacet(, producer=\'\')\\\\nBases\\\\nproducer (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.facet_v2.RunFacet(, producer=\'\')\\\\nBases\\\\nproducer (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.facetv2.setproducer(producer)\\\\n\\\\nParameters\\\\nNone\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.filter module\\\\n\\\\n\\\\nclass openlineage.client.filter.Filter\\\\nBases\\\\nevent (RunEventType)\\\\n\\\\nReturn type Filter\\\\n\\\\nParameters\\\\nevent (RunEventType)\\\\n\\\\nReturn type Filter\\\\n\\\\nParameters\\\\nevent (RunEventType)\\\\n\\\\nReturn type\\\\nconf (dict[str, str])\\\\n\\\\nReturn type Enum\\\\nAn enumeration.\\\\n\\\\n\\\\nSTART = \'START\'\\\\n\\\\n\\\\n\\\\nRUNNING = \'RUNNING\'\\\\n\\\\n\\\\n\\\\nCOMPLETE = \'COMPLETE\'\\\\n\\\\n\\\\n\\\\nABORT = \'ABORT\'\\\\n\\\\n\\\\n\\\\nFAIL = \'FAIL\'\\\\n\\\\n\\\\n\\\\nOTHER = \'OTHER\'\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.run.Dataset(namespace, name, facets=Nothing.NOTHING)\\\\nBases\\\\n\\\\nnamespace (str)\\\\nname (str)\\\\nfacets (Dict[Any, Any])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nnamespace str\\\\n\\\\n\\\\n\\\\nfacets Dataset\\\\n\\\\nParameters Dict[Any, Any]\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.run.OutputDataset(namespace, name, facets=Nothing.NOTHING, outputFacets=Nothing.NOTHING)\\\\nBases\\\\n\\\\nnamespace (str)\\\\nname (str)\\\\nfacets (Dict[Any, Any])\\\\noutputFacets (Dict[Any, Any])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\noutputFacets RedactMixin\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\nproducer str\\\\n\\\\n\\\\n\\\\ndataset RedactMixin\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\nname Dict[Any, Any]\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.run.JobEvent(eventTime, producer, schemaURL, job, inputs=Nothing.NOTHING, outputs=Nothing.NOTHING)\\\\nBases\\\\n\\\\neventTime (str)\\\\nproducer (str)\\\\nschemaURL (str)\\\\njob (Job)\\\\ninputs (Optional[List[Dataset]])\\\\noutputs (Optional[List[Dataset]])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\neventTime str\\\\n\\\\n\\\\n\\\\nschemaURL Job\\\\n\\\\n\\\\n\\\\ninputs Optional[List[Dataset]]\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.run.Run(runId, facets=Nothing.NOTHING)\\\\nBases\\\\n\\\\nrunId (str)\\\\nfacets (Dict[Any, Any])\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nrunId Dict[Any, Any]\\\\n\\\\n\\\\n\\\\ncheck(attribute, value)\\\\n\\\\nParameters\\\\nNone\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.run.RunEvent(eventType, eventTime, run, job, producer, inputs=Nothing.NOTHING, outputs=Nothing.NOTHING, schemaURL=\'https RedactMixin\\\\n\\\\nParameters RunState\\\\n\\\\n\\\\n\\\\neventTime Run\\\\n\\\\n\\\\n\\\\njob str\\\\n\\\\n\\\\n\\\\ninputs Optional[List[Dataset]]\\\\n\\\\n\\\\n\\\\nschemaURL\\\\n\\\\nattribute (str)\\\\nvalue (str)\\\\n\\\\n\\\\nReturn type object\\\\n\\\\n\\\\nclassmethod removenullsandenums(obj)\\\\n\\\\nParameters\\\\nAny\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclassmethod todict(obj)\\\\n\\\\nParameters\\\\ndict[Any, Any]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclassmethod tojson(obj)\\\\n\\\\nParameters\\\\nstr\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.utils module\\\\n\\\\n\\\\nopenlineage.client.utils.importfromstring(path)\\\\n\\\\nParameters\\\\ntype[Any]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.utils.tryimportfromstring(path)\\\\n\\\\nParameters\\\\ntype[Any] | None\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.utils.getonlyspecifiedfields(clazz, params)\\\\n\\\\nParameters\\\\ndict[str, Any]\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.utils.RedactMixin\\\\nBases list[str]\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.uuid module\\\\n\\\\n\\\\nopenlineage.client.uuid.generatenewuuid(instant=None)\\\\nGenerate new UUID for an instant of time. Each function call returns a new UUID value.\\\\nUUID version is an implementation detail, and should not be relied on.\\\\nFor now it is [UUIDv7, so for increasing instant values,\\\\nreturned UUID is always greater than previous one.\\\\nUsing uuid6 lib implementation (MIT License), with few changes\\\\ninstant (datetime | None) \\\\u2013 instant of time used to generate UUID. If not provided, current time is used.\\\\n\\\\nReturn type\\\\nUUID\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.uuid.generatestaticuuid(instant, data)\\\\nGenerate UUID for instant of time and input data.\\\\nCalling function with same arguments always produces the same result.\\\\nUUID version is an implementation detail, and **should not* be relied on.\\\\nFor now it is UUIDv7, so for increasing instant values,\\\\nreturned UUID is always greater than previous one. The only difference from RFC 9562 is that\\\\nleast significant bytes are not random, but instead a SHA-1 hash of input data.\\\\nUsing uuid6 lib implementation (MIT License), with few changes\\\\n\\\\ninstant (datetime) \\\\u2013 instant of time used to generate UUID. If not provided, current time is used.\\\\ndata (bytes) \\\\u2013 input data to generate random part from.\\\\n\\\\n\\\\nReturn type\\\\nUUID\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.base module\\\\n\\\\n\\\\nopenlineage.client.generated.base.setproducer(producer)\\\\n\\\\nParameters\\\\nNone\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.generated.base.BaseEvent(, eventTime, producer=\'\')\\\\nBases\\\\n\\\\neventTime (str)\\\\nproducer (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\neventTime str\\\\n\\\\n\\\\n\\\\nschemaURL\\\\n\\\\nattribute (str)\\\\nvalue (str)\\\\n\\\\n\\\\nReturn type\\\\n\\\\nattribute (str)\\\\nvalue (str)\\\\n\\\\n\\\\nReturn type\\\\n\\\\nattribute (str)\\\\nvalue (str)\\\\n\\\\n\\\\nReturn type RedactMixin\\\\nall fields of the base facet are prefixed with  to avoid name conflicts in facets\\\\n\\\\nParameters RedactMixin\\\\n\\\\nParameters str\\\\nThe namespace containing that dataset\\\\n\\\\n\\\\n\\\\nname dict[str, DatasetFacet] | None\\\\nThe facets for this dataset\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.generated.base.DatasetEvent(, eventTime, producer=\'\', dataset)\\\\nBases\\\\n\\\\neventTime (str)\\\\nproducer (str)\\\\ndataset (StaticDataset)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ndataset BaseFacet\\\\nA Dataset Facet\\\\n\\\\nParameters Enum\\\\nthe current transition of the run state. It is required to issue 1 START event and 1 of [ COMPLETE,\\\\nABORT, FAIL ] event per run. Additional events with OTHER eventType can be added to the same run.\\\\nFor example to send additional metadata after the run is complete\\\\n\\\\n\\\\nSTART = \'START\'\\\\n\\\\n\\\\n\\\\nRUNNING = \'RUNNING\'\\\\n\\\\n\\\\n\\\\nCOMPLETE = \'COMPLETE\'\\\\n\\\\n\\\\n\\\\nABORT = \'ABORT\'\\\\n\\\\n\\\\n\\\\nFAIL = \'FAIL\'\\\\n\\\\n\\\\n\\\\nOTHER = \'OTHER\'\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.generated.base.InputDataset(namespace, name, inputFacets=_Nothing.NOTHING, , facets=Nothing.NOTHING)\\\\nBases dict[str, InputDatasetFacet] | None\\\\nThe input facets for this dataset.\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.generated.base.InputDatasetFacet(*, producer=\'\')\\\\nBases\\\\nproducer (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.generated.base.Job(namespace, name, facets=Nothing.NOTHING)\\\\nBases\\\\n\\\\nnamespace (str)\\\\nname (str)\\\\nfacets (dict[str, JobFacet] | None)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nnamespace str\\\\nThe unique name for that job within that namespace\\\\n\\\\n\\\\n\\\\nfacets BaseEvent\\\\n\\\\n\\\\njob list[InputDataset] | None\\\\nThe set of input datasets.\\\\n\\\\n\\\\n\\\\noutputs BaseFacet\\\\nA Job Facet\\\\n\\\\nParameters Dataset\\\\nAn output dataset\\\\n\\\\n\\\\noutputFacets BaseFacet\\\\nAn Output Dataset Facet\\\\n\\\\nParameters RedactMixin\\\\n\\\\nParameters str\\\\nThe globally unique ID of the run associated with the job.\\\\n\\\\n\\\\n\\\\nfacets\\\\n\\\\nattribute (str)\\\\nvalue (str)\\\\n\\\\n\\\\nReturn type BaseEvent\\\\n\\\\nParameters Run\\\\n\\\\n\\\\n\\\\njob EventType | None\\\\nthe current transition of the run state. It is required to issue 1 START event and 1 of [ COMPLETE,\\\\nABORT, FAIL ] event per run. Additional events with OTHER eventType can be added to the same run.\\\\nFor example to send additional metadata after the run is complete\\\\n\\\\n\\\\n\\\\ninputs list[OutputDataset] | None\\\\nThe set of output datasets.\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.generated.base.RunFacet(, producer=\'\')\\\\nBases\\\\nproducer (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.generated.base.StaticDataset(namespace, name, , facets=Nothing.NOTHING)\\\\nBases\\\\n\\\\nnamespace (str)\\\\nname (str)\\\\nfacets (dict[str, DatasetFacet] | None)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.columnlineagedataset module\\\\n\\\\n\\\\nclass openlineage.client.generated.columnlineagedataset.ColumnLineageDatasetFacet(fields, *, producer=\'\', deleted=None)\\\\nBases\\\\n\\\\nfields (dict[str, Fields])\\\\nproducer (str)\\\\ndeleted (bool | None)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nfields RedactMixin\\\\n\\\\nParameters list[InputField]\\\\n\\\\n\\\\n\\\\ntransformationDescription str | None\\\\nno\\\\noriginal data available (like a hash of PII for example)\\\\n\\\\nType\\\\nexact same as input; MASKED\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.generated.columnlineagedataset.InputField(namespace, name, field, transformations=Nothing.NOTHING)\\\\nBases\\\\n\\\\nnamespace (str)\\\\nname (str)\\\\nfield (str)\\\\ntransformations (list[Transformation] | None)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nnamespace str\\\\nThe input dataset name\\\\n\\\\n\\\\n\\\\nfield list[Transformation] | None\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.generated.columnlineagedataset.Transformation(type, subtype=None, description=None, masking=None)\\\\nBases\\\\n\\\\ntype (str)\\\\nsubtype (str | None)\\\\ndescription (str | None)\\\\nmasking (bool | None)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ntype\\\\nThe type of the transformation. Allowed values are\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nsubtype str | None\\\\na string representation of the transformation applied\\\\n\\\\n\\\\n\\\\nmasking RedactMixin\\\\n\\\\nParameters str\\\\nType of expectation test that dataset is subjected to\\\\n\\\\n\\\\n\\\\nsuccess str | None\\\\nColumn that expectation is testing. It should match the name provided in SchemaDatasetFacet. If\\\\ncolumn field is empty, then expectation refers to whole dataset.\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.generated.dataqualityassertionsdataset.DataQualityAssertionsDatasetFacet(assertions, *, producer=\'\')\\\\nBases\\\\n\\\\nassertions (list[Assertion])\\\\nproducer (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nassertions RedactMixin\\\\n\\\\nParameters int | None\\\\nThe number of null values in this column for the rows evaluated\\\\n\\\\n\\\\n\\\\ndistinctCount float | None\\\\nThe total sum of values in this column for the rows evaluated\\\\n\\\\n\\\\n\\\\ncount float | None\\\\n\\\\n\\\\n\\\\nmax dict[str, float] | None\\\\n0.1 0.25 0.5 0.75 1\\\\n\\\\nType InputDatasetFacet\\\\n\\\\nParameters dict[str, ColumnMetrics]\\\\nThe property key is the column name\\\\n\\\\n\\\\n\\\\nrowCount int | None\\\\nThe size in bytes\\\\n\\\\n\\\\n\\\\nfileCount DatasetFacet\\\\n\\\\nParameters str\\\\nThe version of the dataset.\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.datasourcedataset module\\\\n\\\\n\\\\nclass openlineage.client.generated.datasourcedataset.DatasourceDatasetFacet(name=None, uri=None, , producer=\'\', deleted=None)\\\\nBases\\\\n\\\\nname (str | None)\\\\nuri (str | None)\\\\nproducer (str)\\\\ndeleted (bool | None)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nname str | None\\\\n\\\\n\\\\n\\\\nuri_check(attribute, value)\\\\n\\\\nParameters\\\\nNone\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.documentation_dataset module\\\\n\\\\n\\\\nclass openlineage.client.generated.documentation_dataset.DocumentationDatasetFacet(description, , producer=\'\', deleted=None)\\\\nBases\\\\n\\\\ndescription (str)\\\\nproducer (str)\\\\ndeleted (bool | None)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ndescription JobFacet\\\\n\\\\nParameters str\\\\nThe description of the job.\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.error_message_run module\\\\n\\\\n\\\\nclass openlineage.client.generated.error_message_run.ErrorMessageRunFacet(message, programmingLanguage, stackTrace=None, , producer=\'\')\\\\nBases\\\\n\\\\nmessage (str)\\\\nprogrammingLanguage (str)\\\\nstackTrace (str | None)\\\\nproducer (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nmessage str\\\\nProgramming language the observed system uses.\\\\n\\\\n\\\\n\\\\nstackTrace RunFacet\\\\n\\\\nParameters str\\\\nIdentifier for the external system\\\\n\\\\n\\\\n\\\\nsource RedactMixin\\\\n\\\\nParameters str\\\\nText representation of extraction error message.\\\\n\\\\n\\\\n\\\\nstackTrace str | None\\\\nText representation of task that failed. This can be, for example, SQL statement that parser could\\\\nnot interpret.\\\\n\\\\n\\\\n\\\\ntaskNumber RunFacet\\\\n\\\\nParameters int\\\\nThe number of distinguishable tasks in a run that were processed by OpenLineage, whether\\\\nsuccessfully or not. Those could be, for example, distinct SQL statements.\\\\n\\\\n\\\\n\\\\nfailedTasks list[Error]\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.jobtypejob module\\\\n\\\\n\\\\nclass openlineage.client.generated.jobtypejob.JobTypeJobFacet(processingType, integration, jobType=None, , producer=\'\', deleted=None)\\\\nBases\\\\n\\\\nprocessingType (str)\\\\nintegration (str)\\\\njobType (str | None)\\\\nproducer (str)\\\\ndeleted (bool | None)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nprocessingType\\\\nJob processing type like\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nintegration\\\\nOpenLineage integration type of this job\\\\n\\\\n\\\\n\\\\n\\\\n\\\\njobType\\\\nRun type like\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.lifecycle_state_change_dataset module\\\\n\\\\n\\\\nclass openlineage.client.generated.lifecycle_state_change_dataset.LifecycleStateChange(value)\\\\nBases DatasetFacet\\\\n\\\\nParameters LifecycleStateChange\\\\nThe lifecycle state change.\\\\n\\\\n\\\\n\\\\npreviousIdentifier RedactMixin\\\\nPrevious name of the dataset in case of renaming it.\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\nnamespace RunFacet\\\\n\\\\nParameters str\\\\n//en.wikipedia.org/wiki/ISO8601) timestamp representing the nominal start time\\\\n(included) of the run. AKA the schedule time\\\\n\\\\nType\\\\nAn ISO-8601\\\\n\\\\nParameters\\\\nNone\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nnominalendtimecheck(attribute, value)\\\\n\\\\nParameters\\\\nNone\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.outputstatisticsoutputdataset module\\\\n\\\\n\\\\nclass openlineage.client.generated.outputstatisticsoutputdataset.OutputStatisticsOutputDatasetFacet(rowCount=None, size=None, fileCount=None, *, producer=\'\')\\\\nBases\\\\n\\\\nrowCount (int | None)\\\\nsize (int | None)\\\\nfileCount (int | None)\\\\nproducer (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nrowCount int | None\\\\nThe size in bytes written to the dataset\\\\n\\\\n\\\\n\\\\nfileCount RedactMixin\\\\n\\\\nParameters str\\\\nthe identifier of the owner of the Dataset. It is recommended to define this as a URN. For example\\\\napplicationjdoe, team str | None\\\\nThe type of ownership (optional)\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.generated.ownershipdataset.OwnershipDatasetFacet(owners=Nothing.NOTHING, *, producer=\'\', deleted=None)\\\\nBases\\\\n\\\\nowners (list[Owner] | None)\\\\nproducer (str)\\\\ndeleted (bool | None)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nowners RedactMixin\\\\n\\\\nParameters str\\\\nthe identifier of the owner of the Job. It is recommended to define this as a URN. For example\\\\napplicationjdoe, team str | None\\\\nThe type of ownership (optional)\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.generated.ownershipjob.OwnershipJobFacet(owners=Nothing.NOTHING, *, producer=\'\', deleted=None)\\\\nBases\\\\n\\\\nowners (list[Owner] | None)\\\\nproducer (str)\\\\ndeleted (bool | None)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nowners RedactMixin\\\\n\\\\nParameters str\\\\nThe namespace containing that job\\\\n\\\\n\\\\n\\\\nname RunFacet\\\\nthe id of the parent run and job, iff this run was spawn from an other run (for example, the Dag run\\\\nscheduling its tasks)\\\\n\\\\nParameters Run\\\\n\\\\n\\\\n\\\\njob\\\\n\\\\nrunId (str)\\\\nnamespace (str)\\\\nname (str)\\\\n\\\\n\\\\nReturn type RedactMixin\\\\n\\\\nParameters str\\\\nThe globally unique ID of the run associated with the job.\\\\n\\\\n\\\\n\\\\nrunid_check(attribute, value)\\\\n\\\\nParameters\\\\nNone\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.processing_engine_run module\\\\n\\\\n\\\\nclass openlineage.client.generated.processing_engine_run.ProcessingEngineRunFacet(version, name=None, openlineageAdapterVersion=None, , producer=\'\')\\\\nBases\\\\n\\\\nversion (str)\\\\nname (str | None)\\\\nopenlineageAdapterVersion (str | None)\\\\nproducer (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nversion str | None\\\\nProcessing engine name, e.g. Airflow or Spark\\\\n\\\\n\\\\n\\\\nopenlineageAdapterVersion DatasetFacet\\\\n\\\\nParameters list[SchemaDatasetFacetFields] | None\\\\nThe fields of the data source.\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.generated.schemadataset.SchemaDatasetFacetFields(name, type=None, description=None, fields=Nothing.NOTHING)\\\\nBases\\\\n\\\\nname (str)\\\\ntype (str | None)\\\\ndescription (str | None)\\\\nfields (list[SchemaDatasetFacetFields] | None)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nname str | None\\\\nThe type of the field.\\\\n\\\\n\\\\n\\\\ndescription list[SchemaDatasetFacetFields] | None\\\\nNested struct fields.\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.sourcecodejob module\\\\n\\\\n\\\\nclass openlineage.client.generated.sourcecodejob.SourceCodeJobFacet(language, sourceCode, *, producer=\'\', deleted=None)\\\\nBases\\\\n\\\\nlanguage (str)\\\\nsourceCode (str)\\\\nproducer (str)\\\\ndeleted (bool | None)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nlanguage str\\\\nSource code of this job.\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.sourcecodelocationjob module\\\\n\\\\n\\\\nclass openlineage.client.generated.sourcecodelocationjob.SourceCodeLocationJobFacet(type, url, repoUrl=None, path=None, version=None, tag=None, branch=None, *, producer=\'\', deleted=None)\\\\nBases\\\\n\\\\ntype (str)\\\\nurl (str)\\\\nrepoUrl (str | None)\\\\npath (str | None)\\\\nversion (str | None)\\\\ntag (str | None)\\\\nbranch (str | None)\\\\nproducer (str)\\\\ndeleted (bool | None)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\ntype str\\\\nthe full http URL to locate the file\\\\n\\\\n\\\\n\\\\nrepoUrl str | None\\\\nthe path in the repo containing the source files\\\\n\\\\n\\\\n\\\\nversion str | None\\\\noptional tag name\\\\n\\\\n\\\\n\\\\nbranch\\\\n\\\\nattribute (str)\\\\nvalue (str)\\\\n\\\\n\\\\nReturn type JobFacet\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.storage_dataset module\\\\n\\\\n\\\\nclass openlineage.client.generated.storage_dataset.StorageDatasetFacet(storageLayer, fileFormat=None, , producer=\'\', deleted=None)\\\\nBases\\\\n\\\\nstorageLayer (str)\\\\nfileFormat (str | None)\\\\nproducer (str)\\\\ndeleted (bool | None)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nstorageLayer\\\\nStorage layer provider with allowed values\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nfileFormat\\\\nFile format with allowed values\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.generated.symlinksdataset module\\\\n\\\\n\\\\nclass openlineage.client.generated.symlinksdataset.Identifier(namespace, name, type)\\\\nBases\\\\n\\\\nnamespace (str)\\\\nname (str)\\\\ntype (str)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nnamespace str\\\\nThe dataset name\\\\n\\\\n\\\\n\\\\ntype DatasetFacet\\\\n\\\\nParameters list[Identifier] | None\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.transport.console module\\\\n\\\\n\\\\nclass openlineage.client.transport.console.ConsoleConfig\\\\nBases Transport\\\\n\\\\nParameters str | None = \'console\'\\\\n\\\\n\\\\n\\\\nconfigclass\\\\nalias of ConsoleConfig\\\\n\\\\n\\\\n\\\\nemit(event)\\\\n\\\\nParameters\\\\nNone\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.transport.factory module\\\\n\\\\n\\\\nclass openlineage.client.transport.factory.DefaultTransportFactory\\\\nBases\\\\n\\\\noftype (str)\\\\nclazz (type[Transport] | str)\\\\n\\\\n\\\\nReturn type\\\\nconfig (dict[str, str] | None)\\\\n\\\\nReturn type Config\\\\n\\\\nParameters str\\\\n\\\\n\\\\n\\\\nappend\\\\nparams (dict[str, Any])\\\\n\\\\nReturn type Transport\\\\n\\\\nParameters str | None = \'file\'\\\\n\\\\n\\\\n\\\\nconfigclass\\\\nalias of FileConfig\\\\n\\\\n\\\\n\\\\nemit(event)\\\\n\\\\nParameters\\\\nNone\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.transport.http module\\\\n\\\\n\\\\nclass openlineage.client.transport.http.TokenProvider(config)\\\\nBases\\\\nconfig (dict[str, str])\\\\n\\\\n\\\\n\\\\n\\\\ngetbearer()\\\\n\\\\nReturn type Enum\\\\nAn enumeration.\\\\n\\\\n\\\\nGZIP = \'gzip\'\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.transport.http.ApiKeyTokenProvider(config)\\\\nBases\\\\nconfig (dict[str, str])\\\\n\\\\n\\\\n\\\\n\\\\ngetbearer()\\\\n\\\\nReturn type\\\\nauth (dict[str, str])\\\\n\\\\nReturn type\\\\nSession\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.transport.http.HttpConfig(url, endpoint=\'api/v1/lineage\', timeout=5.0, verify=True, auth=Nothing.NOTHING, compression=None, session=None, adapter=None)\\\\nBases\\\\n\\\\nurl (str)\\\\nendpoint (str)\\\\ntimeout (float)\\\\nverify (bool)\\\\nauth (TokenProvider)\\\\ncompression (HttpCompression | None)\\\\nsession (Session | None)\\\\nadapter (HTTPAdapter | None)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nurl str\\\\n\\\\n\\\\n\\\\ntimeout bool\\\\n\\\\n\\\\n\\\\nauth HttpCompression | None\\\\n\\\\n\\\\n\\\\nsession HTTPAdapter | None\\\\n\\\\n\\\\n\\\\nclassmethod fromdict(params)\\\\n\\\\nParameters\\\\nHttpConfig\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclassmethod fromoptions(url, options, session)\\\\n\\\\nParameters\\\\nHttpConfig\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.transport.http.HttpTransport(config)\\\\nBases\\\\nconfig (HttpConfig)\\\\n\\\\n\\\\n\\\\n\\\\nkind\\\\nadapter (HTTPAdapter)\\\\n\\\\nReturn type\\\\nevent (Union[RunEvent, DatasetEvent, JobEvent, RunEvent, DatasetEvent, JobEvent])\\\\n\\\\nReturn type Config\\\\n\\\\nParameters dict[str, str]\\\\n\\\\n\\\\n\\\\ntopic str | None\\\\n\\\\n\\\\n\\\\nflush\\\\nparams (dict[str, Any])\\\\n\\\\nReturn type\\\\n\\\\nerr (KafkaError)\\\\nmsg (Message)\\\\n\\\\n\\\\nReturn type Transport\\\\n\\\\nParameters str | None = \'kafka\'\\\\n\\\\n\\\\n\\\\nconfigclass\\\\nalias of KafkaConfig\\\\n\\\\n\\\\n\\\\nemit(event)\\\\n\\\\nParameters\\\\nNone\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nopenlineage.client.transport.mskiam module\\\\n\\\\n\\\\nclass openlineage.client.transport.mskiam.MSKIAMConfig(config, topic, messageKey=None, flush=True, region=None, awsprofile=None, rolearn=None, awsdebugcreds=False)\\\\nBases\\\\n\\\\nconfig (dict[str, str])\\\\ntopic (str)\\\\nmessageKey (str | None)\\\\nflush (bool)\\\\nregion (str)\\\\nawsprofile (None | str)\\\\nrolearn (None | str)\\\\nawsdebugcreds (bool)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nregion None | str\\\\n\\\\n\\\\n\\\\nrolearn bool\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.transport.mskiam.MSKIAMTransport(config)\\\\nBases\\\\nconfig (MSKIAMConfig)\\\\n\\\\n\\\\n\\\\n\\\\nkind Config\\\\n\\\\n\\\\n\\\\nclass openlineage.client.transport.noop.NoopTransport(config)\\\\nBases\\\\nconfig (NoopConfig)\\\\n\\\\n\\\\n\\\\n\\\\nkind\\\\nevent (Union[RunEvent, DatasetEvent, JobEvent, RunEvent, DatasetEvent, JobEvent])\\\\n\\\\nReturn type object\\\\n\\\\n\\\\nclassmethod fromdict(params)\\\\n\\\\nParameters\\\\nT\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.transport.transport.Transport\\\\nBases str | None = None\\\\n\\\\n\\\\n\\\\nconfig_class\\\\nalias of Config\\\\n\\\\n\\\\n\\\\nemit(event)\\\\n\\\\nParameters\\\\nAny\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nclass openlineage.client.transport.transport.TransportFactory\\\\nBases\\\\nconfig (dict[str, str] | None)\\\\n\\\\nReturn type:\\\\nTransport\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\"}}>","sidebar":"tutorialSidebar"},"development/developing/python/setup":{"id":"development/developing/python/setup","title":"Setup a development environment","description":"There are four Python OpenLineage packages that you can install locally when setting up a development environment.","sidebar":"tutorialSidebar"},"development/developing/python/tests/airflow":{"id":"development/developing/python/tests/airflow","title":"Airflow","description":"OpenLineage provides an integration with Apache Airflow. As Airflow is actively developed and major changes happen quite often it is advised to test OpenLineage integration against multiple Airflow versions. In the current CI process OpenLineage is tested against following versions:","sidebar":"tutorialSidebar"},"development/developing/python/tests/client":{"id":"development/developing/python/tests/client","title":"Client","description":"This page needs your contribution! Please contribute new examples using the edit link at the bottom.","sidebar":"tutorialSidebar"},"development/developing/python/tests/common":{"id":"development/developing/python/tests/common","title":"Common","description":"This page needs your contribution! Please contribute new examples using the edit link at the bottom.","sidebar":"tutorialSidebar"},"development/developing/python/tests/dagster":{"id":"development/developing/python/tests/dagster","title":"Dagster","description":"This page needs your contribution! Please contribute new examples using the edit link at the bottom.","sidebar":"tutorialSidebar"},"development/developing/python/troubleshooting/logging":{"id":"development/developing/python/troubleshooting/logging","title":"Logging","description":"OpenLineage uses python\'s logging facility when generating logs. Being able to emit logs for various purposes is very helpful when troubleshooting OpenLineage.","sidebar":"tutorialSidebar"},"development/developing/spark/built_in_lineage":{"id":"development/developing/spark/built_in_lineage","title":"Integrating with Spark extensions","description":"Feature available since 1.11.","sidebar":"tutorialSidebar"},"development/developing/spark/setup":{"id":"development/developing/spark/setup","title":"Build","description":"Java 8","sidebar":"tutorialSidebar"},"development/examples":{"id":"development/examples","title":"Example Lineage Events","description":"Simple Examples","sidebar":"tutorialSidebar"},"development/ol-proxy":{"id":"development/ol-proxy","title":"OpenLineage Proxy","description":"OpenLineage Proxy is a simple Java server that can be used to monitor the JSON events that OpenLineage client emits, as well as tunnel the transmission to the OpenLineage backend such as Marquez.","sidebar":"tutorialSidebar"},"faq":{"id":"faq","title":"Frequently Asked Questions","description":"This page needs your contribution! Please contribute new questions (or answers) using the edit link at the bottom.","sidebar":"tutorialSidebar"},"guides/about":{"id":"guides/about","title":"About These Guides","description":"The following tutorials take you through the process of exploiting the lineage metadata provided by Marquez and OpenLineage to solve common data engineering problems and make new analytical and historical insights into your pipelines.","sidebar":"tutorialSidebar"},"guides/airflow_proxy":{"id":"guides/airflow_proxy","title":"Using the OpenLineage Proxy with Airflow","description":"This tutorial introduces you to using the OpenLineage Proxy with Airflow. OpenLineage has various integrations that will enable Airflow to emit OpenLineage events when using Airflow Integrations. In this tutorial, you will be running a local instance of Airflow using Docker Compose and learning how to enable and setup OpenLineage to emit data lineage events. The tutorial will use two backends to check the data lineage, 1) the Proxy, and 2) Marquez.","sidebar":"tutorialSidebar"},"guides/airflow-backfill-dags":{"id":"guides/airflow-backfill-dags","title":"Backfilling Airflow DAGs Using Marquez","description":"Adapted from a blog post by Willy Lulciuc","sidebar":"tutorialSidebar"},"guides/airflow-quickstart":{"id":"guides/airflow-quickstart","title":"Getting Started with Airflow and OpenLineage+Marquez","description":"In this example, we\'ll walk you through how to enable Airflow DAGs to send lineage metadata to Marquez using OpenLineage.","sidebar":"tutorialSidebar"},"guides/dbt":{"id":"guides/dbt","title":"Using Marquez with dbt","description":"Adapted from a blog post by Ross Turk","sidebar":"tutorialSidebar"},"guides/facets":{"id":"guides/facets","title":"Understanding and Using Facets","description":"Adapted from the OpenLineage spec.","sidebar":"tutorialSidebar"},"guides/spark":{"id":"guides/spark","title":"Using OpenLineage with Spark","description":"Adapted from a blog post by Michael Collado","sidebar":"tutorialSidebar"},"index":{"id":"index","title":"About OpenLineage","description":"OpenLineage is an open framework for data lineage collection and analysis. At its core is an extensible specification that systems can use to interoperate with lineage metadata.","sidebar":"tutorialSidebar"},"integrations/about":{"id":"integrations/about","title":"OpenLineage Integrations","description":"Capability Matrix","sidebar":"tutorialSidebar"},"integrations/airflow/airflow":{"id":"integrations/airflow/airflow","title":"Apache Airflow","description":"This page is about Airflow\'s external integration that works mainly for Airflow versions <2.7.","sidebar":"tutorialSidebar"},"integrations/airflow/default-extractors":{"id":"integrations/airflow/default-extractors","title":"Exposing Lineage in Airflow Operators","description":"This page is about Airflow\'s external integration that works mainly for Airflow versions <2.7.","sidebar":"tutorialSidebar"},"integrations/airflow/extractors/custom-extractors":{"id":"integrations/airflow/extractors/custom-extractors","title":"Custom Extractors","description":"This page is about Airflow\'s external integration that works mainly for Airflow versions <2.7.","sidebar":"tutorialSidebar"},"integrations/airflow/extractors/extractor-testing":{"id":"integrations/airflow/extractors/extractor-testing","title":"Testing Custom Extractors","description":"This page is about Airflow\'s external integration that works mainly for Airflow versions <2.7.","sidebar":"tutorialSidebar"},"integrations/airflow/job-hierarchy":{"id":"integrations/airflow/job-hierarchy","title":"Job Hierarchy","description":"This page is about Airflow\'s external integration that works mainly for Airflow versions <2.7.","sidebar":"tutorialSidebar"},"integrations/airflow/manual":{"id":"integrations/airflow/manual","title":"Manually Annotated Lineage","description":"This page is about Airflow\'s external integration that works mainly for Airflow versions <2.7.","sidebar":"tutorialSidebar"},"integrations/airflow/older":{"id":"integrations/airflow/older","title":"Supported Airflow versions","description":"This page is about Airflow\'s external integration that works mainly for Airflow versions <2.7.","sidebar":"tutorialSidebar"},"integrations/airflow/preflight-check-dag":{"id":"integrations/airflow/preflight-check-dag","title":"Preflight check DAG","description":"Purpose","sidebar":"tutorialSidebar"},"integrations/airflow/usage":{"id":"integrations/airflow/usage","title":"Using the Airflow Integration","description":"This page is about Airflow\'s external integration that works mainly for Airflow versions <2.7.","sidebar":"tutorialSidebar"},"integrations/dbt":{"id":"integrations/dbt","title":"dbt","description":"dbt (data build tool) is a powerful transformation engine. It operates on data already within a warehouse, making it easy for data engineers to build complex pipelines from the comfort of their laptops. While it doesn\u2019t perform extraction and loading of data, it\u2019s extremely powerful at transformations.","sidebar":"tutorialSidebar"},"integrations/flink":{"id":"integrations/flink","title":"Apache Flink","description":"Apache Flink is one of the most popular stream processing frameworks. Apache Flink jobs run on clusters,","sidebar":"tutorialSidebar"},"integrations/great-expectations":{"id":"integrations/great-expectations","title":"Great Expectations","description":"Great Expectations is a robust data quality tool. It runs suites of checks, called expectations, over a defined dataset. This dataset can be a table in a database, or a Spark or Pandas dataframe. Expectations are run by checkpoints, which are configuration files that describe not just the expectations to use, but also any batching, runtime configurations, and, importantly, the action list of actions run after the expectation suite completes.","sidebar":"tutorialSidebar"},"integrations/spark/configuration/airflow":{"id":"integrations/spark/configuration/airflow","title":"Scheduling from Airflow","description":"The same parameters passed to spark-submit can be supplied from Airflow and other schedulers. If","sidebar":"tutorialSidebar"},"integrations/spark/configuration/circuit_breaker":{"id":"integrations/spark/configuration/circuit_breaker","title":"Circuit Breaker","description":"","sidebar":"tutorialSidebar"},"integrations/spark/configuration/spark_conf":{"id":"integrations/spark/configuration/spark_conf","title":"Spark Config Parameters","description":"The following parameters can be specified:","sidebar":"tutorialSidebar"},"integrations/spark/configuration/transport":{"id":"integrations/spark/configuration/transport","title":"Transport","description":"","sidebar":"tutorialSidebar"},"integrations/spark/configuration/usage":{"id":"integrations/spark/configuration/usage","title":"Usage","description":"Configuring the OpenLineage Spark integration is straightforward. It uses built-in Spark configuration mechanisms.","sidebar":"tutorialSidebar"},"integrations/spark/extending":{"id":"integrations/spark/extending","title":"Extending","description":"The Spark library is intended to support extension via custom implementations of a handful","sidebar":"tutorialSidebar"},"integrations/spark/installation":{"id":"integrations/spark/installation","title":"Installation","description":"* Version 1.8.0 and earlier only supported Scala 2.12 variants of Apache Spark.","sidebar":"tutorialSidebar"},"integrations/spark/job-hierarchy":{"id":"integrations/spark/job-hierarchy","title":"Job Hierarchy","description":"Please get familiar with OpenLineage Job Hierarchy concept before reading this.","sidebar":"tutorialSidebar"},"integrations/spark/main_concept":{"id":"integrations/spark/main_concept","title":"Main Concepts","description":"Spark jobs typically run on clusters of machines. A single machine hosts the \\"driver\\" application,","sidebar":"tutorialSidebar"},"integrations/spark/metrics":{"id":"integrations/spark/metrics","title":"Spark Integration Metrics","description":"The OpenLineage integration with Spark not only utilizes the Java client\'s metrics but also introduces its own set of metrics specific to Spark operations. Below is a list of these metrics.","sidebar":"tutorialSidebar"},"integrations/spark/quickstart/quickstart_databricks":{"id":"integrations/spark/quickstart/quickstart_databricks","title":"Quickstart with Databricks","description":"OpenLineage\'s Spark Integration can be installed on Databricks leveraging init scripts. Please note, Databricks on Google Cloud does not currently support the DBFS CLI, so the proposed solution will not work on Google Cloud until that feature is enabled.","sidebar":"tutorialSidebar"},"integrations/spark/quickstart/quickstart_local":{"id":"integrations/spark/quickstart/quickstart_local","title":"Quickstart with Jupyter","description":"Trying out the Spark integration is super easy if you already have Docker Desktop and git installed.","sidebar":"tutorialSidebar"},"integrations/spark/spark":{"id":"integrations/spark/spark","title":"Apache Spark","description":"This integration is known to work with latest Spark versions as well as Apache Spark 2.4.","sidebar":"tutorialSidebar"},"integrations/spark/spark_column_lineage":{"id":"integrations/spark/spark_column_lineage","title":"Column-Level Lineage","description":"Column-level lineage works only with Spark 3.","sidebar":"tutorialSidebar"},"integrations/spark/testing":{"id":"integrations/spark/testing","title":"Testing","description":"Configurable Integration Test","sidebar":"tutorialSidebar"},"integrations/trino":{"id":"integrations/trino","title":"Trino","description":"This integration is known to work with Trino 450 and later.","sidebar":"tutorialSidebar"},"releases/0_1_0":{"id":"releases/0_1_0","title":"0.1.0","description":"OpenLineage is an Open Standard for lineage metadata collection designed to record metadata for a job in execution. The initial public release includes:","sidebar":"tutorialSidebar"},"releases/0_10_0":{"id":"releases/0_10_0","title":"0.10.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_11_0":{"id":"releases/0_11_0","title":"0.11.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_12_0":{"id":"releases/0_12_0","title":"0.12.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_13_0":{"id":"releases/0_13_0","title":"0.13.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_13_1":{"id":"releases/0_13_1","title":"0.13.1","description":"Fixed","sidebar":"tutorialSidebar"},"releases/0_14_0":{"id":"releases/0_14_0","title":"0.14.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_14_1":{"id":"releases/0_14_1","title":"0.14.1","description":"Fixed","sidebar":"tutorialSidebar"},"releases/0_15_1":{"id":"releases/0_15_1","title":"0.15.1","description":"Added","sidebar":"tutorialSidebar"},"releases/0_16_1":{"id":"releases/0_16_1","title":"0.16.1","description":"Added","sidebar":"tutorialSidebar"},"releases/0_17_0":{"id":"releases/0_17_0","title":"0.17.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_18_0":{"id":"releases/0_18_0","title":"0.18.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_19_2":{"id":"releases/0_19_2","title":"0.19.2","description":"Added","sidebar":"tutorialSidebar"},"releases/0_2_0":{"id":"releases/0_2_0","title":"0.2.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_2_1":{"id":"releases/0_2_1","title":"0.2.1","description":"Fixed","sidebar":"tutorialSidebar"},"releases/0_2_2":{"id":"releases/0_2_2","title":"0.2.2","description":"Added","sidebar":"tutorialSidebar"},"releases/0_2_3":{"id":"releases/0_2_3","title":"0.2.3","description":"Fixed","sidebar":"tutorialSidebar"},"releases/0_20_4":{"id":"releases/0_20_4","title":"0.20.4","description":"Added","sidebar":"tutorialSidebar"},"releases/0_20_6":{"id":"releases/0_20_6","title":"0.20.6","description":"Added","sidebar":"tutorialSidebar"},"releases/0_21_1":{"id":"releases/0_21_1","title":"0.21.1","description":"Added","sidebar":"tutorialSidebar"},"releases/0_22_0":{"id":"releases/0_22_0","title":"0.22.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_23_0":{"id":"releases/0_23_0","title":"0.23.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_24_0":{"id":"releases/0_24_0","title":"0.24.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_25_0":{"id":"releases/0_25_0","title":"0.25.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_26_0":{"id":"releases/0_26_0","title":"0.26.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_27_1":{"id":"releases/0_27_1","title":"0.27.1","description":"Added","sidebar":"tutorialSidebar"},"releases/0_27_2":{"id":"releases/0_27_2","title":"0.27.2","description":"Fixed","sidebar":"tutorialSidebar"},"releases/0_28_0":{"id":"releases/0_28_0","title":"0.28.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_29_2":{"id":"releases/0_29_2","title":"0.29.2","description":"Added","sidebar":"tutorialSidebar"},"releases/0_3_0":{"id":"releases/0_3_0","title":"0.3.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_3_1":{"id":"releases/0_3_1","title":"0.3.1","description":"Fixed","sidebar":"tutorialSidebar"},"releases/0_30_1":{"id":"releases/0_30_1","title":"0.30.1","description":"Added","sidebar":"tutorialSidebar"},"releases/0_4_0":{"id":"releases/0_4_0","title":"0.4.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_5_1":{"id":"releases/0_5_1","title":"0.5.1","description":"Added","sidebar":"tutorialSidebar"},"releases/0_5_2":{"id":"releases/0_5_2","title":"0.5.2","description":"Added","sidebar":"tutorialSidebar"},"releases/0_6_0":{"id":"releases/0_6_0","title":"0.6.0","description":"Added","sidebar":"tutorialSidebar"},"releases/0_6_1":{"id":"releases/0_6_1","title":"0.6.1","description":"Fixed","sidebar":"tutorialSidebar"},"releases/0_6_2":{"id":"releases/0_6_2","title":"0.6.2","description":"Added","sidebar":"tutorialSidebar"},"releases/0_7_1":{"id":"releases/0_7_1","title":"0.7.1","description":"Added","sidebar":"tutorialSidebar"},"releases/0_8_1":{"id":"releases/0_8_1","title":"0.8.1","description":"Added","sidebar":"tutorialSidebar"},"releases/0_8_2":{"id":"releases/0_8_2","title":"0.8.2","description":"Added","sidebar":"tutorialSidebar"},"releases/0_9_0":{"id":"releases/0_9_0","title":"0.9.0","description":"Added","sidebar":"tutorialSidebar"},"releases/1_0_0":{"id":"releases/1_0_0","title":"1.0.0","description":"Added","sidebar":"tutorialSidebar"},"releases/1_1_0":{"id":"releases/1_1_0","title":"1.1.0","description":"Added","sidebar":"tutorialSidebar"},"releases/1_10_2":{"id":"releases/1_10_2","title":"1.10.2","description":"Added","sidebar":"tutorialSidebar"},"releases/1_11_3":{"id":"releases/1_11_3","title":"1.11.3","description":"Added","sidebar":"tutorialSidebar"},"releases/1_12_0":{"id":"releases/1_12_0","title":"1.12.0","description":"Added","sidebar":"tutorialSidebar"},"releases/1_13_1":{"id":"releases/1_13_1","title":"1.13.1","description":"Added","sidebar":"tutorialSidebar"},"releases/1_14_0":{"id":"releases/1_14_0","title":"1.14.0","description":"Added","sidebar":"tutorialSidebar"},"releases/1_15_0":{"id":"releases/1_15_0","title":"1.15.0","description":"Added","sidebar":"tutorialSidebar"},"releases/1_16_0":{"id":"releases/1_16_0","title":"1.16.0","description":"Added","sidebar":"tutorialSidebar"},"releases/1_2_2":{"id":"releases/1_2_2","title":"1.2.2","description":"Added","sidebar":"tutorialSidebar"},"releases/1_3_1":{"id":"releases/1_3_1","title":"1.3.1","description":"Added","sidebar":"tutorialSidebar"},"releases/1_4_1":{"id":"releases/1_4_1","title":"1.4.1","description":"Added","sidebar":"tutorialSidebar"},"releases/1_5_0":{"id":"releases/1_5_0","title":"1.5.0","description":"Added","sidebar":"tutorialSidebar"},"releases/1_6_2":{"id":"releases/1_6_2","title":"1.6.2","description":"Added","sidebar":"tutorialSidebar"},"releases/1_7_0":{"id":"releases/1_7_0","title":"1.7.0","description":"COMPATIBILITY NOTICE","sidebar":"tutorialSidebar"},"releases/1_8_0":{"id":"releases/1_8_0","title":"1.8.0","description":"Added","sidebar":"tutorialSidebar"},"releases/1_9_1":{"id":"releases/1_9_1","title":"1.9.1","description":"This version adds the capability to publish Scala 2.12 and 2.13 variants of Apache Spark,","sidebar":"tutorialSidebar"},"spec/facets/custom-facets":{"id":"spec/facets/custom-facets","title":"Custom Facets","description":"In addition to the existing facets mentioned in this documentation, users can extend the base facets and provide their own facet definition as part of the payload in OpenLineage event. For example, when OpenLineage event is emitted from the Apache Airflow using OpenLineage\'s Airflow integration, the following facets can be observed:","sidebar":"tutorialSidebar"},"spec/facets/dataset-facets/column_lineage_facet":{"id":"spec/facets/dataset-facets/column_lineage_facet","title":"Column Level Lineage Dataset Facet","description":"Column level lineage provides fine grained information on datasets\' dependencies.","sidebar":"tutorialSidebar"},"spec/facets/dataset-facets/data_quality_assertions":{"id":"spec/facets/dataset-facets/data_quality_assertions","title":"Data Quality Assertions Facet","description":"Example:","sidebar":"tutorialSidebar"},"spec/facets/dataset-facets/data_source":{"id":"spec/facets/dataset-facets/data_source","title":"Datasource Facet","description":"Example:","sidebar":"tutorialSidebar"},"spec/facets/dataset-facets/dataset-facets":{"id":"spec/facets/dataset-facets/dataset-facets","title":"Dataset Facets","description":"Dataset Facets are generally consisted of common facet that is used both in inputs and outputs of the OpenLineage event. There are facets that exist specifically for input or output datasets.","sidebar":"tutorialSidebar"},"spec/facets/dataset-facets/input-dataset-facets/data_quality_metrics":{"id":"spec/facets/dataset-facets/input-dataset-facets/data_quality_metrics","title":"Data Quality Metrics Facet","description":"Example:","sidebar":"tutorialSidebar"},"spec/facets/dataset-facets/lifecycle_state_change":{"id":"spec/facets/dataset-facets/lifecycle_state_change","title":"Lifecycle State Change Facet","description":"Example:","sidebar":"tutorialSidebar"},"spec/facets/dataset-facets/output-dataset-facets/output_statistics":{"id":"spec/facets/dataset-facets/output-dataset-facets/output_statistics","title":"Output Statistics Facet","description":"Example:","sidebar":"tutorialSidebar"},"spec/facets/dataset-facets/ownership":{"id":"spec/facets/dataset-facets/ownership","title":"Ownership Dataset Facet","description":"Example:","sidebar":"tutorialSidebar"},"spec/facets/dataset-facets/schema":{"id":"spec/facets/dataset-facets/schema","title":"Schema Dataset Facet","description":"The schema dataset facet contains the schema of a particular dataset.","sidebar":"tutorialSidebar"},"spec/facets/dataset-facets/storage":{"id":"spec/facets/dataset-facets/storage","title":"Storage Facet","description":"Example:","sidebar":"tutorialSidebar"},"spec/facets/dataset-facets/symlinks":{"id":"spec/facets/dataset-facets/symlinks","title":"Symlinks Facet","description":"Example:","sidebar":"tutorialSidebar"},"spec/facets/dataset-facets/version_facet":{"id":"spec/facets/dataset-facets/version_facet","title":"Version Facet","description":"Example:","sidebar":"tutorialSidebar"},"spec/facets/facets":{"id":"spec/facets/facets","title":"Facets & Extensibility","description":"Facets provide context to the OpenLineage events. Generally, an OpenLineage event contains the type of the event, who created it, and when the event happened. In addition to the basic information related to the event, it provides facets for more details in four general categories:","sidebar":"tutorialSidebar"},"spec/facets/job-facets/documentation":{"id":"spec/facets/job-facets/documentation","title":"Documentation Facet","description":"Contains the documentation or description of the job.","sidebar":"tutorialSidebar"},"spec/facets/job-facets/job-facets":{"id":"spec/facets/job-facets/job-facets","title":"Job Facets","description":"Job Facets apply to a distinct instance of a job: an abstract process that consumes, executes, and produces datasets (defined as its inputs and outputs). It is identified by a unique name within a namespace. The Job evolves over time and this change is captured during the job runs.","sidebar":"tutorialSidebar"},"spec/facets/job-facets/job-type":{"id":"spec/facets/job-facets/job-type","title":"Job type Job Facet","description":"Facet to contain job properties like:","sidebar":"tutorialSidebar"},"spec/facets/job-facets/ownership":{"id":"spec/facets/job-facets/ownership","title":"Ownership Job Facet","description":"The facet that contains the information regarding users or group who owns this particular job.","sidebar":"tutorialSidebar"},"spec/facets/job-facets/source-code":{"id":"spec/facets/job-facets/source-code","title":"Source Code Facet","description":"The source code of a particular job (e.g. Python script)","sidebar":"tutorialSidebar"},"spec/facets/job-facets/source-code-location":{"id":"spec/facets/job-facets/source-code-location","title":"Source Code Location Facet","description":"The facet that indicates where the source code is located.","sidebar":"tutorialSidebar"},"spec/facets/job-facets/sql":{"id":"spec/facets/job-facets/sql","title":"SQL Job Facet","description":"The SQL Job Facet contains a SQL query that was used in a particular job.","sidebar":"tutorialSidebar"},"spec/facets/run-facets/error_message":{"id":"spec/facets/run-facets/error_message","title":"Error Message Facet","description":"The facet to contain information about the failures during the run of the job. A typical payload would be the message, stack trace, etc.","sidebar":"tutorialSidebar"},"spec/facets/run-facets/external_query":{"id":"spec/facets/run-facets/external_query","title":"External Query Facet","description":"The facet that describes the identification of the query that the run is related to which was executed by external systems. Even though the query itself is not contained, using this facet, the user should be able to access the query and its details.","sidebar":"tutorialSidebar"},"spec/facets/run-facets/nominal_time":{"id":"spec/facets/run-facets/nominal_time","title":"Nominal Time Facet","description":"The facet to describe the nominal start and end time of the run. The nominal usually means the time the job run was expected to run (like a scheduled time), and the actual time can be different.","sidebar":"tutorialSidebar"},"spec/facets/run-facets/parent_run":{"id":"spec/facets/run-facets/parent_run","title":"Parent Run Facet","description":"Commonly, scheduler systems like Apache Airflow will trigger processes on remote systems, such as on Apache Spark or Apache Beam jobs.","sidebar":"tutorialSidebar"},"spec/facets/run-facets/run-facets":{"id":"spec/facets/run-facets/run-facets","title":"Run Facets","description":"Run Facets apply to a specific instance of a particular running job. Every run will have a uniquely identifiable run ID that is usually a UUID, that can later be tracked. It is recommended to use UUIDv7 version of the format.","sidebar":"tutorialSidebar"},"spec/job-hierarchy":{"id":"spec/job-hierarchy","title":"Job Hierarchy","description":"This feature is available in OpenLineage versions >= 1.9.0.","sidebar":"tutorialSidebar"},"spec/naming":{"id":"spec/naming","title":"Naming Conventions","description":"Employing a unique naming strategy per resource ensures that the spec is followed uniformly regardless of metadata producer.","sidebar":"tutorialSidebar"},"spec/object-model":{"id":"spec/object-model","title":"Object Model","description":"OpenLineage was designed to enable large-scale observation of datasets as they move through a complex pipeline.","sidebar":"tutorialSidebar"},"spec/producers":{"id":"spec/producers","title":"Producers","description":"This page could use some extra detail! You\'re welcome to contribute using the Edit link at the bottom.","sidebar":"tutorialSidebar"},"spec/run-cycle":{"id":"spec/run-cycle","title":"The Run Cycle","description":"The OpenLineage object model is event-based and updates provide an OpenLineage backend with details about the activities of a Job.","sidebar":"tutorialSidebar"},"spec/schemas":{"id":"spec/schemas","title":"Working with Schemas","description":"OpenLineage is a rapidly growing open source project, and therefore, will face many new changes in its SPEC. The spec file is based on JSON schema specification and defines how the OpenLineage\'s event message would be structured. More details on what are defined in its object model can be found here.","sidebar":"tutorialSidebar"}}}')}}]);