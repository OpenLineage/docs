"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[3338],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>c});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function p(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),s=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},d=function(e){var t=s(e.components);return n.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,d=p(e,["components","mdxType","originalType","parentName"]),m=s(a),c=r,k=m["".concat(l,".").concat(c)]||m[c]||u[c]||i;return a?n.createElement(k,o(o({ref:t},d),{},{components:a})):n.createElement(k,o({ref:t},d))}));function c(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=m;var p={};for(var l in t)hasOwnProperty.call(t,l)&&(p[l]=t[l]);p.originalType=e,p.mdxType="string"==typeof e?e:r,o[1]=p;for(var s=2;s<i;s++)o[s]=a[s];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},56608:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>R,contentTitle:()=>C,default:()=>T,frontMatter:()=>_,metadata:()=>I,toc:()=>w});var n=a(87462),r=a(67294),i=a(3905),o=a(86010),p=a(12466),l=a(76775),s=a(91980),d=a(67392),u=a(50012);function m(e){return function(e){return r.Children.map(e,(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:t,label:a,attributes:n,default:r}}=e;return{value:t,label:a,attributes:n,default:r}}))}function c(e){const{values:t,children:a}=e;return(0,r.useMemo)((()=>{const e=t??m(a);return function(e){const t=(0,d.l)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,a])}function k(e){let{value:t,tabValues:a}=e;return a.some((e=>e.value===t))}function h(e){let{queryString:t=!1,groupId:a}=e;const n=(0,l.k6)(),i=function(e){let{queryString:t=!1,groupId:a}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:t,groupId:a});return[(0,s._X)(i),(0,r.useCallback)((e=>{if(!i)return;const t=new URLSearchParams(n.location.search);t.set(i,e),n.replace({...n.location,search:t.toString()})}),[i,n])]}function g(e){const{defaultValue:t,queryString:a=!1,groupId:n}=e,i=c(e),[o,p]=(0,r.useState)((()=>function(e){let{defaultValue:t,tabValues:a}=e;if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!k({value:t,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const n=a.find((e=>e.default))??a[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:t,tabValues:i}))),[l,s]=h({queryString:a,groupId:n}),[d,m]=function(e){let{groupId:t}=e;const a=function(e){return e?`docusaurus.tab.${e}`:null}(t),[n,i]=(0,u.Nk)(a);return[n,(0,r.useCallback)((e=>{a&&i.set(e)}),[a,i])]}({groupId:n}),g=(()=>{const e=l??d;return k({value:e,tabValues:i})?e:null})();(0,r.useLayoutEffect)((()=>{g&&p(g)}),[g]);return{selectedValue:o,selectValue:(0,r.useCallback)((e=>{if(!k({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);p(e),s(e),m(e)}),[s,m,i]),tabValues:i}}var N=a(72389);const f="tabList__CuJ",b="tabItem_LNqP";function y(e){let{className:t,block:a,selectedValue:i,selectValue:l,tabValues:s}=e;const d=[],{blockElementScrollPositionUntilNextRender:u}=(0,p.o5)(),m=e=>{const t=e.currentTarget,a=d.indexOf(t),n=s[a].value;n!==i&&(u(t),l(n))},c=e=>{let t=null;switch(e.key){case"Enter":m(e);break;case"ArrowRight":{const a=d.indexOf(e.currentTarget)+1;t=d[a]??d[0];break}case"ArrowLeft":{const a=d.indexOf(e.currentTarget)-1;t=d[a]??d[d.length-1];break}}t?.focus()};return r.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.Z)("tabs",{"tabs--block":a},t)},s.map((e=>{let{value:t,label:a,attributes:p}=e;return r.createElement("li",(0,n.Z)({role:"tab",tabIndex:i===t?0:-1,"aria-selected":i===t,key:t,ref:e=>d.push(e),onKeyDown:c,onClick:m},p,{className:(0,o.Z)("tabs__item",b,p?.className,{"tabs__item--active":i===t})}),a??t)})))}function x(e){let{lazy:t,children:a,selectedValue:n}=e;const i=(Array.isArray(a)?a:[a]).filter(Boolean);if(t){const e=i.find((e=>e.props.value===n));return e?(0,r.cloneElement)(e,{className:"margin-top--md"}):null}return r.createElement("div",{className:"margin-top--md"},i.map(((e,t)=>(0,r.cloneElement)(e,{key:t,hidden:e.props.value!==n}))))}function A(e){const t=g(e);return r.createElement("div",{className:(0,o.Z)("tabs-container",f)},r.createElement(y,(0,n.Z)({},e,t)),r.createElement(x,(0,n.Z)({},e,t)))}function v(e){const t=(0,N.Z)();return r.createElement(A,(0,n.Z)({key:String(t)},e))}const E="tabItem_Ymn6";function S(e){let{children:t,hidden:a,className:n}=e;return r.createElement("div",{role:"tabpanel",className:(0,o.Z)(E,n),hidden:a},t)}const _={sidebar_position:1,title:"Apache Spark"},C=void 0,I={unversionedId:"integrations/spark/spark",id:"integrations/spark/spark",title:"Apache Spark",description:"This integration is known to work with Apache Spark 2.4 and later.",source:"@site/docs/integrations/spark/spark.md",sourceDirName:"integrations/spark",slug:"/integrations/spark/",permalink:"/docs/integrations/spark/",draft:!1,editUrl:"https://github.com/OpenLineage/docs/tree/main/docs/integrations/spark/spark.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1,title:"Apache Spark"},sidebar:"tutorialSidebar",previous:{title:"OpenLineage Integrations",permalink:"/docs/integrations/about"},next:{title:"Quickstart with Databricks",permalink:"/docs/integrations/spark/quickstart_databricks"}},R={},w=[{value:"Collecting Lineage in Spark",id:"collecting-lineage-in-spark",level:2},{value:"About the Integration",id:"about-the-integration",level:2},{value:"How to Use the Integration",id:"how-to-use-the-integration",level:2},{value:"Installation",id:"installation",level:3},{value:"Bundle the package with your Apache Spark application project",id:"bundle-the-package-with-your-apache-spark-application-project",level:4},{value:"Place the JAR in your <code>${SPARK_HOME}/jars</code> directory",id:"place-the-jar-in-your-spark_homejars-directory",level:4},{value:"Use the <code>--jars</code> option with <code>spark-submit / spark-shell / pyspark</code>",id:"use-the---jars-option-with-spark-submit--spark-shell--pyspark",level:4},{value:"Use the <code>--packages</code> option with <code>spark-submit / spark-shell / pyspark</code>",id:"use-the---packages-option-with-spark-submit--spark-shell--pyspark",level:4},{value:"Configuration",id:"configuration",level:3},{value:"Setting the properties directly in your application",id:"setting-the-properties-directly-in-your-application",level:4},{value:"Using <code>--conf</code> options with the CLI",id:"using---conf-options-with-the-cli",level:4},{value:"Adding properties to the <code>spark-defaults.conf</code> file in the <code>${SPARK_HOME}/conf</code> directory",id:"adding-properties-to-the-spark-defaultsconf-file-in-the-spark_homeconf-directory",level:4},{value:"Spark Config Parameters",id:"spark-config-parameters",level:4},{value:"HTTP",id:"http",level:5},{value:"URL",id:"url",level:6},{value:"Kinesis",id:"kinesis",level:5},{value:"Kafka",id:"kafka",level:5},{value:"Scheduling from Airflow",id:"scheduling-from-airflow",level:3}],O={toc:w};function T(e){let{components:t,...r}=e;return(0,i.kt)("wrapper",(0,n.Z)({},O,r,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("admonition",{type:"info"},(0,i.kt)("p",{parentName:"admonition"},"This integration is known to work with Apache Spark 2.4 and later.")),(0,i.kt)("p",null,"Spark jobs typically run on clusters of machines. A single machine hosts the \"driver\" application,\nwhich constructs a graph of jobs - e.g., reading data from a source, filtering, transforming, and\njoining records, and writing results to some sink- and manages execution of those jobs. Spark's\nfundamental abstraction is the Resilient Distributed Dataset (RDD), which encapsulates distributed\nreads and modifications of records. While RDDs can be used directly, it is far more common to work\nwith Spark Datasets or Dataframes, which is an API that adds explicit schemas for better performance\nand the ability to interact with datasets using SQL. The Dataframe's declarative API enables Spark\nto optimize jobs by analyzing and manipulating an abstract query plan prior to execution."),(0,i.kt)("h2",{id:"collecting-lineage-in-spark"},"Collecting Lineage in Spark"),(0,i.kt)("p",null,"Collecting lineage requires hooking into Spark's ",(0,i.kt)("inlineCode",{parentName:"p"},"ListenerBus")," in the driver application and\ncollecting and analyzing execution events as they happen. Both raw RDD and Dataframe jobs post events\nto the listener bus during execution. These events expose the structure of the job, including the\noptimized query plan, allowing the Spark integration to analyze the job for datasets consumed and\nproduced, including attributes about the storage, such as location in GCS or S3, table names in a\nrelational database or warehouse, such as Redshift or Bigquery, and schemas. In addition to dataset\nand job lineage, Spark SQL jobs also report logical plans, which can be compared across job runs to\ntrack important changes in query plans, which may affect the correctness or speed of a job."),(0,i.kt)("p",null,"A single Spark application may execute multiple jobs. The Spark OpenLineage integration maps one\nSpark job to a single OpenLineage Job. The application will be assigned a Run id at startup and each\njob that executes will report the application's Run id as its parent job run. Thus, an application\nthat reads one or more source datasets, writes an intermediate dataset, then transforms that\nintermediate dataset and writes a final output dataset will report three jobs- the parent application\njob, the initial job that reads the sources and creates the intermediate dataset, and the final job\nthat consumes the intermediate dataset and produces the final output. As an image:\n",(0,i.kt)("img",{alt:"image",src:a(83189).Z,width:"1289",height:"204"})),(0,i.kt)("h2",{id:"about-the-integration"},"About the Integration"),(0,i.kt)("p",null,"This integration employs the ",(0,i.kt)("inlineCode",{parentName:"p"},"SparkListener")," interface through ",(0,i.kt)("inlineCode",{parentName:"p"},"OpenLineageSparkListener"),", offering\na comprehensive monitoring solution. It examines SparkContext-emitted events to extract metadata\nassociated with jobs and datasets, utilizing the RDD and DataFrame dependency graphs. This method\neffectively gathers information from various data sources, including filesystem sources (e.g., S3\nand GCS), JDBC backends, and data warehouses such as Redshift and Bigquery."),(0,i.kt)("h2",{id:"how-to-use-the-integration"},"How to Use the Integration"),(0,i.kt)("p",null,"Incorporating OpenLineage metadata collection into existing Spark jobs is designed to be simple and\nminimally invasive."),(0,i.kt)("h3",{id:"installation"},"Installation"),(0,i.kt)("admonition",{type:"warning"},(0,i.kt)("ul",{parentName:"admonition"},(0,i.kt)("li",{parentName:"ul"},"Version ",(0,i.kt)("inlineCode",{parentName:"li"},"1.8.0")," and earlier only supported Scala 2.12 variants of Apache Spark."),(0,i.kt)("li",{parentName:"ul"},"Version ",(0,i.kt)("inlineCode",{parentName:"li"},"1.9.0")," and later support both Scala 2.12 and 2.13 variants of Apache Spark.")),(0,i.kt)("p",{parentName:"admonition"},"The above necessitates a change in the artifact identifier for ",(0,i.kt)("inlineCode",{parentName:"p"},"io.openlineage:openlineage-spark"),".\nAfter version ",(0,i.kt)("inlineCode",{parentName:"p"},"1.8.0"),", the artifact identifier has been updated. For subsequent versions, utilize:\n",(0,i.kt)("inlineCode",{parentName:"p"},"io.openlineage:openlineage-spark_${SCALA_BINARY_VERSION}:${OPENLINEAGE_SPARK_VERSION}"),".")),(0,i.kt)("p",null,"To integrate OpenLineage Spark with your application, you can:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#bundle-the-package-with-your-apache-spark-application-project"},"Bundle the package with your Apache Spark application project"),"."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#place-the-jar-in-your-spark_homejars-directory"},"Place the JAR in your ",(0,i.kt)("inlineCode",{parentName:"a"},"${SPARK_HOME}/jars")," directory")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#use-the---jars-option-with-spark-submit--spark-shell--pyspark"},"Use the ",(0,i.kt)("inlineCode",{parentName:"a"},"--jars")," option with ",(0,i.kt)("inlineCode",{parentName:"a"},"spark-submit / spark-shell / pyspark"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#use-the---packages-option-with-spark-submit--spark-shell--pyspark"},"Use the ",(0,i.kt)("inlineCode",{parentName:"a"},"--packages")," option with ",(0,i.kt)("inlineCode",{parentName:"a"},"spark-submit / spark-shell / pyspark")))),(0,i.kt)("h4",{id:"bundle-the-package-with-your-apache-spark-application-project"},"Bundle the package with your Apache Spark application project"),(0,i.kt)("admonition",{type:"info"},(0,i.kt)("p",{parentName:"admonition"},"This approach does not demonstrate how to configure the ",(0,i.kt)("inlineCode",{parentName:"p"},"OpenLineageSparkListener"),".\nPlease refer to the ",(0,i.kt)("a",{parentName:"p",href:"#configuration"},"Configuration")," section.")),(0,i.kt)("p",null,"For Maven, add the following to your ",(0,i.kt)("inlineCode",{parentName:"p"},"pom.xml"),":"),(0,i.kt)(v,{groupId:"spark",mdxType:"Tabs"},(0,i.kt)(S,{value:"after-1.8.0",label:"After 1.8.0",mdxType:"TabItem"},(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml"},"<dependency>\n  <groupId>io.openlineage</groupId>\n  <artifactId>openlineage-spark_${SCALA_BINARY_VERSION}</artifactId>\n  <version>${OPENLINEAGE_SPARK_VERSION}</version>\n</dependency>\n"))),(0,i.kt)(S,{value:"1.8.0-and-earlier",label:"1.8.0 and earlier",mdxType:"TabItem"},(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-xml"},"<dependency>\n  <groupId>io.openlineage</groupId>\n  <artifactId>openlineage-spark</artifactId>\n  <version>${OPENLINEAGE_SPARK_VERSION}</version>\n</dependency>\n")))),(0,i.kt)("p",null,"For Gradle, add this to your ",(0,i.kt)("inlineCode",{parentName:"p"},"build.gradle"),":"),(0,i.kt)(v,{groupId:"spark",mdxType:"Tabs"},(0,i.kt)(S,{value:"after-1.8.0",label:"After 1.8.0",mdxType:"TabItem"},(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-groovy"},'implementation("io.openlineage:openlineage-spark_${SCALA_BINARY_VERSION}:${OPENLINEAGE_SPARK_VERSION}")\n'))),(0,i.kt)(S,{value:"1.8.0-and-earlier",label:"1.8.0 and earlier",mdxType:"TabItem"},(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-groovy"},'implementation("io.openlineage:openlineage-spark:${OPENLINEAGE_SPARK_VERSION}")\n')))),(0,i.kt)("h4",{id:"place-the-jar-in-your-spark_homejars-directory"},"Place the JAR in your ",(0,i.kt)("inlineCode",{parentName:"h4"},"${SPARK_HOME}/jars")," directory"),(0,i.kt)("admonition",{type:"info"},(0,i.kt)("p",{parentName:"admonition"},"This approach does not demonstrate how to configure the ",(0,i.kt)("inlineCode",{parentName:"p"},"OpenLineageSparkListener"),".\nPlease refer to the ",(0,i.kt)("a",{parentName:"p",href:"#configuration"},"Configuration")," section.")),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Download the JAR and its checksum from Maven Central."),(0,i.kt)("li",{parentName:"ol"},"Verify the JAR's integrity using the checksum."),(0,i.kt)("li",{parentName:"ol"},"Upon successful verification, move the JAR to ",(0,i.kt)("inlineCode",{parentName:"li"},"${SPARK_HOME}/jars"),".")),(0,i.kt)("p",null,"This script automates the download and verification process:"),(0,i.kt)(v,{groupId:"spark",mdxType:"Tabs"},(0,i.kt)(S,{value:"after-1.8.0",label:"After 1.8.0",mdxType:"TabItem"},(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},'#!/usr/bin/env bash\n\nif [ -z "$SPARK_HOME" ]; then\n    echo "SPARK_HOME is not set. Please define it as your Spark installation directory."\n    exit 1\nfi\n\nOPENLINEAGE_SPARK_VERSION=\'1.9.0\'  # Example version\nSCALA_BINARY_VERSION=\'2.13\'        # Example Scala version\nARTIFACT_ID="openlineage-spark_${SCALA_BINARY_VERSION}"\nJAR_NAME="${ARTIFACT_ID}-${OPENLINEAGE_SPARK_VERSION}.jar"\nCHECKSUM_NAME="${JAR_NAME}.sha512"\nBASE_URL="https://repo1.maven.org/maven2/io/openlineage/${ARTIFACT_ID}/${OPENLINEAGE_SPARK_VERSION}"\n\ncurl -O "${BASE_URL}/${JAR_NAME}"\ncurl -O "${BASE_URL}/${CHECKSUM_NAME}"\n\necho "$(cat ${CHECKSUM_NAME})  ${JAR_NAME}" | sha512sum -c\n\nif [ $? -eq 0 ]; then\n    mv "${JAR_NAME}" "${SPARK_HOME}/jars"\nelse\n    echo "Checksum verification failed."\n    exit 1\nfi\n'))),(0,i.kt)(S,{value:"1.8.0-and-earlier",label:"1.8.0 and earlier",mdxType:"TabItem"},(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},'#!/usr/bin/env bash\n\nif [ -z "$SPARK_HOME" ]; then\n    echo "SPARK_HOME is not set. Please define it as your Spark installation directory."\n    exit 1\nfi\n\nOPENLINEAGE_SPARK_VERSION=\'1.8.0\'  # Example version\nARTIFACT_ID="openlineage-spark"\nJAR_NAME="${ARTIFACT_ID}-${OPENLINEAGE_SPARK_VERSION}.jar"\nCHECKSUM_NAME="${JAR_NAME}.sha512"\nBASE_URL="https://repo1.maven.org/maven2/io/openlineage/${ARTIFACT_ID}/${OPENLINEAGE_SPARK_VERSION}"\n\ncurl -O "${BASE_URL}/${JAR_NAME}"\ncurl -O "${BASE_URL}/${CHECKSUM_NAME}"\n\necho "$(cat ${CHECKSUM_NAME})  ${JAR_NAME}" | sha512sum -c\n\nif [ $? -eq 0 ]; then\n    mv "${JAR_NAME}" "${SPARK_HOME}/jars"\nelse\n    echo "Checksum verification failed."\n    exit 1\nfi\n')))),(0,i.kt)("h4",{id:"use-the---jars-option-with-spark-submit--spark-shell--pyspark"},"Use the ",(0,i.kt)("inlineCode",{parentName:"h4"},"--jars")," option with ",(0,i.kt)("inlineCode",{parentName:"h4"},"spark-submit / spark-shell / pyspark")),(0,i.kt)("admonition",{type:"info"},(0,i.kt)("p",{parentName:"admonition"},"This approach does not demonstrate how to configure the ",(0,i.kt)("inlineCode",{parentName:"p"},"OpenLineageSparkListener"),".\nPlease refer to the ",(0,i.kt)("a",{parentName:"p",href:"#configuration"},"Configuration")," section.")),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Download the JAR and its checksum from Maven Central."),(0,i.kt)("li",{parentName:"ol"},"Verify the JAR's integrity using the checksum."),(0,i.kt)("li",{parentName:"ol"},"Upon successful verification, submit a Spark application with the JAR using the ",(0,i.kt)("inlineCode",{parentName:"li"},"--jars")," option.")),(0,i.kt)("p",null,"This script demonstrate this process:"),(0,i.kt)(v,{groupId:"spark",mdxType:"Tabs"},(0,i.kt)(S,{value:"after-1.8.0",label:"After 1.8.0",mdxType:"TabItem"},(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},'#!/usr/bin/env bash\n\nOPENLINEAGE_SPARK_VERSION=\'1.9.0\'  # Example version\nSCALA_BINARY_VERSION=\'2.13\'        # Example Scala version\nARTIFACT_ID="openlineage-spark_${SCALA_BINARY_VERSION}"\nJAR_NAME="${ARTIFACT_ID}-${OPENLINEAGE_SPARK_VERSION}.jar"\nCHECKSUM_NAME="${JAR_NAME}.sha512"\nBASE_URL="https://repo1.maven.org/maven2/io/openlineage/${ARTIFACT_ID}/${OPENLINEAGE_SPARK_VERSION}"\n\ncurl -O "${BASE_URL}/${JAR_NAME}"\ncurl -O "${BASE_URL}/${CHECKSUM_NAME}"\n\necho "$(cat ${CHECKSUM_NAME})  ${JAR_NAME}" | sha512sum -c\n\nif [ $? -eq 0 ]; then\n    spark-submit --jars "path/to/${JAR_NAME}" \\\n      # ... other options\nelse\n    echo "Checksum verification failed."\n    exit 1\nfi\n'))),(0,i.kt)(S,{value:"1.8.0-and-earlier",label:"1.8.0 and earlier",mdxType:"TabItem"},(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},'#!/usr/bin/env bash\n\nOPENLINEAGE_SPARK_VERSION=\'1.8.0\'  # Example version\nARTIFACT_ID="openlineage-spark"\nJAR_NAME="${ARTIFACT_ID}-${OPENLINEAGE_SPARK_VERSION}.jar"\nCHECKSUM_NAME="${JAR_NAME}.sha512"\nBASE_URL="https://repo1.maven.org/maven2/io/openlineage/${ARTIFACT_ID}/${OPENLINEAGE_SPARK_VERSION}"\n\ncurl -O "${BASE_URL}/${JAR_NAME}"\ncurl -O "${BASE_URL}/${CHECKSUM_NAME}"\n\necho "$(cat ${CHECKSUM_NAME})  ${JAR_NAME}" | sha512sum -c\n\nif [ $? -eq 0 ]; then\n    spark-submit --jars "path/to/${JAR_NAME}" \\\n      # ... other options\nelse\n    echo "Checksum verification failed."\n    exit 1\nfi\n')))),(0,i.kt)("h4",{id:"use-the---packages-option-with-spark-submit--spark-shell--pyspark"},"Use the ",(0,i.kt)("inlineCode",{parentName:"h4"},"--packages")," option with ",(0,i.kt)("inlineCode",{parentName:"h4"},"spark-submit / spark-shell / pyspark")),(0,i.kt)("admonition",{type:"info"},(0,i.kt)("p",{parentName:"admonition"},"This approach does not demonstrate how to configure the ",(0,i.kt)("inlineCode",{parentName:"p"},"OpenLineageSparkListener"),".\nPlease refer to the ",(0,i.kt)("a",{parentName:"p",href:"#configuration"},"Configuration")," section.")),(0,i.kt)("p",null,"Spark allows you to add packages at runtime using the ",(0,i.kt)("inlineCode",{parentName:"p"},"--packages")," option with ",(0,i.kt)("inlineCode",{parentName:"p"},"spark-submit"),". This\noption automatically downloads the package from Maven Central (or other configured repositories)\nduring runtime and adds it to the classpath of your Spark application."),(0,i.kt)(v,{groupId:"spark",mdxType:"Tabs"},(0,i.kt)(S,{value:"after-1.8.0",label:"After 1.8.0",mdxType:"TabItem"},(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"OPENLINEAGE_SPARK_VERSION='1.9.0'  # Example version\nSCALA_BINARY_VERSION='2.13'        # Example Scala version\n\nspark-submit --packages \"io.openlineage:openlineage-spark_${SCALA_BINARY_VERSION}:${OPENLINEAGE_SPARK_VERSION}\" \\\n    # ... other options\n"))),(0,i.kt)(S,{value:"1.8.0-and-earlier",label:"1.8.0 and earlier",mdxType:"TabItem"},(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"OPENLINEAGE_SPARK_VERSION='1.8.0'  # Example version\n\nspark-submit --packages \"io.openlineage:openlineage-spark::${OPENLINEAGE_SPARK_VERSION}\" \\\n    # ... other options\n")))),(0,i.kt)("h3",{id:"configuration"},"Configuration"),(0,i.kt)("p",null,"Configuring the OpenLineage Spark integration is straightforward. It uses built-in Spark\nconfiguration\nmechanisms."),(0,i.kt)("p",null,"Your options are:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("a",{parentName:"li",href:"#setting-the-properties-directly-in-your-application"},"Setting the properties directly in your application"),"."),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("a",{parentName:"li",href:"#using---conf-options-with-the-cli"},"Using ",(0,i.kt)("inlineCode",{parentName:"a"},"--conf")," options with the CLI"),"."),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("a",{parentName:"li",href:"#adding-properties-to-the-spark-defaultsconf-file-in-the-spark_homeconf-directory"},"Adding properties to the ",(0,i.kt)("inlineCode",{parentName:"a"},"spark-defaults.conf")," file in the ",(0,i.kt)("inlineCode",{parentName:"a"},"${SPARK_HOME}/conf")," directory"),".")),(0,i.kt)("h4",{id:"setting-the-properties-directly-in-your-application"},"Setting the properties directly in your application"),(0,i.kt)("p",null,"The below example demonstrates how to set the properties directly in your application when\nconstructing\na ",(0,i.kt)("inlineCode",{parentName:"p"},"SparkSession"),"."),(0,i.kt)("admonition",{type:"warning"},(0,i.kt)("p",{parentName:"admonition"},"The setting ",(0,i.kt)("inlineCode",{parentName:"p"},'config("spark.extraListeners", "io.openlineage.spark.agent.OpenLineageSparkListener")')," is\n",(0,i.kt)("strong",{parentName:"p"},"extremely important"),". Without it, the OpenLineage Spark integration will not be invoked, rendering\nthe integration ineffective.")),(0,i.kt)(v,{groupId:"spark-app-conf",mdxType:"Tabs"},(0,i.kt)(S,{value:"scala",label:"Scala",mdxType:"TabItem"},(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},'import org.apache.spark.sql.SparkSession\n\nobject OpenLineageExample extends App {\n  val spark = SparkSession.builder()\n    .appName("OpenLineageExample")\n    // This line is EXTREMELY important\n    .config("spark.extraListeners", "io.openlineage.spark.agent.OpenLineageSparkListener")\n    .config("spark.openlineage.transport.type", "http")\n    .config("spark.openlineage.transport.url", "http://localhost:5000/api/v1/lineage")\n    .config("spark.openlineage.namespace", "MyNamespace")\n    .config("spark.openlineage.parentJobName", "ParentJobName")\n    .config("spark.openlineage.parentRunId", "xxxx-xxxx-xxxx-xxxx")\n    .getOrCreate()\n\n  // ... your code\n\n  spark.stop()\n}\n'))),(0,i.kt)(S,{value:"python",label:"Python",mdxType:"TabItem"},(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'from pyspark.sql import SparkSession\n\nspark = SparkSession.builder\n    .appName("OpenLineageExample")\n    .config("spark.extraListeners", "io.openlineage.spark.agent.OpenLineageSparkListener")\n    .config("spark.openlineage.transport.type", "http")\n    .config("spark.openlineage.transport.url", "http://localhost:5000/api/v1/lineage")\n    .config("spark.openlineage.namespace", "MyNamespace")\n    .config("spark.openlineage.parentJobName", "ParentJobName")\n    .config("spark.openlineage.parentRunId", "xxxx-xxxx-xxxx-xxxx")\n    .getOrCreate()\n\n# ... your code\n\nspark.stop()\n')))),(0,i.kt)("h4",{id:"using---conf-options-with-the-cli"},"Using ",(0,i.kt)("inlineCode",{parentName:"h4"},"--conf")," options with the CLI"),(0,i.kt)("p",null,"The below example demonstrates how to use the ",(0,i.kt)("inlineCode",{parentName:"p"},"--conf")," option with ",(0,i.kt)("inlineCode",{parentName:"p"},"spark-submit"),"."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},'spark-submit \\\n  --conf "spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener" \\\n  --conf "spark.openlineage.transport.type=http" \\\n  --conf "spark.openlineage.transport.url=http://localhost:5000/api/v1/lineage" \\\n  --conf "spark.openlineage.namespace=MyNamespace" \\\n  --conf "spark.openlineage.parentJobName=ParentJobName" \\\n  --conf "spark.openlineage.parentRunId=xxxx-xxxx-xxxx-xxxx" \\\n  # ... other options\n')),(0,i.kt)("h4",{id:"adding-properties-to-the-spark-defaultsconf-file-in-the-spark_homeconf-directory"},"Adding properties to the ",(0,i.kt)("inlineCode",{parentName:"h4"},"spark-defaults.conf")," file in the ",(0,i.kt)("inlineCode",{parentName:"h4"},"${SPARK_HOME}/conf")," directory"),(0,i.kt)("admonition",{type:"warning"},(0,i.kt)("p",{parentName:"admonition"},"You may need to create this file if it does not exist. If it does exist, ",(0,i.kt)("strong",{parentName:"p"},"we strongly suggest that\nyou back it up before making any changes"),", particularly if you are not the only user of the Spark\ninstallation. A misconfiguration here can have devastating effects on the operation of your Spark\ninstallation, particularly in a shared environment.")),(0,i.kt)("p",null,"The below example demonstrates how to add properties to the ",(0,i.kt)("inlineCode",{parentName:"p"},"spark-defaults.conf")," file."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-properties"},"spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener\nspark.openlineage.transport.type=http\nspark.openlineage.transport.url=http://localhost:5000/api/v1/lineage\nspark.openlineage.namespace=MyNamespace\n")),(0,i.kt)("admonition",{type:"info"},(0,i.kt)("p",{parentName:"admonition"},"The ",(0,i.kt)("inlineCode",{parentName:"p"},"spark.extraListeners")," configuration parameter is ",(0,i.kt)("strong",{parentName:"p"},"non-additive"),". This means that if you set\n",(0,i.kt)("inlineCode",{parentName:"p"},"spark.extraListeners")," via the CLI or via ",(0,i.kt)("inlineCode",{parentName:"p"},"SparkSession#config"),", it will ",(0,i.kt)("strong",{parentName:"p"},"replace")," the value\nin ",(0,i.kt)("inlineCode",{parentName:"p"},"spark-defaults.conf"),". This is important to remember if you are using ",(0,i.kt)("inlineCode",{parentName:"p"},"spark-defaults.conf")," to\nset a default value for ",(0,i.kt)("inlineCode",{parentName:"p"},"spark.extraListeners")," and then want to override it for a specific job.")),(0,i.kt)("admonition",{type:"info"},(0,i.kt)("p",{parentName:"admonition"},"When it comes to configuration parameters like ",(0,i.kt)("inlineCode",{parentName:"p"},"spark.openlineage.namespace"),", a default value can\nbe supplied in the ",(0,i.kt)("inlineCode",{parentName:"p"},"spark-defaults.conf")," file. This default value can be overridden by the\napplication at runtime, via the previously detailed methods. However, it is ",(0,i.kt)("strong",{parentName:"p"},"strongly")," recommended\nthat more dynamic or quickly changing parameters like ",(0,i.kt)("inlineCode",{parentName:"p"},"spark.openlineage.parentRunId")," or\n",(0,i.kt)("inlineCode",{parentName:"p"},"spark.openlineage.parentJobName")," be set at runtime via the CLI or ",(0,i.kt)("inlineCode",{parentName:"p"},"SparkSession#config")," methods.")),(0,i.kt)("h4",{id:"spark-config-parameters"},"Spark Config Parameters"),(0,i.kt)("p",null,"The following parameters can be specified:"),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"Parameter"),(0,i.kt)("th",{parentName:"tr",align:null},"Definition"),(0,i.kt)("th",{parentName:"tr",align:null},"Example"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"spark.openlineage.transport.type"),(0,i.kt)("td",{parentName:"tr",align:null},"The transport type used for event emit, default type is ",(0,i.kt)("inlineCode",{parentName:"td"},"console")),(0,i.kt)("td",{parentName:"tr",align:null},"http")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"spark.openlineage.namespace"),(0,i.kt)("td",{parentName:"tr",align:null},"The default namespace to be applied for any jobs submitted"),(0,i.kt)("td",{parentName:"tr",align:null},"MyNamespace")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"spark.openlineage.parentJobName"),(0,i.kt)("td",{parentName:"tr",align:null},"The job name to be used for the parent job facet"),(0,i.kt)("td",{parentName:"tr",align:null},"ParentJobName")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"spark.openlineage.parentRunId"),(0,i.kt)("td",{parentName:"tr",align:null},"The RunId of the parent job that initiated this Spark job"),(0,i.kt)("td",{parentName:"tr",align:null},"xxxx-xxxx-xxxx-xxxx")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"spark.openlineage.appName"),(0,i.kt)("td",{parentName:"tr",align:null},"Custom value overwriting Spark app name in events"),(0,i.kt)("td",{parentName:"tr",align:null},"AppName")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"spark.openlineage.facets.disabled"),(0,i.kt)("td",{parentName:"tr",align:null},"List of facets to disable, enclosed in ",(0,i.kt)("inlineCode",{parentName:"td"},"[]")," (required from 0.21.x) and separated by ",(0,i.kt)("inlineCode",{parentName:"td"},";"),", default is ",(0,i.kt)("inlineCode",{parentName:"td"},"[spark_unknown;]")," (currently must contain ",(0,i.kt)("inlineCode",{parentName:"td"},";"),")"),(0,i.kt)("td",{parentName:"tr",align:null},"[","spark_unknown;spark.logicalPlan","]")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"spark.openlineage.capturedProperties"),(0,i.kt)("td",{parentName:"tr",align:null},"comma separated list of properties to be captured in spark properties facet (default ",(0,i.kt)("inlineCode",{parentName:"td"},"spark.master"),", ",(0,i.kt)("inlineCode",{parentName:"td"},"spark.app.name"),")"),(0,i.kt)("td",{parentName:"tr",align:null},'"spark.example1,spark.example2"')),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"spark.openlineage.dataset.removePath.pattern"),(0,i.kt)("td",{parentName:"tr",align:null},"Java regular expression that removes ",(0,i.kt)("inlineCode",{parentName:"td"},"?<remove>")," named group from dataset path. Can be used to last path subdirectories from paths like ",(0,i.kt)("inlineCode",{parentName:"td"},"s3://my-whatever-path/year=2023/month=04")),(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"(.*)(?<remove>\\/.*\\/.*)"))),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"spark.openlineage.jobName.appendDatasetName"),(0,i.kt)("td",{parentName:"tr",align:null},"Decides whether output dataset name should be appended to job name. By default ",(0,i.kt)("inlineCode",{parentName:"td"},"true"),"."),(0,i.kt)("td",{parentName:"tr",align:null},"false")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"spark.openlineage.jobName.replaceDotWithUnderscore"),(0,i.kt)("td",{parentName:"tr",align:null},"Replaces dots in job name with underscore. Can be used to mimic legacy behaviour on Databricks platform. By default ",(0,i.kt)("inlineCode",{parentName:"td"},"false"),"."),(0,i.kt)("td",{parentName:"tr",align:null},"false")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"spark.openlineage.debugFacet"),(0,i.kt)("td",{parentName:"tr",align:null},"Determines whether debug facet shall be generated and included within the event. Set ",(0,i.kt)("inlineCode",{parentName:"td"},"enabled")," to turn it on. By default, facet is disabled."),(0,i.kt)("td",{parentName:"tr",align:null},"enabled")))),(0,i.kt)("h5",{id:"http"},"HTTP"),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"Parameter"),(0,i.kt)("th",{parentName:"tr",align:null},"Definition"),(0,i.kt)("th",{parentName:"tr",align:null},"Example"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"spark.openlineage.transport.endpoint"),(0,i.kt)("td",{parentName:"tr",align:null},"Path to resource"),(0,i.kt)("td",{parentName:"tr",align:null},"/api/v1/lineage")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"spark.openlineage.transport.auth.type"),(0,i.kt)("td",{parentName:"tr",align:null},"The type of authentication method to use"),(0,i.kt)("td",{parentName:"tr",align:null},"api_key")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"spark.openlineage.transport.auth.apiKey"),(0,i.kt)("td",{parentName:"tr",align:null},"An API key to be used when sending events to the OpenLineage server"),(0,i.kt)("td",{parentName:"tr",align:null},"abcdefghijk")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"spark.openlineage.transport.timeout"),(0,i.kt)("td",{parentName:"tr",align:null},"Timeout for sending OpenLineage info in milliseconds"),(0,i.kt)("td",{parentName:"tr",align:null},"5000")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"spark.openlineage.transport.urlParams.xyz"),(0,i.kt)("td",{parentName:"tr",align:null},"A URL parameter (replace xyz) and value to be included in requests to the OpenLineage API server"),(0,i.kt)("td",{parentName:"tr",align:null},"abcdefghijk")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"spark.openlineage.transport.url"),(0,i.kt)("td",{parentName:"tr",align:null},"The hostname of the OpenLineage API server where events should be reported, it can have other properties embeded"),(0,i.kt)("td",{parentName:"tr",align:null},"http://localhost:5000")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"spark.openlineage.transport.headers.xyz"),(0,i.kt)("td",{parentName:"tr",align:null},"Request headers (replace xyz) and value to be included in requests to the OpenLineage API server"),(0,i.kt)("td",{parentName:"tr",align:null},"abcdefghijk")))),(0,i.kt)("h6",{id:"url"},"URL"),(0,i.kt)("p",null,"You can supply http parameters using values in url, the parsed ",(0,i.kt)("inlineCode",{parentName:"p"},"spark.openlineage.*")," properties are located in url as follows:"),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"{transport.url}/{transport.endpoint}/namespaces/{namespace}/jobs/{parentJobName}/runs/{parentRunId}?app_name={appName}&api_key={transport.apiKey}&timeout={transport.timeout}&xxx={transport.urlParams.xxx}")),(0,i.kt)("p",null,"example:"),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"http://localhost:5000/api/v1/namespaces/ns_name/jobs/job_name/runs/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx?app_name=app&api_key=abc&timeout=5000&xxx=xxx")),(0,i.kt)("h5",{id:"kinesis"},"Kinesis"),(0,i.kt)("p",null,"If ",(0,i.kt)("inlineCode",{parentName:"p"},"spark.openlineage.transport.type")," is set to ",(0,i.kt)("inlineCode",{parentName:"p"},"kinesis"),", then the below parameters would be read and used when building KinesisProducer.\nAlso, KinesisTransport depends on you to provide artifact ",(0,i.kt)("inlineCode",{parentName:"p"},"com.amazonaws:amazon-kinesis-producer:0.14.0")," or compatible on your classpath."),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"Parameter"),(0,i.kt)("th",{parentName:"tr",align:null},"Definition"),(0,i.kt)("th",{parentName:"tr",align:null},"Example"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"spark.openlineage.transport.streamName"),(0,i.kt)("td",{parentName:"tr",align:null},"Required, the streamName of the Kinesis Stream"),(0,i.kt)("td",{parentName:"tr",align:null},"some-stream-name")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"spark.openlineage.transport.region"),(0,i.kt)("td",{parentName:"tr",align:null},"Required, the region of the stream"),(0,i.kt)("td",{parentName:"tr",align:null},"us-east-2")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"spark.openlineage.transport.roleArn"),(0,i.kt)("td",{parentName:"tr",align:null},"Optional, the roleArn which is allowed to read/write to Kinesis stream"),(0,i.kt)("td",{parentName:"tr",align:null},"some-role-arn")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"spark.openlineage.transport.properties.","[xxx]"),(0,i.kt)("td",{parentName:"tr",align:null},"Optional, the ","[xxx]"," is property of ",(0,i.kt)("a",{parentName:"td",href:"https://github.com/awslabs/amazon-kinesis-producer/blob/master/java/amazon-kinesis-producer-sample/default_config.properties"},"Kinesis allowd properties")),(0,i.kt)("td",{parentName:"tr",align:null},"1")))),(0,i.kt)("h5",{id:"kafka"},"Kafka"),(0,i.kt)("p",null,"If ",(0,i.kt)("inlineCode",{parentName:"p"},"spark.openlineage.transport.type")," is set to ",(0,i.kt)("inlineCode",{parentName:"p"},"kafka"),", then the below parameters would be read and used when building KafkaProducer."),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"Parameter"),(0,i.kt)("th",{parentName:"tr",align:null},"Definition"),(0,i.kt)("th",{parentName:"tr",align:null},"Example"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"spark.openlineage.transport.topicName"),(0,i.kt)("td",{parentName:"tr",align:null},"Required, name of the topic"),(0,i.kt)("td",{parentName:"tr",align:null},"topic-name")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"spark.openlineage.transport.localServerId"),(0,i.kt)("td",{parentName:"tr",align:null},"Required, id of local server"),(0,i.kt)("td",{parentName:"tr",align:null},"xxxxxxxx")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"spark.openlineage.transport.properties.","[xxx]"),(0,i.kt)("td",{parentName:"tr",align:null},"Optional, the ","[xxx]"," is property of Kafka client"),(0,i.kt)("td",{parentName:"tr",align:null},"1")))),(0,i.kt)("h3",{id:"scheduling-from-airflow"},"Scheduling from Airflow"),(0,i.kt)("p",null,"The same parameters passed to ",(0,i.kt)("inlineCode",{parentName:"p"},"spark-submit")," can be supplied from Airflow and other schedulers. If\nusing the ",(0,i.kt)("a",{parentName:"p",href:"/docs/integrations/airflow/"},"openlineage-airflow")," integration, each task in the DAG has its own Run id\nwhich can be connected to the Spark job run via the ",(0,i.kt)("inlineCode",{parentName:"p"},"spark.openlineage.parentRunId")," parameter. For example,\nhere is an example of a ",(0,i.kt)("inlineCode",{parentName:"p"},"DataProcPySparkOperator")," that submits a Pyspark application on Dataproc:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'t1 = DataProcPySparkOperator(\n    task_id=job_name,\n    gcp_conn_id=\'google_cloud_default\',\n    project_id=\'project_id\',\n    cluster_name=\'cluster-name\',\n    region=\'us-west1\',\n    main=\'gs://bucket/your-prog.py\',\n    job_name=job_name,\n    dataproc_pyspark_properties={\n      "spark.extraListeners": "io.openlineage.spark.agent.OpenLineageSparkListener",\n      "spark.jars.packages": "io.openlineage:openlineage-spark:1.0.0+",\n      "spark.openlineage.transport.url": f"{openlineage_url}/api/v1/namespaces/{openlineage_namespace}/jobs/dump_orders_to_gcs/runs/{{{{lineage_run_id(run_id, task)}}}}?api_key={api_key}",\n      "spark.openlineage.namespace": openlineage_namespace,\n      "spark.openlineage.parentJobName": job_name,\n      "spark.openlineage.parentRunId": f"{{{{lineage_run_id(run_id, task)}}}}\n    },\n    dag=dag)\n')))}T.isMDXComponent=!0},83189:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/spark-job-creation.dot-d3fd1094587dcacc0c8a1566dac60ed5.png"}}]);