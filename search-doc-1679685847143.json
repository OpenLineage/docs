[{"title":"Introducing OpenLineage 0.1.0","type":0,"sectionRef":"#","url":"/blog/0.1-release","content":"We are pleased to announce the initial release of OpenLineage. This release includes the core specification, data model, clients, and integrations with common data tools. We are pleased to announce the initial release of OpenLineage. This is the culmination of a broad community effort, and establishes a common framework for data lineage collection and analysis. We want to thank all the contributors as well all the projects and companies involved in the design (in alphabetical order): Airflow, Astronomer, Datakin, Data Mesh, dbt, Egeria, GetInData, Great Expectations, Iceberg (and others that I am probably forgetting). This release includes: The initial 1-0-0 release of the OpenLineage specificationA core lineage model of Jobs, Runs and Datasets Core facetsData Quality Metrics and statisticsDataset schemaSource code locationSQL Clients that send OpenLineage events to an HTTP backend JavaPython Integrations that collect lineage metadata as OpenLineage events Apache Airflow with support for BigQuery, Great Expectations, Postgres, Redshift, SnowflakeApache Sparkdbt This is only the beginning. We invite everyone interested to consult and contribute to the roadmap. The roadmap currently contains, among other things: adding support for Kafka, BI dashboards, and column level lineage...but you can influence it by participating! Follow the repo to stay updated. And, as always, you can join the conversation on Slack.","keywords":""},{"title":"The Current State of Column-level Lineage","type":0,"sectionRef":"#","url":"/blog/column-lineage","content":"","keywords":""},{"title":"Overview & background‚Äã","type":1,"pageTitle":"The Current State of Column-level Lineage","url":"/blog/column-lineage#overview--background","content":"Long one of our most requested new features, column-level lineage was added to the Spark integration with the release of OpenLineage 0.9.0. Project committer Pawe≈Ç Leszczy≈Ñski (@pawel-big-lebowski) authored the relevant pull requests (#645, #698, #738 and #772). In its current form, column-level lineage in OpenLineage is limited to the Spark integration and not yet visible in the Marquez UI. But this is only the first step in a broader, ongoing project to implement the feature across the project, and we‚Äôd love your help. Column-level lineage is a worthy pursuit. It dramatically extends the reach of OpenLineage‚Äôs metadata capture, providing finely grained information about datasets' dependencies. As Pawe≈Ç and project lead Julien Le Dem (@julienledem) wrote in the initial proposal, ‚ÄúNot only can we know that a dependency exists, but we are also able to understand which input columns are used to produce output columns. This allows [for] answering questions like ‚ÄòWhich root input columns are used to construct column x?‚Äô‚Äù Another reason to pursue column-level lineage: the demands of regulatory compliance. Bodies such as the GDPR, HIPAA, CCPA, BCBS and PCI have instituted requirements for data accuracy and integrity that compel companies and organizations to obtain deeper insight into their datasets and pipelines. "},{"title":"Why start with the Spark integration?‚Äã","type":1,"pageTitle":"The Current State of Column-level Lineage","url":"/blog/column-lineage#why-start-with-the-spark-integration","content":"As Julien and Pawe≈Ç's proposal suggests, the Spark integration was a logical starting point for adding column-level lineage. This is so because the integration relies on implementing visitors that traverse a LogicalPlan and extract meaningful information when encountered. These data include outputs and inputs with their schemas (which we were already identifying, in fact). The LogicalPlan also exposes the expressions that derive the output columns from the input columns. They can be inspected to derive column-level lineage. Traversing the LogicalPlan allows for the capturing of all the dependencies required to build column-level lineage. "},{"title":"A new facet in the spec‚Äã","type":1,"pageTitle":"The Current State of Column-level Lineage","url":"/blog/column-lineage#a-new-facet-in-the-spec","content":"In the process of implementing column-level lineage, Pawe≈Ç and Julien contributed a new facet schema, ColumnLineageDatasetFacet, to the OpenLineage spec. This facet uses fields to relay data points about dependencies. These are properties of items in the InputField property of the facet (namespace, name and field), as well as two human-readable string fields (transformationDescription, transformationType) for conveying information about dataset transformations. The last field, transformationType, may be especially useful for those whose companies or organizations need to track the usage of sensitive personal information. An example of a columnLineage facet in the outputs array of a lineage event: { &quot;namespace&quot;: &quot;{namespace of the outputdataset}&quot;, &quot;name&quot;: &quot;{name of the output dataset}&quot;, &quot;facets&quot;: { &quot;schema&quot;: { &quot;fields&quot;: [ { &quot;name&quot;: &quot;{first column of the output dataset}&quot;, &quot;type&quot;: &quot;{its type}&quot;}, { &quot;name&quot;: &quot;{second column of the output dataset}&quot;, &quot;type&quot;: &quot;{its type}&quot;}, ... ] }, &quot;columnLineage&quot;: { &quot;{first column of the output dataset}&quot;: { &quot;inputFields&quot;: [ { &quot;namespace&quot;: &quot;{input dataset namespace}&quot;, name: &quot;{input dataset name}&quot;, &quot;field&quot;: &quot;{input dataset column name}&quot;}, ... other inputs ], &quot;transformationDescription&quot;: &quot;identical&quot;, &quot;transformationType&quot;: &quot;IDENTITY&quot; }, &quot;{second column of the output dataset}&quot;: ..., ... } } }  "},{"title":"How it works‚Äã","type":1,"pageTitle":"The Current State of Column-level Lineage","url":"/blog/column-lineage#how-it-works","content":"As we‚Äôve seen, column-level lineage is being collected via the new columnLineage dataset facet. For each output, this facet contains a list of the output's fields along with the input fields used to create it. The input fields are identified by a namespace, name and field. But how is OpenLineage obtaining the data about dependencies that the facet relays? In PR #698, Pawe≈Ç describes the mechanism this way: The core mechanism first gets an output schema and logical plan as inputs.Then, the OutputFieldsCollector class traverses the plan to gather the outputs. Outputs can be extracted from Aggregate or Project, and each output field has an ExprId (expression ID) that is attached from the plan.Next, the InputFieldsCollector class is used to collect inputs that can be extracted from DataSourceV2Relation, DataSourceV2ScanRelation, HiveTableRelation or LogicalRelation. Each input field takes its ExprId from the plan, and each input is identified by a DatasetIdentifier, which means it contains the name and namespace of a dataset and an input field.Finally, the FieldDependenciesCollector traverses the plan to identify dependencies between different ExprIds. Dependencies map parent expressions to children expressions. This is used to identify the inputs used to evaluate certain outputs. "},{"title":"What‚Äôs next?‚Äã","type":1,"pageTitle":"The Current State of Column-level Lineage","url":"/blog/column-lineage#whats-next","content":"Work on extending column-level lineage in the project is ongoing. For example, project committer Will Johnson (@wjohnson) has opened a PR (#963) to add support for common dataframe operations not covered due to the initial focus on Spark. As Will writes in the PR, Currently, the Column Lineage Input Field Collectors work mainly for Spark SQL operations and Data Source V2. This leaves out normal dataframe operations like inserting into HDFS without the use of a Hive table. Column Lineage should support this scenario as many users will want to see column lineage for operations outside of SQL and Hive Metastore backed tables. Also, Pawe≈Ç has written enhancements that will enable column-level lineage in the case of altered table and column names and allow one to extend column-level lineage without contributing to OpenLineage (to avoid exposing proprietary code, for example). Meanwhile, over in Marquez, Julien has contributed a proposal to add a column-level endpoint to the project that would leverage OpenLineage‚Äôs ColumnLineageDatasetFacet. This approach would add column lineage to an existing endpoint by embedding the columnLineage facet in the data section of the DATASET nodes. "},{"title":"How can I contribute?‚Äã","type":1,"pageTitle":"The Current State of Column-level Lineage","url":"/blog/column-lineage#how-can-i-contribute","content":"We welcome contributions to this ongoing effort at implementing column-level lineage in OpenLineage! If you‚Äôre interested in contributing, one of our existing integrations might be a good place to start. OpenLineage‚Äôs growing list of integrations includes Airflow, dbt, Dagster and Flink. Sounds fun? Check out our new contributor guide to get started. "},{"title":"Video - OpenLineage at Data Agility Day","type":0,"sectionRef":"#","url":"/blog/data-agility-day","content":"At Data Agility Day 2021, Julien Le Dem and Kevin Mellott outlined their approach to data lineage and discussed various approaches to implementing it in the real world. OpenLineage made an appearance at Data Agility Day 2021, when contributors Julien Le Dem and Kevin Mellott took the virtual stage for a casual conversation about data lineage. The result was both informative and enjoyable. If you couldn't make the event this year, that's okay! The video is now available, and it's almost as good as being there in person. Julien Le Dem is the creator and lead engineer of OpenLineage. Kevin Mellott implemented the Enterprise Data Platform at Northwestern Mutual, and recently shared a post detailing his team‚Äôs experiences. The video is also available at the Data Agility Day site, where you can keep an eye out for future events.","keywords":""},{"title":"Backfilling Airflow DAGs using Marquez","type":0,"sectionRef":"#","url":"/blog/backfilling-airflow-dags-using-marquez","content":"","keywords":""},{"title":"1. Brief Intro to Backfilling Airflow DAGs‚Äã","type":1,"pageTitle":"Backfilling Airflow DAGs using Marquez","url":"/blog/backfilling-airflow-dags-using-marquez#1-brief-intro-to-backfilling-airflow-dags","content":"Airflow supports backfilling DAG runs for a historical time window given a start and end date. Let's say our example.etl_orders_7_days DAG started failing on 2021-06-06, and we wanted to reprocess the daily table partitions for that week (assuming all partitions have been backfilled upstream). In order to run the backfill for example.etl_orders_7_days, using the Airflow CLI, you open up a terminal and execute the following backfill command: # Backfill weekly food orders $ airflow dags backfill \\ --start-date 2021-06-06 \\ --end-date 2021-06-06 \\ example.etl_orders_7_days  Now, the backfill was fairly straightforward but they're not always trivial. That is, we still have the following open questions: How quickly can data quality issues be identified and explored?What alerting rules should be in place to notify downstream DAGs of possible upstream processing issues or failures?What effects (if any) would upstream DAGs have on downstream DAGs if dataset consumption was delayed? Next, we'll demonstrate how lineage metadata managed with Marquez can help answer some of these questions (and more!) by maintaining inter-DAG dependencies and cataloging historical runs of DAGs. "},{"title":"2. Exploring Lineage Metadata using Marquez‚Äã","type":1,"pageTitle":"Backfilling Airflow DAGs using Marquez","url":"/blog/backfilling-airflow-dags-using-marquez#2-exploring-lineage-metadata-using-marquez","content":"Note: To seed the Marquez HTTP API server with the sample lineage metadata used in this blog post, see the Write Sample Lineage Metadata to Marquez section in Marquez's quickstart guide. 2.1 COLLECT DAG LINEAGE METADATA‚Äã  Figure 1: DAG lineage metadata. Marquez is an open source metadata service for the collection, aggregation, and visualization of a data ecosystem‚Äôs metadata. Marquez has integration support for Airflow with minimal configuration. Using the marquez-airflow library, DAG lineage metadata will be collected automatically during DAG execution using the OpenLineage standard, then stored in Marquez‚Äôs centralized data model. To learn more about how lineage metadata is stored and versioned in Marquez, see the Data Model section in Marquez's quickstart guide. The Airflow integration gives us two important benefits: DAG Metadata: Each DAG has a code version, inputs and outputs, run args, and run state transitions. Keeping a global historical log of DAG runs linked to code will quickly highlight upstream dependencies errors and minimize downstream impact.Lineage Metadata: Each DAG may have one or more upstream dependency. Keeping track of inter-DAG dependencies will allow for teams within an organization to safely depend on one another‚Äôs datasets, while also understanding which DAGs will be impacted downstream of a DAG failure. In this blog, we won't go into how to enable lineage metadata collection for Airflow DAGs. But, we encourage you to take a look at Marquez's Airflow example to learn how to troubleshoot DAG failures using Marquez. 2.2 GET LINEAGE METADATA VIA REST API‚Äã In Marquez, each dataset and job has its own globally unique node ID that can be used to query the lineage graph. The LineageAPI returns a set of nodes consisting of edges. An edge is directed and has a defined origin and destination. A lineage graph may contain the following node types: dataset:&lt;namespace&gt;:&lt;dataset&gt;, job:&lt;namespace&gt;:&lt;job&gt;. So, let's start by querying the lineage graph for our example.etl_orders_7_days DAG using the node ID job:food_delivery:example.etl_orders_7_days. You'll notice in the returned lineage graph that the DAG input datasets are public.categories, public.orders, and public.menus with public.orders_7_days as the output dataset: REQUEST‚Äã $ curl -X GET &quot;http://localhost:5000/api/v1-beta/lineage?nodeId=job:food_delivery:example.etl_orders_7_days&quot;  RESPONSE‚Äã 200 OK { &quot;graph&quot;: [{ &quot;id&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;, &quot;type&quot;: &quot;JOB&quot;, &quot;data&quot;: { &quot;type&quot;: &quot;BATCH&quot;, &quot;id&quot;: { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;example.etl_orders_7_days&quot; }, &quot;name&quot;: &quot;example.etl_orders_7_days&quot;, &quot;createdAt&quot;: &quot;2021-06-06T14:50:13.931946Z&quot;, &quot;updatedAt&quot;: &quot;2021-06-06T14:57:54.037399Z&quot;, &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;inputs&quot;: [ {&quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.categories&quot;}, {&quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.menu_items&quot;}, {&quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.orders&quot;}, {&quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.menus&quot;} ], &quot;outputs&quot;: [ {&quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.orders_7_days&quot;} ], &quot;location&quot;: &quot;https://github.com/example/jobs/blob/2294bc15eb49071f38425dc927e48655530a2f2e/etl_orders_7_days.py&quot;, &quot;context&quot;: { &quot;sql&quot;: &quot;INSERT INTO orders_7_days (order_id, placed_on, discount_id, menu_id, restaurant_id, menu_item_id, category_id)\\n SELECT o.id AS order_id, o.placed_on, o.discount_id, m.id AS menu_id, m.restaurant_id, mi.id AS menu_item_id, c.id AS category_id\\n FROM orders AS o\\n INNER JOIN menu_items AS mi\\n ON menu_items.id = o.menu_item_id\\n INNER JOIN categories AS c\\n ON c.id = mi.category_id\\n INNER JOIN menu AS m\\n ON m.id = c.menu_id\\n WHERE o.placed_on &gt;= NOW() - interval '7 days';&quot; }, &quot;description&quot;: &quot;Loads newly placed orders weekly.&quot;, &quot;latestRun&quot;: { &quot;id&quot;: &quot;5c7f0dc4-d3c1-4f16-9ac3-dc86c5da37cc&quot;, &quot;createdAt&quot;: &quot;2021-06-06T14:50:36.853459Z&quot;, &quot;updatedAt&quot;: &quot;2021-06-06T14:57:54.037399Z&quot;, &quot;nominalStartTime&quot;: &quot;2021-06-06T14:54:00Z&quot;, &quot;nominalEndTime&quot;: &quot;2021-06-06T14:57:00Z&quot;, &quot;state&quot;: &quot;FAILED&quot;, &quot;startedAt&quot;: &quot;2021-06-06T14:54:14.037399Z&quot;, &quot;endedAt&quot;: &quot;2021-06-06T14:57:54.037399Z&quot;, &quot;durationMs&quot;: 220000, &quot;args&quot;: {}, &quot;location&quot;: &quot;https://github.com/example/jobs/blob/2294bc15eb49071f38425dc927e48655530a2f2e/etl_orders_7_days.py&quot;, &quot;context&quot;: { &quot;sql&quot;: &quot;INSERT INTO orders_7_days (order_id, placed_on, discount_id, menu_id, restaurant_id, menu_item_id, category_id)\\n SELECT o.id AS order_id, o.placed_on, o.discount_id, m.id AS menu_id, m.restaurant_id, mi.id AS menu_item_id, c.id AS category_id\\n FROM orders AS o\\n INNER JOIN menu_items AS mi\\n ON menu_items.id = o.menu_item_id\\n INNER JOIN categories AS c\\n ON c.id = mi.category_id\\n INNER JOIN menu AS m\\n ON m.id = c.menu_id\\n WHERE o.placed_on &gt;= NOW() - interval '7 days';&quot; }, &quot;facets&quot;: {} } }, &quot;inEdges&quot;: [ {&quot;origin&quot;: &quot;dataset:food_delivery:public.categories&quot;, &quot;destination&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;}, &quot;destination&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;}, {&quot;origin&quot;: &quot;dataset:food_delivery:public.orders&quot;, &quot;destination&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;}, {&quot;origin&quot;: &quot;dataset:food_delivery:public.menus&quot;, &quot;destination&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;} ], &quot;outEdges&quot;: [ {&quot;origin&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;, &quot;destination&quot;: &quot;dataset:food_delivery:public.orders_7_days&quot;} ] } }, ...] }  "},{"title":"3. Using Lineage Metadata to Backfill Airflow DAGs‚Äã","type":1,"pageTitle":"Backfilling Airflow DAGs using Marquez","url":"/blog/backfilling-airflow-dags-using-marquez#3-using-lineage-metadata-to-backfill-airflow-dags","content":"3.1 BACKFILLING‚Äã  Figure 2: Backfilled daily table partitions. To run a backfill for example.etl_orders_7_days using the DAG lineage metadata stored in Marquez, we'll need to query the lineage graph for the upstream DAG where the error originated. Now, let's assume the example.etl_orders DAG upstream of example.etl_orders_7_days failed to write some of the daily table partitions needed for the weekly food order trends report (see Figure 2). To fix the weekly trends report, we'll first need to backfill the missing daily table partitions public.orders_2021_06_04, public.orders_2021_06_05, and public.orders_2021_06_06: # Backfill daily food orders $ airflow dags backfill \\ --start-date 2021-06-04 \\ --end-date 2021-06-06 \\ example.etl_orders   Figure 3: Airflow inter-DAG dependencies. Then, using the script backfill.sh defined below, we can easily backfill all DAGs downstream of example.etl_orders: backfill.sh‚Äã Note: Make sure you have jq installed before running backfill.sh. #!/bin/bash # # Backfill DAGs automatically using lineage metadata stored in Marquez. # # Usage: $ ./backfill.sh &lt;start-date&gt; &lt;end-date&gt; &lt;dag-id&gt; set -e # Backfills DAGs downstream of the given node ID, recursively. backfill_downstream_of() { node_id=&quot;${1}&quot; # Get out edges for node ID out_edges=($(echo $lineage_graph \\ | jq -r --arg NODE_ID &quot;${node_id}&quot; '.graph[] | select(.id==$NODE_ID) | .outEdges[].destination')) for out_edge in &quot;${out_edges[@]}&quot;; do # Run backfill if out edge is a job node (i.e. &lt;dataset&gt; =&gt; &lt;job&gt;) if [[ &quot;${out_edge}&quot; = job:* ]]; then dag_id=&quot;${out_edge##*:}&quot; echo &quot;backfilling ${dag_id}...&quot; airflow backfill --start_date &quot;${start_date}&quot; --end_date &quot;${start_date}&quot; &quot;${dag_id}&quot; fi # Follow out edges downstream, recursively backfill_downstream_of &quot;${out_edge}&quot; done } start_date=&quot;${1}&quot; end_date=&quot;${2}&quot; dag_id=&quot;${3}&quot; # (1) Build job node ID (format: 'job:&lt;namespace&gt;:&lt;job&gt;') node_id=&quot;job:food_delivery:${dag_id}&quot; # (2) Get lineage graph lineage_graph=$(curl -s -X GET &quot;http://localhost:5000/api/v1-beta/lineage?nodeId=${node_id}&quot;) # (3) Run backfill backfill_downstream_of &quot;${node_id}&quot;  When you run the script backfill.sh, it will output all backfilled DAGs to the console: $ ./backfill.sh 2021-06-06 2021-06-06 example.etl_orders backfilling example.etl_orders_7_days... backfilling example.etl_delivery_7_days... backfilling example.delivery_times_7_days...  "},{"title":"4. Conclusion‚Äã","type":1,"pageTitle":"Backfilling Airflow DAGs using Marquez","url":"/blog/backfilling-airflow-dags-using-marquez#4-conclusion","content":"In this blog post, we showed how easy it can be to automate backfilling DAGs downstream of a data quality issue using lineage metadata stored in Marquez. With only two steps, we were able to backfill missing daily table partitions, then automatically re-run failed DAGs downstream of the upstream DAG where the error originated. But, what measures can we put in place to detect low-quality data issues faster, therefore avoiding backfills altogether? Since Marquez collects DAG run metadata that can be viewed using the Runs API, building automated processes that periodically check DAG run states and quickly notifying teams of upstream data quality issue (or missed SLAs) in a timely fashion is just one possible preventive measure. We encourge you to explore Marquez's opinionated Metadata API and define your own automated process(es) for analyzing lineage metadata! If you need help or have any questions, you can always join our Slack channel or reach out to us on Twitter. "},{"title":"Meet Us at Data Council Austin","type":0,"sectionRef":"#","url":"/blog/data-council-meetup","content":"","keywords":""},{"title":"Meetup Details‚Äã","type":1,"pageTitle":"Meet Us at Data Council Austin","url":"/blog/data-council-meetup#meetup-details","content":"Date: March 30, 2023Time: 12:15-1:30 pm CST Place: AT&amp;T Hotel and Conference Center, UT Austin, Room 103 "},{"title":"Happening Soon - Our First Meetup!","type":0,"sectionRef":"#","url":"/blog/data-lineage-meetup","content":"","keywords":""},{"title":"Time, Place & Format‚Äã","type":1,"pageTitle":"Happening Soon - Our First Meetup!","url":"/blog/data-lineage-meetup#time-place--format","content":"Date: March 9, 2023 Format: In-person Time: 6-8 pm ET Address: CIC, 225 Dyer Street, Providence, RI, US 02903  Getting There‚Äã Air: the nearest airport is T.F. Green/PVD. Boston Logan is also within 1.5-2 hours' driving distance. Rail: Amtrak serves PVD, which is within walking distance of CIC. Road: garages and lots are a short walk away from the venue, and metered street parking is also available nearby. Richmond Garage South Street Landing garage Clifford parking lot  Getting In‚Äã Check in with the CIC concierge inside the north entrance. The concierge will direct you to the Hope Island Room on the 3rd floor. Arriving Early?‚Äã Come to the coffee bar in Plant City at 334 South Water Street, Providence RI 02903, which is a short walk from CIC. Other out-of-towners will be meeting up there between 3 and 6 pm. "},{"title":"Hope to see you there!‚Äã","type":1,"pageTitle":"Happening Soon - Our First Meetup!","url":"/blog/data-lineage-meetup#hope-to-see-you-there","content":""},{"title":"Using Marquez to Visualize dbt Models","type":0,"sectionRef":"#","url":"/blog/dbt-with-marquez","content":"Each time dbt runs, it generates a trove of metadata about datasets and the work it performs with them. In this post, I‚Äôd like to show you how to harvest this metadata and put it to good use. The first time I built a data warehouse was in a completely different era, even though it wasn‚Äôt all that long ago. It was a few dozen tables + a collection of loader scripts and an ETL tool. If I‚Äôm honest, calling the whole thing a ‚Äúdata warehouse‚Äù is a bit grandiose, but it worked. At the time, my defining question was ‚Äúhow can I make all of my most important data available for study without spending more than it‚Äôs worth?‚Äù Because my database capacity wasn‚Äôt infinite, I couldn‚Äôt keep all of my data forever. The jobs I wrote would pull data from operational data stores, perform a bunch of slicing and aggregation, and load summary data into the warehouse. They shoveled bits every night from one server to another, performing calculations in between - and that meant they had to run on a beefy server with close proximity to my data. Skip forward to the current day and here I am, building and running models from a cafe over pretty shaky wifi. My, how things have changed. Cloud data warehouses like Google BigQuery, Amazon Redshift, and Snowflake have created a new economic and technological possibility: we can now pretty much just load everything - including our entire operational data stores - into a single warehouse. Once everything is in one place, data can be sliced up and analyzed much more quickly. This is where dbt shines, at making transformations within a cloud data warehouse easy. And we all know what happens when you make something easy: it finds a way to happen a lot. People are doing more complex transformations than ever before, and the need for lineage context is becoming greater than ever. Fortunately, each time dbt runs it generates a trove of metadata about datasets and the work it performs with them. In this post, I‚Äôd like to show you how to harvest this metadata and put it to good use. Our Example For our example, let‚Äôs choose the kind of experiment that I might run in my day-to-day life. I‚Äôm the head of marketing at Datakin, which means the metrics I‚Äôm most interested in are usually about some sort of human behavior. I ask questions like: Does [x] technology space matter, and to whom? Is it waxing or waning?Are there adjacent ecosystems we should be collaborating with?Who are the influencers in this space? Who are the major contributors?What challenges are users facing? What does successful adoption look like? There are a lot of ways to try to answer these questions. None of them are any more reliable than human behavior itself, and every resulting metric requires analysis and judgment. But there are still some pretty fun things to discover. And what better data source to mine to understand technical audiences than Stack Overflow? So let‚Äôs see what we can learn from the Stack Overflow public data set in BigQuery. But not the whole thing; it is very large, so let‚Äôs study just a part of it. I created a sample dbt project that contains a handful of models to study all of the questions and answers we can find about the topic of ELT. These models: Create slices of the key Stack Overflow tables, pulling them into a separate BigQuery project. These slices only contain the rows that are related to questions tagged with ‚Äúelt‚Äù. That way, we can query them tortuously all day long without scanning through gobs of partitions and running up our bill.Augment these slices by performing some helpful calculations - in this case, the number of upvotes/downvotes per question.Populate two summary tables for consumption by a BI system of some sort: a daily summary table that can be used to study trends and a user summary table that can be used to learn about the most influential contributors. This is exactly the kind of experiment I have run multiple times over the years, across numerous stacks. It‚Äôs usually pretty messy. But this time, after running all of these models, we will be rewarded with a gorgeous Marquez lineage graph. We‚Äôll be able to see how everything fits together. Setting Everything Up First, if you haven‚Äôt already, run through the excellent dbt tutorial. It will show you how to create a BigQuery project, provision a service account, download a JSON key, and set up your local dbt environment. The rest of this example assumes that you have created a BigQuery project where our models can be run, and you know how to properly configure dbt to connect to it. Next, let‚Äôs start a local Marquez instance to store our lineage metadata. Make sure you have Docker running, and then: git clone https://github.com/MarquezProject/marquez.git &amp;&amp; cd marquez ./docker/up.sh Check to make sure Marquez is up by visiting http://localhost:3000. You should see an empty Marquez instance with a message saying there isn‚Äôt any data. Also, you should be able to see the server output from your requests in the terminal window where Marquez is running. Keep this window open until we‚Äôre done. Now, let‚Äôs open a new terminal window/pane and clone the GitHub project containing our models: git clone https://github.com/rossturk/stackostudy.git &amp;&amp; cd stackostudy Next we need to install dbt and its integration with OpenLineage. I like to do this in a Python virtual environment because I make mistakes - as we all do - and I enjoy knowing that I can burn everything down and start over quickly if I need to. Virtual environments make this easy. To create one and install everything we need, run the following commands: python -m venv virtualenv source virtualenv/bin/activate pip install dbt dbt-openlineage dbt learns how to connect to your BigQuery project by looking for a matching profile in ~/.dbt/profiles.yml. Create or edit this file so it contains a section with your BigQuery connection details. You will need to point to the location of a file containing the JSON key for your service account. If you aren‚Äôt sure, you can follow this section in the dbt documentation. My profiles.yml looked like this when I was done: stackostudy: target: dev outputs: dev: type: bigquery method: service-account keyfile: /Users/rturk/.dbt/dbt-example.json project: dbt-example dataset: stackostudy threads: 1 timeout_seconds: 300 location: US priority: interactive Run dbt debug to make sure that you have everything configured correctly. % dbt debug Running with dbt=0.20.1 dbt version: 0.20.1 python version: 3.8.12 python path: /opt/homebrew/Cellar/dbt/0.20.1_1/libexec/bin/python3 os info: macOS-11.5.2-arm64-arm-64bit Using profiles.yml file at /Users/rturk/.dbt/profiles.yml Using dbt_project.yml file at /Users/rturk/projects/stackostudy/dbt_project.yml Configuration: profiles.yml file [OK found and valid] dbt_project.yml file [OK found and valid] Required dependencies: - git [OK found] Connection: method: service-account database: stacko-study schema: stackostudy location: US priority: interactive timeout_seconds: 300 maximum_bytes_billed: None Connection test: OK connection ok A Few Important Details There are a couple of considerations to make when designing dbt models for use with OpenLineage. By following these conventions, you can help OpenLineage collect the most complete metadata possible. First, when working with datasets outside of your dbt project, define them in a schema YAML file inside the models/ directory: version: 2 sources: - name: stackoverflow database: bigquery-public-data schema: stackoverflow tables: - name: posts_questions - name: posts_answers - name: users - name: votes This contains the name of the external dataset - in this case, bigquery-public-datasets, and lists the tables that are used by the models in this project. It doesn‚Äôt matter what the file is named, as long as it ends with .yml and is inside the models/ directory, so I called mine schema.yml ü§∑‚Äç‚ôÇÔ∏è If you hardcode dataset and table names into your queries instead, dbt will likely run successfully but dataset metadata will be incompletely collected. When writing queries, be sure to use the {{ ref() }} and {{ source() }} jinja functions when referring to data sources. The {{ ref() }} function can be used to refer to tables within the same model, and the {{ source() }} function refers to tables we have defined in schema.yml. That way, dbt will properly keep track of the relationships between datasets. For example, to select from both an external dataset and one in this model: select * from {{ source('stackoverflow', 'posts_answers') }} where parent_id in (select id from {{ ref('filtered_questions') }} ) Performing a Run Okay! We are ready to perform a run. Before we do, though, there‚Äôs one last step we need to take. Run dbt docs generate. This will cause dbt to create a target/catalog.json file containing the schemas of each dataset referred to in the models. This file will be parsed by the dbt OpenLineage integration and sent to our Marquez server. If it doesn‚Äôt exist, a lineage graph will still be generated but schema details won‚Äôt be available in Marquez. dbt docs generate Running with dbt=0.20.1 Found 8 models, 0 tests, 0 snapshots, 0 analyses, 164 macros, 0 operations, 0 seed files, 4 sources, 0 exposures 12:15:10 | Concurrency: 1 threads (target='dev') 12:15:10 | 12:15:10 | Done. 12:15:10 | Building catalog 12:15:26 | Catalog written to /Users/rturk/projects/stackostudy/target/catalog.json The OpenLineage integration for dbt is implemented as a wrapper, dbt-ol. This wrapper runs dbt and, after it completes, analyzes the target/catalog.json, target/run_results.json and target/manifest.json files. It sends corresponding OpenLineage events to the endpoint specified in the OPENLINEAGE_URL environment variable. To run the models: % OPENLINEAGE_URL=http://localhost:5000 dbt-ol run Running with dbt=0.20.1 Found 8 models, 0 tests, 0 snapshots, 0 analyses, 164 macros, 0 operations, 0 seed files, 4 sources, 0 exposures 12:35:41 | Concurrency: 1 threads (target='dev') 12:35:41 | 12:35:41 | 1 of 8 START incremental model stackostudy.filtered_questions........ [RUN] 12:35:46 | 1 of 8 OK created incremental model stackostudy.filtered_questions... [MERGE (0.0 rows, 34.6 GB processed) in 4.52s] 12:35:46 | 2 of 8 START incremental model stackostudy.filtered_answers.......... [RUN] 12:35:51 | 2 of 8 OK created incremental model stackostudy.filtered_answers..... [MERGE (0.0 rows, 26.8 GB processed) in 5.22s] 12:35:51 | 3 of 8 START incremental model stackostudy.filtered_votes............ [RUN] 12:36:05 | 3 of 8 OK created incremental model stackostudy.filtered_votes....... [MERGE (0.0 rows, 6.5 GB processed) in 14.58s] 12:36:05 | 4 of 8 START incremental model stackostudy.filtered_users............ [RUN] 12:36:21 | 4 of 8 OK created incremental model stackostudy.filtered_users....... [MERGE (0.0 rows, 2.5 GB processed) in 16.09s] 12:36:21 | 5 of 8 START view model stackostudy.summary_daily.................... [RUN] 12:36:23 | 5 of 8 OK created view model stackostudy.summary_daily............... [OK in 1.01s] 12:36:23 | 6 of 8 START view model stackostudy.answer_stats..................... [RUN] 12:36:23 | 6 of 8 OK created view model stackostudy.answer_stats................ [OK in 0.96s] 12:36:23 | 7 of 8 START view model stackostudy.question_stats................... [RUN] 12:36:24 | 7 of 8 OK created view model stackostudy.question_stats.............. [OK in 0.88s] 12:36:24 | 8 of 8 START view model stackostudy.user_stats....................... [RUN] 12:36:26 | 8 of 8 OK created view model stackostudy.user_stats.................. [OK in 1.21s] 12:36:26 | 12:36:26 | Finished running 4 incremental models, 4 view models in 45.39s. Completed successfully Done. PASS=8 WARN=0 ERROR=0 SKIP=0 TOTAL=8 Emitted 16 openlineage events Note the output showing the number of OpenLineage events emitted to Marquez. Reviewing the Output If everything ran successfully you should be able to see a list of jobs when you navigate to http://localhost:3000. Upon clicking a job, you will see a lineage graph that looks similar to this: Our set of models, previously represented by SQL inside text files, has become more easily digestible. The dependencies between datasets are now completely obvious. Data engineers can throw away their remaining whiteboards, hooray! There‚Äôs something satisfying about seeing models represented in two-dimensional space. But more importantly, this integration allows us to capture the state of a dbt pipeline as it runs. Using a long-running instance of Marquez (or another OpenLineage-compatible metadata repository) this information can be studied as it changes over time. To see how the OpenLineage dbt integration works, visit its GitHub repository.","keywords":""},{"title":"Expecting Great Quality with OpenLineage Facets","type":0,"sectionRef":"#","url":"/blog/dataquality_expectations_facet","content":"","keywords":""},{"title":"The Parable of Bad Data‚Äã","type":1,"pageTitle":"Expecting Great Quality with OpenLineage Facets","url":"/blog/dataquality_expectations_facet#the-parable-of-bad-data","content":"Several years ago, I worked as a developer on the experimentation team at Amazon, which owned the code libraries and data processing systems that supported experimentation on the amazon.com website (among other systems). Developers used our libraries and a microservice we maintained to ‚Äútrigger‚Äù an experiment for a customer- that is, the customer was randomized into either control or treatment and the resulting assignment was recorded in the logs, which my team consumed in the analysis of the data later on. One of the interesting parts of my job was helping our users diagnose problems with their experiment results. A classic example was a Kindle developer who was prototyping a new feature for book pages that would make the site more engaging for Kindle owners- perhaps a ‚Äúlook inside‚Äù feature, or maybe some better recommendations. A customer would come to the website and the developer‚Äôs code would determine if the customer belonged in Control or Treatment. If Control, the assignment was logged and no feature was shown- the site looked to the customer as it always had. But if the assignment was Treatment, the code would check the customer‚Äôs account to determine if they owned a Kindle device and, if yes, the assignment was logged and the customer saw the fancy new feature on the books page. The experiment showed the developer‚Äôs feature would be wildly successful- an increase of over $10 in Kindle book purchases per customer on average over the course of the 2 weeks the experiment ran- projected to be tens of billions of dollars in annual revenue due to this one feature! With data in hand, the developer requested tons of resources to build the feature up to production standards. After three months and two dozen people‚Äôs labor, the feature was ready to ship. The developers deployed their new service and the incredible feature was unleashed. For days afterward, everyone watched the metrics dashboards waiting for that hockey stick uptick in the revenue graphs üìà. But it never materialized! The graph was flat. No change at all! Weeks went by. Nothing. How could the experiment results be so far from reality? Of course, if you‚Äôve ever run A/B tests, you probably already recognized the developer‚Äôs mistake. In their randomization logic, customers who were assigned control were logged and forgotten, while customers who were assigned treatment were logged only after validating that they owned a Kindle device. It turned out the total number of customers who came to amazon.com was far greater than the number of customers who owned a Kindle device. And if you divide the total sum of Kindle book sales by all of the amazon.com customers, regardless of whether they own a Kindle device, that average will come out quite a lot lower than if you calculate the average Kindle book revenue from only customers who own Kindles.  In reality, this story never happened. Why? Because we knew the adage- Bad Data is Worse than No Data. In the story, people took data of poor quality and used it to justify bad decisions. In our system, we checked the quality of the data and, if we detected assignment imbalances, we simply invalidated the experiment and hid the results. Over the years, I can‚Äôt count the number of times our users asked us to just give them partial results or just exclude certain segments or to let them know if things were ‚Äútrending‚Äù the right way. Our policy was firm- if we couldn‚Äôt trust the quality of the data, the results were meaningless and we would not surface them in our system. "},{"title":"Data-Driven Depends On Data-Quality‚Äã","type":1,"pageTitle":"Expecting Great Quality with OpenLineage Facets","url":"/blog/dataquality_expectations_facet#data-driven-depends-on-data-quality","content":"Today, most businesses consider themselves data-driven. The stereotype of the maverick CEO leading with his or her gut is mostly antiquated, with a handful of exceptions. And yet, even though people know intellectually that data is only useful if it is correct, we tend to stop digging once we find some data that confirms our pre-existing assumptions. We justify bad decisions by claiming that they are ‚Äúdata-based‚Äù without ever validating the quality of our sources. Where did that data come from? How old is it? Is the logic that generated it correct? Is it derived from some other dataset? What is the quality of that dataset? Thankfully, data quality validation is becoming more and more common in data engineering organizations. In part, this is due to the prevalence of new tools and their integration with common workflow engines which we already use to schedule the jobs that generate and process our data. One such tool that has been gaining in popularity is called Great Expectations, a Python-based framework for defining assertions about data sets which easily integrates with existing workflow tools, such as Airflow. In software development, testing the behavior of our code with unit and integration tests has been common practice for years. Similarly, using Great Expectations, a data engineer can assert that a dataset has a row count that falls within an expected range, that column values are not null, or that values match a specified regular expression. One can even create custom expectations, such as validating that the number of records in treatment is roughly the same as the number of records in control (this post is not intended to be an in-depth tutorial on setting up Great Expectations; if you want to read more on its capabilities and to get started, I recommend the going through the Quick Start tutorial). "},{"title":"A Sample Assertion Suite‚Äã","type":1,"pageTitle":"Expecting Great Quality with OpenLineage Facets","url":"/blog/dataquality_expectations_facet#a-sample-assertion-suite","content":"As a simple example, imagine a table of new customers that you need to import into your Data Warehouse. Before importing, we want to check the data quality of this new batch of customers. One example suite of assertions we could test is below: { &quot;data_asset_type&quot;: &quot;Dataset&quot;, &quot;expectation_suite_name&quot;: &quot;customers_suite&quot;, &quot;expectations&quot;: [ { &quot;expectation_type&quot;: &quot;expect_table_row_count_to_be_between&quot;, &quot;kwargs&quot;: { &quot;max_value&quot;: 1000, &quot;min_value&quot;: 100 }, &quot;meta&quot;: {} }, { &quot;expectation_type&quot;: &quot;expect_table_column_count_to_equal&quot;, &quot;kwargs&quot;: { &quot;value&quot;: 8 }, &quot;meta&quot;: {} }, { &quot;expectation_type&quot;: &quot;expect_table_columns_to_match_ordered_list&quot;, &quot;kwargs&quot;: { &quot;column_list&quot;: [ &quot;id&quot;, &quot;created_at&quot;, &quot;updated_at&quot;, &quot;name&quot;, &quot;email&quot;, &quot;address&quot;, &quot;phone&quot;, &quot;city_id&quot; ] }, &quot;meta&quot;: {} }, { &quot;expectation_type&quot;: &quot;expect_column_values_to_be_unique&quot;, &quot;kwargs&quot;: { &quot;column&quot;: &quot;email&quot; }, &quot;meta&quot;: {} } ], &quot;meta&quot;: { // ... } }  This sample suite contains 4 data quality assertions- that the dataset contains between 100 and 1000 rows, that the table contains exactly 8 columns, that they match the explicit list of column names we expect, and that the email column contains only distinct values. "},{"title":"Adding Data Quality Checks to an Airflow pipeline‚Äã","type":1,"pageTitle":"Expecting Great Quality with OpenLineage Facets","url":"/blog/dataquality_expectations_facet#adding-data-quality-checks-to-an-airflow-pipeline","content":"With a suite of assertions in hand, we can update our Airflow DAG to only import data into our Data Warehouse if it matches our expectations. A simple DAG might look like this from airflow import DAG from airflow.contrib.operators.bigquery_operator import BigQueryOperator from airflow.utils.dates import days_ago from great_expectations_provider.operators.great_expectations import GreatExpectationsOperator dag = DAG( 'etl_customers', schedule_interval='@daily', catchup=False, default_args=default_args, description='Loads newly registered customers daily.' ) t1 = BigQueryOperator( task_id='if_not_exists', sql=''' CREATE TABLE IF NOT EXISTS food_delivery.customers ( id INT64, created_at TIME, updated_at TIME, name STRING, email STRING, address STRING, phone STRING, city_id INT64 ) ''', use_legacy_sql=False, dag=dag ) t2 = GreatExpectationsOperator( expectation_suite_name='customers_suite', batch_kwargs={ 'table': 'tmp_customers', 'datasource': 'food_delivery_db' }, dag=dag task_id='customers_expectation', ) t3 = BigQueryOperator( task_id='etl', sql=''' SELECT id, created_at, updated_at, name, email, address, phone, city_id FROM food_delivery.tmp_customers ''', destination_dataset_table='airflow_marquez.food_delivery.customers', use_legacy_sql=False, dag=dag ) t1 &gt;&gt; t2 &gt;&gt; t3  This is great! Our DAG creates our target table in BigQuery (if it doesn‚Äôt already exist), checks the quality of thetmp_customers table by running the customers_suite defined earlier, then imports only if all of the data quality checks pass. And thus ended all data quality problems forever. Just kidding. Because reality is never so straightforward. In reality, the recommendations team wanted to start generating recommendations for new customers without waiting until the next day, so they built a data pipeline to start consuming from the tmp_customers table directly. And the supply chain folks wanted to start detecting what recipes are gaining popularity so they can predict what supplies will need to be restocked sooner, so they started reading from the bothtmp_orders table and the tmp_customers table before they‚Äôre available in the DW. Oh, and the scheduling team needs an idea of the geography of the various customers that are ordering and what the distances are between restaurants and customers so they can get the right number of drivers in the right neighborhoods and of course the marketing team wants to use all of this data to make predictions about how much to spend on the right search engine and social media ads and they absolutely cannot wait until tomorrow at 8AM to update their models. "},{"title":"Tracing Data Quality With OpenLineage Facets‚Äã","type":1,"pageTitle":"Expecting Great Quality with OpenLineage Facets","url":"/blog/dataquality_expectations_facet#tracing-data-quality-with-openlineage-facets","content":"Users are never satisfied with the way things are supposed to work. There‚Äôs always a reason to work around gatekeepers- oftentimes, very good reasons that have real business impact- and data engineering is full of creative and resourceful people who absolutely will find a way to get at that data. Even at Amazon, the experiment data was available in the click stream logs, so resourceful users could (and sometimes did) calculate their own experiment results if they really wanted to. So it‚Äôs important not just to have data quality checks, but to trace the impact of that data throughout an organization. The OpenLineage standard uses Facets to augment the core data model with useful information about the jobs, runs, and datasets reported on. One interesting detail about facets is that they can be attached to an entity after the fact. In the Marquez reference implementation, a dataset version is created every time a job run writes to or otherwise modifies a dataset. Output facets, such as the new record count or the number of bytes written, are attached directly to the dataset version when the job run completes. But consuming jobs can also attach facets to the version of the dataset that exists at the start time of the job‚Äôs execution. In the OpenLineage Airflow integration, Great Expectations tasks, such as the one in our example DAG above, are evaluated after they run and the expectation results (as well as some other data quality metrics) are collected into aDataQuality Metrics Input Dataset Facet, which is reported to the server along with the rest of the lineage metadata. In Marquez, we recognize the version of the dataset that was read by the job run and the data quality metadata is permanently associated with that dataset version. The impact of this is that any job that reads that data, whether it happens before or after the dataset quality assertion, can be linked to the data quality facet recorded (provided that the dataset version doesn‚Äôt change between the data quality check and the read job). This integration is extremely straightforward to get working. If you already have the Marquez Airflow DAG running in your Airflow workflows, there‚Äôs nothing to do! Great Expectations tasks are already being detected and the metrics and assertion statuses are already being reported to your configured instance of Marquez. If you‚Äôve never integrated Marquez with your Airflow setup, add a couple of environment variablesand change one line of code: - from airflow import DAG + from marquez_airflow import DAG from airflow.contrib.operators.bigquery_operator import BigQueryOperator from airflow.utils.dates import days_ago from great_expectations_provider.operators.great_expectations import GreatExpectationsOperator  I‚Äôve previously written about how to determine the version of the dataset that was read by a particular job run. With Great Expectations now integrated into my Airflow DAG, I want to see what the data quality metrics are for the latest version of the customers dataset that was processed by my ETL job. I‚Äôll hit my datakin demo instance: $ curl &quot;https://demo.datakin.com/api/v1/namespaces/food_delivery/jobs/etl.etl_delivery_7_days&quot; | jq | less { &quot;id&quot;: { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;etl.etl_delivery_7_days&quot; }, &quot;type&quot;: &quot;BATCH&quot;, &quot;name&quot;: &quot;etl.etl_delivery_7_days&quot;, &quot;createdAt&quot;: &quot;2021-07-23T19:32:03.401782Z&quot;, &quot;updatedAt&quot;: &quot;2021-08-06T05:11:03.604573Z&quot;, &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;inputs&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.customers&quot; }, //... ], &quot;latestRun&quot;: { &quot;id&quot;: &quot;1043e596-ccb8-4bfb-8fc2-7ee066253248&quot;, &quot;jobVersion&quot;: { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;etl.etl_delivery_7_days&quot;, &quot;version&quot;: &quot;bc6c294b-b0eb-3160-a06d-1ff9ba3a4e1c&quot; }, &quot;inputVersions&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.customers&quot;, &quot;version&quot;: &quot;4c33f292-40a9-304d-b43f-c7ffb2256e7f&quot; }, // ... ], // ... } }  With the input version of the public.customers dataset, I can query Marquez for all the metadata we have about that specific version of the dataset. $ curl &quot;https://demo.datakin.com/api/v1/namespaces/food_delivery/datasets/public.customers/versions/4c33f292-40a9-304d-b43f-c7ffb2256e7f&quot; | jq | less { &quot;id&quot;: { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.customers&quot; }, &quot;type&quot;: &quot;DB_TABLE&quot;, &quot;name&quot;: &quot;public.customers&quot;, &quot;physicalName&quot;: &quot;public.customers&quot;, &quot;createdAt&quot;: &quot;2021-08-06T05:02:59.189118Z&quot;, &quot;version&quot;: &quot;4c33f292-40a9-304d-b43f-c7ffb2256e7f&quot;, &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;sourceName&quot;: &quot;analytics_db&quot;, &quot;fields&quot;: [ { &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;INTEGER&quot;, &quot;tags&quot;: [], &quot;description&quot;: &quot;The unique ID of the customer.&quot; }, // ... ], &quot;facets&quot;: { &quot;stats&quot;: { &quot;size&quot;: 53362712, &quot;rowCount&quot;: 4969 }, &quot;dataSource&quot;: { &quot;uri&quot;: &quot;jdbc:postgresql://localhost:3306/deliveries&quot;, &quot;name&quot;: &quot;analytics_db&quot; }, &quot;description&quot;: &quot;A table for customers.&quot;, &quot;dataQuality&quot;: { &quot;bytes&quot;: 53362712, &quot;rowCount&quot;: 4969, &quot;columnMetrics&quot;: { &quot;id&quot;: { &quot;nullCount&quot;: 0, &quot;distinctCount&quot;: 4969 }, &quot;name&quot;: { &quot;nullCount&quot;: 0, &quot;distinctCount&quot;: 4969 }, &quot;email&quot;: { &quot;nullCount&quot;: 0, &quot;distinctCount&quot;: 4969 } } }, &quot;greatExpectations_assertions&quot;: { &quot;assertions&quot;: [ { &quot;success&quot;: true, &quot;expectationType&quot;: &quot;expect_table_row_count_to_be_between&quot; }, { &quot;success&quot;: true, &quot;expectationType&quot;: &quot;expect_column_to_exist&quot; }, { &quot;success&quot;: true, &quot;columnId&quot;: &quot;id&quot;, &quot;expectationType&quot;: &quot;expect_column_values_to_be_unique&quot; }, { &quot;success&quot;: true, &quot;columnId&quot;: &quot;id&quot;, &quot;expectationType&quot;: &quot;expect_column_values_to_not_be_null&quot; }, { &quot;success&quot;: true, &quot;columnId&quot;: &quot;created_at&quot;, &quot;expectationType&quot;: &quot;expect_column_values_to_not_be_null&quot; }, //.... ] } } }  Note the facets field contains several properties- stats, dataSource, description, dataQualityand greatExpectations_assertions. Each of those describes some attribute about the dataset version. Some of the facets are attached at write-time, some are attached later- when the dataset is read. In our Datakin demo, we have a lot more assertions than what I included in the sample suite above and that can be seen in this response. In addition to counting rows and columns, we also validate that id columns are unique and non-null, timestamps fall within specified ranges (did you know that if you accidentally write a timestamp too far in the future, certain JDBC drivers will overflow the Calendar instance they use for converting timezones?), and emails match expected regular expressions. With the ability to attach data quality facets to dataset versions and the ability to trace the specific versions of datasets read by and written to by specific job runs, I can trust whether the data I‚Äôm looking at is good data or bad data. And if my data quality checks fail, I can find out whether I need to contact somebody over in marketing or recommendations to backfill their pipelines once the issue has been corrected.  Whether your business is an e-commerce shop that wants to improve its customer experience or a music streaming service that wants to make better listening recommendations or an autonomous vehicle company trying to improve the car‚Äôs ability to detect double parked vehicles, the quality of your data is paramount to making good decisions. Quality testing tools are out there and, chances are, they already work with the pipeline workflow tool you‚Äôre using today. And with OpenLineage support, you can be confident in the quality of the data at every stage in your pipeline. "},{"title":"Extending OpenLineage with Facets","type":0,"sectionRef":"#","url":"/blog/extending-with-facets","content":"","keywords":""},{"title":"Open Source‚Äã","type":1,"pageTitle":"Extending OpenLineage with Facets","url":"/blog/extending-with-facets#open-source","content":"Standardizing an API through open source collaboration can be challenging. On one end, you need to get input and feedback from the people who will use the API in different contexts. On the other, you want to avoid getting stuck in disagreements arising from the different and sometimes incompatible viewpoints that inevitably drive these discussions. Thankfully, there are mechanisms to help organize and decouple those disagreements and drive discussions towards conclusion. A community driven open source project works very differently from a product you buy off the shelf. At the very moment you start using it - maybe starting by reading the doc - you become part of the community and start sharing a little bit of ownership. As with any software, you might encounter problems... but in this case, you immediately become part of the solution. In a healthy community, how much of the solution you become is entirely up to you. Maybe you spotted a typo and reported it. Maybe you opened a pull request to fix it. You might propose an improvement, or even build one yourself. All of those contributions, no matter how small, make the project better for everyone. That very powerful flywheel motion gathers momentum and drives very successful open source projects. One of the success factors of such an open source project is how much it can minimize the friction for new community members who want to contribute. The easier it is to contribute, the faster the project will acquire momentum. It‚Äôs not about getting other people‚Äôs input, it‚Äôs about giving them a share of ownership and encouraging them to drive the areas where they can most effectively contribute. In a multi-faceted domain like data lineage, enabling others to lead discussions is critical. "},{"title":"Making progress‚Äã","type":1,"pageTitle":"Extending OpenLineage with Facets","url":"/blog/extending-with-facets#making-progress","content":"In this context, we need mechanisms to converge often and make incremental progress. You definitely want to avoid having a big monolithic spec that takes a long time to reach consensus on - if you ever do. A discussion around a large ultra-spec that combines specifications from multiple related domains will lose steam. We need to keep conversations focused on the topics that individual contributors care about. It is critical to subdivide the specification in concrete and granular decision points where consistent and significant progress can be made. Not everyone will care about all the aspects of the specification, and that is fine. We need to make sure contributors can easily focus on the aspects they do care about. This need for a very granular decision making process, one where we can make progress independently on different aspects of the spec, leads naturally into decomposition of the specification into smaller independent subsets. This will keep conversations focused and moving. It also decouples workstreams where consensus can be reached from those that are more contentious. For example the contributors interested in data quality might be different from the ones interested in column-level lineage or query performance. "},{"title":"Embracing different points of view‚Äã","type":1,"pageTitle":"Extending OpenLineage with Facets","url":"/blog/extending-with-facets#embracing-different-points-of-view","content":"Depending on their perspective, contributors may have very different opinions on how to model a certain aspect of data. Or they may have different use-cases in mind. Instead of pitting different view-points against each other and forcing alignment on every point, it is sometimes beneficial to allow them to be expressed separately. For example, when you ask a data practitioner &quot;what is data lineage?&quot; they may have very different definitions for it. Some care about how a specific metric is derived from the raw data, and need column level lineage. Some will care about compliance with privacy regulations and need relevant metadata to locate sensitive data and trace its movement.Some will care about the reliability of data delivery and need data freshness and quality metrics - in particular, how they change over time in correlation with changes in the system. All those are valid view points that deserve to be captured appropriately and can be defined independently in a framework that allows them to cohabitate. Mechanics OpenLineage is purposefully providing a faceted model around a minimalistic core spec to enable this granular decision making, minimize friction in contributing, and favor community-driven improvements. The core spec focuses on high-level modeling of jobs, runs, datasets, and their relation. Each OpenLineage event refers to a run of a job and its input and output datasets. A job is a recurring transformation that reads from datasets and writes to datasets. It has a unique name that identifies it across runs.A run identifies an individual execution of a job. It might be an incremental or full batch process. It could also be a streaming job.A dataset could be a table in a warehouse or a folder in a blob store. It is consumed or written to by jobs. Facets are pieces of metadata that can be attached to those core entities. Facets have their own schema and capture various aspects of those entities. "},{"title":"Facets are individual atomic specs‚Äã","type":1,"pageTitle":"Extending OpenLineage with Facets","url":"/blog/extending-with-facets#facets-are-individual-atomic-specs","content":"Like the core model, facets are defined by a JSONSchema. They are a self-contained definition of one aspect of a job, a dataset, or a run at the time the event happened. They make the model extensible. The notion of facets is powerful because it makes it easy to add more information to the model - you just define a new facet. There‚Äôs a clear compatibility model when introducing a new facet, since fields that are defined at the same time are grouped together. For example, there‚Äôs a facet to capture the schema of a dataset. There‚Äôs a facet to capture the version of a job in source control. There‚Äôs a facet to capture the parameters of a run. Facets are optional and may not apply to every instance of an entity. "},{"title":"Facets enable specialization of models‚Äã","type":1,"pageTitle":"Extending OpenLineage with Facets","url":"/blog/extending-with-facets#facets-enable-specialization-of-models","content":"The core entities are fairly generic. A dataset might be a table in a warehouse or a topic in a Kafka broker. A job might be a SQL query or a machine learning training job. This generic high level model of lineage can be specialized by adding facets for that particular type of entity. At-rest data might be versioned, enabling transactions at the run level. Streaming data might capture the offsets and partitions where a streaming job started reading. Datasets might have a schema like a warehouse table, or not (for example, in the case of a machine learning model). By capturing a generic representation of lineage and allowing progressive specialization of those entities, this approach offers a lot of flexibility. "},{"title":"Facets allow expressing different point of views‚Äã","type":1,"pageTitle":"Extending OpenLineage with Facets","url":"/blog/extending-with-facets#facets-allow-expressing-different-point-of-views","content":"There can be divergent points of view on how to model a certain aspect of metadata. Facets allow these models to cohabitate in a common framework. One example of this is capturing the physical plan of a query execution. Each data warehouse might have its own unique way of describing execution plans. It is very valuable to be able to capture both a precise (but maybe too specific) model as well as a generic (but possibly imprecise or lossy) representation. They can be captured as two different facets. This also gives us opportunities to define several competing models and use the resulting information to collaborate on a more unified and generic representation. This emergent modeling is actually extremely useful in an open source setting, and as a way to make incremental progress. "},{"title":"Custom facets make the model decentralized‚Äã","type":1,"pageTitle":"Extending OpenLineage with Facets","url":"/blog/extending-with-facets#custom-facets-make-the-model-decentralized","content":"Most importantly, the OpenLineage spec allows custom facets that are defined elsewhere, completely outside of the spec. This allows others to extend the spec as-needed without having to coordinate with anyone or ask any permission from a governing body. They can make their own opinionated definition of an aspect of metadata. All that is required is that they publish a JSONSchema that describes their facets, prefixed by a unique namespace. This lowers the barrier to experimentation and encourages incremental progress by making the experimentation of others visible. The facets that become broadly useful can eventually be represented in the core spec. Contribute! As a community, we‚Äôve done our best to minimize friction when experimenting with or contributing to OpenLineage. We‚Äôre looking forward to seeing you join us as we make data lineage transparent across the data ecosystem. "},{"title":"Exploring Lineage History via the Marquez API","type":0,"sectionRef":"#","url":"/blog/explore-lineage-api","content":"","keywords":""},{"title":"Getting Started‚Äã","type":1,"pageTitle":"Exploring Lineage History via the Marquez API","url":"/blog/explore-lineage-api#getting-started","content":"To get started, we need a running instance of Marquez with a little bit of seed data. For these exercises, we'll assume you have a terminal with the following programs installed dockergitcurljqless (optional) Download and install any dependencies you don't already have. You'll need the docker daemon running (see the docs for your platform to get that started). Then check out the Marquez repository from Github and run the docker image locally: git clone https://github.com/MarquezProject/marquez cd marquez ./docker/up.sh --seed  This script uses docker-compose to spin up a self-contained installation of Marquez, including a local database container, web frontend, and service instance. Additionally, it populates a set of sample data that's useful for exploring the API. You'll know when the seed job is done when you see the following line in the output logs seed-marquez-with-metadata exited with code 0  Once the seed job is done, we can begin exploring the API. "},{"title":"The Jobs‚Äã","type":1,"pageTitle":"Exploring Lineage History via the Marquez API","url":"/blog/explore-lineage-api#the-jobs","content":"In a separate terminal window, type the following command curl &quot;http://localhost:5000/api/v1/namespaces/food_delivery/jobs/&quot; | jq | less  The output returned should look something like the following { &quot;jobs&quot;: [ { &quot;id&quot;: { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;example.delivery_times_7_days&quot; }, &quot;type&quot;: &quot;BATCH&quot;, &quot;name&quot;: &quot;example.delivery_times_7_days&quot;, &quot;createdAt&quot;: &quot;2021-06-24T21:50:39.229759Z&quot;, &quot;updatedAt&quot;: &quot;2021-06-24T22:05:45.321952Z&quot;, &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;inputs&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot; } ], &quot;outputs&quot;: [], &quot;location&quot;: &quot;https://github.com/example/jobs/blob/2294bc15eb49071f38425dc927e48655530a2f2e/delivery_times_7_days.py&quot;, &quot;context&quot;: { &quot;sql&quot;: &quot;INSERT INTO top_delivery_times (order_id, order_placed_on, order_dispatched_on, order_delivered_on, order_delivery_time,\\n customer_email, restaurant_id, driver_id)\\n SELECT order_id, order_placed_on, order_delivered_on, DATEDIFF(minute, order_placed_on, order_delivered_on) AS order_delivery_time,\\n customer_email, restaurant_id, driver_id\\n FROM delivery_7_days\\nGROUP BY restaurant_id\\nORDER BY order_delivery_time DESC\\n LIMIT 1;&quot; }, &quot;description&quot;: &quot;Determine weekly top delivery times by restaurant.&quot;, &quot;latestRun&quot;: { &quot;id&quot;: &quot;f4fada30-dfcc-400c-9391-2d7a506b9139&quot;, &quot;createdAt&quot;: &quot;2021-06-24T21:50:59.509739Z&quot;, &quot;updatedAt&quot;: &quot;2021-06-24T22:05:45.321952Z&quot;, &quot;nominalStartTime&quot;: &quot;2021-06-24T22:02:00Z&quot;, &quot;nominalEndTime&quot;: &quot;2021-06-24T22:05:00Z&quot;, &quot;state&quot;: &quot;FAILED&quot;, &quot;startedAt&quot;: &quot;2021-06-24T22:02:39.321952Z&quot;, &quot;endedAt&quot;: &quot;2021-06-24T22:05:45.321952Z&quot;, &quot;durationMs&quot;: 186000, &quot;args&quot;: {}, &quot;jobVersion&quot;: { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;example.delivery_times_7_days&quot;, &quot;version&quot;: &quot;e9eafa5b-e334-358d-a3b4-61c8d3de75f3&quot; }, &quot;inputVersions&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot;, &quot;version&quot;: &quot;a40ec54f-b8e1-35f7-b868-58b27383b5ff&quot; } ], &quot;outputVersions&quot;: [], &quot;context&quot;: { &quot;sql&quot;: &quot;INSERT INTO top_delivery_times (order_id, order_placed_on, order_dispatched_on, order_delivered_on, order_delivery_time,\\n customer_email, restaurant_id, driver_id)\\n SELECT order_id, order_placed_on, order_delivered_on, DATEDIFF(minute, order_placed_on, order_delivered_on) AS order_delivery_time,\\n customer_email, restaurant_id, driver_id\\n FROM delivery_7_days\\nGROUP BY restaurant_id\\nORDER BY order_delivery_time DESC\\n LIMIT 1;&quot; }, &quot;facets&quot;: {} }, &quot;facets&quot;: {} }, ... ] }  For brevity, I only included a single job- in this case, a job called example.delivery_times_7_daysin the food_delivery namespace (which we specified in the curl command). Your output will include many more jobs. There are a few things in the job output worth noting. The first is the id of the job:  &quot;id&quot;: { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;example.delivery_times_7_days&quot; },  There is no version information in the id, as this API refers to the unversioned job information. The job itself is mutable, in the sense that each time you query the API, the content of the job may change as new versions are created. The response includes the set of input and output datasets, as well as the current job source location:  &quot;inputs&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot; } ], &quot;outputs&quot;: [], &quot;location&quot;: &quot;https://github.com/example/jobs/blob/2294bc15eb49071f38425dc927e48655530a2f2e/delivery_times_7_days.py&quot;,  If a new version of the job is created, any or all of these fields can change. "},{"title":"The Job Run‚Äã","type":1,"pageTitle":"Exploring Lineage History via the Marquez API","url":"/blog/explore-lineage-api#the-job-run","content":"The next thing to notice is the latestRun field. This includes information about the latest Run of this job:  &quot;latestRun&quot;: { &quot;id&quot;: &quot;f4fada30-dfcc-400c-9391-2d7a506b9139&quot;, &quot;createdAt&quot;: &quot;2021-06-24T21:50:59.509739Z&quot;, &quot;updatedAt&quot;: &quot;2021-06-24T22:05:45.321952Z&quot;, &quot;nominalStartTime&quot;: &quot;2021-06-24T22:02:00Z&quot;, &quot;nominalEndTime&quot;: &quot;2021-06-24T22:05:00Z&quot;, &quot;state&quot;: &quot;FAILED&quot;, &quot;startedAt&quot;: &quot;2021-06-24T22:02:39.321952Z&quot;, &quot;endedAt&quot;: &quot;2021-06-24T22:05:45.321952Z&quot;, &quot;durationMs&quot;: 186000, &quot;args&quot;: {}, &quot;jobVersion&quot;: { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;example.delivery_times_7_days&quot;, &quot;version&quot;: &quot;e9eafa5b-e334-358d-a3b4-61c8d3de75f3&quot; }, &quot;inputVersions&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot;, &quot;version&quot;: &quot;a40ec54f-b8e1-35f7-b868-58b27383b5ff&quot; } ], &quot;outputVersions&quot;: [], &quot;context&quot;: { &quot;sql&quot;: &quot;INSERT INTO top_delivery_times (order_id, order_placed_on, order_dispatched_on, order_delivered_on, order_delivery_time,\\n customer_email, restaurant_id, driver_id)\\n SELECT order_id, order_placed_on, order_delivered_on, DATEDIFF(minute, order_placed_on, order_delivered_on) AS order_delivery_time,\\n customer_email, restaurant_id, driver_id\\n FROM delivery_7_days\\nGROUP BY restaurant_id\\nORDER BY order_delivery_time DESC\\n LIMIT 1;&quot; }, &quot;facets&quot;: {} },  Here, we see explicit version information in the jobVersion, the inputVersions, and theoutputVersions fields. This is included because every Run is tied to exactly one immutable version of a job and one immutable version of each input dataset and each output dataset (it's worth noting that a Run can be tied to one version of a dataset as its input and another version of the same dataset as its output- a SQL MERGE statement is one common use case supported by this). The other important field to notice in the Run structure is the state  &quot;state&quot;: &quot;FAILED&quot;,  Uh-oh. Looks like the last time this job ran, it failed. "},{"title":"Tracing Failures‚Äã","type":1,"pageTitle":"Exploring Lineage History via the Marquez API","url":"/blog/explore-lineage-api#tracing-failures","content":"The first question we have when diagnosing a failure is Is this the first time it's failed? Or has it been broken a while? Let's use the API to find out. Checking previous runs is easily accomplished by hitting the job's runsAPI. Job runs are returned in descending order by start time, so the latest runs should be at the top. Since we only want to check whether (and which) previous runs failed, we can use the following command: curl &quot;http://localhost:5000/api/v1/namespaces/food_delivery/jobs/example.delivery_times_7_days/runs&quot; | \\ jq '.runs | map({&quot;id&quot;: .id, &quot;state&quot;: .state})' | less  I get the following output: [ { &quot;id&quot;: &quot;cb436906-1c66-4ce4-b7ac-ceebfd1babf8&quot;, &quot;state&quot;: &quot;FAILED&quot; }, { &quot;id&quot;: &quot;34bd4d60-82a6-4cac-ad76-815e6d95a93c&quot;, &quot;state&quot;: &quot;COMPLETED&quot; }, { &quot;id&quot;: &quot;352c67c3-c8d7-4b3a-b7da-8532aa9b8335&quot;, &quot;state&quot;: &quot;COMPLETED&quot; }, { &quot;id&quot;: &quot;0c62b1cc-2e43-44d0-9443-0a1d9768fece&quot;, &quot;state&quot;: &quot;COMPLETED&quot; }, { &quot;id&quot;: &quot;5900de19-12f7-4a6e-8118-8e0792d98f65&quot;, &quot;state&quot;: &quot;COMPLETED&quot; }, ... ]  This is an incomplete list of jobs, but it's obvious from this sampling that this is the first job failure in the recent execution history. What we want to see now is what changed between the last successful run and this one. We'll need to grab the id fields of each of the runs we want to compare. The run ids in the seed data are randomly generated, so they'll be different if you're following along. Grab the run ids with the following shell commands: FAILED_RUN_ID=$(curl &quot;http://localhost:5000/api/v1/namespaces/food_delivery/jobs/example.delivery_times_7_days/runs&quot; | jq -r '.runs[0].id') SUCCESSFUL_RUN_ID=$(curl &quot;http://localhost:5000/api/v1/namespaces/food_delivery/jobs/example.delivery_times_7_days/runs&quot; | jq -r '.runs[1].id')  To get a specific run, we call the /jobs/runs API. Since each Run ID is required to be unique, the API doesn't require a namespace or a job name. We can get the failed job run with curl &quot;http://localhost:5000/api/v1/jobs/runs/$FAILED_RUN_ID&quot; | jq | less  The output is the same as the latestRun field of the JobVersions API. Recall the output of that API includes these three important fields: the jobVersion, the inputVersionsand the outputVersions.  &quot;jobVersion&quot;: { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;example.delivery_times_7_days&quot;, &quot;version&quot;: &quot;e9eafa5b-e334-358d-a3b4-61c8d3de75f3&quot; }, &quot;inputVersions&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot;, &quot;version&quot;: &quot;a40ec54f-b8e1-35f7-b868-58b27383b5ff&quot; } ], &quot;outputVersions&quot;: [],  These fields give us what we need to trace the lineage of the specific job runs we want to compare. "},{"title":"Job Versions‚Äã","type":1,"pageTitle":"Exploring Lineage History via the Marquez API","url":"/blog/explore-lineage-api#job-versions","content":"The first thing to look at is the jobVersion. Nearly 100% of the time, a job failure can be traced to a code change. Let's compare the job version of the failed run with the job version of the successful one: diff &lt;(curl -s &quot;http://localhost:5000/api/v1/jobs/runs/$FAILED_RUN_ID&quot; | jq -r '.jobVersion.version') \\ &lt;(curl -s &quot;http://localhost:5000/api/v1/jobs/runs/$SUCCESSFUL_RUN_ID&quot; | jq -r '.jobVersion.version') 1c1 &lt; e9eafa5b-e334-358d-a3b4-61c8d3de75f3 --- &gt; 92d801c0-021e-3c3d-ba18-c9e8504b143d  Right away, we see there is a difference. A number of factors contribute to the job versioning logic in Marquez: The source code locationThe job contextThe list of input datasetsThe list of output datasets The version generation code is a deterministic function of these four inputs, so if any of them change, the version will change. Let's find out what changed between the two job versions. To do the diff, we ought to get rid of anything we expect to differ ahead of time: the version, the createdAtand updatedAt timestamps, and the latestRun. The version field is also nested within the job version's id field, so we'll omit that too. FAILED_JOB_VERSION=$(curl -s &quot;http://localhost:5000/api/v1/jobs/runs/$FAILED_RUN_ID&quot; | jq -r '.jobVersion.version') SUCCESSFUL_JOB_VERSION=$(curl -s &quot;http://localhost:5000/api/v1/jobs/runs/$SUCCESSFUL_RUN_ID&quot; | jq -r '.jobVersion.version') diff &lt;(curl -s &quot;http://localhost:5000/api/v1/namespaces/food_delivery/jobs/example.delivery_times_7_days/versions/$FAILED_JOB_VERSION&quot; | \\ jq 'del(.[&quot;id&quot;, &quot;version&quot;, &quot;createdAt&quot;, &quot;updatedAt&quot;, &quot;latestRun&quot;])') \\ &lt;(curl -s &quot;http://localhost:5000/api/v1/namespaces/food_delivery/jobs/example.delivery_times_7_days/versions/$SUCCESSFUL_JOB_VERSION&quot; | \\ jq 'del(.[&quot;id&quot;, &quot;version&quot;, &quot;createdAt&quot;, &quot;updatedAt&quot;, &quot;latestRun&quot;])') 14c14,23 &lt; &quot;outputs&quot;: [] --- &gt; &quot;outputs&quot;: [ &gt; { &gt; &quot;namespace&quot;: &quot;food_delivery&quot;, &gt; &quot;name&quot;: &quot;public.top_delivery_times&quot; &gt; }, &gt; { &gt; &quot;namespace&quot;: &quot;food_delivery&quot;, &gt; &quot;name&quot;: &quot;public.discounts&quot; &gt; } &gt; ]  Oh, interesting! The two job versions only differ because of the output datasets. This is an interesting point that should be addressed in the Marquez API- the version generation is constructed when the run completes, even if the job run failed. Sometimes this has no impact on the versioning, as the output datasets can be determined before the job run executes. But sometimes we see impacts like this where a job run failed before we had a chance to discover the output datasets. "},{"title":"Tracing Upstream Lineage‚Äã","type":1,"pageTitle":"Exploring Lineage History via the Marquez API","url":"/blog/explore-lineage-api#tracing-upstream-lineage","content":"So what gives? The job code didn't actually change! So what caused the failure? Here's where the lineage tracking becomes useful. Recall again, the run output gave us 3 interesting fields: the jobVersion, the inputVersions, and the outputVersions. We already know that the outputVersions is empty because the latest failed run didn't have a chance to determine the outputs. But we can take a look at the input datasets. "},{"title":"Dataset Versions‚Äã","type":1,"pageTitle":"Exploring Lineage History via the Marquez API","url":"/blog/explore-lineage-api#dataset-versions","content":"diff &lt;(curl -s &quot;http://localhost:5000/api/v1/jobs/runs/$FAILED_RUN_ID&quot; | jq -r '.inputVersions') \\ &lt;(curl -s &quot;http://localhost:5000/api/v1/jobs/runs/$SUCCESSFUL_RUN_ID&quot; | jq -r '.inputVersions') 5c5 &lt; &quot;version&quot;: &quot;a40ec54f-b8e1-35f7-b868-58b27383b5ff&quot; --- &gt; &quot;version&quot;: &quot;5e439f1f-1a44-3700-961f-60c79c75a1ec&quot;  Dataset versions work differently from job versions. They don't only change when the structure changes. Every time a job run modifies or writes to a dataset, the dataset version changes. Unless a job schedule is more frequent than its upstream job's schedule (e.g., an hourly job consuming a daily generated dataset), it is expected that each job run consumes a different version of a dataset. To find out if there is a significant difference, we have to compare the two versions with the dataset's versions API. We know there's only a single input dataset, so we'll keep this simple, but you could also write a loop to check multiple input datasets if needed. In this post, we omit the structure of the datasetVersion, but you can explore it yourself with the following: FAILED_DATASET_VERSION=$(curl -s &quot;http://localhost:5000/api/v1/jobs/runs/$FAILED_RUN_ID&quot; | jq -r '.inputVersions[0].version') curl -s &quot;http://localhost:5000/api/v1/namespaces/food_delivery/datasets/public.delivery_7_days/versions/$FAILED_DATASET_VERSION&quot; | jq | less  As with the job versions, we'll omit some of the data we expect to be different in order to produce a useful diff: FAILED_DATASET_VERSION=$(curl -s &quot;http://localhost:5000/api/v1/jobs/runs/$FAILED_RUN_ID&quot; | jq -r '.inputVersions[0].version') SUCCESSFUL_DATASET_VERSION=$(curl -s &quot;http://localhost:5000/api/v1/jobs/runs/$SUCCESSFUL_RUN_ID&quot; | jq -r '.inputVersions[0].version') diff &lt;(curl -s &quot;http://localhost:5000/api/v1/namespaces/food_delivery/datasets/public.delivery_7_days/versions/$FAILED_DATASET_VERSION&quot; | \\ jq 'del(.[&quot;id&quot;, &quot;version&quot;, &quot;createdAt&quot;, &quot;createdByRun&quot;])') \\ &lt;(curl -s &quot;http://localhost:5000/api/v1/namespaces/food_delivery/datasets/public.delivery_7_days/versions/$SUCCESSFUL_DATASET_VERSION&quot; | \\ jq 'del(.[&quot;id&quot;, &quot;version&quot;, &quot;createdAt&quot;, &quot;createdByRun&quot;])') 58c58 &lt; &quot;type&quot;: &quot;VARCHAR&quot;, --- &gt; &quot;type&quot;: &quot;INTEGER&quot;,  Hey! Somehow one of the fields was converted from a an INT to a VARCHAR! One of the helpful fields in the version API is the createdByRun, which is similar to the jobVersion's latestRun. It provides the job run that last altered the dataset, creating the new version. We can quickly compare the job versions of the runs that created these two dataset versions: diff &lt;(curl -s &quot;http://localhost:5000/api/v1/namespaces/food_delivery/datasets/public.delivery_7_days/versions/$FAILED_DATASET_VERSION&quot; | \\ jq '.createdByRun.jobVersion') \\ &lt;(curl -s &quot;http://localhost:5000/api/v1/namespaces/food_delivery/datasets/public.delivery_7_days/versions/$SUCCESSFUL_DATASET_VERSION&quot; | \\ jq '.createdByRun.jobVersion') 4c4 &lt; &quot;version&quot;: &quot;c222a72e-92cc-3bb6-b3b7-c174cbc76387&quot; --- &gt; &quot;version&quot;: &quot;76c375bf-58ac-3d19-b94f-424fe2784601&quot;  And we can do a quick comparison of the two job versions. Since the job name is different, we'll let jq generate the endpoints for us diff &lt;(curl -s $(curl -s &quot;http://localhost:5000/api/v1/namespaces/food_delivery/datasets/public.delivery_7_days/versions/$FAILED_DATASET_VERSION&quot; | \\ jq -r '.createdByRun.jobVersion | &quot;http://localhost:5000/api/v1/namespaces/&quot; + .namespace + &quot;/jobs/&quot; + .name + &quot;/versions/&quot; + .version') | \\ jq 'del(.[&quot;id&quot;, &quot;version&quot;, &quot;createdAt&quot;, &quot;updatedAt&quot;, &quot;latestRun&quot;])') \\ &lt;(curl -s $(curl -s &quot;http://localhost:5000/api/v1/namespaces/food_delivery/datasets/public.delivery_7_days/versions/$SUCCESSFUL_DATASET_VERSION&quot; | \\ jq -r '.createdByRun.jobVersion | &quot;http://localhost:5000/api/v1/namespaces/&quot; + .namespace + &quot;/jobs/&quot; + .name + &quot;/versions/&quot; + .version') | \\ jq 'del(.[&quot;id&quot;, &quot;version&quot;, &quot;createdAt&quot;, &quot;updatedAt&quot;, &quot;latestRun&quot;])') 4c4 &lt; &quot;location&quot;: &quot;https://github.com/example/jobs/blob/c87f2a40553cfa4ae7178083a068bf1d0c6ca3a8/etl_delivery_7_days.py&quot;, --- &gt; &quot;location&quot;: &quot;https://github.com/example/jobs/blob/4d0b5d374261fdaf60a1fc588dd8f0d124b0e87f/etl_delivery_7_days.py&quot;,  And there it is. Because nearly 100% of the time, a job failure can be traced to a code change. In this example, the job immediately upstream decided to change the output schema of its dataset. In reality, it's not always so straightforward. Sometimes the upstream job is just a passthrough- maybe it applies some filters to a subset of the columns and writes out whatever schema it's given. In that case, the job immediately upstream would have succeeded without a change in the job version. Or the code change in the upstream job could be innocuous. Maybe someone added a comment or fixed an unrelated bug. We might do some follow up and discover we have to continue our search upstream. But the Marquez API actually gives us that ability. Using the /lineage API, we can even explore the downsteam impact of changes. So if you owned the etl_delivery_7_days job and wanted to see what the impact of changing the varchar to an int was on running jobs, the following jq recursive script will let you walk the downstream jobs and show the state of the last run: # For readability, the jq filter is in a file broken into multiple lines cat recurse.jq .graph as $graph | .graph[] | select(.id == &quot;job:food_delivery:example.etl_delivery_7_days&quot;) | recurse(.outEdges[] | .destination as $nodeId | $graph[] | select(.id == $nodeId)) | select(.type == &quot;JOB&quot;) | {&quot;id&quot;: .id, &quot;state&quot;: .data.latestRun.state} curl -s &quot;http://localhost:5000/api/v1-beta/lineage?nodeId=job:food_delivery:example.etl_delivery_7_days&quot; | jq -f recurse.jq less { &quot;id&quot;: &quot;job:food_delivery:example.etl_delivery_7_days&quot;, &quot;state&quot;: &quot;COMPLETED&quot; } { &quot;id&quot;: &quot;job:food_delivery:example.delivery_times_7_days&quot;, &quot;state&quot;: &quot;FAILED&quot; }  In this post, we did everything manually with bash (because the shell is your most powerful tool when debugging a live outage you've never encountered before; and let's be honest- how many outages aren'tsomething you've never encountered before), but this could easily have been done in Java or Go or Python. The openapi specin the Marquez repo can be used to generate a client in whatever language you want to write your ops tool in. So build some tooling and help your next debugging session run a little more smoothly. But wait! What about the times when the job isn't failing, but the data is wrong! Ah, the data quality checks! This is where the extensibility of the OpenLineage model comes to our rescue with a field in the responses that we completely glossed over  &quot;facets&quot;: {}  But I think that's a topic for another post. "},{"title":"Pursuing Lineage from Airflow using Custom Extractors","type":0,"sectionRef":"#","url":"/blog/extractors","content":"","keywords":""},{"title":"Overview‚Äã","type":1,"pageTitle":"Pursuing Lineage from Airflow using Custom Extractors","url":"/blog/extractors#overview","content":"Airflow is built around operators, each having a different function and requiring a different approach to lineage. The OpenLineage Airflow integration detects which Airflow operators your DAG is using and extracts lineage data from them using extractors. The community has already authored a number of extractors to support Airflow‚Äôs Great Expectations, BigQuery, Python, Postgres, SQL and Bash operators (and more ‚Äì you can find all the extractors here.) Nevertheless, in the course of pursuing lineage, you may find yourself needing to write custom extractors. Some teams use custom extractors to automate repeatable work ‚Äì using the same code from PythonOperator across a project, for example. Another common use case is that a team needs to use an operator for which a pre-built extractor does not yet exist. Airflow has literally hundreds of operators. Built-in support for custom extractors makes OpenLineage a flexible, highly adaptable solution for pipelines that use Airflow for orchestration. "},{"title":"How it works‚Äã","type":1,"pageTitle":"Pursuing Lineage from Airflow using Custom Extractors","url":"/blog/extractors#how-it-works","content":"As we explain in the OpenLineage docs, custom extractors must be derived from the BaseExtractor class (import it from openlineage.airflow.extractors.base). Extractors have methods they can implement: extract, extract_on_complete and get_operator_classnames. Either extract or extract_on_complete is required. The get_operator_classnames method, also required, is for providing a list of operators from which your extractor can get lineage. For example: @classmethod def get_operator_classnames(cls) -&gt; List[str]: return ['PostgresOperator']  If the name of the operator matches one of the names on the list, the extractor will be instantiated ‚Äì using the operator passed to the extractor as a parameter and stored in the self.operator property ‚Äì and both the extract and extract_on_complete methods will be called. They both return information used by the OpenLineage integration to emit OpenLineage events. The difference is that extract is called before the operator's execute method to generate a START event, while extract_on_complete is called afterward to generate a COMPLETE event. The latter has access to any additional information that the operator leaves behind following execution. A good example of this is the SnowflakeOperator, which sets query_ids after execution. Both methods return a TaskMetadata structure: @attr.s class TaskMetadata: name: str = attr.ib() # deprecated inputs: List[Dataset] = attr.ib(factory=list) outputs: List[Dataset] = attr.ib(factory=list) run_facets: Dict[str, BaseFacet] = attr.ib(factory=dict) job_facets: Dict[str, BaseFacet] = attr.ib(factory=dict)  The inputs and outputs are lists of plain OpenLineage datasets. The run_facets and job_facets are dictionaries of optional JobFacets and RunFacets that accompany a job. For example, you might want to attach a SqlJobFacet if your operator is executing SQL. Note: in order for a custom extractor to work, it must be registered first, so the OpenLineage integration can import it. You can read about how to use environment variables to do this here. "},{"title":"Example: the RedshiftDataExtractor‚Äã","type":1,"pageTitle":"Pursuing Lineage from Airflow using Custom Extractors","url":"/blog/extractors#example-the-redshiftdataextractor","content":"In the RedshiftDataExtractor, the extract_on_complete method parses SQL, obtains task stats using the get_facets method of the RedshiftDataDatasetsProvider class, and returns a TaskMetadata instance. We can see usage of a SQL statement, and the connection is provided by an actual operator. def extract_on_complete(self, task_instance) -&gt; Optional[TaskMetadata]: log.debug(f&quot;extract_on_complete({task_instance})&quot;) job_facets = {&quot;sql&quot;: SqlJobFacet(self.operator.sql)} log.debug(f&quot;Sending SQL to parser: {self.operator.sql}&quot;) sql_meta: Optional[SqlMeta] = parse(self.operator.sql, self.default_schema) log.debug(f&quot;Got meta {sql_meta}&quot;) try: redshift_job_id = self._get_xcom_redshift_job_id(task_instance) if redshift_job_id is None: raise Exception( &quot;Xcom could not resolve Redshift job id. Job may have failed.&quot; ) except Exception as e: log.error(f&quot;Cannot retrieve job details from {e}&quot;, exc_info=True) return TaskMetadata( name=get_job_name(task=self.operator), run_facets={}, job_facets=job_facets, ) client = self.operator.hook.conn redshift_details = [ &quot;database&quot;, &quot;cluster_identifier&quot;, &quot;db_user&quot;, &quot;secret_arn&quot;, &quot;region&quot;, ] connection_details = { detail: getattr(self.operator, detail) for detail in redshift_details } stats = RedshiftDataDatasetsProvider( client=client, connection_details=connection_details ).get_facets( job_id=redshift_job_id, inputs=sql_meta.in_tables if sql_meta else [], outputs=sql_meta.out_tables if sql_meta else [], ) return TaskMetadata( name=get_job_name(task=self.operator), inputs=[ds.to_openlineage_dataset() for ds in stats.inputs], outputs=[ds.to_openlineage_dataset() for ds in stats.output], run_facets=stats.run_facets, job_facets={&quot;sql&quot;: SqlJobFacet(self.operator.sql)}, )  "},{"title":"Common issues‚Äã","type":1,"pageTitle":"Pursuing Lineage from Airflow using Custom Extractors","url":"/blog/extractors#common-issues","content":"There are two common issues associated with custom extractors. First, when the wrong path is provided to OPENLINEAGE_EXTRACTORS, the extractor isn‚Äôt imported and OpenLineage events aren‚Äôt emitted. The path needs to be exactly the same as the one you are using in your code. Also, make sure that the extractor code is available to import from Airflow‚Äôs Python interpreter. Second, imports from Airflow can be unnoticeably cyclical. This is due to the fact that OpenLineage code gets instantiated when the Airflow worker itself starts, in contrast to DAG code. OpenLineage extraction can fail as a result. To avoid this issue, make sure that all imports from Airflow are local ‚Äì in the extract or extract_on_complete methods. If you need imports for type checking, guard them behind typing.TYPE_CHECKING. "},{"title":"How to get started‚Äã","type":1,"pageTitle":"Pursuing Lineage from Airflow using Custom Extractors","url":"/blog/extractors#how-to-get-started","content":"Check out the existing extractors here. Read the docs about the Airflow integration, including tips on registering and debugging your custom extractor, here. "},{"title":"How to contribute‚Äã","type":1,"pageTitle":"Pursuing Lineage from Airflow using Custom Extractors","url":"/blog/extractors#how-to-contribute","content":"We welcome your contributions! One of our existing integrations might be a good place to start. OpenLineage‚Äôs growing list of partners includes Airflow, dbt, Dagster and Flink. Sounds fun? Check out our new contributor guide to get started. "},{"title":"OpenLineage Advances to Incubation Stage with the LFAI & Data","type":0,"sectionRef":"#","url":"/blog/incubation-stage-lfai","content":"","keywords":""},{"title":"What It Means‚Äã","type":1,"pageTitle":"OpenLineage Advances to Incubation Stage with the LFAI & Data","url":"/blog/incubation-stage-lfai#what-it-means","content":"Now that we‚Äôve cleared this hurdle, we have access to additional services from the foundation, including assistance with creative work, marketing and communication support, and event-planning assistance. Graduation from the program, which will earn us a voting seat on the TAC, is on the horizon. Stay tuned for updates on our progress with the foundation. "},{"title":"About the LFAI & Data‚Äã","type":1,"pageTitle":"OpenLineage Advances to Incubation Stage with the LFAI & Data","url":"/blog/incubation-stage-lfai#about-the-lfai--data","content":"LF AI &amp; Data is an umbrella foundation of the Linux Foundation that supports open source innovation in artificial intelligence (AI) and data. LF AI &amp; Data was created to support open source AI and data, and to create a sustainable open source AI and data ecosystem that makes it easy to create AI and data products and services using open source technologies. They foster collaboration under a neutral environment with an open governance in support of the harmonization and acceleration of open source technical projects. For more info about the foundation and other LFAI &amp; Data projects, visit their website. "},{"title":"What's Next‚Äã","type":1,"pageTitle":"OpenLineage Advances to Incubation Stage with the LFAI & Data","url":"/blog/incubation-stage-lfai#whats-next","content":"The next step for the project is Graduation, which we expect to happen early this summer. Requirements for Graduation include 1000 stars on GitHub and the OpenSSF Gold Badge. Watch this space for updates on our progress. "},{"title":"OpenLineage joins the LF AI & Data Foundation","type":0,"sectionRef":"#","url":"/blog/joining-lfai","content":"Becoming a LF AI &amp; Data project ensures that OpenLineage can never belong to a company, or even a group of developers; it belongs to us all. I am pleased to share that the OpenLineage project is joining the LF AI &amp; Data foundation as a Sandbox Project! This is an important step towards the development of an open ecosystem for lineage metadata collection. The LF AI &amp; Data Foundation provides a vendor-neutral governance structure that can help the project grow broad industry collaboration. Even more importantly, becoming a LF AI &amp; Data project ensures that OpenLineage can never belong to a company, or even a group of developers; it belongs to us all. The license can‚Äôt be changed to protect the business interests of a subset of the community. That‚Äôs important, because in order to succeed we need a whole lot of software projects - open source and proprietary - to adopt this standard and allow their users to begin collecting lineage metadata. In the full announcement, Ibrahim Haddad, Executive Director of LF AI &amp; Data, writes: ‚ÄúWe look forward to working with the OpenLineage project to grow the project‚Äôs footprint in the ecosystem, expand its community of adopters and contributors, and to foster the creation of collaboration opportunities with our members and other related projects.‚Äù","keywords":""},{"title":"At MANTA, OpenLineage Opens Doors to New Insights","type":0,"sectionRef":"#","url":"/blog/manta-integration","content":"Adopting OpenLineage as part of our portfolio allows MANTA to bring detailed run-time lineage to our customers. Here at MANTA, we are very excited to be working closely with OpenLineage and, more importantly, with the OpenLineage Community. As a leader in lineage analysis, we see first-hand the complexity required to achieve effective lineage, and the benefits of having an accepted standard for the sharing of operational lineage metadata. OpenLineage moves everything in the direction of enhanced interoperability, and helps to ensure that enterprises have maximum flexibility for current and future tool selection. Adopting OpenLineage as part of our portfolio allows MANTA to bring detailed run-time lineage to our customers, many of whom are enterprise organizations and need this level of granularity. This is especially important for new technologies such as Apache Airflow, whose integration with OpenLineage continues to evolve. Apache Airflow, as an example, is increasingly being utilized by our customers as part of their process orchestration portfolio; as such, these companies need lineage coverage for these operations. Having a recognized industry standard for lineage capture and reporting is an enabler for enhanced metadata management and governance. OpenLineage helps to ensure increased consistency in pipeline analysis, especially as more and more solutions appear in the Cloud, and in the general marketplace, for the transformation, enrichment, and overall movement of information through new and future dataflows. Vendors like MANTA will continue to offer creative and purposeful solutions that answer key questions and meet the end-to-end requirements of the business. For selected technologies, OpenLineage enables us to do this faster and simpler. Eighteen months ago, we started our investigation into OpenLineage. After working with various customers earlier this year, we decided to double down on our investment and get more involved with the OpenLineage Community. Throughout our journey, we‚Äôve found this growing community to be welcoming, helpful, and collaborative. MANTA is pleased to contribute however we can to this important open source project. Are you ready to join? For more information about MANTA's data lineage solution, visit our website. To learn more about contributing to OpenLineage, check out the project's new contributor guide.","keywords":""},{"title":"OpenLineage Support in Egeria","type":0,"sectionRef":"#","url":"/blog/openlineage-egeria","content":"","keywords":""},{"title":"OpenLineage Support in Egeria‚Äã","type":1,"pageTitle":"OpenLineage Support in Egeria","url":"/blog/openlineage-egeria#openlineage-support-in-egeria","content":"Egeria is a sister open source project to OpenLineage in the LF AI and Data Foundation. Egeria provides Open Metadata and Governance standard types and integration technology to exchange metadata between different technologies. It stitches together different standards to create a complete landscape of metadata about an organization‚Äôs digital operations. OpenLineage is very welcome to the Egeria team since it defines a standard for dynamic lineage capture. This means Egeria can capture open lineage events to detect new assets and activity around them, link this new knowledge into the existing metadata and distribute it to the open metadata ecosystem. Egeria also executes governance processes for maintaining both metadata and the data sources it describes. Since it is running processes, it also makes sense that Egeria produces open lineage for its processes. The diagram below is a big animal picture showing the different features relating to open lineage that Egeria offers. With Egeria‚Äôs plug-and-play architecture you can pick and choose which pieces you need.  The numbers on the diagram refer to the notes below. Egeria can capture open lineage events directly through HTTP or via the proxy backend.OpenLineage metadata is correlated and matched to existing metadata captured through a variety of mechanisms from direct metadata extraction from the hosting data platforms, to updates through dev ops pipelines to metadata discovery analytic tools.Egeria can publish OpenLineage events. These include the OpenLineage events it received (potentially augmented with additional facets), or events generated from its own governance processes. Published OpenLineage events can go to Egeria‚Äôs OpenLineage file-based log store for later processing or to any application that supports the OpenLineage API (Marquez, for example -- another project from LF AI and Data).The metadata extracted from OpenLineage events can be distributed to the open metadata ecosystem using standard approaches. This means it can be picked up by connected data science, governance and lineage tools.Governance processes linked to the open metadata ecosystem can use OpenLineage events to validate that their originating processes are operating as frequently and as accurately as expected. More information on Egeria‚Äôs open lineage support can be found here. The Egeria community would like to thank the OpenLineage community for their great support while we created this integration. We look forward to continuing to work together as both our projects mature. "},{"title":"How Northwestern Mutual Simplified Data Observability with OpenLineage & Marquez","type":0,"sectionRef":"#","url":"/blog/openlineage-at-northwestern-mutual","content":"","keywords":""},{"title":"Embrace cloud managed services‚Äã","type":1,"pageTitle":"How Northwestern Mutual Simplified Data Observability with OpenLineage & Marquez","url":"/blog/openlineage-at-northwestern-mutual#embrace-cloud-managed-services","content":"Many of the foundational needs of an Enterprise Data Platform can be accomplished using a cloud-first mindset. While we may not all agree which cloud provider is best, we can all agree that the level of scale and sophistication accomplished around things like storage, compute, and redundancy are going to be MUCH greater when relying on a cloud provider than when rolling your own solution. "},{"title":"We are software engineers‚Äã","type":1,"pageTitle":"How Northwestern Mutual Simplified Data Observability with OpenLineage & Marquez","url":"/blog/openlineage-at-northwestern-mutual#we-are-software-engineers","content":"The Data Mesh evolution has reminded the industry that centralized data teams do not scale or empower anybody. With this principle in mind, our platform teams embraced full automation from the beginning and designed for self-service workflows. We do not want to become the bottleneck to insights; rather, we want to enable data owners to manage and share their datasets throughout the company. We want to empower data engineers with transformation and machine learning capabilities, so that they can author pipelines and deliver insights. "},{"title":"Aim for simplicity through consistency‚Äã","type":1,"pageTitle":"How Northwestern Mutual Simplified Data Observability with OpenLineage & Marquez","url":"/blog/openlineage-at-northwestern-mutual#aim-for-simplicity-through-consistency","content":"Traditionally, data platforms have gathered and constructed technical metadata based on events of the past. For example, there are many crawlers that will examine various database systems and build a catalog to make those datasets ‚Äúdiscoverable.‚Äù Logs from various jobs can be parsed in extremely specific ways to identify datasets consumed and produced by a given pipeline to infer data lineage. We viewed these traditional methods as a massive impediment to activating DataOps, due to differing technology solutions and the historical-based approach of their designs. Our platform aimed to achieve dynamic decisions based on what is happening as it is happening. We also recognize and appreciate the complexity of this portion of the platform and did not find it wise to build from the ground up. Especially with the industry momentum towards real-time data observability, why add another custom solution to the stack? With such an evolving technical landscape, it was important for us to avoid vendor lock to allow us flexibility in future decisions. NM hearts OL When we first learned of the OpenLineage specification, we were very intrigued and hopeful. An open specification focused on observing real-time events AND unifying tools and frameworks?!? Fast forward nine months, and we cannot believe how much capability we have developed around data observability in such a brief time. Let me back up a little... Marquez is a metadata management framework that implements the OpenLineage specification. It transforms your data runtime events into a searchable catalog of technical metadata. It was a perfect fit to the skills of our Platform Data Engineers - it is written in Java, runs in Kubernetes, and integrates well with our backend services via web-based APIs. We were able to quickly deploy this framework into our own environment, which provided us with several immediate wins. "},{"title":"Flexible framework‚Äã","type":1,"pageTitle":"How Northwestern Mutual Simplified Data Observability with OpenLineage & Marquez","url":"/blog/openlineage-at-northwestern-mutual#flexible-framework","content":"Since it is aligned with the OpenLineage framework, Marquez can process messages from ANY data producer that is publishing compliant events. The Marquez and OpenLineage communities have been doing an excellent job maturing the integration library, which allows you to tackle this challenge at the infrastructure level. This is the ultimate easy button approach and our own ideal state; configure an environment on behalf of your user base and sit back while it automatically detects and logs the activity within! In the cases when an integration either does not exist or you need to address a more custom workflow, you can construct and emit your own OpenLineage event messages. Marquez will still be able to process and store custom OpenLineage events, provided they meet the requirements of the open standard. For example, our teams have been able to programmatically construct OpenLineage messages within code that pulls data from various on-premises database servers and publishes it into our Data Platform. Using the OpenLineage specification, we extract the actual table schema from the source system as part of the Dataset entity and log the executing SQL query as part of the Job entity. This code was simplistic and allowed us to meet our immediate needs around observing data movement and recording those event details. "},{"title":"Alignment with enterprise‚Äã","type":1,"pageTitle":"How Northwestern Mutual Simplified Data Observability with OpenLineage & Marquez","url":"/blog/openlineage-at-northwestern-mutual#alignment-with-enterprise","content":"Marquez already supported Kubernetes when we got involved, which provided us with many different deployment options. Our first contributions to the project were made to mature the Helm chart and to enhance security around the base images and Kubernetes secrets usage. These changes allowed us to fully automate our deployments using GitOps and incorporate internal security measures involving container vulnerability management. The flexibility offered by the Marquez deployment architecture and our ability to customize its details allowed us to activate new production use cases in about a month. We were happy with this timeline, given the series of security checkpoints that were validated and the wealth of functionality we had just unlocked. "},{"title":"Collaborative working group‚Äã","type":1,"pageTitle":"How Northwestern Mutual Simplified Data Observability with OpenLineage & Marquez","url":"/blog/openlineage-at-northwestern-mutual#collaborative-working-group","content":"Both the Marquez and OpenLineage communities have been extremely welcoming, and that has been a huge factor in our success at Northwestern Mutual. Our feedback and ideas have been encouraged and heard, which is evidenced by evolving project roadmaps and accepted developer contributions. We have learned quite a bit from the community members and feel fortunate to be a part of this group. Monthly community meetings are informative yet have an amazingly informal feel to them. Where are we headed? The Unified Data Platform at Northwestern Mutual relies on the OpenLineage standard to formulate technical metadata within our various platform services. Publishing these events into Marquez has provided us with an effortless way to understand our running jobs. We can easily trace a downstream dataset to the job that produced it, as well as examine individual runs of that job or any preceding ones. Gaining the ability to observe lineage throughout our platform has been huge, and we are just getting started. Our teams are working to apply standard OpenLineage integrations into our environment and introduce data quality facets into our events. We have also been establishing operational workflows using job run information, to allow our DataOps team to monitor durations and measure against SLAs. "},{"title":"Microsoft Purview Accelerates Lineage Extraction from Azure Databricks","type":0,"sectionRef":"#","url":"/blog/openlineage-microsoft-purview","content":"","keywords":""},{"title":"Summary‚Äã","type":1,"pageTitle":"Microsoft Purview Accelerates Lineage Extraction from Azure Databricks","url":"/blog/openlineage-microsoft-purview#summary","content":"Microsoft Purview provides a comprehensive platform to populate native and custom data lineage metadata from on-prem, OSS, SaaS, and multi-cloud data systems. The Azure Databricks to Microsoft Purview Solution Accelerator takes advantage of the robust Spark integration inside OpenLineage and translates OpenLineage events into Microsoft Purview native assets supporting: Azure Data Lake Gen 2Azure Blob StorageAzure SQLAzure Synapse SQL Pools Customers of Azure Databricks and Microsoft Purview can try the solution today by following the demo instructions or connector only instructions. "},{"title":"What is Microsoft Purview?‚Äã","type":1,"pageTitle":"Microsoft Purview Accelerates Lineage Extraction from Azure Databricks","url":"/blog/openlineage-microsoft-purview#what-is-microsoft-purview","content":"Microsoft Purview provides an ambient data governance solution that helps you unify and manage your data wherever it exists ‚Äì on-premises, in the cloud, or on a software-as-a-service (SaaS) platform. With Microsoft Purview, you can: create a holistic, up-to-date map of your data landscape with automated data discovery, sensitive data classification, and end-to-end data lineage.enable data curators to manage and secure your data estate.empower data consumers to find valuable, trustworthy data. Figure 1. Microsoft Purview is an ambient data governance platform for an enterprise. Microsoft Purview automates data discovery by providing data scanning and classification as a service for assets across your data estate. Microsoft Purview integrates metadata and descriptions of discovered data assets into a holistic map of your data ecosystem. Layered on this map are purpose-built apps that create environments for data discovery, policy management, and insights into your data landscape. "},{"title":"Data Lineage in Microsoft Purview‚Äã","type":1,"pageTitle":"Microsoft Purview Accelerates Lineage Extraction from Azure Databricks","url":"/blog/openlineage-microsoft-purview#data-lineage-in-microsoft-purview","content":"Organizations need data to conduct business, and they need trustworthy data to perform analysis and make key decisions. Data lineage and provenance provide insights into data pedigree, which relates to operational information, runtime analysis, historical lineage, and ownership information. Users rely on pedigree when taking insights from data. Critical scenarios involving root cause analysis, impact analysis, quality control, compliance, and audit tracing are served by data lineage and provenance. Data Lineage in Microsoft Purview is a core platform capability that populates the Microsoft Purview Data Map with data movement and transformations across systems. With the backing of Apache Atlas 2.2, lineage is captured as it flows in the enterprise and stitched without gaps irrespective of its source. Data lineage in Microsoft Purview enables data analysts and data stewards to conduct root cause analysis, troubleshoot, and perform impact analysis of data moving upstream and downstream in data estates. With a combined platform and interactive lineage visualization tool, data investigations related to quality, trust, and compliance can be self-served in a few clicks rather than requested from a third party. Microsoft Purview has native data lineage support for 20+ sources, many of which are integrated at engine runtimes. For example, data lineage is pushed from Azure Data Factory when pipelines are run. This deep integration allows Microsoft Purview to capture operational metadata such as job start/end times, the number of rows impacted, job run status and more. In addition to native support, the open APIs can be used to integrate with enterprise systems to support custom lineage.  Figure 2. Native data lineage visualization in Microsoft Purview. "},{"title":"OpenLineage + Microsoft‚Äã","type":1,"pageTitle":"Microsoft Purview Accelerates Lineage Extraction from Azure Databricks","url":"/blog/openlineage-microsoft-purview#openlineage--microsoft","content":"This integration came about because Microsoft Purview sought a lineage solution for Azure Databricks users, ideally one that would support all Azure data repository types, from Azure Blob Storage to Azure SQL. The team that took on this challenge was the Early Access Engineering (EAE) team, a group of data experts at Microsoft who forge competitive differentiation and value by using groundbreaking technology and features before they become available to the general enterprise landscape. A History of Contributions to OpenLineage‚Äã The EAE team at Microsoft has a long history of contributions to open source projects in general and to OpenLineage in particular. In December of 2021, Will Johnson contributed a PR to OpenLineage to add support for arbitrary parameters in the OpenLineage URL. This change supported key-based authentication via URL and eased the process of sending metadata from OpenLineage to repositories other than Marquez, OpenLineage‚Äôs sister project. This in turn supported additional integrations and collaboration and has helped to increase adoption of the OpenLineage standard. Over the course of seven months, the Microsoft team contributed eight pull requests to enable: better support for the Azure Blob File System (Azure Data Lake Gen 2).use of an Azure Function as the lineage endpoint.lineage extraction for Azure Synapse as a data source.extraction of Databricks environment properties such as notebook paths and job ids. Contributing open source integrations to OpenLineage benefits not only Microsoft Purview but also the data landscape as a whole. Collaborations like this one help increase adoption of the OpenLineage standard across the industry, which gets us closer to the single standard we need for consistently powerful and reliable lineage across the wide diversity of tooling in today‚Äôs data pipelines. At Microsoft, this kind of work is not unique to the EAE team. Across the company, cross-functional, community-driven teams foster innovation through open source collaboration. "},{"title":"Why Contribute to OpenLineage?‚Äã","type":1,"pageTitle":"Microsoft Purview Accelerates Lineage Extraction from Azure Databricks","url":"/blog/openlineage-microsoft-purview#why-contribute-to-openlineage","content":"Most enterprise data environments are convoluted, with data systems spread across on-prem, multi-cloud, SaaS, and open-source platforms. The data moves between a variety of storage, processing, analytical, and SaaS data systems. Azure Databricks is one such data system in an enterprise with a lakehouse platform in the cloud that combines data warehouses and data lakes to offer an open and unified platform for data and AI. Microsoft Purview customers have long asked for the ability to populate and govern Azure Databricks assets in the Microsoft Purview DataMap. With OpenLineage, we are bringing runtime Data Lineage capture from Azure Databricks Spark workloads to Microsoft Purview. In addition, by contributing to OpenLineage, Microsoft can offer users of OpenLineage on other platforms the ability to represent metadata models of Microsoft data sources accurately in their lineage graphs. For example, users of Spark on any other platform can represent the metadata models of Microsoft data sources more accurately. Lastly, OpenLineage benefits from Microsoft‚Äôs contributions as they will add hundreds if not thousands of new users to the OpenLineage standard. This will spur more contributions by the OpenLineage community as more users request that new implementations and features be added to the specification. "},{"title":"About the Solution‚Äã","type":1,"pageTitle":"Microsoft Purview Accelerates Lineage Extraction from Azure Databricks","url":"/blog/openlineage-microsoft-purview#about-the-solution","content":"Figure 3. The flow of metadata from Azure Databricks to Microsoft Purview using OpenLineage. An Azure administrator deploys an Azure Function (serverless C# application) and an Event Hub (to store OpenLineage events) by running a deployment script.An administrator configures a Databricks cluster as per the OpenLineage install instructions along with the Azure Function key and OpenLineage host pointing to the Azure Function.The OpenLineage Spark jar extracts the necessary inputs and outputs and emits them to the Azure Function.The Azure Functions transform the OpenLineage payload and push lineage to Microsoft Purview through the Apache Atlas REST APIs.Databricks Lineage is then visible inside Microsoft Purview! "},{"title":"Getting Started with Microsoft Purview‚Äã","type":1,"pageTitle":"Microsoft Purview Accelerates Lineage Extraction from Azure Databricks","url":"/blog/openlineage-microsoft-purview#getting-started-with-microsoft-purview","content":"Quickly and easily create a Microsoft Purview account to explore the new features. Try out the Azure Databricks to Microsoft Purview Solution Accelerator. Learn how to deploy the solution. "},{"title":"What the Future Holds‚Äã","type":1,"pageTitle":"Microsoft Purview Accelerates Lineage Extraction from Azure Databricks","url":"/blog/openlineage-microsoft-purview#what-the-future-holds","content":"Microsoft plans to continue contributing to OpenLineage to ensure that users can extract lineage from additional Azure data sources such as Azure Data Explorer (Kusto), Azure Cosmos DB, and Azure Event Hubs, and that OpenLineage continues to perform well on Azure Databricks. In addition, Microsoft plans to keep up-to-date with advancements made by the OpenLineage community, such as the exciting recent contribution of column-level lineage to the project. "},{"title":"Acknowledging the Contributors‚Äã","type":1,"pageTitle":"Microsoft Purview Accelerates Lineage Extraction from Azure Databricks","url":"/blog/openlineage-microsoft-purview#acknowledging-the-contributors","content":"The OpenLineage Spark integration is the product of hard work by teams inside and outside Microsoft. Contributors from the Microsoft Early Access Engineering team include: Mark Taylor, Principal Technical Specialist (@marktayl1)Will Johnson, Global Black Belt - Big Data, Analytics, and ML Specialist (@wjohnson)Rodrigo Monteiro, Global Black Belt - Big Data, Analytics (@rodrigomonteiro-gbb)Travis Hilbert, Technical Specialist (@travishilbert)Matt Savarino, Sr. Technical Specialist (@mattsavarino) Outside Microsoft, contributors to the OpenLineage Spark integration are based at a range of internationally distributed companies and organizations. Additional contributors to the integration include: Michael Collado, Staff Software Engineer, Astronomer (@collado-mike)Oleksandr Dvornik, Senior Java Developer, UBS (@OleksandrDvornik)Pawe≈Ç Leszczy≈Ñski, Data Engineer, GetInData (@pawel-big-lebowski)Tomasz Nazarewicz, Data Engineer, GetInData (@tnazarew)Maciej Obuchowski, Software Engineer, GetInData (@mobuchowski) Kengo Seki, PMC Member and Committer, Apache Software Foundation (@sekikn)Ziyoiddin Yusupov, Senior Software Engineer, UBS (@mr-yusupov) Try the Azure Databricks to Microsoft Purview Solution Accelerator today! "},{"title":"Data Lineage with Snowflake","type":0,"sectionRef":"#","url":"/blog/openlineage-snowflake","content":"","keywords":""},{"title":"Introduction‚Äã","type":1,"pageTitle":"Data Lineage with Snowflake","url":"/blog/openlineage-snowflake#introduction","content":"We are excited to reveal a new way to gather lineage metadata directly from Snowflake: the OpenLineage Adapter. This integration offers Snowflake‚Äôs enterprise users a powerful tool for analyzing and diagnosing issues with their data pipelines. This new integration will add new diagnostic capability to one of the world‚Äôs largest data platforms. Snowflake‚Äôs Data Cloud currently empowers more than 5,900 companies, including 241 of the Fortune 500 as of January 2022, to unite siloed data, securely share data, and execute diverse analytic workloads across their organizations. Legacy platforms struggled to provide a single, secure, and universally accessible platform for organizations to warehouse and analyze their data, but Snowflake‚Äôs Data Cloud provides a global ecosystem where customers, providers, and partners can finally break down data silos and derive value from rapidly growing data sets in secure, compliant, and governed ways. "},{"title":"Background‚Äã","type":1,"pageTitle":"Data Lineage with Snowflake","url":"/blog/openlineage-snowflake#background","content":"An open source LF AI &amp; Data Foundation sandbox project, OpenLineage provides an open standard for metadata and lineage collection that instruments jobs as they are running. OpenLineage not only automates the process of generating lineage and metadata about datasets, jobs, and runs in a data flow, but also does this in real time behind the scenes. With OpenLineage‚Äôs open standard and extensible backend, users can easily identify the root causes of slow or failing jobs and issues with data quality in their ecosystems without parsing queries. The magic of OpenLineage is its standard API for capturing lineage events. Any number of tools ‚Äì from schedulers to SQL engines ‚Äì can send metadata from this endpoint to a compatible tool such as Marquez for visualization and further analysis of a pipeline. Historically, the process of producing lineage and collecting metadata has been laborious and error-prone. Extracting data from query logs via parsing, for example, required one to reimplement database parsing logic, which added complexity and introduced opportunities for user error. In addition, the lineage collected was incomplete. One could learn about the view that was queried but not about the underlying tables in the pipeline, much less about the upstream and downstream dependencies of the datasets. OpenLineage, by contrast, exploits what the database already knows and does to maintain an up-to-date, end-to-end graph of a pipeline ‚Äì and makes the graph available via an API. OpenLineage and Snowflake play nicely because the latter is unusual among cloud data platforms for offering lineage information out of the box in a view (ACCESS_HISTORY). The integration of OpenLineage builds on this foundation to offer automated generation of lineage and metadata. The value proposition of Snowflake + OpenLineage lies in the combination of an open standard tool, which supports multiple data systems to provide lineage in a single format, to Snowflake‚Äôs existing production of lineage information on an enterprise scale. The integration gives customers the ability to consume enterprise-wide table lineage and process lineage together in a consolidated OpenLineage format. "},{"title":"Approach‚Äã","type":1,"pageTitle":"Data Lineage with Snowflake","url":"/blog/openlineage-snowflake#approach","content":"The process of integrating OpenLineage benefited from an existing query logging tool already available to Snowflake enterprise customers: the ACCESS_HISTORY view. As its name suggests, this feature, designed initially for governance use cases, offers users a detailed view of read operations conducted on Snowflake objects (e.g., tables, views, and columns) on an on-demand basis in response to SQL queries. (Write operations are viewable as a preview feature.) As developed primarily by former Snowflake intern Aly Hirani with support from Datakin Senior Engineer Minkyu Park, the OpenLineage integration makes Access History the basis of automated production of lineage and metadata. But rather than produce a view for querying, OpenLineage produces a holistic lineage graph. To create the graph, the integration takes the data used to populate the Access History view and sends it to the OpenLineage backend as a standard OpenLineage event. Events in OpenLineage are JSON objects that employ a consistent naming strategy for database entities and enrich those entities with facets: { &quot;eventType&quot;: &quot;COMPLETE&quot;, &quot;eventTime&quot;: &quot;2020-12-28T20:52:00.001+10:00&quot;, &quot;run&quot;: { &quot;runId&quot;: &quot;d46e465b-d358-4d32-83d4-df660ff614dd&quot; }, &quot;job&quot;: { &quot;namespace&quot;: &quot;my-namespace&quot;, &quot;name&quot;: &quot;my-job&quot; }, &quot;outputs&quot;: [{ &quot;namespace&quot;: &quot;my-namespace&quot;, &quot;name&quot;: &quot;my-output&quot;, &quot;facets&quot;: { &quot;schema&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client&quot;, &quot;_schemaURL&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/spec/OpenLineage.json#/definitions/SchemaDatasetFacet&quot;, &quot;fields&quot;: [ { &quot;name&quot;: &quot;a&quot;, &quot;type&quot;: &quot;VARCHAR&quot;}, { &quot;name&quot;: &quot;b&quot;, &quot;type&quot;: &quot;VARCHAR&quot;} ] } } }], &quot;producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client&quot; }  "},{"title":"A DAG-based Solution‚Äã","type":1,"pageTitle":"Data Lineage with Snowflake","url":"/blog/openlineage-snowflake#a-dag-based-solution","content":"Automating lineage production from the Access History view required a two-DAG solution. Minkyu had initially planned to use one DAG to scan the view and produce the lineage graph, but the timing of the logs used for the view precluded the production of lineage data with a single DAG. The solution Minkyu found was a separate DAG with a schedule for scanning the Access History view on a regular interval. def send_ol_events(): client = OpenLineageClient.from_environment() with connect(user=SNOWFLAKE_USER, password=SNOWFLAKE_PASSWORD, account=SNOWFLAKE_ACCOUNT, database='OPENLINEAGE', schema='PUBLIC') as conn: with conn.cursor() as cursor: ol_view = 'OPENLINEAGE_ACCESS_HISTORY' ol_event_time_tag = 'OL_LATEST_EVENT_TIME' var_query = f''' set current_organization='{SNOWFLAKE_ACCOUNT}'; ''' cursor.execute(var_query) ol_query = f''' SELECT * FROM {ol_view} WHERE EVENT:eventTime &gt; system$get_tag('{ol_event_time_tag}', '{ol_view}', 'table') ORDER BY EVENT:eventTime ASC; ''' cursor.execute(ol_query) ol_events = [json.loads(ol_event[0]) for ol_event in cursor.fetchall()] for ol_event in ol_events: client.emit(ol_event) if len(ol_events) &gt; 0: latest_event_time = ol_events[-1]['eventTime'] cursor.execute(f''' ALTER VIEW {ol_view} SET TAG {ol_event_time_tag} = '{latest_event_time}'; ''') default_args = { 'owner': 'openlineage', 'depends_on_past': False, 'start_date': days_ago(1), 'email_on_failure': False, 'email_on_retry': False, 'email': ['demo@openlineage.io'], 'snowflake_conn_id': 'openlineage_snowflake' } with DAG('etl_openlineage', schedule_interval='@hourly', catchup=False, default_args=default_args, description='Send OL events every minutes', tags=[&quot;extract&quot;]) as dag: t1 = PythonOperator(task_id='ol_event', python_callable=send_ol_events)  "},{"title":"Getting Started with an Example‚Äã","type":1,"pageTitle":"Data Lineage with Snowflake","url":"/blog/openlineage-snowflake#getting-started-with-an-example","content":"This example uses Airflow to run a collection of Snowflake queries for a fictional food delivery service. Lineage data for these queries will be recorded within Snowflake ACCESS_HISTORY and, using the OpenLineage Access History View, emitted to an OpenLineage backend. This is done using a series of DAGs in dags/etl that each use SnowflakeOperator to run queries, along with a DAG in dags/lineage that uses PythonOperator to send generated OpenLineage events to the configured backend. "},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"Data Lineage with Snowflake","url":"/blog/openlineage-snowflake#prerequisites","content":"Installing Marquez‚Äã First, checkout the Marquez repository: % git clone https://github.com/MarquezProject/marquez.git % cd marquez  Then, run Marquez in detached mode: % docker/up.sh -d %  Preparing Snowflake‚Äã First, check out the OpenLineage Access History View repository: % git clone https://github.com/Snowflake-Labs/OpenLineage-AccessHistory-Setup.git % cd OpenLineage-AccessHistory-Setup  The OPENLINEAGE database and FOOD_DELIVERY schema in Snowflake need to be created. This can be done using the SnowSQL command-line tool, or by pasting the queries into a new Snowflake Worksheet. This example uses SnowSQL. % snowsql -u &lt;snowflake-user&gt; -a &lt;snowflake-account&gt; SnowSQL&gt; CREATE DATABASE OPENLINEAGE; SnowSQL&gt; CREATE SCHEMA OPENLINEAGE.FOOD_DELIVERY;  The view defined in open_lineage_access_history.sql also needs to be created. This view represents the entries in ACCESS_HISTORY as specially-constructed JSON objects containing RunEvents that can be emitted to an OpenLineage backend. To create it, use SnowSQL to set the current_organization session variable and execute the SQL file. SnowSQL&gt; SET current_organization='&lt;snowflake-organization&gt;'; SnowSQL&gt; USE SCHEMA OPENLINEAGE.PUBLIC; SnowSQL&gt; !source open_lineage_access_history.sql  Finally, our lineage extraction DAG relies upon a tag on the view to keep track of which lineage events have been processed. This tag needs to be initialized: SnowSQL&gt; CREATE TAG OL_LATEST_EVENT_TIME; SnowSQL&gt; ALTER VIEW OPENLINEAGE.PUBLIC.OPENLINEAGE_ACCESS_HISTORY SET TAG OL_LATEST_EVENT_TIME = '1970-01-01T00:00:00.000'; SnowSQL&gt; !quit %  "},{"title":"Preparing the Environment‚Äã","type":1,"pageTitle":"Data Lineage with Snowflake","url":"/blog/openlineage-snowflake#preparing-the-environment","content":"The following environment variables need to be set in order for the query DAGs to connect to Snowflake, and so that the extraction DAG can send lineage events to your OpenLineage backend: SNOWFLAKE_USERSNOWFLAKE_PASSWORDSNOWFLAKE_ACCOUNTOPENLINEAGE_URLAIRFLOW_CONN_OPENLINEAGE_SNOWFLAKE To do this, copy the .env-example file to .env, and edit it to provide the appropriate values for your environment. The variables in this file will be set for each service in the Airflow deployment. % cd examples/airflow % cp .env-example .env % vi .env  "},{"title":"Preparing Airflow‚Äã","type":1,"pageTitle":"Data Lineage with Snowflake","url":"/blog/openlineage-snowflake#preparing-airflow","content":"Once the environment is prepared, initialize Airflow with docker-compose: % docker-compose up airflow-init  This will take several minutes. When it has finished, bring up the Airflow services: % docker-compose up  This will also take several minutes. Eventually, the webserver will be up at http://localhost:8080. Log in using the default credentials (airflow/airflow) and navigate to the DAGs page. When you see 12 DAGs in the list, you can be confident that Airflow has completed its initialization of the example. "},{"title":"Running the Example‚Äã","type":1,"pageTitle":"Data Lineage with Snowflake","url":"/blog/openlineage-snowflake#running-the-example","content":"Each of the DAGs is paused by default. Enable each one, skipping the etl_openlineage DAG for now. They may not all run successfully on the first try, since they have interdependencies that this example leaves unmanaged.  After each DAG has completed at least one successful run, enable etl_openlineage. Wait for it to complete its run. "},{"title":"Result‚Äã","type":1,"pageTitle":"Data Lineage with Snowflake","url":"/blog/openlineage-snowflake#result","content":"Navigate to your Marquez deployment and view the resulting lineage graph:  "},{"title":"Potential Improvements‚Äã","type":1,"pageTitle":"Data Lineage with Snowflake","url":"/blog/openlineage-snowflake#potential-improvements","content":"This new integration paves the way for an exciting set of potential future capabilities. These include support for Object_Dependencies and the addition of Granular Lineage (column-level lineage). We are interested in feedback from users, which will help the team at Snowflake and the members of the OpenLineage community prioritize future work. "},{"title":"Conclusion‚Äã","type":1,"pageTitle":"Data Lineage with Snowflake","url":"/blog/openlineage-snowflake#conclusion","content":"Snowflake‚Äôs integration of the OpenLineage standard promises to dramatically improve enterprise users‚Äô ability to diagnose issues with quality and performance in their pipelines. This project is cause for optimism about future collaboration with OpenLineage. The fit between Snowflake‚Äôs enterprise product and OpenLineage is already fairly seamless. Further collaboration would likely yield additional features and, by extension, more value for Snowflake‚Äôs customers. Also, the fact that OpenLineage is an open standard offers opportunities for fruitful integrations with other partners. Supporters of OpenLineage already include Spark, Airflow, and dbt, and the list is growing. For more information or to contribute to OpenLineage, reach out on twitter or Slack, and check out the repositories on Github. "},{"title":"Tracing Data Lineage with OpenLineage and Apache Spark","type":0,"sectionRef":"#","url":"/blog/openlineage-spark","content":"","keywords":""},{"title":"The Age of Data Democratization‚Äã","type":1,"pageTitle":"Tracing Data Lineage with OpenLineage and Apache Spark","url":"/blog/openlineage-spark#the-age-of-data-democratization","content":"In 2015, Apache Spark seemed to be taking over the world. Many of us had spent the prior few years moving our large datasets out of the Data Warehouse into &quot;Data Lakes&quot;- repositories of structured and unstructured data in distributed file systems or object stores, like HDFS or S3. This enabled us to build analytic systems that could handle traditional, table-structured data alongside flexible, unstructured JSON blobs, giving us access to more data and allowing us to move much faster than we‚Äôd previously been able to. The problem was that taking the data out of Data Warehouses meant that the people who really needed access to the data, analysts and statisticians, could no longer use the tools they were comfortable with in order to read that data. Where previously, SQL and Python were all that was needed to start exploring and analyzing a dataset, now people needed to write Java or use specialized scripting languages, like Pig, to get at the data. Systems that did support SQL, such as Hive, were unbearably slow for any but the most basic operations. In many places, the statisticians were dependent on software engineers to build custom tools for access, meaning the bottleneck had moved from the systems that needed to store and process the data to the humans who were supposed to tell us what systems to build. Then along came Apache Spark, which gave back to analysts the ability to use their beloved Python (and eventually SQL) tools to process raw data in object stores without the dependency on software engineers. While others were attracted to its ability to perform multiple operations on data without the I/O overhead of alternatives, like Pig or Hive, data scientists were thrilled to start piping that data through their NumPy and Pandas scripts. "},{"title":"A Colossal Mess‚Äã","type":1,"pageTitle":"Tracing Data Lineage with OpenLineage and Apache Spark","url":"/blog/openlineage-spark#a-colossal-mess","content":"Of course, the natural consequence of this data democratization is that it becomes difficult to keep track of who is using the data and for what purpose. Hidden dependencies and Hyrum‚Äôs Law suddenly meant that changes to the data schema would inadvertently break downstream processes or that stale, deprecated datasets were still being consumed, and that corrupted datasets would leak into unknown processes making recovery difficult or even impossible.  The goal of OpenLineage is to reduce issues and speed up recovery by exposing those hidden dependencies and informing both producers and consumers of data about the state of that data and the potential blast radius of changes and software bugs. Naturally, support for Apache Spark seemed like a good idea and, while the Spark 2.4 branch has been supported for some time, the recent OpenLineage 0.3 release has explicit support for Spark 3.1. üéâ "},{"title":"Getting Started‚Äã","type":1,"pageTitle":"Tracing Data Lineage with OpenLineage and Apache Spark","url":"/blog/openlineage-spark#getting-started","content":"Our approach to integrating with Spark is not super novel nor is it complicated to integrate into your own system. Spark has had a SparkListener interface since before the 1.x days. If you're a heavy Spark user, it's likely you're already familiar with it and how it's used in Spark applications. OpenLineage integrates with Spark by implementing that interface and collecting information about jobs that are executed inside a Spark application. To activate the listener, add the following properties to your Spark configuration: spark.jars.packages io.openlineage:openlineage-spark:0.3.+ spark.extraListeners io.openlineage.spark.agent.OpenLineageSparkListener  This can be added to your cluster‚Äôs spark-defaults.conf file, in which case it will record lineage for every job executed on the cluster, or added to specific jobs on submission via the spark-submit command. Once the listener is activated, it needs to know where to report lineage events, as well as the namespace of your jobs. Add the following additional configuration lines to your spark-defaults.conf file or your Spark submission script: spark.openlineage.host {your.openlineage.host} spark.openlineage.namespace {your.openlineage.namespace}  "},{"title":"The Demo‚Äã","type":1,"pageTitle":"Tracing Data Lineage with OpenLineage and Apache Spark","url":"/blog/openlineage-spark#the-demo","content":"Trying out the Spark integration is super easy if you already have Docker Desktop and git installed. To follow along with this demo, you‚Äôll also need a Google Cloud account and a Service Account JSON key file for an account that has access to BigQuery and read/write access to your GCS bucket. I added mine to a file called bq-spark-demo.json. Note: If you're on macOS Monterey (macOS 12) you'll have to release port 5000 before beginning by disabling the AirPlay Receiver. Check out the OpenLineage project into your workspace with: git clone https://github.com/OpenLineage/OpenLineage  Then cd into the integration/spark directory. Run mkdir -p docker/notebooks/gcs and copy your service account credentials file into that directory. Then run: docker-compose up  This launches a Jupyter notebook with Spark already installed as well as a Marquez API endpoint to report lineage. Once the notebook server is up and running, you should see something like the following text in the logs: notebook_1 | [I 21:43:39.014 NotebookApp] Jupyter Notebook 6.4.4 is running at: notebook_1 | [I 21:43:39.014 NotebookApp] http://082cb836f1ec:8888/?token=507af3cf9c22f627f6c5211d6861fe0804d9f7b19a93ca48 notebook_1 | [I 21:43:39.014 NotebookApp] or http://127.0.0.1:8888/?token=507af3cf9c22f627f6c5211d6861fe0804d9f7b19a93ca48 notebook_1 | [I 21:43:39.015 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).  Copy the URL with 127.0.0.1 as the hostname from your own log (the token will be different from mine) and paste it into your browser window. You should have a blank Jupyter notebook environment ready to go.  Once your notebook environment is ready, click on the notebooks directory, then click on the New button to create a new Python 3 notebook.  In the first cell in the window paste the following text. Update the GCP project and bucket names and the service account credentials file, then run the code: from pyspark.sql import SparkSession import urllib.request # download dependencies for BigQuery and GCS gc_jars = ['https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.1.1/gcs-connector-hadoop3-2.1.1-shaded.jar', 'https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/bigquery-connector/hadoop3-1.2.0/bigquery-connector-hadoop3-1.2.0-shaded.jar', 'https://repo1.maven.org/maven2/com/google/cloud/spark/spark-bigquery-with-dependencies_2.12/0.22.2/spark-bigquery-with-dependencies_2.12-0.22.2.jar'] files = [urllib.request.urlretrieve(url)[0] for url in gc_jars] # Set these to your own project and bucket project_id = 'bq-openlineage-spark-demo' gcs_bucket = 'bq-openlineage-spark-demo-bucket' credentials_file = '/home/jovyan/notebooks/gcs/bq-spark-demo.json' spark = (SparkSession.builder.master('local').appName('openlineage_spark_test') .config('spark.jars', &quot;,&quot;.join(files)) # Install and set up the OpenLineage listener .config('spark.jars.packages', 'io.openlineage:openlineage-spark:0.3.+') .config('spark.extraListeners', 'io.openlineage.spark.agent.OpenLineageSparkListener') .config('spark.openlineage.host', 'http://marquez-api:5000') .config('spark.openlineage.namespace', 'spark_integration') # Configure the Google credentials and project id .config('spark.executorEnv.GCS_PROJECT_ID', project_id) .config('spark.executorEnv.GOOGLE_APPLICATION_CREDENTIALS', '/home/jovyan/notebooks/gcs/bq-spark-demo.json') .config('spark.hadoop.google.cloud.auth.service.account.enable', 'true') .config('spark.hadoop.google.cloud.auth.service.account.json.keyfile', credentials_file) .config('spark.hadoop.fs.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem') .config('spark.hadoop.fs.AbstractFileSystem.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS') .config(&quot;spark.hadoop.fs.gs.project.id&quot;, project_id) .getOrCreate())  Most of this is boilerplate- we need the BigQuery and GCS libraries installed in the notebook environment, then we need to set the configuration parameters to tell the libraries what GCP project we want to use and how to authenticate with Google. The parameters specific to OpenLineage are the four we already covered- spark.jars.packages,spark.extraListeners, spark.openlineage.host, spark.openlineage.namespace. Here, we‚Äôve configured the host to be the marquez-api container started by Docker. Google has a wealth of information available as public datasets in BigQuery. If you‚Äôre ever bored one Saturday night, browse the datasets available- you‚Äôll find census data, crime data, liquor sales, and even a black hole database. For the demo, I thought I‚Äôd browse some of the Covid19 related datasets they have. Specifically, there‚Äôs a dataset that reports the likelihood of people in a given county to wear masks (broken up into five categories: always, frequently,sometimes, rarely, and never). There‚Äôs also a giant dataset called covid19_open_data that contains things like vaccination rates, current totals of confirmed cases, hospitalizations, deaths, population breakdowns, and policies on mask-wearing, contact tracing, and vaccination-mandates. Both datasets contain the county FIPS code for US counties, meaning we can join the two datasets and start exploring. Create a new cell in the notebook and paste the following code: from pyspark.sql.functions import expr, col mask_use = spark.read.format('bigquery') \\ .option('parentProject', project_id) \\ .option('table', 'bigquery-public-data:covid19_nyt.mask_use_by_county') \\ .load() \\ .select(expr(&quot;always + frequently&quot;).alias(&quot;frequent&quot;), expr(&quot;never + rarely&quot;).alias(&quot;rare&quot;), &quot;county_fips_code&quot;) opendata = spark.read.format('bigquery') \\ .option('parentProject', project_id) \\ .option('table', 'bigquery-public-data.covid19_open_data.covid19_open_data') \\ .load() \\ .filter(&quot;country_name == 'United States of America'&quot;) \\ .filter(&quot;date == '2021-10-31'&quot;) \\ .select(&quot;location_key&quot;, expr('cumulative_deceased/(population/100000)').alias('deaths_per_100k'), expr('cumulative_persons_fully_vaccinated/(population - population_age_00_09)').alias('vaccination_rate'), col('subregion2_code').alias('county_fips_code')) joined = mask_use.join(opendata, 'county_fips_code') joined.write.mode('overwrite').parquet(f'gs://{gcs_bucket}/demodata/covid_deaths_and_mask_usage/')  Again, this is standard Spark DataFrame usage. The particulars are completely irrelevant to the OpenLineage data collection- we don‚Äôt need to call any new APIs or change our code in any way. Here, I‚Äôve filtered the covid19_open_data table to include only U.S. data and to include the data for Halloween 2021. That dataset has a large number of columns, but for my own purposes, I‚Äôm only interested in a few of them. I calculate deaths_per_100kusing the existing cumulative_deceased and population columns and I calculate the vaccination_rate using the total population, subtracting the 0-9 year olds, since they weren‚Äôt eligible for vaccination at the time. For the mask_use_by_county data, I don't really care about the difference between rarely and never, so I combine them into a single number. I do the same for frequently and always. I join the few columns I want from the two datasets and store the combined result in GCS. Add one more cell to the notebook and paste the following: spark.read.parquet(f'gs://{gcs_bucket}/demodata/covid_deaths_and_mask_usage/').count()  The notebook will likely spit out a warning and a stacktrace (it should probably be a debug statement), then give you a total of 3142 records. So far, so good. Now what? If this was a data science blog, we might start generating some scatter plots or doing a linear regression to determine whether frequent mask usage was a predictor of high death rates or vaccination rates. But since we're really focused on lineage collection, I'll leave the rest of the analysis up to those with the time and inclination to dig further. Instead, let's switch to exploring the lineage records we just created. The docker-compose.yml file that ships with the OpenLineage repo includes only the Jupyter notebook and the Marquez API. For exploring visually, we‚Äôll also want to start up the Marquez web project. Without terminating the existing docker containers, run the following command in a new terminal: docker run --network spark_default -p 3000:3000 -e MARQUEZ_HOST=marquez-api -e MARQUEZ_PORT=5000 --link marquez-api:marquez-api marquezproject/marquez-web:0.19.1  Now open a new browser tab and navigate to http://localhost:3000. You should see a screen like the following:  Note the spark_integration namespace was found for us and automatically chosen, since there are no other namespaces available. We can see three jobs listed on the jobs page of the UI. They all start with openlineage_spark_test, which is the appName we passed to the SparkSession we were building in the first cell of the notebook. Each query execution or RDD action is represented as a distinct job and the name of the action is appended to the application name to form the name of the job. Clicking on the openlineage_spark_test.execute_insert_into_hadoop_fs_relation_command node, we can see the lineage graph for our notebook:  The graph shows the openlineage_spark_test.execute_insert_into_hadoop_fs_relation_command job reads from two input datasets, bigquery-public-data.covid19_nyt.mask_use_by_countyand bigquery-public-data.covid19_open_data.covid19_open_data, and writes to a third dataset,/demodata/covid_deaths_and_mask_usage. The namespace is missing from that third dataset- the fully qualified name isgs://&lt;your_bucket&gt;/demodata/covid_deaths_and_mask_usage. Before clicking on the datasets, though, the bottom bar shows some really interesting data that was collected from the Spark job. Dragging the bar up expands the view so we can get a better look at that data.  Two facets that are always collected from Spark jobs are the spark_version and the spark.logicalPlan. The first simply reports what version of Spark was executing, as well as the version of the openlineage-spark library. This is helpful information to collect when trying to debug a job run. The second facet is the serialized optimized LogicalPlan Spark reports when the job runs. Spark‚Äôs query optimization can have dramatic effects on the execution time and efficiency of the query job. Tracking how query plans change over time can significantly aid in debugging slow queries or OutOfMemory errors in production. Clicking on the first BigQuery dataset gives us information about the data we read:  Here, we can see the schema of the dataset as well as the datasource ‚Äî namely BigQuery. We can get similar information about the dataset written to in GCS:  As in the BigQuery dataset, we can see the output schema and the datasource ‚Äî here, the gs:// scheme and the name of the bucket we wrote to. In addition to the schema, we also see a stats facet, reporting the number of output records and bytes as -1. A somewhat recent change to the OpenLineage schema resulted in output facets being recorded in a new field- one that Marquez is not yet reading from. The old, deprecated facet reports the output stats incorrectly. An upcoming bugfix should correct the stats view so that we can see the number of rows written as well as the number of output bytes (the statistics are actually recorded correctly- the API simply needs to start returning the correct values). You may have noticed the VERSIONS tab on the bottom bar. We can click it, but since the job has only ever run once, we‚Äôll only see one version. Clicking on the version, we‚Äôll see the same schema and statistics facets, but specific to the version we clicked.  In production, this dataset would have many versions, as each time the job runs a new version of the dataset is created. This allows us to track changes to the statistics and schema over time, again aiding in debugging slow jobs (suddenly, we started writing 50% more records!) or data quality issues (suddenly, we‚Äôre only writing half as many records as normal!) and job failures (somebody changed the output schema and downstream jobs are failing!). The final job in the UI is a HashAggregate job- this represents the count() method we called at the end to show the number of records in the dataset. Rather than a count(), this could easily be a toPandas() call or some other job that reads and processes that data- perhaps storing output back into GCS or updating a Postgres database or publishing a new model, etc. Regardless of where the output gets stored, the OpenLineage integration allows you to see the entire lineage graph, unifying datasets in object stores, relational databases, and more traditional data warehouses.  The Spark integration is still a work in progress, but users are already getting insights into their graphs of datasets stored in object stores like S3, GCS, and Azure Blob Storage, as well as BigQuery and relational databases like Postgres. Now with Spark 3.1 supported, we can gain visibility into more environments, like Databricks, EMR, and Dataproc clusters. Data lineage gives visibility to the (hopefully) high quality, (hopefully) regularly updated datasets that everyone depends on, maybe without even realizing it. Spark helped usher in a welcome age of data democratization. Now data observability can help ensure we‚Äôre making the best possible use of the data available. "},{"title":"How OpenLineage takes inspiration from OpenTelemetry","type":0,"sectionRef":"#","url":"/blog/openlineage-takes-inspiration-from-opentelemetry","content":"","keywords":""},{"title":"Data vs Services‚Äã","type":1,"pageTitle":"How OpenLineage takes inspiration from OpenTelemetry","url":"/blog/openlineage-takes-inspiration-from-opentelemetry#data-vs-services","content":"The data world and the service world have many similarities but also a few crucial differences. Let‚Äôs start by drawing the similarities: The contract for services is the API, in the data world the contract is the dataset schema. Properly tracked Data lineage is as powerful as distributed request tracing for services.Data Quality checks are the data pipelines equivalent of services‚Äô circuit breakers.The Data catalog is data‚Äôs service discoveryData quality metrics are similar to service metrics and both can define SLOs. \tData\tServicesContract\tDataset schema\tService API Tracking Dependencies\tData lineage\tDistributed traces Preventing cascading failures\tData Quality checks\tCircuit breakers Discovery\tData catalog\tService Discovery SLOs\tFreshness, data quality\tAvailability, latency "},{"title":"Observability in the Services world‚Äã","type":1,"pageTitle":"How OpenLineage takes inspiration from OpenTelemetry","url":"/blog/openlineage-takes-inspiration-from-opentelemetry#observability-in-the-services-world","content":"In many ways, observability is a lot more mature in the services world than it is in the data world. Service telemetry data is usually described as traces, metrics and logs that allow us to observe how services behave and interact with each other. Recognizing how telemetry data is connected across service layers is key to understanding and mitigating complex distributed failures in today‚Äôs environments. OpenTelemetry is the standard that allows collection of telemetry data in a uniform, vendor-agnostic way across services and databases. For example, it enables the understanding of dependencies between microservices, facilitating investigation into how a single failure might impact services several layers removed. The creation of OpenTelemetry removed the need for every monitoring, tracing analysis and log indexing platform to create unique integrations to collect that information from the environment. "},{"title":"Observability in the Data world‚Äã","type":1,"pageTitle":"How OpenLineage takes inspiration from OpenTelemetry","url":"/blog/openlineage-takes-inspiration-from-opentelemetry#observability-in-the-data-world","content":"The data world is organized in a slightly different manner. Services strive for high availability and expose a contract to be requested from. Data pipelines consume datasets and produce datasets. They could be executed as batch processes or using streaming engines but their fundamental contract is to consume data from given inputs and produce data to given outputs. The contract is now the schema of the dataset and an expectation of a rate of update. In this world observability cares about a few things: Is the data being delivered? We might be happy with data being delivered at an hourly or daily rate but we want to know if the job responsible for this is failing and won‚Äôt be updating it at all. As a consequence all the datasets depending on this will also not be updated. Correlated SLA misses likes this must be identified to avoid many teams investigating the same problem independently.Is the data being delivered on time? Batch processing for example is relatively forgiving and can still deliver outputs according to a time SLA even when it failed and had to retry or was slower than usual because of disruptions in its environment. However critical data will need to be delivered according to pre-defined SLA. We want to be able to understand where in the data pipeline dependencies, a bottleneck caused a delay and resolve the issue.Is the data correct? Now the worst thing that can happen is not a data pipeline failing. This case is relatively easy to recover from. Once the issue is fixed and the pipeline restarted, it will typically catch up and get back to normal operation after the delay induced by the failure. The worst case scenario for a data engineer or data scientist, is the pipeline carrying through and producing bad data. This usually propagates to downstream consumers and all over the place. Recovering requires understanding that the data is incorrect (usually using a data quality library like Great Expectations or Deequ), identifying the upstream dataset where the problem originated, identifying the downstream datasets where the problem propagated, and restating all those datasets to the correct result.Auditing what happened: Another common need whether it‚Äôs for compliance or governance reasons is being able to know if specific sensitive datasets are used according to a defined set of rules. This can be used to protect user privacy, comply with financial regulation, or ensure that key metrics are derived from trusted data sources. The key common element in all those use cases is understanding dependencies through data lineage, just like services care about understanding dependencies through service traces. "},{"title":"Differences Between the Data world and the service world‚Äã","type":1,"pageTitle":"How OpenLineage takes inspiration from OpenTelemetry","url":"/blog/openlineage-takes-inspiration-from-opentelemetry#differences-between-the-data-world-and-the-service-world","content":"If the data world has exactly the same needs for observability than the service world, there are key differences between the two that create the need for a different API to instrument data pipelines. "},{"title":"Overall dependency structure:‚Äã","type":1,"pageTitle":"How OpenLineage takes inspiration from OpenTelemetry","url":"/blog/openlineage-takes-inspiration-from-opentelemetry#overall-dependency-structure","content":"Services dependencies can vary at the request level. Different requests to the same service may trigger very different downstream requests to other services. Service logic may create very different dependency patterns depending on input, timing and context. Services depend on other services that can be persistent or stateless.Data pipelines tend to be expressed in terms of a transformation from a defined set of input datasets to one or several output datasets. Their input/output structure tends to be a lot more stable and not vary much from record to record in the dataset. It‚Äôs effectively a bigraph: jobs consume datasets and datasets are produced by jobs. "},{"title":"Push vs Pull:‚Äã","type":1,"pageTitle":"How OpenLineage takes inspiration from OpenTelemetry","url":"/blog/openlineage-takes-inspiration-from-opentelemetry#push-vs-pull","content":"Services send or push requests to downstream services. Whether it‚Äôs synchronous or asynchronous, they can add a traceid to their request that will be propagated downstream. An upstream request spawns one or more downstream requests in a tree structure.Data pipelines pull data from the datasets they consume from. They aggregate and join datasets together. The structure of dependencies is typically a Directed Acyclic Graph at the dataset level instead of a tree at the request level. This means that the granularity of updates does not match one to one in a lot of cases. The frequency of updates can be different from one pipeline to the next and does not neatly align with a trace flowing down the dependencies. "},{"title":"Granularity of data updates‚Äã","type":1,"pageTitle":"How OpenLineage takes inspiration from OpenTelemetry","url":"/blog/openlineage-takes-inspiration-from-opentelemetry#granularity-of-data-updates","content":"Services treat one request at a time and tend to optimize for latency of the request being processed.Data pipelines consume entire datasets and tend to prioritize throughput over latency. The result output can be combining many records from various inputs. When a service request spawns multiple requests downstream a data pipeline tends to do the opposite at the record level while producing multiple derived datasets. "},{"title":"Parallels between OpenLineage and OpenTelemetry‚Äã","type":1,"pageTitle":"How OpenLineage takes inspiration from OpenTelemetry","url":"/blog/openlineage-takes-inspiration-from-opentelemetry#parallels-between-openlineage-and-opentelemetry","content":""},{"title":"An API first‚Äã","type":1,"pageTitle":"How OpenLineage takes inspiration from OpenTelemetry","url":"/blog/openlineage-takes-inspiration-from-opentelemetry#an-api-first","content":"Like OpenTelemetry is an API to collect traces, logs and metrics, OpenLineage is first an API to collect lineage. It is agnostic to the backend collecting the data and aspires to be integrated in every data processing engine. Data lineage is the equivalent of traces for services. It keeps track of dependencies between datasets and data pipelines and how they change over time. "},{"title":"Broad language support‚Äã","type":1,"pageTitle":"How OpenLineage takes inspiration from OpenTelemetry","url":"/blog/openlineage-takes-inspiration-from-opentelemetry#broad-language-support","content":"Like OpenTelemetry, OpenLineage has broad language support through the definition of its API in the standard JSONSchema representation. It also has dedicated clients to simplify using its semantics in the languages most commonly used for data processing (Java and Python). "},{"title":"Backend agnostic‚Äã","type":1,"pageTitle":"How OpenLineage takes inspiration from OpenTelemetry","url":"/blog/openlineage-takes-inspiration-from-opentelemetry#backend-agnostic","content":"Like OpenTelemetry, OpenLineage allows the implementation of multiple backends to consume lineage events for a variety of use cases. For example Marquez is an Open Source reference implementation that keeps track of all the changes in your environment and will help you understand what happened if something goes wrong. Other metadata projects like Egeria, DataHub, Atlas or Amundsen can also benefit from OpenLineage. Egeria in particular is committed to support OpenLineage as a Metadata bus. Like OpenTelemetry, anybody can consume and leverage OpenLineage events. "},{"title":"Integrates with popular frameworks and libraries‚Äã","type":1,"pageTitle":"How OpenLineage takes inspiration from OpenTelemetry","url":"/blog/openlineage-takes-inspiration-from-opentelemetry#integrates-with-popular-frameworks-and-libraries","content":"Like OpenTelemetry, OpenLineage aspires to be integrated in every data processing tool in the ecosystem. It also provides integration with popular tools that are not integrated yet. For example today you can cover Apache Spark, BigQuery, Snowflake, Redshift, Apache Airflow and others. "},{"title":"OpenLineage specific capabilities‚Äã","type":1,"pageTitle":"How OpenLineage takes inspiration from OpenTelemetry","url":"/blog/openlineage-takes-inspiration-from-opentelemetry#openlineage-specific-capabilities","content":"In addition to those, OpenLineage is extensible to cover various aspects of metadata that are specific to the data world. OpenLineage defines a notion of facet that lets attach well defined pieces of metadata to the OpenLineage entities (Jobs, Runs and Datasets). Facets can be either part of the core spec or be defined as custom facets by third parties. This flexible mechanism lets define independent specs for dataset schema, query profiles or data quality metrics for example. But this is a topic for another post. "},{"title":"How Operators and Extractors Work Under-the-Hook","type":0,"sectionRef":"#","url":"/blog/operators-and-extractors-technical-deep-dive","content":"","keywords":""},{"title":"Overview‚Äã","type":1,"pageTitle":"How Operators and Extractors Work Under-the-Hook","url":"/blog/operators-and-extractors-technical-deep-dive#overview","content":"Airflow Operators and OpenLineage Extractors have a specific, if quirky, way of working together. Recently, the way they work together has seen a bit of an overhaul, and the new SQL Check Extractors added a new and unique way that the extractors work and interact with operators. In this blog, we'll demystify these relationships. Note: This blog post describes the relationships of the operators and extractors only for Airflow&gt;=2.3 and OpenLineage&gt;=0.12.0. "},{"title":"Integration‚Äã","type":1,"pageTitle":"How Operators and Extractors Work Under-the-Hook","url":"/blog/operators-and-extractors-technical-deep-dive#integration","content":""},{"title":"The Operator and the Extractor‚Äã","type":1,"pageTitle":"How Operators and Extractors Work Under-the-Hook","url":"/blog/operators-and-extractors-technical-deep-dive#the-operator-and-the-extractor","content":"Some quick definitions are in order before we continue. The Airflow Operator defines a task, which is the unit of work in Airflow. All operators inherit from the BaseOperator, and in addition to taking the arguments of the BaseOperator, they can take arguments specific to the kind of task they are going to perform, such as a specific conn_id to connect to a datasource or a dictionary of checks to perform on that datasource. The OpenLineage Extractor is somewhat analogous to the Airflow Operator: it is a unit of work in OpenLineage, which takes the relevant input and output data from an operator, creates OpenLineage data facets, and sends those facets to be displayed in Marquez or Datakin. Each extractor maps to a specific set of operators via the get_operator_classnames() class method. The extractors all inherit from a BaseExtractor, which defines a few abstract methods, importantly execute() and execute_on_complete(). Briefly, the two other major OpenLineage constructs in this story are the ExtractorManager, which is responsible for identifying the correct extractor to use, and the Listener, which is the connecting piece between OpenLineage and Airflow. "},{"title":"Interplay‚Äã","type":1,"pageTitle":"How Operators and Extractors Work Under-the-Hook","url":"/blog/operators-and-extractors-technical-deep-dive#interplay","content":"Next, we'll walk through what happens when an Airflow instance with OpenLineage support runs a DAG, and how that operator data makes it to the Marquez or Datakin UI. First, a DAG is born. When the DAG is run, the scheduler runs the operators in order by calling their execute() method. This is the first time the OpenLineage Listener responds. Triggered by the execute() event, it calls the Manager, which identifies the correct extractor based on the task_id. Then the Extractor's execute() method is run, potentially returning lineage data in the form of a metadata object. When the operator is done--either succeeded or failed--, the Listener calls the Manager again, and this time the Manager triggers the Extractor's execute_on_complete() method, which may also return metadata based on the result of the task. The metadata object is then sent to Marquez or Datakin, where the data is displayed. "},{"title":"SQL Check Operators/Extractors‚Äã","type":1,"pageTitle":"How Operators and Extractors Work Under-the-Hook","url":"/blog/operators-and-extractors-technical-deep-dive#sql-check-operatorsextractors","content":"The SQLCheckOperators and SQLCheckExtractors work slightly differently than the interplay outlined above. The biggest difference is that the SQLCheckExtractors all inherit from a BaseSqlCheckExtractor, which in turn dynamically inherits from the appropriate extractor at run time. The appropriate extractor is always some existing SQL database extractor. The BaseSqlCheckExtractor itself only implements the extract_on_complete() method by determining whether the super class‚Äôs extract() or extract_on_complete() method should be run to gather metadata. The _get_input_facets() methods are all implemented by the particular check extractors, and are called in the super class‚Äôs extract() or extract_on_complete() method. The dynamic inheritance is done by defining the SQLCheckExtractors inside the get_check_extractors() function that takes a class as a parameter and passes that class to the constructor of the BaseSqlCheckExtractor. When a SQLCheckOperator is run, and the Manager searches for the correct extractor to use, it calls instantiate_abstract_extractors() with the given task instance. In this function, the task instance is used to find the correct extractor that will be the superclass of the BaseSqlCheckOperator. To do this, the function uses a set of hard-coded operator names whose extractors will dynamically inherit from the found superclass. Currently, this list is the set of SQLCheckOperators, which corresponds to a dictionary of extractor keys and conn_type values. The given task‚Äôs class name is checked against the set of operator names, and if it is in the set, a loop compares the existing extractor‚Äôs corresponding conn_type from the aforementioned dictionary to the given task instance‚Äôs conn_type retrieved from Airflow connections. If there‚Äôs a match, the get_check_extractors() method is called with the matched extractor, instantiating all the operators with the correct superclass. The SQLCheckExtractors only rely on the extract_on_complete() method, as the values needed from the operators, i.e. the results of the query and the success or failures of the checks, are only available after the operator completes. "},{"title":"The Python Client -- the Foundation of OpenLineage Integrations","type":0,"sectionRef":"#","url":"/blog/python-client","content":"","keywords":""},{"title":"Introduction‚Äã","type":1,"pageTitle":"The Python Client -- the Foundation of OpenLineage Integrations","url":"/blog/python-client#introduction","content":"Thanks to the OpenLineage community‚Äôs active work on integrations, the pursuit of lineage is getting more efficient and effective all the time. And our growing list of partners and adapters makes OpenLineage plenty powerful out of the box. At the same time, the nature of the data engineering field means that lineage capture is an ongoing process ‚Äì simply put, the work of lineage is never done. Hence, as lineage capture becomes integral to your pipelines, situations can arise that require new custom integrations. Enter the Python client, one of two built-in clients included in the project (the other being the Java client). The OpenLineage spec is defined using JSON schema, but we have created these clients so that new integrations do not have to reinvent the wheel. OpenLineage‚Äôs Python client enables the creation of lineage metadata events with Python code. The core data structures currently offered by the client include the RunEvent, RunState, Run, Job, Dataset, and Transport classes. These either configure or collect data for the emission of lineage events. In the history of the project, the client has been useful in helping us avoid unnecessary duplication of code. It is also integral to OpenLineage‚Äôs existing integrations, serving as the basis of the Airflow and dbt integrations, for example. It could also act as the foundation of your own custom integration should you need to write one. (Another use case: tests for a new Airflow extractor.) For this reason, an existing integration can serve as a useful example of how to use the client to write a new integration (and, hopefully, contribute it back to the project!). What follows is an overview of the Python client and the dbt integration, which uses the Python client. You‚Äôll see how the client receives metadata from dbt to make it available to a consumer of OpenLineage such as Microsoft Purview, Amundsen, Astronomer, Egeria or Marquez. "},{"title":"Python Client Data Structures‚Äã","type":1,"pageTitle":"The Python Client -- the Foundation of OpenLineage Integrations","url":"/blog/python-client#python-client-data-structures","content":"The core structures of the client organize metadata about the foundational objects of the OpenLineage spec: runs, jobs and datasets. A dataset is a class consisting of a name, namespace and, optionally, facets array: @attr.s class Dataset: namespace: str = attr.ib() name: str - attr.ib() Facets: Dict = attr.ib(factory=dict)  The same goes for a job: @attr.s class Job: namespace: str = attr.ib() name: str - attr.ib() Facets: Dict = attr.ib(factory=dict)  A RunEvent sends the time, state, job, run, producer, input and output information needed to construct an OpenLineage job run event: @attr.s class RunEvent: eventType: RunState = attr.ib(validator=attr.validators.in_(RunState)) eventTime: str = attr.ib() run: Run = attr.ib() job: Job = attr.ib() producer: str = attr.ib() inputs: Optional[List[Dataset]] = attr.ib(factory=list) outputs: Optional[List[Dataset]] = attr.ib(factory=list)  "},{"title":"The OpenLineage-dbt Integration‚Äã","type":1,"pageTitle":"The Python Client -- the Foundation of OpenLineage Integrations","url":"/blog/python-client#the-openlineage-dbt-integration","content":"At a high level, the dbt integration uses the Python client to push metadata to the OpenLineage backend. The metadata it makes available comprises the run lifecycle, including any dataset inputs and outputs accessed during a job run. In the dbt-ol script, the integration uses the project‚Äôs ParentRunMetadata and DbtArtifactProcessor classes, both of which can be found in the OpenLineage common integration, to parse metadata from the dbt manifest and run_result in order to produce OpenLineage events: from openlineage.common.provider.dbt import DbtArtifactProcessor, ParentRunMetadata #‚Ä¶ if parent_id: parent_namespace, parent_job_name, parent_run_id = parent_id.split(‚Äò/‚Äô) parent_run_metadata = ParentRunMetadata( run_id=parent_run_id, job_name=parent_job_name, job_namespace=parent_namespace ) processor = DbtArtifactProcessor( producer=PRODUCER, target=target, job_namespace=job_namespace, project_dir=project_dir, profile_name=profile_name, logger=logger )  The integration uses a wrapper for dbt runs because start and complete events are not emitted until execution concludes: dbt_run_event = dbt_run_event_start( job_name=f‚Äúdbt-run-{processor.project[‚Äòname‚Äô]}‚Äù, job_namespace=job_namespace, parent_run_metadata=parent_run_metadata ) dbt_run_metadata = ParentRunMetadata( run_id=dbt_run_event.run.runId, job_name=dbt_run_event.job.name, job_namespace=dbt_run_event.job.namespace ) processor.dbt_run_metadata = dbt_run_metadata  After executing dbt, the script parses the metadata using the processor and emits a run event: events = processor.parse().events() client.emit(dbt_run_event_end( run_id=dbt_run_metadata.run_id, job_namespace=dbt_run_metadata.job_namespace, job_name=dbt_run_metadata.job_name, parent_run_metadata=parent_run_metadata )) logger.info(f&quot;Emitted {len(events) + 2} openlineage events&quot;)  "},{"title":"Additional Resources‚Äã","type":1,"pageTitle":"The Python Client -- the Foundation of OpenLineage Integrations","url":"/blog/python-client#additional-resources","content":"Check out the source code here: https://github.com/OpenLineage/OpenLineage/tree/main/client/python. Interested in contributing to the project? Read our guide for new contributors: https://github.com/OpenLineage/OpenLineage/blob/main/CONTRIBUTING.md. Join us on Slack: https://bit.ly/OLslack. Attend a community meeting: https://bit.ly/OLwikitsc. "},{"title":"About OpenLineage","type":0,"sectionRef":"#","url":"/docs/","content":"","keywords":""},{"title":"Design‚Äã","type":1,"pageTitle":"About OpenLineage","url":"/docs/#design","content":"OpenLineage is an Open Standard for lineage metadata collection designed to record metadata for a job in execution. The standard defines a generic model of dataset, job, and run entities uniquely identified using consistent naming strategies. The core model is highly extensible via facets. A facet is user-defined metadata and enables entity enrichment. We encourage you to familiarize yourself with the core model below:  "},{"title":"How OpenLineage Benefits the Ecosystem‚Äã","type":1,"pageTitle":"About OpenLineage","url":"/docs/#how-openlineage-benefits-the-ecosystem","content":"Below, we illustrate the challenges of collecting lineage metadata from multiple sources, schedulers and/or data processing frameworks. We then outline the design benefits of defining an Open Standard for lineage metadata collection. BEFORE:‚Äã  Each project has to instrument its own custom metadata collection integration, therefore duplicating efforts.Integrations are external and can break with new versions of the underlying scheduler and/or data processing framework, requiring projects to ensure backwards compatibility. WITH OPENLINEAGE:‚Äã  Integration efforts are shared across projects.Integrations can be pushed to the underlying scheduler and/or data processing framework; no longer does one need to play catch up and ensure compatibility! "},{"title":"Scope‚Äã","type":1,"pageTitle":"About OpenLineage","url":"/docs/#scope","content":"OpenLineage defines the metadata for running jobs and their corresponding events. A configurable backend allows the user to choose what protocol to send the events to. "},{"title":"Core model‚Äã","type":1,"pageTitle":"About OpenLineage","url":"/docs/#core-model","content":" A facet is an atomic piece of metadata attached to one of the core entities. See the spec for more details. "},{"title":"Spec‚Äã","type":1,"pageTitle":"About OpenLineage","url":"/docs/#spec","content":"The specification is defined using OpenAPI and allows extension through custom facets. "},{"title":"Integrations‚Äã","type":1,"pageTitle":"About OpenLineage","url":"/docs/#integrations","content":"The OpenLineage repository contains integrations with several systems. Apache SparkApache AirflowDagsterdbt "},{"title":"Related projects‚Äã","type":1,"pageTitle":"About OpenLineage","url":"/docs/#related-projects","content":"Marquez: Marquez is an LF AI &amp; DATA project to collect, aggregate, and visualize a data ecosystem's metadata. It is the reference implementation of the OpenLineage API. OpenLineage collection implementation Egeria: Egeria Open Metadata and Governance. A metadata bus. "},{"title":"What's in a Namespace?","type":0,"sectionRef":"#","url":"/blog/whats-in-a-namespace","content":"","keywords":""},{"title":"Background‚Äã","type":1,"pageTitle":"What's in a Namespace?","url":"/blog/whats-in-a-namespace#background","content":"With all due respect to Shakespeare's Juliet, in the OpenLineage spec at least, names in general -- and namespaces in particular -- are everything. OK, that‚Äôs an exaggeration, but not by much. The function of namespaces is to provide unique IDs for everything in the lineage graph so that jobs and datasets can be rendered as nodes. This means namespaces make stitching input and output datasets together as pipelines possible ‚Äì which is to say they effectively make lineage possible. In the broader context of the spec, namespaces reflect the importance of naming and naming conventions to the way OpenLineage constructs lineage.  In creating pipelines organized according to data sources (in the case of datasets) or schedulers (in the case of jobs), namespaces enable focused insight into data flows, even when datasets and workflows are distributed across an organization. This focus enabled by namespaces is key to the production of useful lineage. If everything lived in a single namespace, every lineage graph would show everything that happened in an ecosystem ‚Äì and be too cluttered to be useful. "},{"title":"Namespaces in the Spec‚Äã","type":1,"pageTitle":"What's in a Namespace?","url":"/blog/whats-in-a-namespace#namespaces-in-the-spec","content":"A look at the spec provides additional detail about namespaces. In the spec, namespaces are at the top of the hierarchy, which means that they have priority over datasets, jobs, and the graphs that connect them. Namespaces contain graphs, in fact, along with just about everything else in a datasource or scheduler‚Äôs domain. Ultimately, this reflects the spec‚Äôs bias towards tracking dataset and job transformations. To wit: the same code applied to different input datasets results in different jobs (not different runs of the same job). If those jobs share a scheduler, they will also share a namespace ‚Äì but not a graph, which makes tracking the transformations much easier. Similarly, if different input datasets share a datasource, they will also share a namespace (but not a graph). As you can see, the track switching can get a little complicated, but the namespace abstraction has some clear advantages. "},{"title":"Namespaces in the Wild‚Äã","type":1,"pageTitle":"What's in a Namespace?","url":"/blog/whats-in-a-namespace#namespaces-in-the-wild","content":"Consider the common scenario in which multiple teams in an organization maintain pipelines that access the same datasets. Now, imagine trying to collect and display lineage from the organization‚Äôs ecosystem without having a way to distinguish between the different pipelines that use the same datasets. The ambiguous metadata would make any graph so cluttered as to be practically meaningless. Suffice it to say, without a strategy for naming at that macro level of the ecosystem, creating a meaningful graph and tracking transformations is much more difficult. Namespaces also ensure that lineage is meaningful irrespective of the various sources of a job‚Äôs metadata. A scope above the dataset and run makes heterogeneous, holistic lineage possible in the case of datasets and jobs. We define the unique name strategy per resource to ensure it is followed uniformly independently from who is producing metadata and we can connect lineage from various sources.  In sum, namespaces make operational lineage possible ‚Äì which is, while maybe not everything, close to it. "},{"title":"Consulting the Marquez API‚Äã","type":1,"pageTitle":"What's in a Namespace?","url":"/blog/whats-in-a-namespace#consulting-the-marquez-api","content":"Thanks to the Marquez API reference, we know that a namespaces endpoint is available that we can query for all namespaces. If you use curl to do so, here‚Äôs what you‚Äôll get after building Marquez from source with seed data (using ./docker/up.sh --build --seed): ‚ûú marquez git:(main) ‚úó curl -v http://localhost:5000/api/v1/namespaces | jq { &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;default&quot;, &quot;createdAt&quot;: &quot;2022-12-07T15:02:24.135154Z&quot;, &quot;updatedAt&quot;: &quot;2022-12-07T15:02:24.135154Z&quot;, &quot;ownerName&quot;: &quot;anonymous&quot;, &quot;description&quot;: &quot;The default global namespace for dataset, job, and run metadata not belonging to a user-specified namespace.&quot;, &quot;isHidden&quot;: false }, { &quot;name&quot;: &quot;food_delivery&quot;, &quot;createdAt&quot;: &quot;2020-02-22T22:42:42Z&quot;, &quot;updatedAt&quot;: &quot;2020-02-22T22:42:42Z&quot;, &quot;ownerName&quot;: &quot;anonymous&quot;, &quot;description&quot;: null, &quot;isHidden&quot;: false } ] }  The namespaces endpoint returns all the available namespaces, which is helpful because, as we‚Äôll see, so much of the information available from the API requires a namespace. For this reason alone, you might say the namespace is the ‚Äúroot‚Äù of the object model. Say you want to retrieve one or more datasets from the API. First, you‚Äôll need a namespace: ‚ûú marquez git:(main) ‚úó curl -v http://localhost:5000/api/v1/namespaces/food_delivery/datasets/public.drivers | jq { &quot;id&quot;: { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.drivers&quot; }, &quot;type&quot;: &quot;DB_TABLE&quot;, &quot;name&quot;: &quot;public.drivers&quot;, &quot;physicalName&quot;: &quot;public.drivers&quot;, &quot;createdAt&quot;: &quot;2020-02-22T22:42:42Z&quot;, &quot;updatedAt&quot;: &quot;2020-02-22T22:42:42Z&quot;, &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;sourceName&quot;: &quot;default&quot;, &quot;fields&quot;: [ { &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;INTEGER&quot;, &quot;tags&quot;: [], &quot;description&quot;: &quot;The unique ID of the driver.&quot; }, ‚Ä¶  Say you want to retrieve a job. You‚Äôll need a namespace: ‚ûú marquez git:(main) ‚úó curl -v http://localhost:5000/api/v1/namespaces/food_delivery/jobs/etl_order_status | jq { &quot;id&quot;: { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;etl_order_status&quot; }, &quot;type&quot;: &quot;BATCH&quot;, &quot;name&quot;: &quot;etl_order_status&quot;, &quot;simpleName&quot;: &quot;etl_order_status&quot;, &quot;parentJobName&quot;: null, &quot;createdAt&quot;: &quot;2020-02-22T22:42:42Z&quot;, &quot;updatedAt&quot;: &quot;2020-02-22T22:44:52Z&quot;, &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;inputs&quot;: [], &quot;outputs&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.order_status&quot; } ], ‚Ä¶  Runs? You‚Äôll need a namespace for those: ‚ûú marquez git:(main) ‚úó curl -v http://localhost:5000/api/v1/namespaces/food_delivery/jobs/etl_order_status/runs | jq { &quot;runs&quot;: [ { &quot;id&quot;: &quot;b7098939-87f0-4207-878f-dfd8e8804d8a&quot;, &quot;createdAt&quot;: &quot;2020-02-22T22:42:42Z&quot;, &quot;updatedAt&quot;: &quot;2020-02-22T22:44:52Z&quot;, &quot;nominalStartTime&quot;: null, &quot;nominalEndTime&quot;: null, &quot;state&quot;: &quot;COMPLETED&quot;, &quot;startedAt&quot;: &quot;2020-02-22T22:42:42Z&quot;, &quot;endedAt&quot;: &quot;2020-02-22T22:44:52Z&quot;, &quot;durationMs&quot;: 130000, &quot;args&quot;: {}, &quot;jobVersion&quot;: { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;etl_order_status&quot;, &quot;version&quot;: &quot;44ca508b-43cc-392f-bbd2-9ca77d501afa&quot; }, &quot;inputVersions&quot;: [], &quot;outputVersions&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.order_status&quot;, &quot;version&quot;: &quot;0c16298c-cbe2-3547-8429-309917290570&quot; } ], &quot;context&quot;: { &quot;sql&quot;: &quot;INSERT INTO order_status (id, transitioned_at, status, order_id, customer_id, restaurant_id, driver_id)\\n SELECT id, transitioned_at, status, order_id, customer_id, restaurant_id, driver_id\\n FROM tmp_order_status;&quot; }, &quot;facets&quot;: {} } ] }  As the API reveals, namespaces really are key in the spec: they organize and unlock most of the insights OpenLineage has to offer. "},{"title":"Dataset Namespaces‚Äã","type":1,"pageTitle":"What's in a Namespace?","url":"/blog/whats-in-a-namespace#dataset-namespaces","content":"Having explored the thinking behind namespaces and their role in the spec, we can get into how they organize datasets and jobs. Let‚Äôs start with datasets, the simpler of the two cases due to the more straightforward way datasets‚Äô namespaces are constructed. In short, a dataset‚Äôs namespace is always tied to its datasource. As the spec says, the namespace for a dataset is the unique name for its datasource. Data sources vary, however, so the specific construction of dataset namespaces also varies across datasource types. (But they tend to map to databases.) In the case of Postgres, Mysql, and Trino, for example, we derive the namespace of a dataset from a combination of the scheme, host, and port of the service instance: * Namespace: postgres://{host}:{port} of the service instance. * Scheme = postgres * Authority = {host}:{port} * Namespace: mysql://{host}:{port} of the service instance. * Scheme = mysql * Authority = {host}:{port} * Namespace: trino://{host}:{port} of the service instance. * Scheme = trino * Authority = {host}:{port}  Redshift requires a different strategy. It‚Äôs possible to interact with Redshift via SQL and an API, so a Redshift namespace is composed of a cluster identifier, region name and port ‚Äì the only common unique ID available to both methods: * Namespace: redshift://{cluster_identifier}.{region_name}:{port} of the cluster instance. * Scheme = redshift * Authority = {cluster_identifier}:{port}  Snowflake and Amazon‚Äôs serverless Athena warehouse service, which do not require a port, are even simpler: * Namespace: awsathena://athena.{region_name}.amazonaws.com of the service instance. * Scheme = awsathena * Authority = athena.{region_name}.amazonaws.com * Namespace: snowflake://{account name} * Scheme = snowflake * Authority = {account name}  And so on. As you can see, the provenance of dataset namespaces is pretty straightforward: it‚Äôs the data source. That said, sometimes deriving the data source of a dataset is not a simple operation. Some datasets can be identified two different ways, for example. A Spark dataset can be written using a Hive metastore and table name but read using the physical location of the data. Before we added the SymlinksDatasetFacet, this naming conflict could break the lineage graph. Symlinks both provide alternative dataset names as a workaround in such cases and contain extra information about the datasets. "},{"title":"Job Namespaces‚Äã","type":1,"pageTitle":"What's in a Namespace?","url":"/blog/whats-in-a-namespace#job-namespaces","content":"We‚Äôve seen that for datasets the namespace is determined by the data source. But jobs are a different animal, so their namespaces are also different. How is a job‚Äôs namespace derived? As in the case of datasets, the unique naming of jobs is essential, and a job‚Äôs unique name consists of a namespace and name. Unlike datasets, jobs descend from schedulers, not data sources. Also unlike datasets, jobs are reducible: a job is composed of executions, or runs (as you can see from the ‚ÄúHistorial de Ejecuciones‚Äù tab ‚Äì if you were to take the new language switcher for a spin and select Spanish, that is!).  Consulting the spec, we find more detail about the naming of jobs: Jobs have a name that is unique to them in their namespace by construction. The Namespace is the root of the naming hierarchy. The job name is constructed to identify the job within that namespace. Example: * Airflow: * Namespace: the namespace is assigned to the airflow instance. Ex: airflow-staging, airflow-prod * Job: each task in a DAG is a job. name: {dag name}.{task name} * Spark: * Namespace: the namespace is provided as a configuration parameter as in airflow. If there's a parent job, we use the same namespace, otherwise it is provided by configuration. * Spark app job name: the spark.app.name * Spark action job name: {spark.app.name}.{node.name}  So, while for datasets it‚Äôs all about the datasource, for jobs it‚Äôs all about the scheduler. And the ParentRun facet makes tracking job namespaces possible. { &quot;run&quot;: { &quot;runId&quot;: &quot;run_uuid&quot; }, &quot;job&quot;: { &quot;namespace&quot;: &quot;job_namespace&quot;, &quot;name&quot;: &quot;job_name&quot; } }  For all events, if a parent job exists, the facet‚Äôs namespace value is used to assign a namespace. Otherwise, one is provided by configuration. "},{"title":"What's the Point?‚Äã","type":1,"pageTitle":"What's in a Namespace?","url":"/blog/whats-in-a-namespace#whats-the-point","content":"But why bother with dataset namespaces in the first place? One answer to this question gets to what is, for some users, a primary value proposition of OpenLineage. A common use case for lineage collection involves tracking dataset access and transformation across jobs and teams in an organization ‚Äì for monitoring the use of PII, for example. Tags are supported by OpenLineage and can be used to meet this need, but, depending on how an organization‚Äôs ecosystem is constructed, namespaces can also help with this common governance use case. Let‚Äôs explore a simple example constructed using the Python client. Imagine that a library‚Äôs website has two components, a catalog and a blog, and that both features access the same user and profile tables, both of which contain PII.  In the spec, a dataset is unique only within a namespace ‚Äì not across multiple namespaces ‚Äì so a number of different graphs are possible depending on how the datasets are produced and accessed across an organization. For example, if for some reason the users and profiles tables shared two different data sources, the tables would belong to two different namespaces (let‚Äôs call them catalog_project and blog_project). While not a typical scenario, this would result in two different, uncluttered graphs of the multiple flows making use of the shared datasets:   The reason for the simplicity? The users and profiles tables belong to both the catalog_project and blog_project namespaces. A more typical scenario might involve single versions of the tables being produced by one data source, which would assign them to a single OpenLineage namespace. Ironically, a simpler approach like this results in a more complicated visualization. Notice that the graph remains the same regardless of the namespace selected:   One advantage of this architecture is that it results in graphs making clear that the datasets containing PII are shared by the two jobs. Depending on an organization‚Äôs needs, developers might also find it more convenient to be able to see both jobs and their shared datasets in the same graph. A third scenario might involve the isolation of PII by the use of a dedicated database and, by extension, a dedicated namespace (e.g., user_data). In the resulting visualization, the job views above remain the same, but the shared datasets containing PII are now collected in the user_data namespace on the datasets page of the Marquez UI:  Namespaces offer organizations a range of insights into how their teams are accessing and transforming sensitive data. "},{"title":"How to Learn More‚Äã","type":1,"pageTitle":"What's in a Namespace?","url":"/blog/whats-in-a-namespace#how-to-learn-more","content":"If you‚Äôre new to OpenLineage and want to check out namespaces in action, a good entry point is the Getting Started guide. There you can learn about the core model, collect run-level metadata using Marquez as the HTTP backend, and explore lineage in the Marquez UI. Helpful resources for learning more about the namespaces include the spec, where Naming.md is the Rosetta stone for namespace construction and naming conventions in the project. We also welcome contributions to the project. One of the existing integrations might be a good place to start. Our growing list of partners includes Airflow, dbt, Dagster and Flink. Sound fun? Check out the new contributor guide to get started. "},{"title":"Acknowledgments‚Äã","type":1,"pageTitle":"What's in a Namespace?","url":"/blog/whats-in-a-namespace#acknowledgments","content":"Ross Turk (@rossturk) and Pawe≈Ç Leszczy≈Ñski (@pawel-big-lebowski) contributed valuable feedback and suggestions. Any faults are the author's own. "},{"title":"Java","type":0,"sectionRef":"#","url":"/docs/client/java","content":"","keywords":""},{"title":"Overview‚Äã","type":1,"pageTitle":"Java","url":"/docs/client/java#overview","content":"The OpenLineage Java client enables the creation of lineage metadata events with Java code. The core data structures currently offered by the client are the RunEvent, RunState, Run, Job, Dataset, and Transport classes, along with various Facets that can come under run, job, and dataset. There are various transport classes that the library provides that carry the lineage events into various target endpoints (e.g. HTTP). You can also use the Java client to create your own custom integrations. "},{"title":"Installation‚Äã","type":1,"pageTitle":"Java","url":"/docs/client/java#installation","content":"Java client is provided as library that can either be imported into your Java project using Maven or Gradle. Maven: &lt;dependency&gt; &lt;groupId&gt;io.openlineage&lt;/groupId&gt; &lt;artifactId&gt;openlineage-java&lt;/artifactId&gt; &lt;version&gt;0.12.0&lt;/version&gt; &lt;/dependency&gt;  or Gradle: implementation 'io.openlineage:openlineage-java:0.12.0'  For more information on the available versions of the openlineage-java, please refer to the maven repository. "},{"title":"Configuration‚Äã","type":1,"pageTitle":"Java","url":"/docs/client/java#configuration","content":"Use the following options to configure the client: An openlineage.yml in the user's current working directoryAn openlineage.yml under .openlineage/ in the user's home directory (ex: ~/.openlineage/openlineage.yml)Environment variables Note: By default, the client will give you sane defaults, but you can easily override them. "},{"title":"Environment Variables‚Äã","type":1,"pageTitle":"Java","url":"/docs/client/java#environment-variables","content":"The list of available environment varaibles can be found here. YAML transport: type: &lt;type&gt; # ... transport specific configuration  Here is an example of using HTTP transport for your client: transport: type: http url: http://localhost:5000  Note: For a full list of supported transports, see transports. "},{"title":"Transports‚Äã","type":1,"pageTitle":"Java","url":"/docs/client/java#transports","content":"The Transport abstraction defines an emit() method for OpenLineage.RunEvent. There are three built-in transports: ConsoleTransport, HttpTransport, and KafkaTransport. ConsoleTransport‚Äã in YAML: transport: type: CONSOLE  You can also specify the ConsoleTransport when building a new client instance. OpenLineageClient client = OpenLineageClient.builder() .transport( new ConsoleTransport() .build();  HttpTransport‚Äã in YAML: transport: type: HTTP url: http://localhost:5000 auth: type: api_key api_key: f38d2189-c603-4b46-bdea-e573a3b5a7d5  You can override the default configuration of the HttpTransport by specifying the URL and API key when creating a new client: OpenLineageClient client = OpenLineageClient.builder() .transport( HttpTransport.builder() .uri(&quot;http://localhost:5000&quot;) .apiKey(&quot;f38d2189-c603-4b46-bdea-e573a3b5a7d5&quot;) .build()) .build();  To configure the client with query params appended on each HTTP request, use: Map&lt;String, String&gt; queryParamsToAppend = Map.of( &quot;param0&quot;,&quot;value0&quot;, &quot;param1&quot;, &quot;value1&quot; ); // Connect to http://localhost:5000 OpenLineageClient client = OpenLineageClient.builder() .transport( HttpTransport.builder() .uri(&quot;http://localhost:5000&quot;, queryParamsToAppend) .apiKey(&quot;f38d2189-c603-4b46-bdea-e573a3b5a7d5&quot;) .build()) .build(); // Define a simple OpenLineage START or COMPLETE event OpenLineage.RunEvent startOrCompleteRun = ... // Emit OpenLineage event to http://localhost:5000/api/v1/lineage?param0=value0&amp;param1=value1 client.emit(startOrCompleteRun);  Alternatively, use the following environment variables to configure the HttpTransport: OPENLINEAGE_URL: the URL for the HTTP transport (default: http://localhost:8080)OPENLINEAGE_API_KEY: the API key to be set on each HTTP request Not everything will be supported while using this method. KafkaTransport‚Äã in YAML: transport: type: Kafka topicName: openlineage.events # Kafka properties (see: http://kafka.apache.org/0100/documentation.html#producerconfigs) properties: bootstrap.servers: localhost:9092,another.host:9092 acks: all retries: 3 key.serializer: org.apache.kafka.common.serialization.StringSerializer value.serializer: org.apache.kafka.common.serialization.StringSerializer  KafkaTransport depends on you to provide artifact org.apache.kafka:kafka-clients:3.1.0 or compatible on your classpath. "},{"title":"Python","type":0,"sectionRef":"#","url":"/docs/client/python","content":"","keywords":""},{"title":"Overview‚Äã","type":1,"pageTitle":"Python","url":"/docs/client/python#overview","content":"The Python client is the basis of existing OpenLineage integrations such as Airflow and dbt. The client enables the creation of lineage metadata events with Python code. The core data structures currently offered by the client are the RunEvent, RunState, Run, Job, Dataset, and Transport classes. These either configure or collect data for the emission of lineage events. You can use the client to create your own custom integrations. "},{"title":"Installation‚Äã","type":1,"pageTitle":"Python","url":"/docs/client/python#installation","content":"Download the package using pip with pip install openlineage-python  To install the package from source, use python setup.py install  "},{"title":"Configuration‚Äã","type":1,"pageTitle":"Python","url":"/docs/client/python#configuration","content":"We recommend configuring the client with an openlineage.yml file that tells the client how to connect to an OpenLineage backend. You can make this file available to the client three ways: Set an environment variable to a file path: OPENLINEAGE_CONFIG=path/to/openlineage.yml.Put the file in the working directory.Put the file in $HOME/.openlineage. In openlineage.yml, use a standard transport interface to specify the transport type (http, console, kafka, or custom) and authorization parameters: transport: type: &quot;http&quot; url: &quot;https://backend:5000&quot; auth: type: &quot;api_key&quot; api_key: &quot;f048521b-dfe8-47cd-9c65-0cb07d57591e&quot;  The type property (required) is a fully qualified class name that can be imported. "},{"title":"Environment Variables‚Äã","type":1,"pageTitle":"Python","url":"/docs/client/python#environment-variables","content":"The list of available environment varaibles can be found here. "},{"title":"Built-in Transport Types‚Äã","type":1,"pageTitle":"Python","url":"/docs/client/python#built-in-transport-types","content":"HTTP‚Äã type - string (required)url - string (required)timeout - float specifying a timeout value when sending an event. Default: 5 seconds. (optional)verify - boolean specifying whether or not the client should verify TLS certificates from the backend. Default: true. (optional)auth - dictionary specifying authentication options. Requires the type property. (optional)type - string specifying the &quot;api_key&quot; or the fully qualified class name of your TokenProvider. (required if auth is provided)api_key - string setting the Authentication HTTP header as the Bearer. (required if api_key is set) Kafka‚Äã Kafka transport requires confluent-kafka package to be additionally installed. It can be installed also by specifying kafka client extension: pip install openlineage-python[kafka] type - string (required)config - string containing a Kafka producer config (required)topic - string specifying the topic (required)flush - boolean specifying whether or not Kafka should flush after each event. Default: true. (optional) "},{"title":"Custom Transport Type‚Äã","type":1,"pageTitle":"Python","url":"/docs/client/python#custom-transport-type","content":"To implement a custom transport, follow the instructions in transport.py. "},{"title":"Getting Started‚Äã","type":1,"pageTitle":"Python","url":"/docs/client/python#getting-started","content":"To try out the client, follow the steps below to install and explore OpenLineage, Marquez (the reference implementation of OpenLineage), and the client itself. Then, the instructions will show you how to use these tools to add a run event and datasets to an existing namespace. "},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"Python","url":"/docs/client/python#prerequisites","content":"Docker 17.05+Docker Compose 1.29.1+Git (preinstalled on most versions of MacOS; verify your version with git version)4 GB of available memory (the minimum for Docker ‚Äî more is strongly recommended) "},{"title":"Install OpenLineage and Marquez‚Äã","type":1,"pageTitle":"Python","url":"/docs/client/python#install-openlineage-and-marquez","content":"Clone the Marquez Github repository: git clone https://github.com/MarquezProject/marquez.git  "},{"title":"Install the Python client‚Äã","type":1,"pageTitle":"Python","url":"/docs/client/python#install-the-python-client","content":"pip install openlineage-python  "},{"title":"Start Docker and Marquez‚Äã","type":1,"pageTitle":"Python","url":"/docs/client/python#start-docker-and-marquez","content":"Start Docker Desktop Run Marquez with preloaded data: cd marquez ./docker/up.sh --seed  Marquez should be up and running at http://localhost:3000. Take a moment to explore Marquez to get a sense of how metadata is displayed in the UI. Namespaces ‚Äì the global contexts for runs and datasets ‚Äì can be found in the top right corner, and icons for jobs and runs can be found in a tray along the left side. Next, configure OpenLineage and add a script to your project that will generate a new job and new datasets within an existing namespace (here we‚Äôre using the food_delivery namespace that got passed to Marquez with the ‚Äìseed argument we used earlier). Create a directory for your script: .. mkdir python_scripts &amp;&amp; cd python_scripts  In the python_scripts directory, create a Python script (we used the name generate_events.py for ours) and an openlineage.yml file. In openlineage.yml, define a transport type and URL to tell OpenLineage where and how to send metadata: Transport: Type: ‚Äúhttp‚Äù Url: ‚Äúhttp://localhost:5000‚Äù  In generate_events.py, import the Python client and the methods needed to create a job and datasets. Also required (to create a run): the datetime and uuid packages: from openlineage.client.run import RunEvent, RunState, Run, Job, Dataset from openlineage.client import OpenLineageClient from datetime import datetime from uuid import uuid4  Then, in the same file, initialize the Python client: client = OpenLineageClient.from_environment()  It is also possible to specify parameters such as URL for client to connect to, without using environment variables or openlineage.yaml file, by directly setting it up when instantiating OpenLineageClient: client = OpenLineageClient(url=&quot;http://localhost:5000&quot;)  For more details about options to setup OpenLineageClient such as API tokens or HTTP transport settings, please refer to the following example Specify the producer of the new lineage metadata with a string: producer = ‚ÄúOpenLineage.io/website/blog‚Äù  Now you can create some basic dataset objects. These require a namespace and name: inventory = Dataset(namespace=‚Äúfood_delivery‚Äù, name=‚Äúpublic.inventory‚Äù) menus = Dataset(namespace=‚Äúfood_delivery‚Äù, name=‚Äúpublic.menus_1‚Äù) orders = Dataset(namespace=‚Äúfood_delivery‚Äù, name=‚Äúpublic.orders_1‚Äù)  You can also create a job object (we‚Äôve borrowed this one from the existing food_delivery namespace): job = Job(namespace=‚Äúfood_delivery‚Äù, name=‚Äúexample.order_data‚Äù)  To create a run object you‚Äôll need to specify a unique ID: run = Run(str(uuid4()))  a START run event: client.emit( RunEvent( RunState.START, datetime.now().isoformat(), run, job, producer ) )  and, finally, a COMPLETE run event: client.emit( RunEvent( RunState.COMPLETE, datetime.now().isoformat(), run, job, producer, inputs=[inventory], outputs=[menus, orders], ) )  Now you have a complete script for creating datasets and a run event! Execute it in the terminal to send the metadata to Marquez: python3 generate_scripts.py  Marquez will update itself automatically, so the new job and datasets should now be visible in the UI. Clicking on the jobs icon (the icon with the three interlocking gears), will make the example.order_data job appear in the list of jobs:  When you click on the job, you will see a new map displaying the job, input and outputs we created with our script:  "},{"title":"Full Example Source Code‚Äã","type":1,"pageTitle":"Python","url":"/docs/client/python#full-example-source-code","content":"#!/usr/bin/env python3 from openlineage.client.run import ( RunEvent, RunState, Run, Job, Dataset, OutputDataset, InputDataset, ) from openlineage.client.client import OpenLineageClient, OpenLineageClientOptions from openlineage.client.facet import ( SqlJobFacet, SchemaDatasetFacet, SchemaField, OutputStatisticsOutputDatasetFacet, SourceCodeLocationJobFacet, NominalTimeRunFacet, DataQualityMetricsInputDatasetFacet, ColumnMetric, ) import uuid from datetime import datetime, timezone, timedelta import time from random import random PRODUCER = f&quot;https://github.com/openlineage-user&quot; namespace = &quot;python_client&quot; dag_name = &quot;user_trends&quot; url = &quot;http://mymarquez.host:5000&quot; api_key = &quot;1234567890ckcu028rzu5l&quot; client = OpenLineageClient( url=url, # optional api key in case marquez requires it. When running marquez in # your local environment, you usually do not need this. options=OpenLineageClientOptions(api_key=api_key), ) # generates job facet def job(job_name, sql, location): facets = {&quot;sql&quot;: SqlJobFacet(sql)} if location != None: facets.update( {&quot;sourceCodeLocation&quot;: SourceCodeLocationJobFacet(&quot;git&quot;, location)} ) return Job(namespace=namespace, name=job_name, facets=facets) # geneartes run racet def run(run_id, hour): return Run( runId=run_id, facets={ &quot;nominalTime&quot;: NominalTimeRunFacet( nominalStartTime=f&quot;2022-04-14T{twoDigits(hour)}:12:00Z&quot; ) }, ) # generates dataset def dataset(name, schema=None, ns=namespace): if schema == None: facets = {} else: facets = {&quot;schema&quot;: schema} return Dataset(namespace, name, facets) # generates output dataset def outputDataset(dataset, stats): output_facets = {&quot;stats&quot;: stats, &quot;outputStatistics&quot;: stats} return OutputDataset(dataset.namespace, dataset.name, dataset.facets, output_facets) # generates input dataset def inputDataset(dataset, dq): input_facets = { &quot;dataQuality&quot;: dq, } return InputDataset(dataset.namespace, dataset.name, dataset.facets, input_facets) def twoDigits(n): if n &lt; 10: result = f&quot;0{n}&quot; elif n &lt; 100: result = f&quot;{n}&quot; else: raise f&quot;error: {n}&quot; return result now = datetime.now(timezone.utc) # generates run Event def runEvents(job_name, sql, inputs, outputs, hour, min, location, duration): run_id = str(uuid.uuid4()) myjob = job(job_name, sql, location) myrun = run(run_id, hour) st = now + timedelta(hours=hour, minutes=min, seconds=20 + round(random() * 10)) end = st + timedelta(minutes=duration, seconds=20 + round(random() * 10)) started_at = st.isoformat() ended_at = end.isoformat() return ( RunEvent( eventType=RunState.START, eventTime=started_at, run=myrun, job=myjob, producer=PRODUCER, inputs=inputs, outputs=outputs, ), RunEvent( eventType=RunState.COMPLETE, eventTime=ended_at, run=myrun, job=myjob, producer=PRODUCER, inputs=inputs, outputs=outputs, ), ) # add run event to the events list def addRunEvents( events, job_name, sql, inputs, outputs, hour, minutes, location=None, duration=2 ): (start, complete) = runEvents( job_name, sql, inputs, outputs, hour, minutes, location, duration ) events.append(start) events.append(complete) events = [] # create dataset data for i in range(0, 5): user_counts = dataset(&quot;tmp_demo.user_counts&quot;) user_history = dataset( &quot;temp_demo.user_history&quot;, SchemaDatasetFacet( fields=[ SchemaField(name=&quot;id&quot;, type=&quot;BIGINT&quot;, description=&quot;the user id&quot;), SchemaField( name=&quot;email_domain&quot;, type=&quot;VARCHAR&quot;, description=&quot;the user id&quot; ), SchemaField(name=&quot;status&quot;, type=&quot;BIGINT&quot;, description=&quot;the user id&quot;), SchemaField( name=&quot;created_at&quot;, type=&quot;DATETIME&quot;, description=&quot;date and time of creation of the user&quot;, ), SchemaField( name=&quot;updated_at&quot;, type=&quot;DATETIME&quot;, description=&quot;the last time this row was updated&quot;, ), SchemaField( name=&quot;fetch_time_utc&quot;, type=&quot;DATETIME&quot;, description=&quot;the time the data was fetched&quot;, ), SchemaField( name=&quot;load_filename&quot;, type=&quot;VARCHAR&quot;, description=&quot;the original file this data was ingested from&quot;, ), SchemaField( name=&quot;load_filerow&quot;, type=&quot;INT&quot;, description=&quot;the row number in the original file&quot;, ), SchemaField( name=&quot;load_timestamp&quot;, type=&quot;DATETIME&quot;, description=&quot;the time the data was ingested&quot;, ), ] ), &quot;snowflake://&quot;, ) create_user_counts_sql = &quot;&quot;&quot;CREATE OR REPLACE TABLE TMP_DEMO.USER_COUNTS AS ( SELECT DATE_TRUNC(DAY, created_at) date, COUNT(id) as user_count FROM TMP_DEMO.USER_HISTORY GROUP BY date )&quot;&quot;&quot; # location of the source code location = &quot;https://github.com/some/airflow/dags/example/user_trends.py&quot; # run simulating Airflow DAG with snowflake operator addRunEvents( events, dag_name + &quot;.create_user_counts&quot;, create_user_counts_sql, [user_history], [user_counts], i, 11, location, ) for event in events: from openlineage.client.serde import Serde print(event) print(Serde.to_json(event)) # time.sleep(1) client.emit(event)  The resulting lineage events received by Marquez would look like this.  "},{"title":"Error Handling via Transport‚Äã","type":1,"pageTitle":"Java","url":"/docs/client/java#error-handling-via-transport","content":"// Connect to http://localhost:5000 OpenLineageClient client = OpenLineageClient.builder() .transport( HttpTransport.builder() .uri(&quot;http://localhost:5000&quot;) .apiKey(&quot;f38d2189-c603-4b46-bdea-e573a3b5a7d5&quot;) .build()) .registerErrorHandler(new EmitErrorHandler() { @Override public void handleError(Throwable throwable) { // Handle emit error here } }).build();  "},{"title":"Defining Your Own Transport‚Äã","type":1,"pageTitle":"Java","url":"/docs/client/java#defining-your-own-transport","content":"OpenLineageClient client = OpenLineageClient.builder() .transport( new MyTransport() { @Override public void emit(OpenLineage.RunEvent runEvent) { // Add emit logic here } }).build();  "},{"title":"Usage‚Äã","type":1,"pageTitle":"Java","url":"/docs/client/java#usage","content":""},{"title":"1. Simple OpenLineage Client Test for Console Transport‚Äã","type":1,"pageTitle":"Java","url":"/docs/client/java#1-simple-openlineage-client-test-for-console-transport","content":"First, let's explore how we can create OpenLineage client instance, but not using any actual transport to emit the data yet, except only to our Console. This would be a good exercise to run tests and check the data payloads.  OpenLineageClient client = OpenLineageClient.builder() .transport(new ConsoleTransport()).build();  Also, we will then get a sample payload to produce a RunEvent:  // create one start event for testing RunEvent event = buildEvent(EventType.START);  Lastly, we will emit this event using the client that we instantiated\\:  // emit the event client.emit(event);  Here is the full source code of the test client application: package ol.test; import io.openlineage.client.OpenLineage; import io.openlineage.client.OpenLineageClient; import io.openlineage.client.OpenLineage.RunEvent; import io.openlineage.client.OpenLineage.InputDataset; import io.openlineage.client.OpenLineage.Job; import io.openlineage.client.OpenLineage.JobFacets; import io.openlineage.client.OpenLineage.OutputDataset; import io.openlineage.client.OpenLineage.Run; import io.openlineage.client.OpenLineage.RunFacets; import io.openlineage.client.OpenLineage.RunEvent.EventType; import io.openlineage.client.transports.ConsoleTransport; import java.net.URI; import java.time.ZoneId; import java.time.ZonedDateTime; import java.util.Arrays; import java.util.List; import java.util.UUID; /** * My first openlinage client code */ public class OpenLineageClientTest { public static void main( String[] args ) { try { OpenLineageClient client = OpenLineageClient.builder() .transport(new ConsoleTransport()).build(); // create one start event for testing RunEvent event = buildEvent(EventType.START); // emit the event client.emit(event); } catch (Exception e) { e.printStackTrace(); } } // sample code to build event public static RunEvent buildEvent(EventType eventType) { ZonedDateTime now = ZonedDateTime.now(ZoneId.of(&quot;UTC&quot;)); URI producer = URI.create(&quot;producer&quot;); OpenLineage ol = new OpenLineage(producer); UUID runId = UUID.randomUUID(); // run facets RunFacets runFacets = ol.newRunFacetsBuilder() .nominalTime( ol.newNominalTimeRunFacetBuilder() .nominalStartTime(now) .nominalEndTime(now) .build()) .build(); // a run is composed of run id, and run facets Run run = ol.newRunBuilder().runId(runId).facets(runFacets).build(); // job facets JobFacets jobFacets = ol.newJobFacetsBuilder().build(); // job String name = &quot;jobName&quot;; String namespace = &quot;namespace&quot;; Job job = ol.newJobBuilder().namespace(namespace).name(name).facets(jobFacets).build(); // input dataset List&lt;InputDataset&gt; inputs = Arrays.asList( ol.newInputDatasetBuilder() .namespace(&quot;ins&quot;) .name(&quot;input&quot;) .facets( ol.newDatasetFacetsBuilder() .version(ol.newDatasetVersionDatasetFacet(&quot;input-version&quot;)) .build()) .inputFacets( ol.newInputDatasetInputFacetsBuilder() .dataQualityMetrics( ol.newDataQualityMetricsInputDatasetFacetBuilder() .rowCount(10L) .bytes(20L) .columnMetrics( ol.newDataQualityMetricsInputDatasetFacetColumnMetricsBuilder() .put( &quot;mycol&quot;, ol.newDataQualityMetricsInputDatasetFacetColumnMetricsAdditionalBuilder() .count(10D) .distinctCount(10L) .max(30D) .min(5D) .nullCount(1L) .sum(3000D) .quantiles( ol.newDataQualityMetricsInputDatasetFacetColumnMetricsAdditionalQuantilesBuilder() .put(&quot;25&quot;, 52D) .build()) .build()) .build()) .build()) .build()) .build()); // output dataset List&lt;OutputDataset&gt; outputs = Arrays.asList( ol.newOutputDatasetBuilder() .namespace(&quot;ons&quot;) .name(&quot;output&quot;) .facets( ol.newDatasetFacetsBuilder() .version(ol.newDatasetVersionDatasetFacet(&quot;output-version&quot;)) .build()) .outputFacets( ol.newOutputDatasetOutputFacetsBuilder() .outputStatistics(ol.newOutputStatisticsOutputDatasetFacet(10L, 20L)) .build()) .build()); // run state udpate which encapsulates all - with START event in this case RunEvent runStateUpdate = ol.newRunEventBuilder() .eventType(OpenLineage.RunEvent.EventType.START) .eventTime(now) .run(run) .job(job) .inputs(inputs) .outputs(outputs) .build(); return runStateUpdate; } }  The result of running this will result in the following output from your Java application: [main] INFO io.openlineage.client.transports.ConsoleTransport - {&quot;eventType&quot;:&quot;START&quot;,&quot;eventTime&quot;:&quot;2022-08-05T15:11:24.858414Z&quot;,&quot;run&quot;:{&quot;runId&quot;:&quot;bb46bbc4-fb1a-495a-ad3b-8d837f566749&quot;,&quot;facets&quot;:{&quot;nominalTime&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/NominalTimeRunFacet.json#/$defs/NominalTimeRunFacet&quot;,&quot;nominalStartTime&quot;:&quot;2022-08-05T15:11:24.858414Z&quot;,&quot;nominalEndTime&quot;:&quot;2022-08-05T15:11:24.858414Z&quot;}}},&quot;job&quot;:{&quot;namespace&quot;:&quot;namespace&quot;,&quot;name&quot;:&quot;jobName&quot;,&quot;facets&quot;:{}},&quot;inputs&quot;:[{&quot;namespace&quot;:&quot;ins&quot;,&quot;name&quot;:&quot;input&quot;,&quot;facets&quot;:{&quot;version&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasetVersionDatasetFacet.json#/$defs/DatasetVersionDatasetFacet&quot;,&quot;datasetVersion&quot;:&quot;input-version&quot;}},&quot;inputFacets&quot;:{&quot;dataQualityMetrics&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DataQualityMetricsInputDatasetFacet.json#/$defs/DataQualityMetricsInputDatasetFacet&quot;,&quot;rowCount&quot;:10,&quot;bytes&quot;:20,&quot;columnMetrics&quot;:{&quot;mycol&quot;:{&quot;nullCount&quot;:1,&quot;distinctCount&quot;:10,&quot;sum&quot;:3000.0,&quot;count&quot;:10.0,&quot;min&quot;:5.0,&quot;max&quot;:30.0,&quot;quantiles&quot;:{&quot;25&quot;:52.0}}}}}}],&quot;outputs&quot;:[{&quot;namespace&quot;:&quot;ons&quot;,&quot;name&quot;:&quot;output&quot;,&quot;facets&quot;:{&quot;version&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasetVersionDatasetFacet.json#/$defs/DatasetVersionDatasetFacet&quot;,&quot;datasetVersion&quot;:&quot;output-version&quot;}},&quot;outputFacets&quot;:{&quot;outputStatistics&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/OutputStatisticsOutputDatasetFacet.json#/$defs/OutputStatisticsOutputDatasetFacet&quot;,&quot;rowCount&quot;:10,&quot;size&quot;:20}}}],&quot;producer&quot;:&quot;producer&quot;,&quot;schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-2/OpenLineage.json#/$defs/RunEvent&quot;}  "},{"title":"2. Simple OpenLineage Client Test for Http Transport‚Äã","type":1,"pageTitle":"Java","url":"/docs/client/java#2-simple-openlineage-client-test-for-http-transport","content":"Now, using the same code base, we will change how the client application works by switching the Console transport into Http Transport as shown below. This code will now be able to send the OpenLineage events into a compatible backends such as Marquez. Before making this change and running it, make sure you have an instance of Marquez running on your local environment. Setting up and running Marquez can be found here. OpenLineageClient client = OpenLineageClient.builder() .transport( HttpTransport.builder() .uri(&quot;http://localhost:5000&quot;) .build()) .build();  If we ran the same application, you will now see the event data not emitted in the output console, but rather via the HTTP transport to the marquez backend that was running.  Notice that the Status of this job run will be in RUNNING state, as it will be in that state until it receives an end event that will close off its gaps. That is how the OpenLineage events would work. Now, let's change the previous example to have lineage event doing a complete cycle of START -&gt; COMPLETE: package ol.test; import io.openlineage.client.OpenLineage; import io.openlineage.client.OpenLineageClient; import io.openlineage.client.OpenLineage.RunEvent; import io.openlineage.client.OpenLineage.InputDataset; import io.openlineage.client.OpenLineage.Job; import io.openlineage.client.OpenLineage.JobFacets; import io.openlineage.client.OpenLineage.OutputDataset; import io.openlineage.client.OpenLineage.Run; import io.openlineage.client.OpenLineage.RunFacets; import io.openlineage.client.OpenLineage.RunEvent.EventType; import io.openlineage.client.transports.HttpTransport; import java.net.URI; import java.time.ZoneId; import java.time.ZonedDateTime; import java.util.Arrays; import java.util.List; import java.util.UUID; /** * My first openlinage client code */ public class OpenLineageClientTest { public static void main( String[] args ) { try { OpenLineageClient client = OpenLineageClient.builder() .transport( HttpTransport.builder() .uri(&quot;http://localhost:5000&quot;) .build()) .build(); // create one start event for testing RunEvent event = buildEvent(EventType.START, null); // emit the event client.emit(event); // another event to COMPLETE the run event = buildEvent(EventType.COMPLETE, event.getRun().getRunId()); // emit the second COMPLETE event client.emit(event); } catch (Exception e) { e.printStackTrace(); } } // sample code to build event public static RunEvent buildEvent(EventType eventType, UUID runId) { ZonedDateTime now = ZonedDateTime.now(ZoneId.of(&quot;UTC&quot;)); URI producer = URI.create(&quot;producer&quot;); OpenLineage ol = new OpenLineage(producer); if (runId == null) { runId = UUID.randomUUID(); } // run facets RunFacets runFacets = ol.newRunFacetsBuilder() .nominalTime( ol.newNominalTimeRunFacetBuilder() .nominalStartTime(now) .nominalEndTime(now) .build()) .build(); // a run is composed of run id, and run facets Run run = ol.newRunBuilder().runId(runId).facets(runFacets).build(); // job facets JobFacets jobFacets = ol.newJobFacetsBuilder().build(); // job String name = &quot;jobName&quot;; String namespace = &quot;namespace&quot;; Job job = ol.newJobBuilder().namespace(namespace).name(name).facets(jobFacets).build(); // input dataset List&lt;InputDataset&gt; inputs = Arrays.asList( ol.newInputDatasetBuilder() .namespace(&quot;ins&quot;) .name(&quot;input&quot;) .facets( ol.newDatasetFacetsBuilder() .version(ol.newDatasetVersionDatasetFacet(&quot;input-version&quot;)) .build()) .inputFacets( ol.newInputDatasetInputFacetsBuilder() .dataQualityMetrics( ol.newDataQualityMetricsInputDatasetFacetBuilder() .rowCount(10L) .bytes(20L) .columnMetrics( ol.newDataQualityMetricsInputDatasetFacetColumnMetricsBuilder() .put( &quot;mycol&quot;, ol.newDataQualityMetricsInputDatasetFacetColumnMetricsAdditionalBuilder() .count(10D) .distinctCount(10L) .max(30D) .min(5D) .nullCount(1L) .sum(3000D) .quantiles( ol.newDataQualityMetricsInputDatasetFacetColumnMetricsAdditionalQuantilesBuilder() .put(&quot;25&quot;, 52D) .build()) .build()) .build()) .build()) .build()) .build()); // output dataset List&lt;OutputDataset&gt; outputs = Arrays.asList( ol.newOutputDatasetBuilder() .namespace(&quot;ons&quot;) .name(&quot;output&quot;) .facets( ol.newDatasetFacetsBuilder() .version(ol.newDatasetVersionDatasetFacet(&quot;output-version&quot;)) .build()) .outputFacets( ol.newOutputDatasetOutputFacetsBuilder() .outputStatistics(ol.newOutputStatisticsOutputDatasetFacet(10L, 20L)) .build()) .build()); // run state udpate which encapsulates all - with START event in this case RunEvent runStateUpdate = ol.newRunEventBuilder() .eventType(eventType) .eventTime(now) .run(run) .job(job) .inputs(inputs) .outputs(outputs) .build(); return runStateUpdate; } }  Now, when you run this application, the Marquez would have an output that would looke like this:  "},{"title":"Developing With OpenLineage","type":0,"sectionRef":"#","url":"/docs/development/developing/","content":"","keywords":""},{"title":"Clients‚Äã","type":1,"pageTitle":"Developing With OpenLineage","url":"/docs/development/developing/#clients","content":"For Python and Java, we've created clients that you can use to properly create and emit OpenLineage events to HTTP, Kafka, and other consumers. "},{"title":"API Documentation‚Äã","type":1,"pageTitle":"Developing With OpenLineage","url":"/docs/development/developing/#api-documentation","content":"OpenAPI documentationJava Doc "},{"title":"Common Library (Python)‚Äã","type":1,"pageTitle":"Developing With OpenLineage","url":"/docs/development/developing/#common-library-python","content":"Getting lineage from systems like BigQuery or Redshift isn't necessarily tied to orchestrator or processing engine you're using. For this reason, we've extracted that functionality from our Airflow library and packaged it for separate use. "},{"title":"Environment Variables‚Äã","type":1,"pageTitle":"Developing With OpenLineage","url":"/docs/development/developing/#environment-variables","content":"The following environment variables are available commonly for both Java and Python languages. Name\tDescription\tSinceOPENLINEAGE_API_KEY\tThe optional API key to be set on each lineage request. This will be set as a Bearer token in case authentication is required. OPENLINEAGE_CONFIG\tThe optional path to locate the configuration file. The configuration file is in YAML format. Example: openlineage.yml OPENLINEAGE_DISABLED\tWhen set to true, will prevent OpenLineage from emitting events to the receiving backend\t0.9.0 OPENLINEAGE_URL\tThe URL for the HTTP transport of where to emit lineage events to. If not yet, no lineage data will be emitted, and event data (JSON) will be written to standard output. Example: http://localhost:8080\t "},{"title":"SQL parser‚Äã","type":1,"pageTitle":"Developing With OpenLineage","url":"/docs/development/developing/#sql-parser","content":"We've created SQL parser that allows you to extract lineage from SQL statements. The parser is implemented in Rust, however, it's also available as a Python library. You can take a look at it's documentation here or code on GitHub. "},{"title":"Setup a development environment","type":0,"sectionRef":"#","url":"/docs/development/developing/java/setup","content":"Setup a development environment info This page needs your contribution! Please contribute new examples using the edit link at the bottom.","keywords":""},{"title":"Setup a development environment","type":0,"sectionRef":"#","url":"/docs/development/developing/python/setup","content":"","keywords":""},{"title":"Docker Compose development environment‚Äã","type":1,"pageTitle":"Setup a development environment","url":"/docs/development/developing/python/setup#docker-compose-development-environment","content":"There is also possibility to create local Docker-based development environment that has OpenLineage libraries setup along with Airflow and some helpful services. To do that you should run run-dev-airflow.sh script located here. The script uses the same Docker Compose files as integration tests. Two main differences are: it runs in non-blocking wayit mounts OpenLineage Python packages as editable and mounted to Airflow containers. This allows to change code and test it live without need to rebuild whole environment. When using above script, you can add the -i flag or --attach-integration flag. This can be helpful when you need to run arbitrary integration tests during development. For example, the following command run in the integration container... python -m pytest test_integration.py::test_integration[great_expectations_validation-requests/great_expectations.json]  ...runs a single test which you can repeat after changes in code. "},{"title":"Logging","type":0,"sectionRef":"#","url":"/docs/development/developing/java/troubleshooting/logging","content":"","keywords":""},{"title":"Maven‚Äã","type":1,"pageTitle":"Logging","url":"/docs/development/developing/java/troubleshooting/logging#maven","content":"pom.xml  &lt;dependencies&gt; ... &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-api&lt;/artifactId&gt; &lt;version&gt;2.7&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.7&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-slf4j-impl&lt;/artifactId&gt; &lt;version&gt;2.7&lt;/version&gt; &lt;/dependency&gt; ... &lt;/dependencies&gt;  "},{"title":"Gradle‚Äã","type":1,"pageTitle":"Logging","url":"/docs/development/developing/java/troubleshooting/logging#gradle","content":"build.gradle dependencies { ... implementation &quot;org.apache.logging.log4j:log4j-api:2.7&quot; implementation &quot;org.apache.logging.log4j:log4j-core:2.7&quot; implementation &quot;org.apache.logging.log4j:log4j-slf4j-impl:2.7&quot; ... }  You also need to create a log4j configuration file, log4j2.properties on the classpath. Here is the sample log configuration. # Set to debug or trace if log4j initialization is failing status = warn # Name of the configuration name = ConsoleLogConfigDemo # Console appender configuration appender.console.type = Console appender.console.name = consoleLogger appender.console.layout.type = PatternLayout appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n # Root logger level rootLogger.level = debug # Root logger referring to console appender rootLogger.appenderRef.stdout.ref = consoleLogger  Re-compiling and running the ol.test.OpenLineageClientTest again will produce the following outputs: 2022-12-07 08:57:24 INFO OpenLineageClientTest:33 - Running OpenLineage Client Test... 2022-12-07 08:57:25 DEBUG HttpTransport:96 - POST http://localhost:5000/api/v1/lineage: {&quot;eventType&quot;:&quot;START&quot;,&quot;eventTime&quot;:&quot;2022-12-07T14:57:25.072781Z&quot;,&quot;run&quot;:{&quot;runId&quot;:&quot;0142c998-3416-49e7-92aa-d025c4c93697&quot;,&quot;facets&quot;:{&quot;nominalTime&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/NominalTimeRunFacet.json#/$defs/NominalTimeRunFacet&quot;,&quot;nominalStartTime&quot;:&quot;2022-12-07T14:57:25.072781Z&quot;,&quot;nominalEndTime&quot;:&quot;2022-12-07T14:57:25.072781Z&quot;}}},&quot;job&quot;:{&quot;namespace&quot;:&quot;namespace&quot;,&quot;name&quot;:&quot;jobName&quot;,&quot;facets&quot;:{}},&quot;inputs&quot;:[{&quot;namespace&quot;:&quot;ins&quot;,&quot;name&quot;:&quot;input&quot;,&quot;facets&quot;:{&quot;version&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasetVersionDatasetFacet.json#/$defs/DatasetVersionDatasetFacet&quot;,&quot;datasetVersion&quot;:&quot;input-version&quot;}},&quot;inputFacets&quot;:{&quot;dataQualityMetrics&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DataQualityMetricsInputDatasetFacet.json#/$defs/DataQualityMetricsInputDatasetFacet&quot;,&quot;rowCount&quot;:10,&quot;bytes&quot;:20,&quot;columnMetrics&quot;:{&quot;mycol&quot;:{&quot;nullCount&quot;:1,&quot;distinctCount&quot;:10,&quot;sum&quot;:3000.0,&quot;count&quot;:10.0,&quot;min&quot;:5.0,&quot;max&quot;:30.0,&quot;quantiles&quot;:{&quot;25&quot;:52.0}}}}}}],&quot;outputs&quot;:[{&quot;namespace&quot;:&quot;ons&quot;,&quot;name&quot;:&quot;output&quot;,&quot;facets&quot;:{&quot;version&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasetVersionDatasetFacet.json#/$defs/DatasetVersionDatasetFacet&quot;,&quot;datasetVersion&quot;:&quot;output-version&quot;}},&quot;outputFacets&quot;:{&quot;outputStatistics&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/OutputStatisticsOutputDatasetFacet.json#/$defs/OutputStatisticsOutputDatasetFacet&quot;,&quot;rowCount&quot;:10,&quot;size&quot;:20}}}],&quot;producer&quot;:&quot;producer&quot;,&quot;schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-4/OpenLineage.json#/$defs/RunEvent&quot;} 2022-12-07 08:57:25 DEBUG HttpTransport:96 - POST http://localhost:5000/api/v1/lineage: {&quot;eventType&quot;:&quot;COMPLETE&quot;,&quot;eventTime&quot;:&quot;2022-12-07T14:57:25.42041Z&quot;,&quot;run&quot;:{&quot;runId&quot;:&quot;0142c998-3416-49e7-92aa-d025c4c93697&quot;,&quot;facets&quot;:{&quot;nominalTime&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/NominalTimeRunFacet.json#/$defs/NominalTimeRunFacet&quot;,&quot;nominalStartTime&quot;:&quot;2022-12-07T14:57:25.42041Z&quot;,&quot;nominalEndTime&quot;:&quot;2022-12-07T14:57:25.42041Z&quot;}}},&quot;job&quot;:{&quot;namespace&quot;:&quot;namespace&quot;,&quot;name&quot;:&quot;jobName&quot;,&quot;facets&quot;:{}},&quot;inputs&quot;:[{&quot;namespace&quot;:&quot;ins&quot;,&quot;name&quot;:&quot;input&quot;,&quot;facets&quot;:{&quot;version&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasetVersionDatasetFacet.json#/$defs/DatasetVersionDatasetFacet&quot;,&quot;datasetVersion&quot;:&quot;input-version&quot;}},&quot;inputFacets&quot;:{&quot;dataQualityMetrics&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DataQualityMetricsInputDatasetFacet.json#/$defs/DataQualityMetricsInputDatasetFacet&quot;,&quot;rowCount&quot;:10,&quot;bytes&quot;:20,&quot;columnMetrics&quot;:{&quot;mycol&quot;:{&quot;nullCount&quot;:1,&quot;distinctCount&quot;:10,&quot;sum&quot;:3000.0,&quot;count&quot;:10.0,&quot;min&quot;:5.0,&quot;max&quot;:30.0,&quot;quantiles&quot;:{&quot;25&quot;:52.0}}}}}}],&quot;outputs&quot;:[{&quot;namespace&quot;:&quot;ons&quot;,&quot;name&quot;:&quot;output&quot;,&quot;facets&quot;:{&quot;version&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasetVersionDatasetFacet.json#/$defs/DatasetVersionDatasetFacet&quot;,&quot;datasetVersion&quot;:&quot;output-version&quot;}},&quot;outputFacets&quot;:{&quot;outputStatistics&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/OutputStatisticsOutputDatasetFacet.json#/$defs/OutputStatisticsOutputDatasetFacet&quot;,&quot;rowCount&quot;:10,&quot;size&quot;:20}}}],&quot;producer&quot;:&quot;producer&quot;,&quot;schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-4/OpenLineage.json#/$defs/RunEvent&quot;}  Logs will also produce meaningful error messages when something does not work correctly. For example, if the backend server does not exist, you would get the following messages in your console output: 2022-12-07 09:15:16 INFO OpenLineageClientTest:33 - Running OpenLineage Client Test... 2022-12-07 09:15:16 DEBUG HttpTransport:96 - POST http://localhost:5000/api/v1/lineage: {&quot;eventType&quot;:&quot;START&quot;,&quot;eventTime&quot;:&quot;2022-12-07T15:15:16.668979Z&quot;,&quot;run&quot;:{&quot;runId&quot;:&quot;69861937-55ba-43f5-ab5e-fe78ef6a283d&quot;,&quot;facets&quot;:{&quot;nominalTime&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/NominalTimeRunFacet.json#/$defs/NominalTimeRunFacet&quot;,&quot;nominalStartTime&quot;:&quot;2022-12-07T15:15:16.668979Z&quot;,&quot;nominalEndTime&quot;:&quot;2022-12-07T15:15:16.668979Z&quot;}}},&quot;job&quot;:{&quot;namespace&quot;:&quot;namespace&quot;,&quot;name&quot;:&quot;jobName&quot;,&quot;facets&quot;:{}},&quot;inputs&quot;:[{&quot;namespace&quot;:&quot;ins&quot;,&quot;name&quot;:&quot;input&quot;,&quot;facets&quot;:{&quot;version&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasetVersionDatasetFacet.json#/$defs/DatasetVersionDatasetFacet&quot;,&quot;datasetVersion&quot;:&quot;input-version&quot;}},&quot;inputFacets&quot;:{&quot;dataQualityMetrics&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DataQualityMetricsInputDatasetFacet.json#/$defs/DataQualityMetricsInputDatasetFacet&quot;,&quot;rowCount&quot;:10,&quot;bytes&quot;:20,&quot;columnMetrics&quot;:{&quot;mycol&quot;:{&quot;nullCount&quot;:1,&quot;distinctCount&quot;:10,&quot;sum&quot;:3000.0,&quot;count&quot;:10.0,&quot;min&quot;:5.0,&quot;max&quot;:30.0,&quot;quantiles&quot;:{&quot;25&quot;:52.0}}}}}}],&quot;outputs&quot;:[{&quot;namespace&quot;:&quot;ons&quot;,&quot;name&quot;:&quot;output&quot;,&quot;facets&quot;:{&quot;version&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasetVersionDatasetFacet.json#/$defs/DatasetVersionDatasetFacet&quot;,&quot;datasetVersion&quot;:&quot;output-version&quot;}},&quot;outputFacets&quot;:{&quot;outputStatistics&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/OutputStatisticsOutputDatasetFacet.json#/$defs/OutputStatisticsOutputDatasetFacet&quot;,&quot;rowCount&quot;:10,&quot;size&quot;:20}}}],&quot;producer&quot;:&quot;producer&quot;,&quot;schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-4/OpenLineage.json#/$defs/RunEvent&quot;} io.openlineage.client.OpenLineageClientException: org.apache.http.conn.HttpHostConnectException: Connect to localhost:5000 [localhost/127.0.0.1, localhost/0:0:0:0:0:0:0:1] failed: Connection refused at io.openlineage.client.transports.HttpTransport.emit(HttpTransport.java:113) at io.openlineage.client.OpenLineageClient.emit(OpenLineageClient.java:42) at ol.test.OpenLineageClientTest.main(OpenLineageClientTest.java:48) Caused by: org.apache.http.conn.HttpHostConnectException: Connect to localhost:5000 [localhost/127.0.0.1, localhost/0:0:0:0:0:0:0:1] failed: Connection refused at org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:156) at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:376) at org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393) at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236) at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186) at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:89) at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110) at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:108) at io.openlineage.client.transports.HttpTransport.emit(HttpTransport.java:108) ... 2 more Caused by: java.net.ConnectException: Connection refused at java.base/sun.nio.ch.Net.pollConnect(Native Method) at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672) at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:542) at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:585) at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327) at java.base/java.net.Socket.connect(Socket.java:666) at org.apache.http.conn.socket.PlainConnectionSocketFactory.connectSocket(PlainConnectionSocketFactory.java:75) at org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142) ... 12 more  If you wish to output loggigng message to a file, you can modify the basic configuration by adding a file appender configuration as follows: # Set to debug or trace if log4j initialization is failing status = warn # Name of the configuration name = ConsoleLogConfigDemo # Console appender configuration appender.console.type = Console appender.console.name = consoleLogger appender.console.layout.type = PatternLayout appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n # File appender configuration appender.file.type = File appender.file.name = fileLogger appender.file.fileName = app.log appender.file.layout.type = PatternLayout appender.file.layout.pattern = %d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n # Root logger level rootLogger.level = debug # Root logger referring to console appender rootLogger.appenderRef.stdout.ref = consoleLogger rootLogger.appenderRef.file.ref = fileLogger  And the logs will be saved to a file app.log. Outputting logs using log4j2 is just one way of doing it, so below are some additional resources of undersatnding how Java logging works, and other ways to output the logs. "},{"title":"Further readings‚Äã","type":1,"pageTitle":"Logging","url":"/docs/development/developing/java/troubleshooting/logging#further-readings","content":"https://www.baeldung.com/java-logging-introhttps://www.baeldung.com/slf4j-with-log4j2-logback#Log4j2https://mkyong.com/logging/log4j2-properties-example/ "},{"title":"Client","type":0,"sectionRef":"#","url":"/docs/development/developing/python/tests/client","content":"Client info This page needs your contribution! Please contribute new examples using the edit link at the bottom. There are unit tests available for OpenLineage Python client. You can run them with a simple pytest command with directory set to client base path.","keywords":""},{"title":"Airflow","type":0,"sectionRef":"#","url":"/docs/development/developing/python/tests/airflow","content":"","keywords":""},{"title":"Unit tests‚Äã","type":1,"pageTitle":"Airflow","url":"/docs/development/developing/python/tests/airflow#unit-tests","content":"In order to make running unit tests against multiple Airflow versions easier there is possibility to use tox. To run unit tests against all configured Airflow versions just run: tox  You can also list existing environments with: tox -l  that should list: py3-airflow-2.1.4 py3-airflow-2.2.4 py3-airflow-2.3.4 py3-airflow-2.4.3 py3-airflow.2.5.0  Then you can run tests in chosen environment, e.g.: tox -e py3-airflow-2.3.4  setup.cfg contains tox-related configuration. By default tox command runs: flake8 lintingpytest command Additionally, outside of tox you should run mypy static code analysis. You can do that with: python -m mypy openlineage  "},{"title":"Integration tests‚Äã","type":1,"pageTitle":"Airflow","url":"/docs/development/developing/python/tests/airflow#integration-tests","content":"Integration tests are located in tests/integration/tests directory. They require running Docker containers to provision local test environment: Airflow components (worker, scheduler), databases (PostgreSQL, MySQL) and OpenLineage events consumer. How to run‚Äã Integration tests require usage of docker compose. There are scripts prepared to make build images and run tests easier. AIRFLOW_IMAGE=&lt;name-of-airflow-image&gt; ./tests/integration/docker/up.sh  e.g. AIRFLOW_IMAGE=apache/airflow:2.3.4-python3.7 ./tests/integration/docker/up.sh  What tests are ran‚Äã The actual setup is to run all defined Airflow DAGs, collect OpenLineage events and check if they meet requirements. The test you should pay most attention to is test_integration. It compares produced events to expected JSON structures recursively, with a respect if fields are not missing. Some of the tests are skipped if database connection specific environment variables are not set. The example is set of SNOWFLAKE_PASSWORD and SNOWFLAKE_ACCOUNT_ID variables. View stored OpenLineage events‚Äã OpenLineage events produced from Airflow runs are stored locally in ./tests/integration/tests/events directory. The files are not overwritten, rather new events are appended to existing files. Example how to add new integration test‚Äã Let's take following CustomOperator for which we should add CustomExtractor and test it. First we create DAG in integration tests DAGs folder: airflow/tests/integration/tests/airflow/dags. from airflow.models import BaseOperator from airflow.utils.dates import days_ago from airflow import DAG default_args = { 'depends_on_past': False, 'start_date': days_ago(7) } dag = DAG( 'custom_extractor', schedule_interval='@once', default_args=default_args ) class CustomOperator(BaseOperator): def execute(self, context: Any): for i in range(10): print(i) t1 = CustomOperator( task_id='custom_extractor', dag=dag )  In the same folder we create custom_extractor.py: from typing import Union, Optional, List from openlineage.client.run import Dataset from openlineage.airflow.extractors import TaskMetadata from openlineage.airflow.extractors.base import BaseExtractor class CustomExtractor(BaseExtractor): @classmethod def get_operator_classnames(cls) -&gt; List[str]: return ['CustomOperator'] def extract(self) -&gt; Union[Optional[TaskMetadata], List[TaskMetadata]]: return TaskMetadata( &quot;test&quot;, inputs=[ Dataset( namespace=&quot;test&quot;, name=&quot;dataset&quot;, facets={} ) ] )  Typically we want to compare produced metadata against expected. In order to do that we create JSON file custom_extractor.json in airflow/tests/integration/requests:  [{ &quot;eventType&quot;: &quot;START&quot;, &quot;inputs&quot;: [{ &quot;facets&quot;: {}, &quot;name&quot;: &quot;dataset&quot;, &quot;namespace&quot;: &quot;test&quot; }], &quot;job&quot;: { &quot;facets&quot;: { &quot;documentation&quot;: { &quot;description&quot;: &quot;Test dag.&quot; } }, &quot;name&quot;: &quot;custom_extractor.custom_extractor&quot;, &quot;namespace&quot;: &quot;food_delivery&quot; }, &quot;run&quot;: { &quot;facets&quot;: { &quot;airflow_runArgs&quot;: { &quot;externalTrigger&quot;: false }, &quot;parent&quot;: { &quot;job&quot;: { &quot;name&quot;: &quot;custom_extractor&quot;, &quot;namespace&quot;: &quot;food_delivery&quot; } } } } }, { &quot;eventType&quot;: &quot;COMPLETE&quot;, &quot;inputs&quot;: [{ &quot;facets&quot;: {}, &quot;name&quot;: &quot;dataset&quot;, &quot;namespace&quot;: &quot;test&quot; }], &quot;job&quot;: { &quot;facets&quot;: {}, &quot;name&quot;: &quot;custom_extractor.custom_extractor&quot;, &quot;namespace&quot;: &quot;food_delivery&quot; } } ]  and add parameter for test_integration in airflow/tests/integration/test_integration.py: (&quot;source_code_dag&quot;, &quot;requests/source_code.json&quot;), + (&quot;custom_extractor&quot;, &quot;requests/custom_extractor.json&quot;), (&quot;unknown_operator_dag&quot;, &quot;requests/unknown_operator.json&quot;),  That should setup a check for existence of both START and COMPLETE events, custom input facet and correct job facet. Full example can be found in source code available in integration tests directory. "},{"title":"Common","type":0,"sectionRef":"#","url":"/docs/development/developing/python/tests/common","content":"Common info This page needs your contribution! Please contribute new examples using the edit link at the bottom. There are unit tests available for OpenLineage common package. You can run them with a simple pytest command with directory set to package base path.","keywords":""},{"title":"Dagster","type":0,"sectionRef":"#","url":"/docs/development/developing/python/tests/dagster","content":"Dagster info This page needs your contribution! Please contribute new examples using the edit link at the bottom. There are unit tests available for Dagster integration. You can run them with a simple pytest command with directory set to integration base path.","keywords":""},{"title":"Example Lineage Events","type":0,"sectionRef":"#","url":"/docs/development/examples","content":"","keywords":""},{"title":"Simple Examples‚Äã","type":1,"pageTitle":"Example Lineage Events","url":"/docs/development/examples#simple-examples","content":""},{"title":"START event with single input‚Äã","type":1,"pageTitle":"Example Lineage Events","url":"/docs/development/examples#start-event-with-single-input","content":"This is a START event with a single PostgreSQL input dataset. { &quot;eventType&quot;: &quot;START&quot;, &quot;eventTime&quot;: &quot;2020-12-28T19:52:00.001+10:00&quot;, &quot;run&quot;: { &quot;runId&quot;: &quot;d46e465b-d358-4d32-83d4-df660ff614dd&quot; }, &quot;job&quot;: { &quot;namespace&quot;: &quot;workshop&quot;, &quot;name&quot;: &quot;process_taxes&quot; }, &quot;inputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://workshop-db:None&quot;, &quot;name&quot;: &quot;workshop.public.taxes&quot; }], &quot;producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client&quot; }  "},{"title":"COMPLETE event with single output‚Äã","type":1,"pageTitle":"Example Lineage Events","url":"/docs/development/examples#complete-event-with-single-output","content":"This is a COMPLETE event with a single PostgreSQL output dataset. { &quot;eventType&quot;: &quot;COMPLETE&quot;, &quot;eventTime&quot;: &quot;2020-12-28T20:52:00.001+10:00&quot;, &quot;run&quot;: { &quot;runId&quot;: &quot;d46e465b-d358-4d32-83d4-df660ff614dd&quot; }, &quot;job&quot;: { &quot;namespace&quot;: &quot;workshop&quot;, &quot;name&quot;: &quot;process_taxes&quot; }, &quot;outputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://workshop-db:None&quot;, &quot;name&quot;: &quot;workshop.public.unpaid_taxes&quot; }], &quot;producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client&quot; }  "},{"title":"Complex Examples‚Äã","type":1,"pageTitle":"Example Lineage Events","url":"/docs/development/examples#complex-examples","content":""},{"title":"START event with Facets (run and job)‚Äã","type":1,"pageTitle":"Example Lineage Events","url":"/docs/development/examples#start-event-with-facets-run-and-job","content":"This is a START event with run and job facets of Apache Airflow. { &quot;eventType&quot;: &quot;START&quot;, &quot;eventTime&quot;: &quot;2020-12-28T19:52:00.001+10:00&quot;, &quot;run&quot;: { &quot;runId&quot;: &quot;d46e465b-d358-4d32-83d4-df660ff614dd&quot; &quot;facets&quot;: { &quot;airflow_runArgs&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.10.0/integration/airflow&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/BaseFacet&quot;, &quot;externalTrigger&quot;: true }, &quot;nominalTime&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.10.0/integration/airflow&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/NominalTimeRunFacet&quot;, &quot;nominalStartTime&quot;: &quot;2022-07-29T14:14:31.458067Z&quot; }, &quot;parentRun&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.10.0/integration/airflow&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/ParentRunFacet&quot;, &quot;job&quot;: { &quot;name&quot;: &quot;etl_orders&quot;, &quot;namespace&quot;: &quot;cosmic_energy&quot; }, &quot;run&quot;: { &quot;runId&quot;: &quot;1ba6fdaa-fb80-36ce-9c5b-295f544ec462&quot; } } } }, &quot;job&quot;: { &quot;namespace&quot;: &quot;workshop&quot;, &quot;name&quot;: &quot;process_taxes&quot;, &quot;facets&quot;: { &quot;documentation&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.10.0/integration/airflow&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/DocumentationJobFacet&quot;, &quot;description&quot;: &quot;Process taxes.&quot; }, &quot;sql&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.10.0/integration/airflow&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/SqlJobFacet&quot;, &quot;query&quot;: &quot;INSERT into taxes values(1, 100, 1000, 4000);&quot; } }, }, &quot;inputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://workshop-db:None&quot;, &quot;name&quot;: &quot;workshop.public.taxes&quot; }], &quot;producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client&quot; }  "},{"title":"COMPLETE event with Facets (dataset)‚Äã","type":1,"pageTitle":"Example Lineage Events","url":"/docs/development/examples#complete-event-with-facets-dataset","content":"This is a COMPLETE event with dataset facet of Database table. { &quot;eventType&quot;: &quot;COMPLETE&quot;, &quot;eventTime&quot;: &quot;2020-12-28T20:52:00.001+10:00&quot;, &quot;run&quot;: { &quot;runId&quot;: &quot;d46e465b-d358-4d32-83d4-df660ff614dd&quot; }, &quot;job&quot;: { &quot;namespace&quot;: &quot;workshop&quot;, &quot;name&quot;: &quot;process_taxes&quot; }, &quot;outputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://workshop-db:None&quot;, &quot;name&quot;: &quot;workshop.public.unpaid_taxes&quot;, &quot;facets&quot;: { &quot;dataSource&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.10.0/integration/airflow&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/DataSourceDatasetFacet&quot;, &quot;name&quot;: &quot;postgres://workshop-db:None&quot;, &quot;uri&quot;: &quot;workshop-db&quot; }, &quot;schema&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.10.0/integration/airflow&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/SchemaDatasetFacet&quot;, &quot;fields&quot;: [ { &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;SERIAL PRIMARY KEY&quot; }, { &quot;name&quot;: &quot;tax_dt&quot;, &quot;type&quot;: &quot;TIMESTAMP NOT NULL&quot; }, { &quot;name&quot;: &quot;tax_item_id&quot;, &quot;type&quot;: &quot;INTEGER REFERENCES tax_itemsid&quot; }, { &quot;name&quot;: &quot;amount&quot;, &quot;type&quot;: &quot;INTEGER NOT NULL&quot; }, { &quot;name&quot;: &quot;ref_id&quot;, &quot;type&quot;: &quot;INTEGER REFERENCES refid&quot; }, { &quot;name&quot;: &quot;comment&quot;, &quot;type&quot;: &quot;TEXT&quot; } ] } } }], &quot;producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client&quot; }  "},{"title":"Logging","type":0,"sectionRef":"#","url":"/docs/development/developing/python/troubleshooting/logging","content":"","keywords":""},{"title":"Further readings‚Äã","type":1,"pageTitle":"Logging","url":"/docs/development/developing/python/troubleshooting/logging#further-readings","content":"https://docs.python.org/3/library/logging.htmlhttps://realpython.com/python-logging/ "},{"title":"OpenLineage Proxy","type":0,"sectionRef":"#","url":"/docs/development/ol-proxy","content":"","keywords":""},{"title":"Accessing the proxy‚Äã","type":1,"pageTitle":"OpenLineage Proxy","url":"/docs/development/ol-proxy#accessing-the-proxy","content":"OpenLineage proxy can be obtained via github: git clone https://github.com/OpenLineage/OpenLineage.git cd OpenLineage/proxy  "},{"title":"Building the proxy‚Äã","type":1,"pageTitle":"OpenLineage Proxy","url":"/docs/development/ol-proxy#building-the-proxy","content":"To build the proxy jar, run $ ./gradlew build  The packaged jar file can be found under ./build/libs/ "},{"title":"Running the proxy‚Äã","type":1,"pageTitle":"OpenLineage Proxy","url":"/docs/development/ol-proxy#running-the-proxy","content":"OpenLineage Proxy requires configuration file named proxy.yml. There is an example that you can copy and name it as proxy.yml. cp proxy.example.yml proxy.yml  By default, the OpenLineage proxy uses the following ports: TCP port 8080 is available for the HTTP API server.TCP port 8081 is available for the admin interface. You can then run the proxy using gradlew: $ ./gradlew runShadow  "},{"title":"Monitoring OpenLineage events via Proxy‚Äã","type":1,"pageTitle":"OpenLineage Proxy","url":"/docs/development/ol-proxy#monitoring-openlineage-events-via-proxy","content":"When proxy is running, you can start sending your OpenLineage events just as the same way as you would be sending to any OpenLineage backend server. For example, in your URL for the OpenLineage backend, you can specify it as http://localhost:8080/api/v1/lineage. Once the message is sent to the proxy, you will see the OpenLineage message content (JSON) to the console output of the proxy. You can also specify in the configuration to store the messages into the log file. You might have noticed that OpenLineage client (python, java) simply requires http://localhost:8080 as the URL endpoint. This is possible because the client code adds the /api/v1/lineage internally before it makes the request. If you are not using OpenLineage client library to emit OpenLineage events, you must use the full URL in order for the proxy to receive the data correctly. "},{"title":"Forwarding the data‚Äã","type":1,"pageTitle":"OpenLineage Proxy","url":"/docs/development/ol-proxy#forwarding-the-data","content":"Not only the OpenLineage proxy is useful in receiving the monitoring the OpenLineage events, it can also be used to relay the events to other endpoints. Please see the example of how to set the proxy to relay the events via Kafka topic or HTTP endpoint. "},{"title":"Other ways to run OpenLineage Proxy‚Äã","type":1,"pageTitle":"OpenLineage Proxy","url":"/docs/development/ol-proxy#other-ways-to-run-openlineage-proxy","content":"You do not have to clone the git repo and build all the time. OpenLineage proxy is published and available in Maven Repository.You can also run OpenLineage Proxy as a docker container.There is also a helm chart for Kubernetes available. "},{"title":"SQL parser","type":0,"sectionRef":"#","url":"/docs/development/sql","content":"","keywords":""},{"title":"Interface‚Äã","type":1,"pageTitle":"SQL parser","url":"/docs/development/sql#interface","content":"SQL parser interface expressed in pseudo-python. class DbTableMeta: database: Optional[str] schema: Optional[str] name: str class SqlMeta: in_tables: List[DbTableMeta] out_tables: List[DbTableMeta] def parse( sql: Union[List[str], str], # Setting dialect allows you to enable some dialect-specific processing # like using backticks &quot;`&quot; as delimiters in BigQuery tables. dialect: Optional[str] = None, # Setting this will make parser use this schema for every table that # does not specify schema. default_schema: Optional[str] = None ) -&gt; Optional[SqlMeta]  "},{"title":"SQL dialects‚Äã","type":1,"pageTitle":"SQL parser","url":"/docs/development/sql#sql-dialects","content":"Optional dialect can be specified when using the parser to specify a specific flavor of SQL statement that is required to be parsed. The following dialects are currently available: ansibigqueryhivemssqlmysqlpostgrespostgresqlredshiftsnowflakesqlite If no dialect is specified, the dialect defaults to generic which parses generic SQL statements. "},{"title":"Default databases and schemas‚Äã","type":1,"pageTitle":"SQL parser","url":"/docs/development/sql#default-databases-and-schemas","content":"SQL processing engines and databases sometimes rely on some implicit information. For example, they often allow you to set current database or schema, instead of forcing you to specify fully-qualified table name every time you're refering to it. For this reason, bare SQL parser might be insufficient to fully understand which tables the query refers to. We recommend to process the data that you acquired from SQL parser to take that into account. "},{"title":"Usage‚Äã","type":1,"pageTitle":"SQL parser","url":"/docs/development/sql#usage","content":"OpenLineage SQL parser is available as part of the integrations that contains extractors that need to parse SQL statements. An example would be Apache Airflow integrations which houses multiple operators that executes SQL statements. SQL parser is included as part of the library for such integrations (e.g. integration-airflow). However, you can explicitly install and use the SQL parser via pip: pip install openlineage-sql  For details about using the SQL parser, please refer to its git README There is also a python tester script here which you can use to run parsing tests against any arbitrary SQL statements to verify whether the SQL parser can properly parse them. "},{"title":"Frequently Asked Questions","type":0,"sectionRef":"#","url":"/docs/faq","content":"","keywords":""},{"title":"Is OpenLineage a metadata server?‚Äã","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#is-openlineage-a-metadata-server","content":"No. OpenLineage is, at its core, a specification for lineage metadata. But it also contains a collection of integrations, examples, and tools. If you are looking for a metadata server that can receive and analyze OpenLineage events, check out Marquez. "},{"title":"Is there room for another question on this page?‚Äã","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#is-there-room-for-another-question-on-this-page","content":"You bet! There's always room. Submit an issue or pull request using the edit button at the bottom. "},{"title":"About These Guides","type":0,"sectionRef":"#","url":"/docs/guides/about","content":"About These Guides The following tutorials take you through the process of exploiting the lineage metadata provided by Marquez and OpenLineage to solve common data engineering problems and make new analytical and historical insights into your pipelines. The first tutorial, &quot;Using OpenLineage with Spark,&quot; provides an introduction to OpenLineage's integration with Apache Spark. You will learn how to use Marquez and the OpenLineage standard to produce lineage metadata about jobs and datasets created using Spark and BigQuery in a Jupyter notebook environment. The second tutorial, &quot;Using OpenLineage with Airflow,&quot; shows you how to use OpenLineage on Apache Airflow to produce data lineage on supported operators to emit lineage events to Marquez backend. The tutorial also introduces you to the OpenLineage proxy to monitor the event data being emitted. The third tutorial, &quot;Backfilling Airflow DAGs Using Marquez,&quot; shows you how to use Marquez's Airflow integration and the Marquez CLI to backfill failing runs with the help of lineage metadata. You will learn how data lineage can be used to automate the backfilling process. The fourth tutorial, &quot;Using Marquez with dbt,&quot; takes you through the process of setting up Marquez's dbt integration to harvest metadata produced by dbt. You will learn how to create a Marquez instance, install the integration, configure your dbt installation, and test the configuration using dbt.","keywords":""},{"title":"Getting Started","type":0,"sectionRef":"#","url":"/docs/getting-started","content":"","keywords":""},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"Getting Started","url":"/docs/getting-started#prerequisites","content":"Before you begin, make sure you have installed: Docker 17.05+Docker Compose info In this guide, we'll be using Marquez as the OpenLineage HTTP backend and running the HTTP server via Docker. "},{"title":"Run Marquez with Docker‚Äã","type":1,"pageTitle":"Getting Started","url":"/docs/getting-started#run-marquez-with-docker","content":"The easiest way to get up and running with Marquez is Docker. Check out the Marquez source code and run the ./docker/up.sh script: $ git clone git@github.com:MarquezProject/marquez.git &amp;&amp; cd marquez $ ./docker/up.sh  info Pass the --build flag to the script to build images from source, or --tag X.Y.Z to use a tagged image. To view the Marquez UI and verify it's running, open http://localhost:3000. The UI enables you to discover dependencies between jobs and the datasets they produce and consume via the lineage graph, view run-level metadata of current and previous job runs, and much more. "},{"title":"Collect Run-Level Metadata‚Äã","type":1,"pageTitle":"Getting Started","url":"/docs/getting-started#collect-run-level-metadata","content":"Marquez is an LF AI &amp; DATA incubation project to collect, aggregate, and visualize a data ecosystem‚Äôs metadata. Marquez is the reference implementation of the OpenLineage standard. In this example, we show how you can collect dataset and job metadata using Marquez, Using the LineageAPI. When you submit a lineage event, you first need to define an unique run ID that would look something like d46e465b-d358-4d32-83d4-df660ff614dd. This is usually in UUID format, and should be unique. This run ID will enable the tracking of run-level metadata over time for a job which may have a name, like my-job. So, let's get started! info The example shows how to collect metadata via direct HTTP API calls using curl. But, you can also get started using our client library for Java or Python. "},{"title":"Step 1: Start a Run‚Äã","type":1,"pageTitle":"Getting Started","url":"/docs/getting-started#step-1-start-a-run","content":"Use the run ID d46e465b-d358-4d32-83d4-df660ff614dd to start the run for my-job with my-input as the input dataset: REQUEST‚Äã $ curl -X POST http://localhost:5000/api/v1/lineage \\ -H 'Content-Type: application/json' \\ -d '{ &quot;eventType&quot;: &quot;START&quot;, &quot;eventTime&quot;: &quot;2020-12-28T19:52:00.001+10:00&quot;, &quot;run&quot;: { &quot;runId&quot;: &quot;d46e465b-d358-4d32-83d4-df660ff614dd&quot; }, &quot;job&quot;: { &quot;namespace&quot;: &quot;my-namespace&quot;, &quot;name&quot;: &quot;my-job&quot; }, &quot;inputs&quot;: [{ &quot;namespace&quot;: &quot;my-namespace&quot;, &quot;name&quot;: &quot;my-input&quot; }], &quot;producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client&quot; }'  RESPONSE‚Äã 201 CREATED "},{"title":"Step 2: Complete a Run‚Äã","type":1,"pageTitle":"Getting Started","url":"/docs/getting-started#step-2-complete-a-run","content":"Use d46e465b-d358-4d32-83d4-df660ff614dd to complete the run for my-job with my-output as the output dataset. We also specify the schema facet to collect the schema for my-output before marking the run as completed. Note, you don't have to specify the input dataset my-input again for the run since it already has been associated with the run ID: REQUEST‚Äã $ curl -X POST http://localhost:5000/api/v1/lineage \\ -H 'Content-Type: application/json' \\ -d '{ &quot;eventType&quot;: &quot;COMPLETE&quot;, &quot;eventTime&quot;: &quot;2020-12-28T20:52:00.001+10:00&quot;, &quot;run&quot;: { &quot;runId&quot;: &quot;d46e465b-d358-4d32-83d4-df660ff614dd&quot; }, &quot;job&quot;: { &quot;namespace&quot;: &quot;my-namespace&quot;, &quot;name&quot;: &quot;my-job&quot; }, &quot;outputs&quot;: [{ &quot;namespace&quot;: &quot;my-namespace&quot;, &quot;name&quot;: &quot;my-output&quot;, &quot;facets&quot;: { &quot;schema&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client&quot;, &quot;_schemaURL&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/spec/OpenLineage.json#/definitions/SchemaDatasetFacet&quot;, &quot;fields&quot;: [ { &quot;name&quot;: &quot;a&quot;, &quot;type&quot;: &quot;VARCHAR&quot;}, { &quot;name&quot;: &quot;b&quot;, &quot;type&quot;: &quot;VARCHAR&quot;} ] } } }], &quot;producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client&quot; }'  RESPONSE‚Äã 201 CREATED "},{"title":"View Collected Lineage Metadata‚Äã","type":1,"pageTitle":"Getting Started","url":"/docs/getting-started#view-collected-lineage-metadata","content":""},{"title":"Search Job Metadata‚Äã","type":1,"pageTitle":"Getting Started","url":"/docs/getting-started#search-job-metadata","content":"To view lineage metadata collected by Marquez, browse to the UI by visiting http://localhost:3000. Then, use the search bar in the upper right-side of the page and search for the job my-job. To view lineage metadata for my-job, click on the job from the drop-down list:  "},{"title":"View Job Metadata‚Äã","type":1,"pageTitle":"Getting Started","url":"/docs/getting-started#view-job-metadata","content":"You should see the job namespace, name, my-input as an input dataset and my-output as an output dataset in the lineage graph and the job run marked as COMPLETED :  "},{"title":"View Input Dataset Metadata‚Äã","type":1,"pageTitle":"Getting Started","url":"/docs/getting-started#view-input-dataset-metadata","content":"Finally, click on the output dataset my-output for my-job. You should see the dataset name, schema, and description:  "},{"title":"Summary‚Äã","type":1,"pageTitle":"Getting Started","url":"/docs/getting-started#summary","content":"In this simple example, we showed you how to use Marquez to collect dataset and job metadata with Openlineage. We also walked you through the set of HTTP API calls to successfully mark a run as complete and view the lineage metadata collected with Marquez. "},{"title":"Next Steps‚Äã","type":1,"pageTitle":"Getting Started","url":"/docs/getting-started#next-steps","content":"Take a look at Marquez's Airflow example to learn how to enable OpenLineage metadata collection for Airflow DAGs and troubleshoot failing DAGs using Marquez.Listen to Solving Data Lineage Tracking And Data Discovery At WeWork.Listen to Unlocking The Power of Data Lineage In Your Platform with OpenLineage. "},{"title":"Feedback‚Äã","type":1,"pageTitle":"Getting Started","url":"/docs/getting-started#feedback","content":"What did you think of this guide? We would love to hear feedback, and we can be found on the OpenLineage Slack. "},{"title":"Backfilling Airflow DAGs Using Marquez","type":0,"sectionRef":"#","url":"/docs/guides/airflow-backfill-dags","content":"","keywords":""},{"title":"Exploring Lineage Metadata using Marquez‚Äã","type":1,"pageTitle":"Backfilling Airflow DAGs Using Marquez","url":"/docs/guides/airflow-backfill-dags#exploring-lineage-metadata-using-marquez","content":""},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"Backfilling Airflow DAGs Using Marquez","url":"/docs/guides/airflow-backfill-dags#prerequisites","content":"Sample data (for the dataset used here, follow the instructions in the Write Sample Lineage Metadata to Marquez section of Marquez's quickstart guide)Docker 17.05+Docker DesktopDocker Composejq info If you are using macOS Monterey (macOS 12), port 5000 will have to be released by disabling the AirPlay Receiver. Also, port 3000 will need to be free if access to the Marquez web UI is desired. "},{"title":"Query the Lineage Graph‚Äã","type":1,"pageTitle":"Backfilling Airflow DAGs Using Marquez","url":"/docs/guides/airflow-backfill-dags#query-the-lineage-graph","content":"After running the seed command in the quickstart guide, check to make sure Marquez is up by visiting http://localhost:3000. The page should display an empty Marquez instance and a message saying there is no data. Also, it should be possible to see the server output from requests in the terminal window where Marquez is running. This window should remain open. As you progress through the tutorial, feel free to experiment with the web UI. Use truncated strings (e.g., &quot;example.etl_orders_7_days&quot; instead of &quot;job:food_delivery:example.etl_orders_7_days&quot;) to find the datasets referenced below. In Marquez, each dataset and job has its own globally unique node ID that can be used to query the lineage graph. The LineageAPI returns a set of nodes consisting of edges. An edge is directed and has a defined origin and destination. A lineage graph may contain the following node types: dataset:&lt;namespace&gt;:&lt;dataset&gt;, job:&lt;namespace&gt;:&lt;job&gt;. Start by querying the lineage graph of the seed data via the CLI. The etl_orders_7_days DAG has the node ID job:food_delivery:example.etl_orders_7_days. To see the graph, run the following in a new terminal window: $ curl -X GET &quot;http://localhost:5000/api/v1-beta/lineage?nodeId=job:food_delivery:example.etl_orders_7_days&quot;  Notice in the returned lineage graph that the DAG input datasets are public.categories, public.orders, and public.menus, while public.orders_7_days is the output dataset. The response should look something like this: { &quot;graph&quot;: [{ &quot;id&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;, &quot;type&quot;: &quot;JOB&quot;, &quot;data&quot;: { &quot;type&quot;: &quot;BATCH&quot;, &quot;id&quot;: { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;example.etl_orders_7_days&quot; }, &quot;name&quot;: &quot;example.etl_orders_7_days&quot;, &quot;createdAt&quot;: &quot;2021-06-06T14:50:13.931946Z&quot;, &quot;updatedAt&quot;: &quot;2021-06-06T14:57:54.037399Z&quot;, &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;inputs&quot;: [ {&quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.categories&quot;}, {&quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.menu_items&quot;}, {&quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.orders&quot;}, {&quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.menus&quot;} ], &quot;outputs&quot;: [ {&quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.orders_7_days&quot;} ], &quot;location&quot;: &quot;https://github.com/example/jobs/blob/2294bc15eb49071f38425dc927e48655530a2f2e/etl_orders_7_days.py&quot;, &quot;context&quot;: { &quot;sql&quot;: &quot;INSERT INTO orders_7_days (order_id, placed_on, discount_id, menu_id, restaurant_id, menu_item_id, category_id)\\n SELECT o.id AS order_id, o.placed_on, o.discount_id, m.id AS menu_id, m.restaurant_id, mi.id AS menu_item_id, c.id AS category_id\\n FROM orders AS o\\n INNER JOIN menu_items AS mi\\n ON menu_items.id = o.menu_item_id\\n INNER JOIN categories AS c\\n ON c.id = mi.category_id\\n INNER JOIN menu AS m\\n ON m.id = c.menu_id\\n WHERE o.placed_on &gt;= NOW() - interval '7 days';&quot; }, &quot;description&quot;: &quot;Loads newly placed orders weekly.&quot;, &quot;latestRun&quot;: { &quot;id&quot;: &quot;5c7f0dc4-d3c1-4f16-9ac3-dc86c5da37cc&quot;, &quot;createdAt&quot;: &quot;2021-06-06T14:50:36.853459Z&quot;, &quot;updatedAt&quot;: &quot;2021-06-06T14:57:54.037399Z&quot;, &quot;nominalStartTime&quot;: &quot;2021-06-06T14:54:00Z&quot;, &quot;nominalEndTime&quot;: &quot;2021-06-06T14:57:00Z&quot;, &quot;state&quot;: &quot;FAILED&quot;, &quot;startedAt&quot;: &quot;2021-06-06T14:54:14.037399Z&quot;, &quot;endedAt&quot;: &quot;2021-06-06T14:57:54.037399Z&quot;, &quot;durationMs&quot;: 220000, &quot;args&quot;: {}, &quot;location&quot;: &quot;https://github.com/example/jobs/blob/2294bc15eb49071f38425dc927e48655530a2f2e/etl_orders_7_days.py&quot;, &quot;context&quot;: { &quot;sql&quot;: &quot;INSERT INTO orders_7_days (order_id, placed_on, discount_id, menu_id, restaurant_id, menu_item_id, category_id)\\n SELECT o.id AS order_id, o.placed_on, o.discount_id, m.id AS menu_id, m.restaurant_id, mi.id AS menu_item_id, c.id AS category_id\\n FROM orders AS o\\n INNER JOIN menu_items AS mi\\n ON menu_items.id = o.menu_item_id\\n INNER JOIN categories AS c\\n ON c.id = mi.category_id\\n INNER JOIN menu AS m\\n ON m.id = c.menu_id\\n WHERE o.placed_on &gt;= NOW() - interval '7 days';&quot; }, &quot;facets&quot;: {} } }, &quot;inEdges&quot;: [ {&quot;origin&quot;: &quot;dataset:food_delivery:public.categories&quot;, &quot;destination&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;}, &quot;destination&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;}, {&quot;origin&quot;: &quot;dataset:food_delivery:public.orders&quot;, &quot;destination&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;}, {&quot;origin&quot;: &quot;dataset:food_delivery:public.menus&quot;, &quot;destination&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;} ], &quot;outEdges&quot;: [ {&quot;origin&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;, &quot;destination&quot;: &quot;dataset:food_delivery:public.orders_7_days&quot;} ] } }, ...] }  To see a visualization of the graph, search the web UI with public.delivery_7_days. "},{"title":"Backfill a DAG Run‚Äã","type":1,"pageTitle":"Backfilling Airflow DAGs Using Marquez","url":"/docs/guides/airflow-backfill-dags#backfill-a-dag-run","content":" Figure 1: Backfilled daily table partitions To run a backfill for example.etl_orders_7_days using the DAG lineage metadata stored in Marquez, query the lineage graph for the upstream DAG where an error originated. In this case, the example.etl_orders DAG upstream of example.etl_orders_7_days failed to write some of the daily table partitions needed for the weekly food order trends report. To fix the weekly trends report, backfill the missing daily table partitions public.orders_2021_06_04, public.orders_2021_06_05, and public.orders_2021_06_06 using the Airflow CLI: # Backfill daily food orders $ airflow dags backfill \\ --start-date 2021-06-04 \\ --end-date 2021-06-06 \\ example.etl_orders   Figure 2: Airflow inter-DAG dependencies Then, using the script backfill.sh defined below, we can easily backfill all DAGs downstream of example.etl_orders: (Note: Make sure you have jq installed before running backfill.sh.) #!/bin/bash # # Backfill DAGs automatically using lineage metadata stored in Marquez. # # Usage: $ ./backfill.sh &lt;start-date&gt; &lt;end-date&gt; &lt;dag-id&gt; ‚Äã set -e ‚Äã # Backfills DAGs downstream of the given node ID, recursively. backfill_downstream_of() { node_id=&quot;${1}&quot; # Get out edges for node ID out_edges=($(echo $lineage_graph \\ | jq -r --arg NODE_ID &quot;${node_id}&quot; '.graph[] | select(.id==$NODE_ID) | .outEdges[].destination')) for out_edge in &quot;${out_edges[@]}&quot;; do # Run backfill if out edge is a job node (i.e. &lt;dataset&gt; =&gt; &lt;job&gt;) if [[ &quot;${out_edge}&quot; = job:* ]]; then dag_id=&quot;${out_edge##*:}&quot; echo &quot;backfilling ${dag_id}...&quot; airflow backfill --start_date &quot;${start_date}&quot; --end_date &quot;${start_date}&quot; &quot;${dag_id}&quot; fi # Follow out edges downstream, recursively backfill_downstream_of &quot;${out_edge}&quot; done } ‚Äã start_date=&quot;${1}&quot; end_date=&quot;${2}&quot; dag_id=&quot;${3}&quot; ‚Äã # (1) Build job node ID (format: 'job:&lt;namespace&gt;:&lt;job&gt;') node_id=&quot;job:food_delivery:${dag_id}&quot; ‚Äã # (2) Get lineage graph lineage_graph=$(curl -s -X GET &quot;http://localhost:5000/api/v1-beta/lineage?nodeId=${node_id}&quot;) ‚Äã # (3) Run backfill backfill_downstream_of &quot;${node_id}&quot;  When run, the script should output all backfilled DAGs to the console: $ ./backfill.sh 2021-06-06 2021-06-06 example.etl_orders backfilling example.etl_orders_7_days... backfilling example.etl_delivery_7_days... backfilling example.delivery_times_7_days...  "},{"title":"Conclusion‚Äã","type":1,"pageTitle":"Backfilling Airflow DAGs Using Marquez","url":"/docs/guides/airflow-backfill-dags#conclusion","content":"The lineage metadata provided by Marquez can make the task of backfilling much easier. But lineage metadata can also help avoid the need to backfill altogether. Since Marquez collects DAG run metadata that can be viewed using the Runs API, building automated processes to check DAG run states and notify teams of upstream data quality issues is just one possible preventive measure. Explore Marquez's opinionated Metadata API and define your own automated process(es) for analyzing lineage metadata! Also, join our Slack channel or reach out to us on Twitter if you have questions. "},{"title":"Using Marquez with dbt","type":0,"sectionRef":"#","url":"/docs/guides/dbt","content":"","keywords":""},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"Using Marquez with dbt","url":"/docs/guides/dbt#prerequisites","content":"dbtDocker DesktopgitGoogle Cloud Service account Google Cloud Service account JSON key file Note: your Google Cloud account should have access to BigQuery and read/write access to your GCS bucket. Giving your key file an easy-to-remember name (bq-dbt-demo.json) is recommended. Finally, if using macOS Monterey (macOS 12), you will need to release port 5000 by disabling the AirPlay Receiver. "},{"title":"Instructions‚Äã","type":1,"pageTitle":"Using Marquez with dbt","url":"/docs/guides/dbt#instructions","content":"First, run through this excellent dbt tutorial. It explains how to create a BigQuery project, provision a service account, download a JSON key, and set up a local dbt environment. The rest of this example assumes the existence of a BigQuery project where models can be run, as well as proper configuration of dbt to connect to the project. Next, start a local Marquez instance to store lineage metadata. Make sure Docker is running, and then clone the Marquez repository: git clone https://github.com/MarquezProject/marquez.git &amp;&amp; cd marquez ./docker/up.sh  Check to make sure Marquez is up by visiting http://localhost:3000. The page should display an empty Marquez instance and a message saying there is no data. Also, it should be possible to see the server output from requests in the terminal window where Marquez is running. This window should remain open. Now, in a new terminal window/pane, clone the following GitHub project, which contains some database models: git clone https://github.com/rossturk/stackostudy.git &amp;&amp; cd stackostudy  Now it is time to install dbt and its integration with OpenLineage. Doing this in a Python virtual environment is recommended. To create one and install necessary packages, run the following commands: python -m venv virtualenv source virtualenv/bin/activate pip install dbt dbt-openlineage  Keep in mind that dbt learns how to connect to a BigQuery project by looking for a matching profile in ~/.dbt/profiles.yml. Create or edit this file so it contains a section with the project's BigQuery connection details. Also, point to the location of the JSON key for the service account. Consult this section in the dbt documentation for more help with dbt profiles. At this point, profiles.yml should look something like this: stackostudy: target: dev outputs: dev: type: bigquery method: service-account keyfile: /Users/rturk/.dbt/dbt-example.json project: dbt-example dataset: stackostudy threads: 1 timeout_seconds: 300 location: US priority: interactive  The dbt debug command checks to see that everything has been configured correctly. Running it now should produce output like the following: % dbt debug Running with dbt=0.20.1 dbt version: 0.20.1 python version: 3.8.12 python path: /opt/homebrew/Cellar/dbt/0.20.1_1/libexec/bin/python3 os info: macOS-11.5.2-arm64-arm-64bit Using profiles.yml file at /Users/rturk/.dbt/profiles.yml Using dbt_project.yml file at /Users/rturk/projects/stackostudy/dbt_project.yml ‚Äã Configuration: profiles.yml file [OK found and valid] dbt_project.yml file [OK found and valid] ‚Äã Required dependencies: - git [OK found] ‚Äã Connection: method: service-account database: stacko-study schema: stackostudy location: US priority: interactive timeout_seconds: 300 maximum_bytes_billed: None Connection test: OK connection ok  "},{"title":"Important Details‚Äã","type":1,"pageTitle":"Using Marquez with dbt","url":"/docs/guides/dbt#important-details","content":"Some important conventions should be followed when designing dbt models for use with OpenLineage. Following these conventions will help ensure that OpenLineage collects the most complete metadata possible. First, any datasets existing outside the dbt project should be defined in a schema YAML file inside the models/ directory: version: 2 ‚Äã sources: - name: stackoverflow database: bigquery-public-data schema: stackoverflow tables: - name: posts_questions - name: posts_answers - name: users - name: votes  This contains the name of the external dataset - in this case, bigquery-public-datasets - and lists the tables that are used by the models in this project. The name of the file does not matter, as long as it ends with .yml and is inside models/. Hardcoding dataset and table names into queries can result in incomplete data. When writing queries, be sure to use the {{ ref() }} and {{ source() }} jinja functions when referring to data sources. The {{ ref() }} function can be used to refer to tables within the same model, and the {{ source() }} function refers to tables we have defined in schema.yml. That way, dbt will properly keep track of the relationships between datasets. For example, to select from both an external dataset and one in this model: select * from {{ source('stackoverflow', 'posts_answers') }} where parent_id in (select id from {{ ref('filtered_questions') }} )  "},{"title":"Understanding and Using Facets","type":0,"sectionRef":"#","url":"/docs/guides/facets","content":"","keywords":""},{"title":"Standard Facets‚Äã","type":1,"pageTitle":"Understanding and Using Facets","url":"/docs/guides/facets#standard-facets","content":"Run Facets‚Äã nominalTime: Captures the time this run is scheduled for. This is a typical usage for time based scheduled job. The job has a nominal schedule time that will be different from the actual time it is running at. parent: Captures the parent job and Run when the run was spawn from a parent run. For example in the case of Airflow, there's a run for the DAG that then spawns runs for individual tasks that would refer to the parent run as the DAG run. Similarly when a SparkOperator starts a Spark job, this creates a separate run that refers to the task run as its parent. errorMessage: Captures potential error message, programming language - and optionally stack trace - with which the run failed. Job Facets‚Äã sourceCodeLocation: Captures the source code location and version (e.g., the git sha) of the job. sourceCode: Captures the language (e.g., Python) and actual source code of the job. sql: Capture the SQL query if this job is a SQL query. ownership: Captures the owners of the job. Dataset Facets‚Äã schema: Captures the schema of the dataset. dataSource: Captures the database instance containing this dataset (e.g., Database schema, Object store bucket, etc.) lifecycleStateChange: Captures the lifecycle states of the dataset (e.g., alter, create, drop, overwrite, rename, truncate). version: Captures the dataset version when versioning is defined by database (e.g., Iceberg snapshot ID). columnLineage: Captures the column-level lineage. ownership: Captures the owners of the dataset. Input Dataset Facets‚Äã dataQualityMetrics: Captures dataset-level and column-level data quality metrics when scanning a dataset whith a DataQuality library (row count, byte size, null count, distinct count, average, min, max, quantiles). dataQualityAssertions: Captures the result of running data tests on a dataset or its columns. Output Dataset Facets‚Äã outputStatistics: Captures the size of the output written to a dataset (row count and byte size). "},{"title":"Using OpenLineage with Airflow","type":0,"sectionRef":"#","url":"/docs/guides/airflow","content":"","keywords":""},{"title":"Setting up Local Airflow Environment using Docker Compose‚Äã","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#setting-up-local-airflow-environment-using-docker-compose","content":"Airflow has a convenient way to setup and run a fully functional environment using docker compose. The followings are therefore required to be installed before we start this tutorial. "},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#prerequisites","content":"Docker 20.10.0+Docker DesktopDocker Compose info If you are using macOS Monterey (macOS 12), port 5000 will have to be released by disabling the AirPlay Receiver. Also, port 3000 will need to be free if access to the Marquez web UI is desired. Use the following instructions to setup and run Airflow using docker-compose. First, let's start out by create a new directory which will contain all of our works. mkdir ~/airflow-ol cd ~/airflow-ol  And then, let's download the docker compose file that we'll be running in it. curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.3.3/docker-compose.yaml'  Open the file docker-compose.yaml which got downloaded, and edit the file to add an entry OPENLINEAGE_URL environment variable in line 61: ... --- version: '3' x-airflow-common: &amp;airflow-common # In order to add custom dependencies or upgrade provider packages you can use your extended image. # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml # and uncomment the &quot;build&quot; line below, Then run `docker-compose build` to build the images. image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.3.3} # build: . environment: &amp;airflow-common-env AIRFLOW__CORE__EXECUTOR: CeleryExecutor AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow # For backward compatibility, with Airflow &lt;2.3 AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0 AIRFLOW__CORE__FERNET_KEY: '' AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true' AIRFLOW__CORE__LOAD_EXAMPLES: 'true' AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth' _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-} OPENLINEAGE_URL: ${OPENLINEAGE_URL:-} ...  This will allow a new environment variable OPENLINEAGE_URL to be passed to the docker containers, which is needed for the OpenLineage to work. Then, let's create the following directories which will be mounted and used by the docker compose that will start the Airflow. mkdir dags mkdir logs mkdir plugins  Also, create a file .env that will contain environment variable that is going to be used by Airflow to install additional python packages that is needed. In our tutorial, we are going to have openlineage-airflow be installed. echo &quot;_PIP_ADDITIONAL_REQUIREMENTS=openlineage-airflow&quot; &gt; .env  You also need to let OpenLineage know which backend to emit those lineage data into. echo &quot;OPENLINEAGE_URL=http://host.docker.internal:4433&quot; &gt;&gt; .env  The reason why we are setting the backend to host.docker.internal is because we are going to be running OpenLineage Proxy outside of airflow's docker environment, and on the host machine itself. The port 4433 is the port which the proxy will be listening for lineage data. "},{"title":"Setting up OpenLineage Proxy as receiving end‚Äã","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#setting-up-openlineage-proxy-as-receiving-end","content":"OpenLineage Proxy is a simple tool that you can easily setup and run to receive for OpenLineage data. The proxy does not do anything other than display what it received into the standard output. Optionally, it can also forward the data into any OpenLineage backend via HTTP. Let's download the proxy code from git, and build it. cd ~ git clone https://github.com/OpenLineage/OpenLineage.git cd OpenLineage/proxy ./gradlew build  Now, copy the proxy.dev.yml and edit its content as the following, and save it as proxy.yml. # Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an &quot;AS IS&quot; BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. server: applicationConnectors: - type: http port: ${OPENLINEAGE_PROXY_PORT:-4433} adminConnectors: - type: http port: ${OPENLINEAGE_PROXY_ADMIN_PORT:-4434} logging: level: ${LOG_LEVEL:-INFO} appenders: - type: console proxy: source: openLineageProxyBackend streams: - type: Console - type: Http url: http://localhost:5000/api/v1/lineage  "},{"title":"Setting up Marquez‚Äã","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#setting-up-marquez","content":"The last piece of the setup is the Marquez backend. Using Marquez's quickstart document, setup the Marquez environment. cd ~ git clone https://github.com/MarquezProject/marquez.git  "},{"title":"Running Everything‚Äã","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#running-everything","content":""},{"title":"Running Marquez‚Äã","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#running-marquez","content":"cd ~/marquez ./docker/up.sh  "},{"title":"Running OpenLineage proxy‚Äã","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#running-openlineage-proxy","content":"cd ~/OpenLineage ./gradlew runShadow  "},{"title":"Running Airflow‚Äã","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#running-airflow","content":"cd ~/airflow-ol docker-compose up   Running everything would be the Apache Airflow setup and emitting lineage data into OpenLineage Proxy, and OpenLineage Proxy forwarding those into Marquez, so we can both inspect the data payload entering, as well as see the lineage data in graph form. "},{"title":"Accessing Airflow UI‚Äã","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#accessing-airflow-ui","content":"After everything is up and running, we can now login to Airflow's UI by opening up the browser, and accessing http://localhost:8080. Initial ID and password to login would be airflow/airflow. "},{"title":"Running an example DAG‚Äã","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#running-an-example-dag","content":"When you log into Airflow UI, you will notice that there are several example DAGs already populated when it started up. We can start running some of them to see what kind of OpenLineage event they generate. "},{"title":"Running Bash Operator‚Äã","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#running-bash-operator","content":"In the DAGs page, locate the example_bash_operator.  Clicke the ‚ñ∫ button at the right, which will show up a popup. Select Trigger DAG to trigger and run the DAG manually. You should see DAG running, and eventually completing. "},{"title":"Check the OpenLineage events‚Äã","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#check-the-openlineage-events","content":"Once everything is finished, you should be able to see a number of JSON data payloads output in OpenLineage proxy's console. INFO [2022-08-16 21:39:41,411] io.openlineage.proxy.api.models.ConsoleLineageStream: { &quot;eventTime&quot; : &quot;2022-08-16T21:39:40.854926Z&quot;, &quot;eventType&quot; : &quot;START&quot;, &quot;inputs&quot; : [ ], &quot;job&quot; : { &quot;facets&quot; : { }, &quot;name&quot; : &quot;example_bash_operator.runme_2&quot;, &quot;namespace&quot; : &quot;default&quot; }, &quot;outputs&quot; : [ ], &quot;producer&quot; : &quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0/integration/airflow&quot;, &quot;run&quot; : { &quot;facets&quot; : { &quot;airflow_runArgs&quot; : { &quot;_producer&quot; : &quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0/integration/airflow&quot;, &quot;_schemaURL&quot; : &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/BaseFacet&quot;, &quot;externalTrigger&quot; : true }, &quot;airflow_version&quot; : { &quot;_producer&quot; : &quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0/integration/airflow&quot;, &quot;_schemaURL&quot; : &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/BaseFacet&quot;, &quot;airflowVersion&quot; : &quot;2.3.3&quot;, &quot;openlineageAirflowVersion&quot; : &quot;0.12.0&quot;, &quot;operator&quot; : &quot;airflow.operators.bash.BashOperator&quot;, &quot;taskInfo&quot; : &quot;{'_BaseOperator__init_kwargs': {'task_id': 'runme_2', 'params': &lt;***.models.param.ParamsDict object at 0xffff7467b610&gt;, 'bash_command': 'echo \\&quot;example_bash_operator__runme_2__20220816\\&quot; &amp;&amp; sleep 1'}, '_BaseOperator__from_mapped': False, 'task_id': 'runme_2', 'task_group': &lt;weakproxy at 0xffff74676ef0 to TaskGroup at 0xffff7467ba50&gt;, 'owner': '***', 'email': None, 'email_on_retry': True, 'email_on_failure': True, 'execution_timeout': None, 'on_execute_callback': None, 'on_failure_callback': None, 'on_success_callback': None, 'on_retry_callback': None, '_pre_execute_hook': None, '_post_execute_hook': None, 'executor_config': {}, 'run_as_user': None, 'retries': 0, 'queue': 'default', 'pool': 'default_pool', 'pool_slots': 1, 'sla': None, 'trigger_rule': &lt;TriggerRule.ALL_SUCCESS: 'all_success'&gt;, 'depends_on_past': False, 'ignore_first_depends_on_past': True, 'wait_for_downstream': False, 'retry_delay': datetime.timedelta(seconds=300), 'retry_exponential_backoff': False, 'max_retry_delay': None, 'params': &lt;***.models.param.ParamsDict object at 0xffff7467b4d0&gt;, 'priority_weight': 1, 'weight_rule': &lt;WeightRule.DOWNSTREAM: 'downstream'&gt;, 'resources': None, 'max_active_tis_per_dag': None, 'do_xcom_push': True, 'doc_md': None, 'doc_json': None, 'doc_yaml': None, 'doc_rst': None, 'doc': None, 'upstream_task_ids': set(), 'downstream_task_ids': {'run_after_loop'}, 'start_date': DateTime(2021, 1, 1, 0, 0, 0, tzinfo=Timezone('UTC')), 'end_date': None, '_dag': &lt;DAG: example_bash_operator&gt;, '_log': &lt;Logger ***.task.operators (INFO)&gt;, 'inlets': [], 'outlets': [], '_inlets': [], '_outlets': [], '_BaseOperator__instantiated': True, 'bash_command': 'echo \\&quot;example_bash_operator__runme_2__20220816\\&quot; &amp;&amp; sleep 1', 'env': None, 'output_encoding': 'utf-8', 'skip_exit_code': 99, 'cwd': None, 'append_env': False}&quot; }, &quot;nominalTime&quot; : { &quot;_producer&quot; : &quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0/integration/airflow&quot;, &quot;_schemaURL&quot; : &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/NominalTimeRunFacet&quot;, &quot;nominalStartTime&quot; : &quot;2022-08-16T21:39:38.005668Z&quot; }, &quot;parentRun&quot; : { &quot;_producer&quot; : &quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0/integration/airflow&quot;, &quot;_schemaURL&quot; : &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/ParentRunFacet&quot;, &quot;job&quot; : { &quot;name&quot; : &quot;example_bash_operator&quot;, &quot;namespace&quot; : &quot;default&quot; }, &quot;run&quot; : { &quot;runId&quot; : &quot;39ad10d1-72d9-3fe9-b2a4-860c651b98b7&quot; } } }, &quot;runId&quot; : &quot;313b4e71-9cde-4c83-b641-dd6773bf114b&quot; } }  "},{"title":"Check Marquez‚Äã","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#check-marquez","content":"You can also open up the browser and visit http://localhost:3000 to access Marquez UI, and take a look at the OpenLineage events originating from Airflow.  "},{"title":"Running other DAGs‚Äã","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#running-other-dags","content":"Due to the length of this tutorial, we are not going to be running additional example DAGs, but you can try running them and it would be interesting to see how each of them are going to be emitting OpenLineage events. Please try running other examples like example_python_operator which will also emit OpenLineage events. Normally, DataLineage will be much more complete and useful if a DAG run involves certain datasets that either get used or created during the runtime of it. When you run those DAGs, you will be able to see the connection between different DAGs and Tasks touching the same dataset that will eventually turn into Data Lineage graph that may look something like this:  Currently, these are the Airflow operators that have extractors that can extract and emit OpenLineage events. PostgresOperatorMySqlOperatorBigQueryOperatorSnowflakeOperatorGreatExpectationsOperatorPythonOperator See additional Apache Examples for DAGs that you can run in Airflow for OpenLineage. "},{"title":"Troubleshooting‚Äã","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#troubleshooting","content":"You might not see any data going through the proxy or via Marquez. In that case, please check the task log of Airflow and see if you see the following message: [2022-08-16, 21:23:19 UTC] {factory.py:122} ERROR - Did not find openlineage.yml and OPENLINEAGE_URL is not set. In that case, it means that the environment variable OPENLINEAGE_URL was not set properly, thus OpenLineage was not able to emit any events. Please make sure to follow instructions in setting up the proper environment variable when setting up the Airflow via docker compose.Sometimes, Marquez would not respond and fail to receive any data via its API port 5000. You should be able to notice that if you start receiving response code 500 from Marquez or the Marquez UI hangs. In that case, simply stop and restart Marquez. "},{"title":"Conclusion‚Äã","type":1,"pageTitle":"Using OpenLineage with Airflow","url":"/docs/guides/airflow#conclusion","content":"In this short tutorial, we have learned how to setup and run a simple Apache Airflow environment that can emit OpenLineage events during its DAG run. We have also monitored and received the lineage events using combination of OpenLineage proxy and Marquez. We hope this tutorial was helpful in understanding how Airflow could be setup with OpenLineage and how you can easily monitor its data and end result using proxy and Marquez. "},{"title":"Using OpenLineage with Spark","type":0,"sectionRef":"#","url":"/docs/guides/spark","content":"","keywords":""},{"title":"Running Spark with OpenLineage‚Äã","type":1,"pageTitle":"Using OpenLineage with Spark","url":"/docs/guides/spark#running-spark-with-openlineage","content":""},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"Using OpenLineage with Spark","url":"/docs/guides/spark#prerequisites","content":"Docker DesktopgitGoogle Cloud Service account Google Cloud Service account JSON key file Note: your Google Cloud account should have access to BigQuery and read/write access to your GCS bucket. Giving your key file an easy-to-remember name (bq-spark-demo.json) is recommended. Finally, if using macOS Monterey (macOS 12), port 5000 will have to be released by disabling the AirPlay Receiver. "},{"title":"Instructions‚Äã","type":1,"pageTitle":"Using OpenLineage with Spark","url":"/docs/guides/spark#instructions","content":"Clone the OpenLineage project, navigate to the spark directory, and create a directory for your Google Cloud Service credentials: git clone https://github.com/OpenLineage/OpenLineage cd integration/spark mkdir -p docker/notebooks/gcs  Copy your Google Cloud Service credentials file into that directory, then run: docker-compose up  This launches a Jupyter notebook with Spark as well as a Marquez API endpoint already installed to report lineage. Once the notebook server is up and running, you should see something like the following in the logs: notebook_1 | [I 21:43:39.014 NotebookApp] Jupyter Notebook 6.4.4 is running at: notebook_1 | [I 21:43:39.014 NotebookApp] http://082cb836f1ec:8888/?token=507af3cf9c22f627f6c5211d6861fe0804d9f7b19a93ca48 notebook_1 | [I 21:43:39.014 NotebookApp] or http://127.0.0.1:8888/?token=507af3cf9c22f627f6c5211d6861fe0804d9f7b19a93ca48 notebook_1 | [I 21:43:39.015 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).  Copy the URL with 127.0.0.1 as the hostname from your own log (the token will be different from this one) and paste it into your browser window. You should have a blank Jupyter notebook environment ready to go.  Click on the notebooks directory, then click on the New button to create a new Python 3 notebook.  In the first cell in the window paste the below text. Update the GCP project and bucket names and the service account credentials file, then run the code: from pyspark.sql import SparkSession import urllib.request # Download dependencies for BigQuery and GCS gc_jars = ['https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.1.1/gcs-connector-hadoop3-2.1.1-shaded.jar', 'https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/bigquery-connector/hadoop3-1.2.0/bigquery-connector-hadoop3-1.2.0-shaded.jar', 'https://repo1.maven.org/maven2/com/google/cloud/spark/spark-bigquery-with-dependencies_2.12/0.22.2/spark-bigquery-with-dependencies_2.12-0.22.2.jar'] files = [urllib.request.urlretrieve(url)[0] for url in gc_jars] # Set these to your own project and bucket project_id = 'bq-openlineage-spark-demo' gcs_bucket = 'bq-openlineage-spark-demo-bucket' credentials_file = '/home/jovyan/notebooks/gcs/bq-spark-demo.json' spark = (SparkSession.builder.master('local').appName('openlineage_spark_test') .config('spark.jars', &quot;,&quot;.join(files)) # Install and set up the OpenLineage listener .config('spark.jars.packages', 'io.openlineage:openlineage-spark:0.3.+') .config('spark.extraListeners', 'io.openlineage.spark.agent.OpenLineageSparkListener') .config('spark.openlineage.host', 'http://marquez-api:5000') .config('spark.openlineage.namespace', 'spark_integration') # Configure the Google credentials and project id .config('spark.executorEnv.GCS_PROJECT_ID', project_id) .config('spark.executorEnv.GOOGLE_APPLICATION_CREDENTIALS', '/home/jovyan/notebooks/gcs/bq-spark-demo.json') .config('spark.hadoop.google.cloud.auth.service.account.enable', 'true') .config('spark.hadoop.google.cloud.auth.service.account.json.keyfile', credentials_file) .config('spark.hadoop.fs.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem') .config('spark.hadoop.fs.AbstractFileSystem.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS') .config(&quot;spark.hadoop.fs.gs.project.id&quot;, project_id) .getOrCreate())  Most of this is boilerplate for installing the BigQuery and GCS libraries in the notebook environment. This also sets the configuration parameters to tell the libraries what GCP project to use and how to authenticate with Google. The parameters specific to OpenLineage are the four already mentioned: spark.jars.packages, spark.extraListeners, spark.openlineage.host, spark.openlineage.namespace. Here, the host has been configured to be the marquez-api container started by Docker. With OpenLineage configured, it's time to get some data. The below code populates Spark DataFrames with data from two COVID-19 public data sets. Create a new cell in the notebook and paste the following: from pyspark.sql.functions import expr, col mask_use = spark.read.format('bigquery') \\ .option('parentProject', project_id) \\ .option('table', 'bigquery-public-data:covid19_nyt.mask_use_by_county') \\ .load() \\ .select(expr(&quot;always + frequently&quot;).alias(&quot;frequent&quot;), expr(&quot;never + rarely&quot;).alias(&quot;rare&quot;), &quot;county_fips_code&quot;) opendata = spark.read.format('bigquery') \\ .option('parentProject', project_id) \\ .option('table', 'bigquery-public-data.covid19_open_data.covid19_open_data') \\ .load() \\ .filter(&quot;country_name == 'United States of America'&quot;) \\ .filter(&quot;date == '2021-10-31'&quot;) \\ .select(&quot;location_key&quot;, expr('cumulative_deceased/(population/100000)').alias('deaths_per_100k'), expr('cumulative_persons_fully_vaccinated/(population - population_age_00_09)').alias('vaccination_rate'), col('subregion2_code').alias('county_fips_code')) joined = mask_use.join(opendata, 'county_fips_code') joined.write.mode('overwrite').parquet(f'gs://{gcs_bucket}/demodata/covid_deaths_and_mask_usage/')  Some background on the above: the covid19_open_data table is being filtered to include only U.S. data and data for Halloween 2021. The deaths_per_100k data point is being calculated using the existing cumulative_deceased and population columns and the vaccination_rate using the total population, subtracting the 0-9 year olds, since they were ineligible for vaccination at the time. For the mask_use_by_county data, &quot;rarely&quot; and &quot;never&quot; data are being combined into a single number, as are &quot;frequently&quot; and &quot;always.&quot; The columns selected from the two datasets are then stored in GCS. Now, add a cell to the notebook and paste this line: spark.read.parquet(f'gs://{gcs_bucket}/demodata/covid_deaths_and_mask_usage/').count()  The notebook should print a warning and a stacktrace (probably a debug statement), then return a total of 3142 records. Now that the pipeline is operational it is available for lineage collection. The docker-compose.yml file that ships with the OpenLineage repo includes only the Jupyter notebook and the Marquez API. To explore the lineage visually, start up the Marquez web project. Without terminating the existing docker containers, run the following command in a new terminal: docker run --network spark_default -p 3000:3000 -e MARQUEZ_HOST=marquez-api -e MARQUEZ_PORT=5000 --link marquez-api:marquez-api marquezproject/marquez-web:0.19.1  Next, open a new browser tab and navigate to http://localhost:3000, which should look like this:  Note: the spark_integration namespace is automatically chosen because there are no other namespaces available. Three jobs are listed on the jobs page of the UI. They all start with openlineage_spark_test, which is the appName passed to the SparkSession when the first cell of the notebook was built. Each query execution or RDD action is represented as a distinct job and the name of the action is appended to the application name to form the name of the job. Clicking on the openlineage_spark_test.execute_insert_into_hadoop_fs_relation_command node calls up the lineage graph for our notebook:  The graph shows that the openlineage_spark_test.execute_insert_into_hadoop_fs_relation_command job reads from two input datasets, bigquery-public-data.covid19_nyt.mask_use_by_county and bigquery-public-data.covid19_open_data.covid19_open_data, and writes to a third dataset, /demodata/covid_deaths_and_mask_usage. The namespace is missing from that third dataset, but the fully qualified name is gs://&lt;your_bucket&gt;/demodata/covid_deaths_and_mask_usage. The bottom bar shows some interesting data that was collected from the Spark job. Dragging the bar up expands the view to offer a closer look.  Two facets always collected from Spark jobs are the spark_version and the spark.logicalPlan. The first simply reports what version of Spark was executing, as well as the version of the openlineage-spark library. This is helpful for debugging job runs. The second facet is the serialized optimized LogicalPlan Spark reports when the job runs. Spark‚Äôs query optimization can have dramatic effects on the execution time and efficiency of the query job. Tracking how query plans change over time can significantly aid in debugging slow queries or OutOfMemory errors in production. Clicking on the first BigQuery dataset provides information about the data:  One can see the schema of the dataset as well as the datasource. Similar information is available about the dataset written to in GCS:  As in the BigQuery dataset, one can see the output schema and the datasource ‚Äî in this case, the gs:// scheme and the name of the bucket written to. In addition to the schema, one can also see a stats facet, reporting the number of output records and bytes as -1. The VERSIONS tab on the bottom bar would display multiple versions if there were any (not the case here). Clicking on the version shows the same schema and statistics facets, but they are specific to the version selected.  In production, this dataset would have many versions, as each time a job runs a new version of the dataset is created. This permits the tracking of changes to the statistics and schema over time, aiding in debugging slow jobs or data quality issues and job failures. The final job in the UI is a HashAggregate job. This represents the count() method called at the end to show the number of records in the dataset. Rather than a count(), this could easily be a toPandas() call or some other job that reads and processes that data -- perhaps one that stores output back into GCS or updates a Postgres database, publishes a new model, etc. Regardless of where the output gets stored, the OpenLineage integration allows one to see the entire lineage graph, unifying datasets in object stores, relational databases, and more traditional data warehouses. "},{"title":"Conclusion‚Äã","type":1,"pageTitle":"Using OpenLineage with Spark","url":"/docs/guides/spark#conclusion","content":"The Spark integration from OpenLineage offers users insights into graphs of datasets stored in object stores like S3, GCS, and Azure Blob Storage, as well as BigQuery and relational databases like Postgres. Now with support for Spark 3.1, OpenLineage offers visibility into more environments, such as Databricks, EMR, and Dataproc clusters. "},{"title":"OpenLineage Integrations","type":0,"sectionRef":"#","url":"/docs/integrations/about","content":"","keywords":""},{"title":"Capability Matrix‚Äã","type":1,"pageTitle":"OpenLineage Integrations","url":"/docs/integrations/about#capability-matrix","content":"caution This matrix is not yet complete. The matrix below shows the relationship between an input facet and various mechanisms OpenLineage uses to gather metadata. Not all mechanisms collect data to fill in all facets, and some facets are specific to one integration. ‚úîÔ∏è: The mechanism does implement this facet. ‚úñÔ∏è: The mechanism does not implement this facet. An empty column means it is not yet documented if the mechanism implements this facet. Mechanism\tIntegration\tMetadata Gathered\tInputDatasetFacet\tOutputDatasetFacet\tSqlJobFacet\tSchemaDatasetFacet\tDataSourceDatasetFacet\tDataQualityMetricsInputDatasetFacet\tDataQualityAssertionsDatasetFacet\tSourceCodeJobFacet\tExternalQueryRunFacet\tDocumentationDatasetFacet\tSourceCodeLocationJobFacet\tDocumentationJobFacet\tParentRunFacetSnowflakeOperator*\tAirflow Extractor\tLineage Job duration\t‚úîÔ∏è\t‚úîÔ∏è\t‚úîÔ∏è\t‚úîÔ∏è\t‚úîÔ∏è\t‚úñÔ∏è\t‚úñÔ∏è BigQueryOperator**\tAirflow Extractor\tLineage Schema details Job duration\t‚úîÔ∏è\t‚úîÔ∏è ‚úîÔ∏è PostgresOperator*\tAirflow Extractor\tLineage Job duration\t‚úîÔ∏è\t‚úîÔ∏è\t‚úîÔ∏è\t‚úîÔ∏è\t‚úîÔ∏è SqlCheckOperators\tAirflow Extractor\tLineage Data quality assertions\t‚úîÔ∏è\t‚úñÔ∏è\t‚úîÔ∏è\t‚úîÔ∏è\t‚úîÔ∏è\t‚úîÔ∏è\t‚úîÔ∏è dbt\tdbt Project Files\tLineage Row count Byte count.\t‚úîÔ∏è Great Expectations\tAction\tData quality assertions\t‚úîÔ∏è ‚úîÔ∏è\t‚úîÔ∏è Spark\tSparkListener\tSchema Row count Column lineage\t‚úîÔ∏è Snowflake***\tAccess History\tLineage  Uses the Rest SQL parser Uses the BigQuery API * Uses Snowflake query logs "},{"title":"Compatibility matrix‚Äã","type":1,"pageTitle":"OpenLineage Integrations","url":"/docs/integrations/about#compatibility-matrix","content":"This matrix shows which data sources are known to work with each integration, along with the minimum versions required in the target system or framework. Platform\tVersion\tData SourcesApache Airflow\t1.10+ 2.0+\tPostgreSQL MySQL Snowflake Amazon Athena Amazon Redshift Amazon SageMaker Amazon S3 Copy Google BigQuery Great Expectations SFTP Apache Spark\t2.4+\tJDBC HDFS Google Cloud Storage Google BigQuery Amazon S3 Azure Blob Storage Azure Data Lake Gen2 Azure Synapse dbt\t0.20+\tSnowflake Google BigQuery "},{"title":"Integration strategies‚Äã","type":1,"pageTitle":"OpenLineage Integrations","url":"/docs/integrations/about#integration-strategies","content":"info This section could use some more detail! You're welcome to contribute using the Edit link at the bottom. "},{"title":"Integrating with pipelines‚Äã","type":1,"pageTitle":"OpenLineage Integrations","url":"/docs/integrations/about#integrating-with-pipelines","content":" "},{"title":"Integrating with data sources‚Äã","type":1,"pageTitle":"OpenLineage Integrations","url":"/docs/integrations/about#integrating-with-data-sources","content":" "},{"title":"Apache Airflow","type":0,"sectionRef":"#","url":"/docs/integrations/airflow/","content":"","keywords":""},{"title":"How does Airflow work with OpenLineage?‚Äã","type":1,"pageTitle":"Apache Airflow","url":"/docs/integrations/airflow/#how-does-airflow-work-with-openlineage","content":"Understanding complex inter-DAG dependencies and providing up-to-date runtime visibility into DAG execution can be challenging. OpenLineage integrates with Airflow to collect DAG lineage metadata so that inter-DAG dependencies are easily maintained and viewable via a lineage graph, while also keeping a catalog of historical runs of DAGs.  The DAG metadata collected can answer questions like: Why has a DAG failed?Why has the DAG runtime increased after a code change?What are the upstream dependencies of a DAG? "},{"title":"How can I use this integration?‚Äã","type":1,"pageTitle":"Apache Airflow","url":"/docs/integrations/airflow/#how-can-i-use-this-integration","content":"To instrument your Airflow instance with OpenLineage, follow these instructions. "},{"title":"How to add lineage coverage for more operators?‚Äã","type":1,"pageTitle":"Apache Airflow","url":"/docs/integrations/airflow/#how-to-add-lineage-coverage-for-more-operators","content":"OpenLineage provides a set of extractors that extract lineage from operators. If you want to add lineage coverage for your own custom operators, follow these instructions to add lineage to operators. If you want to add coverage for operators you can not modify, follow instructions to add custom extractors. If you want to expose lineage as a one off in your workflow, you can also manually annotate the tasks in your DAG. "},{"title":"Where can I learn more?‚Äã","type":1,"pageTitle":"Apache Airflow","url":"/docs/integrations/airflow/#where-can-i-learn-more","content":"Take a look at Marquez's Airflow example to learn how to enable OpenLineage metadata collection for Airflow DAGs and troubleshoot failing DAGs using Marquez.Watch Data Lineage with OpenLineage and Airflow "},{"title":"Feedback‚Äã","type":1,"pageTitle":"Apache Airflow","url":"/docs/integrations/airflow/#feedback","content":"You can reach out to us on slack and leave us feedback! "},{"title":"Custom extractors","type":0,"sectionRef":"#","url":"/docs/integrations/airflow/extractors/custom-extractors","content":"","keywords":""},{"title":"Interface‚Äã","type":1,"pageTitle":"Custom extractors","url":"/docs/integrations/airflow/extractors/custom-extractors#interface","content":"Custom extractors have to derive from BaseExtractor. Extractors have three methods to implement: extract, extract_on_complete and get_operator_classnames. The last one is a classmethod that is used to provide list of operators that your extractor can get lineage from. For example: @classmethod def get_operator_classnames(cls) -&gt; List[str]: return ['PostgresOperator']  If the name of the operator matches one of the names on the list, the extractor will be instantiated - with operator provided in the extractor's self.operator property - and both extract and extract_on_complete methods will be called. They are used to provide actual information data. The difference is that extract is called before operator's executemethod, while extract_on_complete is called after. This can be used to extract any additional information that the operator sets on it's own properties. Good example is SnowflakeOperator that sets query_ids after execution. Both methods return TaskMetadata structure: @attr.s class TaskMetadata: name: str = attr.ib() # deprecated inputs: List[Dataset] = attr.ib(factory=list) outputs: List[Dataset] = attr.ib(factory=list) run_facets: Dict[str, BaseFacet] = attr.ib(factory=dict) job_facets: Dict[str, BaseFacet] = attr.ib(factory=dict)  Inputs and outputs are lists of plain OpenLineage datasets run_facets and job_facets are dictionaries of optional JobFacets and RunFacets that would be attached to the job - for example, you might want to attach SqlJobFacet if your operator is executing SQL. To learn more about facets in OpenLineage, please visit this section. "},{"title":"Registering custom extractor‚Äã","type":1,"pageTitle":"Custom extractors","url":"/docs/integrations/airflow/extractors/custom-extractors#registering-custom-extractor","content":"OpenLineage integration does not know that you've provided an extractor unless you'll register it. The way to do that is to add them to OPENLINEAGE_EXTRACTORS environment variable. OPENLINEAGE_EXTRACTORS=full.path.to.ExtractorClass  If you have multiple custom extractors, separate the paths with comma (;) OPENLINEAGE_EXTRACTORS=full.path.to.ExtractorClass;full.path.to.AnotherExtractorClass  Optionally, you can separate them with whitespace. It's useful if you're providing them as part of some YAML file. OPENLINEAGE_EXTRACTORS: &gt;- full.path.to.FirstExtractor; full.path.to.SecondExtractor  "},{"title":"Adding extractor to OpenLineage Airflow integration package‚Äã","type":1,"pageTitle":"Custom extractors","url":"/docs/integrations/airflow/extractors/custom-extractors#adding-extractor-to-openlineage-airflow-integration-package","content":"All Openlineage extractors are defined in this path. In order to add new extractor you should put your code in this directory. Additionally, you need to add the class to _extractors list in extractors.py, e.g.: _extractors = list( filter( lambda t: t is not None, [ try_import_from_string( 'openlineage.airflow.extractors.postgres_extractor.PostgresExtractor' ), ... # other extractors are listed here + try_import_from_string( + 'openlineage.airflow.extractors.new_extractor.ExtractorClass' + ), ] ) )  "},{"title":"Debugging issues‚Äã","type":1,"pageTitle":"Custom extractors","url":"/docs/integrations/airflow/extractors/custom-extractors#debugging-issues","content":"There are two common problems associated with custom extractors. First, is wrong path provided to OPENLINEAGE_EXTRACTORS. The path needs to be exactly the same as one you'd use from your code. If the path is wrong, the extractor won't get imported and OpenLineage events won't be emitted. Second one, and maybe more insidious, are imports from Airflow. Due to the fact that OpenLineage code gets instantiated when Airflow worker itself starts, any import from Airflow can be unnoticeably cyclical. This causes OpenLineage extraction to fail. To avoid this issue, import from Airflow only locally - in extract or extract_on_complete methods. If you need imports for type checking, guard them behind typing.TYPE_CHECKING. You can also check Development section to learn more about how to setup development environment and create tests. "},{"title":"Testing custom extractors","type":0,"sectionRef":"#","url":"/docs/integrations/airflow/extractors/extractor-testing","content":"","keywords":""},{"title":"Testing set-up‚Äã","type":1,"pageTitle":"Testing custom extractors","url":"/docs/integrations/airflow/extractors/extractor-testing#testing-set-up","content":"We‚Äôll use the same extractor that we built in the blog post, the RedshiftDataExtractor. When testing an extractor, we want to verify a few different sets of assumptions. The first set of assumptions are about the TaskMetadata object being created, specifically verifying that the object is being built with the correct input and output datasets and relevant facets. This is done in OpenLineage via pytest, with appropriate mocking and patching for connections and objects. In the OpenLineage repository, extractor unit tests are found in under [integration/airflow/tests](https://github.com/OpenLineage/OpenLineage/tree/main/integration/airflow/tests). For custom extractors, these tests should go under a tests directory at the top level of your project hierarchy.  An Astro project directory structure, with extractors in an extractors/ folder under include/, and tests under a top-level tests/ folder. "},{"title":"Testing the TaskMetadata object‚Äã","type":1,"pageTitle":"Testing custom extractors","url":"/docs/integrations/airflow/extractors/extractor-testing#testing-the-taskmetadata-object","content":"For the RedshiftDataExtractor, this core extract test is actually run on extract_on_complete(), as the extract() method is empty. We‚Äôll walk through a test function to see how we can ensure the output dataset is being built as expected (full test code here) # First, we add patching to mock our connection to Redshift. @mock.patch( &quot;airflow.providers.amazon.aws.operators.redshift_data.RedshiftDataOperator.hook&quot;, new_callable=PropertyMock, ) @mock.patch(&quot;botocore.client&quot;) def test_extract_e2e(self, mock_client, mock_hook): # Mock the descriptions we can expect from a real call. mock_client.describe_statement.return_value = self.read_file_json( &quot;tests/extractors/redshift_statement_details.json&quot; ) mock_client.describe_table.return_value = self.read_file_json( &quot;tests/extractors/redshift_table_details.json&quot; ) # Finish setting mock objects' expected values. job_id = &quot;test_id&quot; mock_client.execute_statement.return_value = {&quot;Id&quot;: job_id} mock_hook.return_value.conn = mock_client # Set the extractor and ensure that the extract() method is not returning anything, as expected. extractor = RedshiftDataExtractor(self.task) task_meta_extract = extractor.extract() assert task_meta_extract is None # Run an instance of RedshiftDataOperator with the predefined test values. self.ti.run() # Run extract_on_complete() with the task instance object. task_meta = extractor.extract_on_complete(self.ti) # Assert that the correct job_id was used in the client call. mock_client.describe_statement.assert_called_with(Id=job_id) # Assert there is a list of output datasets. assert task_meta.outputs # Assert there is only dataset in the list. assert len(task_meta.outputs) == 1 # Assert the output dataset name is the same as the table created by the operator query. assert task_meta.outputs[0].name == &quot;dev.public.fruit&quot; # Assert the output dataset has a parsed schema. assert task_meta.outputs[0].facets[&quot;schema&quot;].fields is not None # Assert the datasource is the correct Redshift URI. assert ( task_meta.outputs[0].facets[&quot;dataSource&quot;].name == f&quot;redshift://{CLUSTER_IDENTIFIER}.{REGION_NAME}:5439&quot; ) # Assert the uri is None (as it already exists in dataSource). assert task_meta.outputs[0].facets[&quot;dataSource&quot;].uri is None # Assert the schema fields match the numnber of fields of the table created by the operator query. assert len(task_meta.outputs[0].facets[&quot;schema&quot;].fields) == 3 # Assert the output statistics match the results of the operator query. assert ( OutputStatisticsOutputDatasetFacet( rowCount=1, size=11, ) == task_meta.outputs[0].facets['stats'] )  Most of the assertions above are straightforward, yet all are important in ensuring that no unexpected behavior occurs when building the metadata object. Testing each facet is important, as data or graphs in the UI can render incorrectly if the facets are wrong. For example, if the task_meta.outputs[0].facets[&quot;dataSource&quot;].name is created incorrectly in the extractor, then the operator‚Äôs task will not show up in the lineage graph, creating a gap in pipeline observability. "},{"title":"Testing private functions‚Äã","type":1,"pageTitle":"Testing custom extractors","url":"/docs/integrations/airflow/extractors/extractor-testing#testing-private-functions","content":"Private functions with any complexity beyond returning a string should be unit tested as well. An example of this is the _get_xcom_redshift_job_id() private function in the RedshiftDataExtractor. The unit test is shown below: @mock.patch(&quot;airflow.models.TaskInstance.xcom_pull&quot;) def test_get_xcom_redshift_job_id(self, mock_xcom_pull): self.extractor._get_xcom_redshift_job_id(self.ti) mock_xcom_pull.assert_called_once_with(task_ids=self.ti.task_id)  Unit tests do not have to be particularly complex, and in this instance the single assertion is enough to cover the expected behavior that the function was called only once. "},{"title":"Troubleshooting‚Äã","type":1,"pageTitle":"Testing custom extractors","url":"/docs/integrations/airflow/extractors/extractor-testing#troubleshooting","content":"Even with unit tests, an extractor may still not be operating as expected. The easiest way to tell if data isn‚Äôt coming through correctly is if the UI elements are not showing up correctly in the Lineage tab. When testing code locally, Marquez can be used to inspect the data being emitted‚Äîor not being emitted. Using Marquez will allow you to figure out if the error is being caused by the extractor or the API. If data is being emitted from the extractor as expected but isn‚Äôt making it to the UI, then the extractor is fine and an issue should be opened up in OpenLineage. However, if data is not being emitted properly, it is likely that more unit tests are needed to cover extractor behavior. Marquez can help you pinpoint which facets are not being formed properly so you know where to add test coverage. "},{"title":"Manually annotated lineage","type":0,"sectionRef":"#","url":"/docs/integrations/airflow/manual","content":"","keywords":""},{"title":"Example‚Äã","type":1,"pageTitle":"Manually annotated lineage","url":"/docs/integrations/airflow/manual#example","content":"An operator insider the Airflow DAG can be annotated with inlets and outlets like - &quot;&quot;&quot;Example DAG demonstrating the usage of the extraction via Inlets and Outlets.&quot;&quot;&quot; import pendulum import datetime from airflow import DAG from airflow.operators.bash import BashOperator from airflow.lineage.entities import Table, File def create_table(cluster, database, name): return Table( database=database, cluster=cluster, name=name, ) t1 = create_table(&quot;c1&quot;, &quot;d1&quot;, &quot;t1&quot;) t2 = create_table(&quot;c1&quot;, &quot;d1&quot;, &quot;t2&quot;) t3 = create_table(&quot;c1&quot;, &quot;d1&quot;, &quot;t3&quot;) t4 = create_table(&quot;c1&quot;, &quot;d1&quot;, &quot;t4&quot;) f1 = File(url = &quot;http://randomfile&quot;) with DAG( dag_id='example_operator', schedule_interval='0 0 * * *', start_date=pendulum.datetime(2021, 1, 1, tz=&quot;UTC&quot;), dagrun_timeout=datetime.timedelta(minutes=60), params={&quot;example_key&quot;: &quot;example_value&quot;}, ) as dag: task1 = BashOperator( task_id='task_with_inlet_outlet', bash_command='echo &quot;{{ task_instance_key_str }}&quot; &amp;&amp; sleep 1', inlets=[t1, t2], outlets=[t3], ) task2 = BashOperator( task_id='task_with_inlet_outlet', bash_command='echo &quot;{{ task_instance_key_str }}&quot; &amp;&amp; sleep 1', inlets=[t3, f1], outlets=[t4], ) task1 &gt;&gt; task2 if __name__ == &quot;__main__&quot;: dag.cli()   The corresponding lineage graph will be -  (The image is shown with the Marquez UI (metadata collector of OpenLineage events). More info can be found here. Also note that the File entity is not captured by the lineage event currently.  "},{"title":"Conversion from Airflow Table entity to Openlineage Dataset‚Äã","type":1,"pageTitle":"Manually annotated lineage","url":"/docs/integrations/airflow/manual#conversion-from-airflow-table-entity-to-openlineage-dataset","content":"The naming convention followed here is: CLUSTER of the table entity becomes the namespace of OpenLineage's DatasetThe name of the dataset is formed by {{DATABASE}}.{{NAME}} where DATABASE and NAME are attributes specified by Airflow's Table entity. "},{"title":"Default extractors","type":0,"sectionRef":"#","url":"/docs/integrations/airflow/extractors/default-extractors","content":"","keywords":""},{"title":"Implementing Default Extractors‚Äã","type":1,"pageTitle":"Default extractors","url":"/docs/integrations/airflow/extractors/default-extractors#implementing-default-extractors","content":"To implement a default extractor, first you need an operator class. In this example, we‚Äôll use the DfToGcsOperator, a custom operator created by the Astronomer Data team to load arbitrary dataframes to our GCS bucket. We‚Äôll implement both get_openlineage_facets_on_start() and get_openlineage_facets_on_complete() for our custom operator. The specific details of the implementation will vary from operator to operator, but there will always be five basic steps that these functions will share. Both the methods return an OperatorLineage object, which itself is a collection of facets. Four of the five steps mentioned above are creating these facets where necessary, and the fifth is creating the DataSourceDatasetFacet. First, though, we‚Äôll need to import some OpenLineage objects: from openlineage.airflow.extractors.base import OperatorLineage from openlineage.client.facet import ( DataSourceDatasetFacet, SchemaDatasetFacet, SchemaField, ) from openlineage.client.run import Dataset  Now, we‚Äôll start building the facets for the OperatorLineage object in the get_openlineage_facets_on_start() method. "},{"title":"1. DataSourceDatasetFacet‚Äã","type":1,"pageTitle":"Default extractors","url":"/docs/integrations/airflow/extractors/default-extractors#1-datasourcedatasetfacet","content":"The DataSourceDatasestFacet is a simple object, containing two fields, name and uri, which should be populated with the unique name of the data source and the URI. We‚Äôll make two of these objects, an input_source to specify where the data came from, and an output_source to specify where the data is going. A quick note about the philosophy behind the name and uri: the uri is built from the namespace and the name, and each is expected to be unique with respect to its environment. This means a namespace should be globally unique in the OpenLineage universe, and the name unique within the namespace. The two are then concatenated to form the uri, so that uri = namespace + name. The full OpenLineage naming spec can be found here. In our case, the input name will be the table we are pulling data from, self.table, and the namespace will be our self.data_source. input_source = DataSourceDatasetFacet( name=self.table, uri=&quot;://&quot;.join([self.data_source, self.table]), )  The output data source object‚Äôs name will always be the base path given to the operator, self.base_path. The namespace is always in GCS, so we use the OpenLineage spec‚Äôs gs:// as the scheme and our bucket as the authority, giving us gs://{ol_bucket}. The uri is simply the concatenation of the two. if not self.bucket: ol_bucket = get_env_bucket() else: ol_bucket = self.bucket output_namespace = &quot;gs://&quot; + ol_bucket output_name = self.base_path output_uri = &quot;/&quot;.join( [ output_namespace, output_name, ] ) output_source = DataSourceDatasetFacet( name=output_name, uri=output_uri, )  "},{"title":"2. Inputs‚Äã","type":1,"pageTitle":"Default extractors","url":"/docs/integrations/airflow/extractors/default-extractors#2-inputs","content":"Next we‚Äôll create the input dataset object. As we are moving data from a dataframe to GCS in this operator, we‚Äôll make sure that we are capturing all the info in the dataframe being extracted in a Dataset. To create the Dataset object, we‚Äôll need namespace, name, and facets objects. The first two are strings, and facets is a dictionary. Our namespace will come from the operator, where we use self.data_source again. The name parameter for this facet will be the table, again coming from the operator‚Äôs parameter list. The facets will contain two entries, the first is our DataSourceDatasetFacet with the key &quot;datasource&quot; coming from the previous step and input_source as the value. The second has the key &quot;schema&quot;, with the value being a SchemaDatasetFacet, which itself is a collection of SchemaField objects, one for each column, created via a list comprehension over the operator's self.col_types parameter. The inputs parameter to OperatorLineage is a list of Dataset objects, so we‚Äôll end up adding a single Dataset object to the list later. The creation of the Dataset object looks as follows: input_facet = { &quot;datasource&quot;: input_source, &quot;schema&quot;: SchemaDatasetFacet( fields=[ SchemaField(name=col_name, type=col_type) for col_name, col_type in self.col_types.items() ] ), } input = Dataset(namespace=self.data_source, name=self.table, facets=input_facet)  "},{"title":"3. Outputs‚Äã","type":1,"pageTitle":"Default extractors","url":"/docs/integrations/airflow/extractors/default-extractors#3-outputs","content":"Our output facet will look almost identical to the input facet, except it will use the output_source we previously created, and will also have a different namespace. Our output facet object will be built as follows: output_facet = { &quot;datasource&quot;: output_source, &quot;schema&quot;: SchemaDatasetFacet( fields=[ SchemaField(name=col_name, type=col_type) for col_name, col_type in self.col_types.items() ] ), } output = Dataset( namespace=output_namespace, name=output_name, facets=output_facet, )  "},{"title":"4. Job Facets‚Äã","type":1,"pageTitle":"Default extractors","url":"/docs/integrations/airflow/extractors/default-extractors#4-job-facets","content":"A Job in OpenLineage is a process definition that consumes and produces datasets. The Job evolves over time, and that change is captured when the job runs. This means the facets we would want to capture in at the Job level are independent of the state the Job is in. Custom facets can be created to capture this job data. For our operator, we stuck with pre-existing job facets, the DocumentationJobFacet and the OwnershipJobFacet: job_facets = { &quot;documentation&quot;: DocumentationJobFacet( description=f&quot;&quot;&quot; Takes data from the data source {input_uri} and puts it in GCS at the path: {output_uri} &quot;&quot;&quot; ), &quot;ownership&quot;: OwnershipJobFacet( owners=[OwnershipJobFacetOwners(name=self.owner, type=self.email)] ) }  "},{"title":"5. Run facets‚Äã","type":1,"pageTitle":"Default extractors","url":"/docs/integrations/airflow/extractors/default-extractors#5-run-facets","content":"A Run is and instance of a Job execution. For example, when an Airflow Operator begins execution, the Run state of the OpenLineage Job transitions to Start, then to Running. When writing an Extractor, this means a Run facet should contain information pertinent to the specific instance of the job, something that could change every Run. In this example, we will emit an error message when there is an empty dataframe, using the existing ErrorMessageRunFacet. starting_facets.run_facets = { &quot;errorMessage&quot;: ErrorMessageRunFacet( message=&quot;Empty dataframe, no artifact saved to GCS.&quot;, programmingLanguage=&quot;python&quot; ) }  "},{"title":"6. On Complete‚Äã","type":1,"pageTitle":"Default extractors","url":"/docs/integrations/airflow/extractors/default-extractors#6-on-complete","content":"Finally, we‚Äôll implement the get_openlineage_metadata_on_complete() method. Most our work is already done for us, so we will start by calling get_openlineage_metadata_on_start() and then modify the returned object slightly before returning it again. The two main additions here are replacing the original SchemaDatasetFacet fields and adding a potential error message to the run_facets. For the SchemaDatasetFacet update, we replace the old fields facet with updated ones based on the now-filled-out df_meta dict, which is populated during the operator‚Äôs execute() method and is therefore unavailable to get_openlineage_metadata_on_start(). Because df_meta is already a list of SchemaField objects, we can set the property directly. Although we use a for-loop here, the operator ensures only one dataframe will ever be extracted per execute, so the for loop will only ever run once and we therefore do not have to worry about multiple input dataframes updating. The run_facets update is done only if there is an error, which is a mutually exclusive event to updating the fields facets. We pass the same message to this facet that is printed in the execute() method when an empty dataframe is found. This error message does not halt operator execution, as it gets added *after* execution, but it does create an alert in the OpenLineage UI. def get_openlineage_facets_on_complete(self, task_instance): &quot;&quot;&quot;Add lineage to DfToGcsOperator on task completion.&quot;&quot;&quot; starting_facets = self.get_openlineage_facets_on_start() if task_instance.task.df_meta is not None: for i in starting_facets.inputs: i.facets[&quot;SchemaDatasetFacet&quot;].fields = task_instance.task.df_meta else: starting_facets.run_facets = { &quot;errorMessage&quot;: ErrorMessageRunFacet( message=&quot;Empty dataframe, no artifact saved to GCS.&quot;, programmingLanguage=&quot;python&quot; ) } return starting_facets  And with that final piece of the puzzle, we have a working extractor for our custom operator! "},{"title":"Custom Facets‚Äã","type":1,"pageTitle":"Default extractors","url":"/docs/integrations/airflow/extractors/default-extractors#custom-facets","content":"The OpenLineage spec may not contain all the facets you need to write your extractor, in which case you will have to make your own custom facets. More on creating custom facets can be found here. "},{"title":"Testing‚Äã","type":1,"pageTitle":"Default extractors","url":"/docs/integrations/airflow/extractors/default-extractors#testing","content":"See the doc on testing custom extractors. "},{"title":"Using OpenLineage with older versions of Airflow","type":0,"sectionRef":"#","url":"/docs/integrations/airflow/older","content":"","keywords":""},{"title":"Airflow 2.1 - 2.2‚Äã","type":1,"pageTitle":"Using OpenLineage with older versions of Airflow","url":"/docs/integrations/airflow/older#airflow-21---22","content":"Integration for those versions has limitations: it does not support tracking failed jobs, and job starts are registered only when job ends. To make OpenLineage work, in addition to installing openlineage-airflow you need to set your LineageBackendin your airflow.cfg or via environmental variable AIRFLOW__LINEAGE__BACKEND to openlineage.lineage_backend.OpenLineageBackend  "},{"title":"Using the Airflow integration","type":0,"sectionRef":"#","url":"/docs/integrations/airflow/usage","content":"","keywords":""},{"title":"Environment Variables‚Äã","type":1,"pageTitle":"Using the Airflow integration","url":"/docs/integrations/airflow/usage#environment-variables","content":"The following environment variables are available specifically for the Airflow integration. Name\tDescription\tSinceOPENLINEAGE_AIRFLOW_DISABLE_SOURCE_CODE\tSet to False if you want source code of callables provided in PythonOperator or BashOperator NOT to be included in OpenLineage events. OPENLINEAGE_EXTRACTORS\tThe optional list of extractors class in case you need to use custom extractors. For example: OPENLINEAGE_EXTRACTORS=full.path.to.ExtractorClass;full.path.to.AnotherExtractorClass OPENLINEAGE_NAMESPACE\tThe optional namespace that the lineage data belongs to. If not specified, defaults to default.\t USAGE‚Äã When enabled, the integration will: On TaskInstance start, collect metadata for each task.Collect task input / output metadata (source, schema, etc.).Collect task run-level metadata (execution time, state, parameters, etc.)On TaskInstance complete, also mark the task as complete in Marquez. "},{"title":"Exposing lineage in Airflow operators","type":0,"sectionRef":"#","url":"/docs/integrations/airflow/operator","content":"","keywords":""},{"title":"Scope‚Äã","type":1,"pageTitle":"Exposing lineage in Airflow operators","url":"/docs/integrations/airflow/operator#scope","content":"Lineage extraction logic should be as close as possible to the operator logic. For Airflow's included operators, this logic would ideally live in the Airflow repository; for external providers, it would live in their own repository. This makes lineage extraction more stable, as it lives with the operators. Previously the OpenLineage library required one Extractor for each supported Operator which is brittle and can break when operator internals change. It's too cumbersome for people who own operators, and want to add the default implementation of OpenLineage for their operators for external users. This is still an option when you can't modify the operator itself: See add custom extractors. Each operator is responsible for describing lineage per the spec below, but the actual lineage events are still being sent by the OpenLineage library in the TaskInstanceListener. "},{"title":"Context‚Äã","type":1,"pageTitle":"Exposing lineage in Airflow operators","url":"/docs/integrations/airflow/operator#context","content":"OpenLineage collects the following information regarding the Datasets being read and written by a task: Dataset name and namespace [required] - the format for naming is outlined in the naming specification.Dataset schema [optional] - The column names and types, if known. Complex types, like structs and arrays are supportedQuery id [optional] - for systems that expose an identifier, the ID of the query. This is a Run facet, not a Dataset facet, but it is often exposed by the Data Source‚Äôs proprietary API. For example, operators for Bigquery, Redshift, and Snowflake should all allow this.Input/output statistics [optional] - The number of records and/or bytes consumed or written. Example in the BigQuery extractor: Creating the relevant facet.BigQuery API. plan info Data quality metrics [optional] - Metrics associated with quality checks performed on the dataset. For example implemented by the Great Expectations integration. Operators that intend to share information about the datasets being read and written should also expose either some of the above-mentioned information or some minimal information necessary to retrieve that information. The absolute minimum information the operators need to share is The type of datasource being accessed (e.g., BigQuery, Snowflake, PostgreSQL)The host or authority - this is often where the data is being hosted, such as the postgres server URL, the Hive metastore URL, the GCS bucket, the Snowflake account identifier...The fully qualified data path - this may be a table name, such as public.MyDataset.MyTable or a path in a bucket, e.g., path/to/my/data as defined in the OpenLineage spec for consistency across operators. This information needs to be shared for each dataset being read and written in a task. The naming spec in the OpenLineage repository uses the above information to construct a Dataset namespace and name; together they uniquely identify the dataset. For metadata about the execution of the task, a queryId or executionId should be exposed for data sources that support them. With that identifier, we can query the data source about the execution and gather statistics, such as &quot;d&quot; number of records read/written. An operator can also includes data quality assertions. The DataQuality facet specification can be found in here. "},{"title":"Implementation‚Äã","type":1,"pageTitle":"Exposing lineage in Airflow operators","url":"/docs/integrations/airflow/operator#implementation","content":"Each Operator implements the following methods returning the structure defined below: get_openlineage_facets_on_start()get_openlineage_facets_on_complete(ti) Facets are the JSON facets defined in the OpenLineage specification TaskInstanceLineage: runFacets: dict{} jobFacets: dict{} inputs: [InputDataset] outputs: [OutputDataset] InputDataset: namespace: string name: string facets: dict{} inputFacets: dict{} OutputDataset: namespace: string name: string facets: dict{} outputFacets: dict{}  (all facets are optional) When the task starts/completes, the OpenLineage TaskInstanceListener uses the selected method if available to construct lineage events. The order of selection of the method is as follows: if there is no extractor defined (based on getoperator_classnames) it will fall back to DefaultExtractor. DefaultExtractor uses get_openlineage_facets* methods. If the get_openlineage_facets_on_complete(ti) is not available it falls back to get_openlineage_facets_on_start(). Example: { &quot;runFacets&quot;: { &quot;errorMessage&quot;: { &quot;message&quot;: &quot;could not connect to foo&quot;, &quot;language&quot;: &quot;python&quot; } }, &quot;jobFacets&quot;: { &quot;sql&quot;: { &quot;query&quot;: &quot;CREATE TABLE FOO AS SELECT * FROM BAR&quot; } }, &quot;inputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://example&quot;, &quot;name&quot;: &quot;workshop.public.wealth&quot;, &quot;facets&quot;: { &quot;schema&quot;: { &quot;fields&quot;: [{ &quot;name&quot;: &quot;foo&quot; &quot;type&quot;: &quot;char&quot; &quot;description&quot;: &quot;my first field&quot; }, ] } }, &quot;inputFacets&quot;: { &quot;dataQualityMetrics&quot;: { &quot;rowCount&quot; : 1345 } } }], &quot;outputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://example&quot;, &quot;name&quot;: &quot;workshop.public.death&quot;, &quot;facets&quot;: { &quot;schema&quot;: { &quot;fields&quot;: [{ &quot;name&quot;: &quot;foo&quot; &quot;type&quot;: &quot;char&quot; &quot;description&quot;: &quot;my first field&quot; }, ] } }, &quot;outputFacets&quot;: { &quot;outputStatistics&quot;: { &quot;rowCount&quot;: 10, &quot;size&quot;: 1000 } } }, { &quot;namespace&quot;: &quot;postgres://example&quot;, &quot;name&quot;: &quot;workshop.public.taxes&quot; &quot;facets&quot;: { &quot;schema&quot;: { &quot;fields&quot;: [{ &quot;name&quot;: &quot;foo&quot; &quot;type&quot;: &quot;char&quot; &quot;description&quot;: &quot;my first field&quot; }, ] } }, &quot;outputFacets&quot;: { &quot;outputStatistics&quot;: { &quot;rowCount&quot;: 10, &quot;size&quot;: 1000 } } }], }  "},{"title":"Relevant facets‚Äã","type":1,"pageTitle":"Exposing lineage in Airflow operators","url":"/docs/integrations/airflow/operator#relevant-facets","content":"Here are some relevant examples of facets that can be added. Please consult the spec for the full list. Custom facets can also be added, using a common facet name prefix. Dataset facets‚Äã Schema schema: { fields: [{ Name: ‚Äù‚Äù Type: ‚Äù‚Äù Description: ‚Äù‚Äù }, ‚Ä¶] }  Output facets‚Äã OutputStatistics outputStatistics: { rowCount: 10 Size: 1000 }  Run facets‚Äã ErrorMessage errorMessage: { Message: ‚Äù‚Äù programmingLanguage: ‚Äù‚Äù stackTrace: ‚Äù‚Äù }  All facets‚Äã Facets "},{"title":"Apache Flink","type":0,"sectionRef":"#","url":"/docs/integrations/flink","content":"","keywords":""},{"title":"Getting lineage from Flink‚Äã","type":1,"pageTitle":"Apache Flink","url":"/docs/integrations/flink#getting-lineage-from-flink","content":"OpenLineage utilizes Flink's JobListener interface. This interface is used by Flink to notify user of job submission, successful finish of job, or job failure. Implementations of this interface are executed on JobClient. When OpenLineage listener receives information that job was submitted, it extracts Transformations from job'sExecutionEnvironment. The Transformations represent logical operations in the dataflow graph; they are composed of both Flink's build-in operators, but also user-provided Sources, Sinks and functions. To get the lineage, OpenLineage integration processes dataflow graph. Currently, OpenLineage is interested only in information contained in Sources and Sinks, as they are the places where Flink interacts with external systems. After job submission, OpenLineage integration starts actively listening to checkpoints - this gives insight into whether the job runs properly. "},{"title":"Limitations‚Äã","type":1,"pageTitle":"Apache Flink","url":"/docs/integrations/flink#limitations","content":"Currently OpenLineage's Flink integration is limited to getting information from jobs running in Application Mode. OpenLineage integration extracts lineage only from following Sources and Sinks: Sources\tSinks KafkaSource\tKafkaSink FlinkKafkaConsumer\tFlinkKafkaProducer IcebergFlinkSource\t We expect this list to grow as we add support for more connectors. "},{"title":"Usage‚Äã","type":1,"pageTitle":"Apache Flink","url":"/docs/integrations/flink#usage","content":"In your job, you need to set up OpenLineageFlinkJobListener. For example:  JobListener listener = new OpenLineageFlinkJobListener(streamExecutionEnvironment); streamExecutionEnvironment.registerJobListener(listener);  Also, OpenLineage needs certain parameters to be set in flink-conf.yaml: Configuration Key\tDescription\tExpected Value\tDefault execution.attached\tThis setting needs to be true if OpenLineage is to detect job start and failure\ttrue\tfalse OpenLineage jar needs to be present on JobManager. When the JobListener is configured, you need to point the OpenLineage integration where the events should end up. If you're using Marquez, simplest way to do that is to set up OPENLINEAGE_URL environment variable to Marquez URL. More advanced settings are in the client documentation.. "},{"title":"dbt","type":0,"sectionRef":"#","url":"/docs/integrations/dbt","content":"","keywords":""},{"title":"How does dbt work with OpenLineage?‚Äã","type":1,"pageTitle":"dbt","url":"/docs/integrations/dbt#how-does-dbt-work-with-openlineage","content":"Fortunately, dbt already collects a lot of the data required to create and emit OpenLineage events. When it runs, it creates a target/manifest.json file containing information about jobs and the datasets they affect, and a target/run_results.json file containing information about the run-cycle. These files can be used to trace lineage and job performance. In addition, by using the create catalog command, a user can instruct dbt to create a target/catalog.json file containing information about dataset schemas. These files contain everything needed to trace lineage. However, the target/manifest.json and target/run_results.json files are only populated with comprehensive metadata after completion of a run-cycle. This integration is implemented as a wrapper script, dbt-ol, that calls dbt and, after the run has completed, collects information from the three json files and calls the OpenLineage API accordingly. For most users, enabling OpenLineage metadata collection can be accomplished by simply substituting dbt-ol for dbt when performing a run. "},{"title":"Preparing a dbt project for OpenLineage‚Äã","type":1,"pageTitle":"dbt","url":"/docs/integrations/dbt#preparing-a-dbt-project-for-openlineage","content":"First, we need to install the integration: pip3 install openlineage-dbt  Next, we specify where we want dbt to send OpenLineage events by setting the OPENLINEAGE_URL environment variable. For example, to send OpenLineage events to a local instance of Marquez, use: OPENLINEAGE_URL=http://localhost:5000  Finally, we can optionally specify a namespace where the lineage events will be stored. For example, to use the namespace &quot;dev&quot;: OPENLINEAGE_NAMESPACE=dev  "},{"title":"Running dbt with OpenLineage‚Äã","type":1,"pageTitle":"dbt","url":"/docs/integrations/dbt#running-dbt-with-openlineage","content":"To run your dbt project with OpenLineage collection, simply replace dbt with dbt-ol: dbt-ol run  The dbt-ol wrapper supports all of the standard dbt subcommands, and is safe to use as a substitutuon (i.e., in an alias). Once the run has completed, you will see output containing the number of events sent via the OpenLineage API: Completed successfully Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2 Emitted 4 openlineage events  "},{"title":"Where can I learn more?‚Äã","type":1,"pageTitle":"dbt","url":"/docs/integrations/dbt#where-can-i-learn-more","content":"Watch a short demonstration of the integration in action "},{"title":"Feedback‚Äã","type":1,"pageTitle":"dbt","url":"/docs/integrations/dbt#feedback","content":"What did you think of this guide? You can reach out to us on slack and leave us feedback! "},{"title":"Great Expectations","type":0,"sectionRef":"#","url":"/docs/integrations/great-expectations","content":"","keywords":""},{"title":"How does Great Expectations work with OpenLineage?‚Äã","type":1,"pageTitle":"Great Expectations","url":"/docs/integrations/great-expectations#how-does-great-expectations-work-with-openlineage","content":"Great Expecations integrates with OpenLineage through the action list in a checkpoint. An OpenLineage action can be specified, which is triggered when all expectations are run. Data from the checkpoint is sent to OpenLineage, which can then be viewed in Marquez or Datakin. "},{"title":"Preparing a Great Expectations project for OpenLineage‚Äã","type":1,"pageTitle":"Great Expectations","url":"/docs/integrations/great-expectations#preparing-a-great-expectations-project-for-openlineage","content":"First, we specify where we want Great Expectations to send OpenLineage events by setting the OPENLINEAGE_URL environment variable. For example, to send OpenLineage events to a local instance of Marquez, use: OPENLINEAGE_URL=http://localhost:5000  If data is being sent to an endpoint with an API key, then that key must be supplied as well: OPENLINEAGE_API_KEY=123456789  We can optionally specify a namespace where the lineage events will be stored. For example, to use the namespace &quot;dev&quot;: OPENLINEAGE_NAMESPACE=dev  With these environment variables set, we can add the OpenLineage action to the action list of the Great Expecations checkpoint. Note: this must be done for each checkpoint. Note: when using the GreatExpectationsOperator&gt;=0.2.0 in Airflow, there is a boolean parameter, defaulting to True, that will automatically create this action list item when it detects the OpenLineage environment specified in the previous step. In a python checkpoint, this looks like: action_list = [ { &quot;name&quot;: &quot;store_validation_result&quot;, &quot;action&quot;: {&quot;class_name&quot;: &quot;StoreValidationResultAction&quot;}, }, { &quot;name&quot;: &quot;store_evaluation_params&quot;, &quot;action&quot;: {&quot;class_name&quot;: &quot;StoreEvaluationParametersAction&quot;}, }, { &quot;name&quot;: &quot;update_data_docs&quot;, &quot;action&quot;: {&quot;class_name&quot;: &quot;UpdateDataDocsAction&quot;, &quot;site_names&quot;: []}, }, { &quot;name&quot;: &quot;open_lineage&quot;, &quot;action&quot;: { &quot;class_name&quot;: &quot;OpenLineageValidationAction&quot;, &quot;module_name&quot;: &quot;openlineage.common.provider.great_expectations&quot;, &quot;openlineage_host&quot;: os.getenv(&quot;OPENLINEAGE_URL&quot;), &quot;openlineage_apiKey&quot;: os.getenv(&quot;OPENLINEAGE_API_KEY&quot;), &quot;openlineage_namespace&quot;: oss.getenv(&quot;OPENLINEAGE_NAMESPACE&quot;), &quot;job_name&quot;: &quot;openlineage_job&quot;, }, } ]  And in yaml: name: openlineage action: class_name: OpenLineageValidationAction module_name: openlineage.common.provider.great_expectations openlineage_host: &lt;HOST&gt; openlineage_apiKey: &lt;API_KEY&gt; openlineage_namespace: &lt;NAMESPACE_NAME&gt; # Replace with your job namespace; we recommend a meaningful namespace like `dev` or `prod`, etc. job_name: validate_my_dataset  Then run your Great Expecations checkpoint with the CLI or your integration of choice. "},{"title":"Feedback‚Äã","type":1,"pageTitle":"Great Expectations","url":"/docs/integrations/great-expectations#feedback","content":"What did you think of this guide? You can reach out to us on slack and leave us feedback! "},{"title":"Quickstart with Databricks","type":0,"sectionRef":"#","url":"/docs/integrations/spark/quickstart_databricks","content":"","keywords":""},{"title":"Enable OpenLineage‚Äã","type":1,"pageTitle":"Quickstart with Databricks","url":"/docs/integrations/spark/quickstart_databricks#enable-openlineage","content":"Follow the steps below to enable OpenLineage on Databricks. Build the jar via Gradle or download the latest release.Configure the Databricks CLI with your desired workspace: Azure Databricks CLIGCP Databricks CLIAWS Databricks CLI Run upload-to-databricks.sh or upload-to-databricks.ps1. This will: create a folder in DBFS to store the OpenLineage jar.copy the jar to the DBFS foldercopy the init script to the DBFS folder Create an interactive or job cluster with the relevant Spark configs: spark.openlineage.consoleTransport true spark.extraListeners io.openlineage.spark.agent.OpenLineageSparkListener spark.openlineage.version v1 Set the cluster init script to be: dbfs:/databricks/openlineage/open-lineage-init-script.sh. info Please note that the init script approach is currently obligatory to install OpenLineage on Databricks. The Openlineage integration relies on providing a custom extra listener class io.openlineage.spark.agent.OpenLineageSparkListener that has to be available on the classpath at the driver startup. Providing it with spark.jars.packages does not work on the Databricks platform as of August 2022. "},{"title":"Verify Initialization‚Äã","type":1,"pageTitle":"Quickstart with Databricks","url":"/docs/integrations/spark/quickstart_databricks#verify-initialization","content":"A successful initialization will emit logs in the Log4j output that look similar to the following: YY/MM/DD HH:mm:ss INFO SparkContext: Registered listener io.openlineage.spark.agent.OpenLineageSparkListener YY/MM/DD HH:mm:ss INFO OpenLineageContext: Init OpenLineageContext: Args: ArgumentParser(host=https://YOURHOST, version=v1, namespace=YOURNAMESPACE, jobName=default, parentRunId=null, apiKey=Optional.empty) URI: https://YOURHOST/api/v1/lineage YY/MM/DD HH:mm:ss INFO AsyncEventQueue: Process of event SparkListenerApplicationStart(Databricks Shell,Some(app-XXX-0000),YYYY,root,None,None,None) by listener OpenLineageSparkListener took Xs.  "},{"title":"Create a Dataset‚Äã","type":1,"pageTitle":"Quickstart with Databricks","url":"/docs/integrations/spark/quickstart_databricks#create-a-dataset","content":"Open a notebook and create an example dataset with: spark.createDataFrame([ {'a': 1, 'b': 2}, {'a': 3, 'b': 4} ]).write.mode(&quot;overwrite&quot;).saveAsTable(&quot;default.temp&quot;)  "},{"title":"Observe OpenLineage Events‚Äã","type":1,"pageTitle":"Quickstart with Databricks","url":"/docs/integrations/spark/quickstart_databricks#observe-openlineage-events","content":"To troubleshoot or observe OpenLineage information in Databricks, see the Log4j output in the Cluster definition's Driver Logs. The Log4j output should contain entries starting with a message INFO ConsoleTransport that contain generated OpenLineage events: {&quot;eventType&quot;:&quot;COMPLETE&quot;,&quot;eventTime&quot;:&quot;2022-08-01T08:36:21.633Z&quot;,&quot;run&quot;:{&quot;runId&quot;:&quot;64537bbd-00ac-498d-ad49-1c77e9c2aabd&quot;,&quot;facets&quot;:{&quot;spark_unknown&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-2/OpenLineage.json#/$defs/RunFacet&quot;,&quot;inputs&quot;:[{&quot;description&quot;:{&quot;@class&quot;:&quot;org.apache.spark.sql.catalyst.analysis.ResolvedTableName&quot;,&quot;id&quot;:1,&quot;traceEnabled&quot;:false,&quot;streaming&quot;:false,&quot;cacheId&quot;:{&quot;id&quot;:2,&quot;empty&quot;:true,&quot;defined&quot;:false},&quot;canonicalizedPlan&quot;:false,&quot;defaultTreePatternBits&quot;:{&quot;id&quot;:3}},&quot;inputAttributes&quot;:[],&quot;outputAttributes&quot;:[]},{&quot;description&quot;:{&quot;@class&quot;:&quot;org.apache.spark.sql.execution.LogicalRDD&quot;,&quot;id&quot;:1,&quot;streaming&quot;:false,&quot;traceEnabled&quot;:false,&quot;cacheId&quot;:{&quot;id&quot;:2,&quot;empty&quot;:true,&quot;defined&quot;:false},&quot;canonicalizedPlan&quot;:false,&quot;defaultTreePatternBits&quot;:{&quot;id&quot;:3}},&quot;inputAttributes&quot;:[],&quot;outputAttributes&quot;:[{&quot;name&quot;:&quot;a&quot;,&quot;type&quot;:&quot;long&quot;,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;b&quot;,&quot;type&quot;:&quot;long&quot;,&quot;metadata&quot;:{}}]}]},&quot;spark.logicalPlan&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-2/OpenLineage.json#/$defs/RunFacet&quot;,&quot;plan&quot;:[{&quot;class&quot;:&quot;org.apache.spark.sql.catalyst.plans.logical.ReplaceTableAsSelect&quot;,&quot;num-children&quot;:2,&quot;name&quot;:0,&quot;partitioning&quot;:[],&quot;query&quot;:1,&quot;tableSpec&quot;:null,&quot;writeOptions&quot;:null,&quot;orCreate&quot;:true},{&quot;class&quot;:&quot;org.apache.spark.sql.catalyst.analysis.ResolvedTableName&quot;,&quot;num-children&quot;:0,&quot;catalog&quot;:null,&quot;ident&quot;:null},{&quot;class&quot;:&quot;org.apache.spark.sql.execution.LogicalRDD&quot;,&quot;num-children&quot;:0,&quot;output&quot;:[[{&quot;class&quot;:&quot;org.apache.spark.sql.catalyst.expressions.AttributeReference&quot;,&quot;num-children&quot;:0,&quot;name&quot;:&quot;a&quot;,&quot;dataType&quot;:&quot;long&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{},&quot;exprId&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.expressions.ExprId&quot;,&quot;id&quot;:18,&quot;jvmId&quot;:&quot;481bebf6-f861-400e-bb00-ea105ed8afef&quot;},&quot;qualifier&quot;:[]}],[{&quot;class&quot;:&quot;org.apache.spark.sql.catalyst.expressions.AttributeReference&quot;,&quot;num-children&quot;:0,&quot;name&quot;:&quot;b&quot;,&quot;dataType&quot;:&quot;long&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{},&quot;exprId&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.expressions.ExprId&quot;,&quot;id&quot;:19,&quot;jvmId&quot;:&quot;481bebf6-f861-400e-bb00-ea105ed8afef&quot;},&quot;qualifier&quot;:[]}]],&quot;rdd&quot;:null,&quot;outputPartitioning&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning&quot;,&quot;numPartitions&quot;:0},&quot;outputOrdering&quot;:[],&quot;isStreaming&quot;:false,&quot;session&quot;:null}]},&quot;spark_version&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-2/OpenLineage.json#/$defs/RunFacet&quot;,&quot;spark-version&quot;:&quot;3.2.1&quot;,&quot;openlineage-spark-version&quot;:&quot;0.12.0-SNAPSHOT&quot;}}},&quot;job&quot;:{&quot;namespace&quot;:&quot;default&quot;,&quot;name&quot;:&quot;databricks_shell.atomic_replace_table_as_select&quot;,&quot;facets&quot;:{}},&quot;inputs&quot;:[],&quot;outputs&quot;:[{&quot;namespace&quot;:&quot;dbfs&quot;,&quot;name&quot;:&quot;/user/hive/warehouse/temp&quot;,&quot;facets&quot;:{&quot;dataSource&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet&quot;,&quot;name&quot;:&quot;dbfs&quot;,&quot;uri&quot;:&quot;dbfs&quot;},&quot;schema&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;a&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;b&quot;,&quot;type&quot;:&quot;long&quot;}]},&quot;storage&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/StorageDatasetFacet.json#/$defs/StorageDatasetFacet&quot;,&quot;storageLayer&quot;:&quot;delta&quot;,&quot;fileFormat&quot;:&quot;parquet&quot;},&quot;lifecycleStateChange&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet&quot;,&quot;lifecycleStateChange&quot;:&quot;OVERWRITE&quot;}},&quot;outputFacets&quot;:{}}],&quot;producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-2/OpenLineage.json#/$defs/RunEvent&quot;}  The generated JSON contains the output dataset name and location {&quot;namespace&quot;:&quot;dbfs&quot;,&quot;name&quot;:&quot;/user/hive/warehouse/temp&quot;&quot; metadata, schema fields [{&quot;name&quot;:&quot;a&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;b&quot;,&quot;type&quot;:&quot;long&quot;}], and more. "},{"title":"spark","type":0,"sectionRef":"#","url":"/docs/integrations/spark/","content":"","keywords":""},{"title":"Collecting Lineage in Spark‚Äã","type":1,"pageTitle":"spark","url":"/docs/integrations/spark/#collecting-lineage-in-spark","content":"Collecting lineage requires hooking into Spark's ListenerBus in the driver application and collecting and analyzing execution events as they happen. Both raw RDD and Dataframe jobs post events to the listener bus during execution. These events expose the structure of the job, including the optimized query plan, allowing the Spark integration to analyze the job for datasets consumed and produced, including attributes about the storage, such as location in GCS or S3, table names in a relational database or warehouse, such as Redshift or Bigquery, and schemas. In addition to dataset and job lineage, Spark SQL jobs also report logical plans, which can be compared across job runs to track important changes in query plans, which may affect the correctness or speed of a job. A single Spark application may execute multiple jobs. The Spark OpenLineage integration maps one Spark job to a single OpenLineage Job. The application will be assigned a Run id at startup and each job that executes will report the application's Run id as its parent job run. Thus, an application that reads one or more source datasets, writes an intermediate dataset, then transforms that intermediate dataset and writes a final output dataset will report three jobs- the parent application job, the initial job that reads the sources and creates the intermediate dataset, and the final job that consumes the intermediate dataset and produces the final output. As an image: "},{"title":"How to Use the Integration‚Äã","type":1,"pageTitle":"spark","url":"/docs/integrations/spark/#how-to-use-the-integration","content":"Adding OpenLineage metadata collection to existing Spark jobs was designed to be straightforward and unobtrusive to the application. "},{"title":"SparkListener‚Äã","type":1,"pageTitle":"spark","url":"/docs/integrations/spark/#sparklistener","content":"The SparkListener approach is very simple and covers most cases. The listener simply analyzes events, as they are posted by the SparkContext, and extracts job and dataset metadata that are exposed by the RDD and Dataframe dependency graphs. Most data sources, such as filesystem sources (including S3 and GCS), JDBC backends, and warehouses such as Redshift and Bigquery can be analyzed and reported in this way. Installation requires adding a following package: &lt;dependency&gt; &lt;groupId&gt;io.openlineage&lt;/groupId&gt; &lt;artifactId&gt;openlineage-spark&lt;/artifactId&gt; &lt;version&gt;{spark-openlineage-version}&lt;/version&gt; &lt;/dependency&gt;  or gradle: implementation 'io.openlineage:openlineage-spark:{spark-openlineage-version}'  spark-submit‚Äã The listener can be enabled by adding the following configuration to a spark-submit command: spark-submit --conf &quot;spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener&quot; \\ --packages &quot;io.openlineage:openlineage-spark:&lt;spark-openlineage-version&gt;&quot; \\ --conf &quot;spark.openlineage.url=http://{openlineage.client.host}/api/v1/namespaces/spark_integration/&quot; \\ --class com.mycompany.MySparkApp my_application.jar  The SparkListener reads its configuration from SparkConf parameters. These can be specified on the command line (e.g., --conf &quot;spark.openlineage.url=http://{openlineage.client.host}/api/v1/namespaces/my_namespace/job/the_job&quot;) or from the conf/spark-defaults.conf file. The following parameters can be specified Parameter\tDefinition\tExamplespark.openlineage.host\tThe hostname of the OpenLineage API server where events should be reported\thttp://localhost:5000 spark.openlineage.version\tThe API version of the OpenLineage API server\t1 spark.openlineage.namespace\tThe default namespace to be applied for any jobs submitted\tMyNamespace spark.openlineage.parentJobName\tThe job name to be used for the parent job facet\tParentJobName spark.openlineage.parentRunId\tThe RunId of the parent job that initiated this Spark job\txxxx-xxxx-xxxx-xxxx spark.openlineage.apiKey\tAn API key to be used when sending events to the OpenLineage server\tabcdefghijk spark.openlineage.url.param.xyz\tA url parameter (replace xyz) and value to be included in requests to the OpenLineage API server\tabcdefghijk spark.openlineage.consoleTransport\tEvents will be emitted to a console, no additional backend is required\ttrue "},{"title":"Scheduling from Airflow‚Äã","type":1,"pageTitle":"spark","url":"/docs/integrations/spark/#scheduling-from-airflow","content":"The same parameters passed to spark-submit can be supplied from Airflow and other schedulers. If using the marquez-airflow integration, each task in the DAG has its own Run id which can be connected to the Spark job run via the openlineage.parentRunId parameter. For example, here is an example of a DataProcPySparkOperator that submits a Pyspark application on Dataproc: t1 = DataProcPySparkOperator( task_id=job_name, gcp_conn_id='google_cloud_default', project_id='project_id', cluster_name='cluster-name', region='us-west1', main='gs://bucket/your-prog.py', job_name=job_name, dataproc_pyspark_properties={ &quot;spark.extraListeners&quot;: &quot;marquez.spark.agent.SparkListener&quot;, &quot;spark.jars.packages&quot;: &quot;io.github.marquezproject:marquez-spark:0.15.+&quot;, &quot;openlineage.url&quot;: f&quot;{marquez_url}/api/v1/namespaces/{marquez_namespace}/jobs/dump_orders_to_gcs/runs/{{{{task_run_id(run_id, task)}}}}?api_key={api_key}&quot; }, dag=dag)  "},{"title":"Column Level Lineage","type":0,"sectionRef":"#","url":"/docs/integrations/spark/spark_column_lineage","content":"","keywords":""},{"title":"Standard specification‚Äã","type":1,"pageTitle":"Column Level Lineage","url":"/docs/integrations/spark/spark_column_lineage#standard-specification","content":"Collected information is sent in OpenLineage event within columnLineage dataset facet described here. "},{"title":"Code architecture and its mechanics‚Äã","type":1,"pageTitle":"Column Level Lineage","url":"/docs/integrations/spark/spark_column_lineage#code-architecture-and-its-mechanics","content":"Column level lineage has been implemented separately from the rest of builders and visitors extracting lineage information from Spark logical plans. As a result the codebase is stored in io.openlineage.spark3.agent.lifecycle.plan.columnLineage package within classes responsible only for this feature. Class ColumnLevelLineageUtils.java is an entry point to run the mechanism and is used within OpenLineageRunEventBuilder. Classes ColumnLevelLineageUtilsNonV2CatalogTest and ColumnLevelLineageUtilsV2CatalogTest contain real-life test cases which run Spark jobs and get an access to the last query plan executed. They evaluate column level lineage based on the plan and expected output schema. Then, they verify if this meets the requirements. This allows testing column level lineage behavior in real scenarios. The more tests and scenarios put here, the better. Class ColumnLevelLineageBuilder is used when traversing logical plans to store all the information required to produce column lineage. It allows storing input/output columns. It also stores dependencies between the expressions contained in query plan. Once inputs, outputs and dependencies are filled, build method is used to produce output facet (ColumnLineageDatasetFacetFields). The mechanism gets output schema and logical plan as input. Output schemas are tightly coupled with root nodes of execution plans, however we do already have this information extracted within the other visitors and dataset input builders.OutputFieldsCollector class is used to traverse the plan to gather the outputs. Outputs can be extracted from Aggregate or Project and each output field has its ExprId (expression id) attached from the plan. InputFieldsCollector class is used to collect the inputs which can be extracted from DataSourceV2Relation, DataSourceV2ScanRelation, HiveTableRelation or LogicalRelation. Each input field has its ExprId within the plan. Each input is identified by DatasetIdentifier, which means it contains name and namespace, of a dataset and an input field. FieldDependenciesCollector traverses the plan to identify dependencies between different ExprId. Dependencies map parent expressions to children expressions'. This is used to identify inputs used to evaluate certain output. "},{"title":"Writing custom extensions‚Äã","type":1,"pageTitle":"Column Level Lineage","url":"/docs/integrations/spark/spark_column_lineage#writing-custom-extensions","content":"Spark framework is known for its great ability to be extended by custom libraries capable of reading or writing to anything. In case of having a custom implementation, we prepared an ability to extend column lineage implementation to be able to retrieve information from other input or output LogicalPlan nodes. Creating such an extension requires implementing a following interface: /** Interface for implementing custom collectors of column level lineage. */ interface CustomColumnLineageVisitor { /** * Collect inputs for a given {@link LogicalPlan}. Column level lineage mechanism traverses * LogicalPlan on its node. This method will be called for each traversed node. Input information * should be put into builder. * * @param node * @param builder */ void collectInputs(LogicalPlan node, ColumnLevelLineageBuilder builder); /** * Collect outputs for a given {@link LogicalPlan}. Column level lineage mechanism traverses * LogicalPlan on its node. This method will be called for each traversed node. Output information * should be put into builder. * * @param node * @param builder */ void collectOutputs(LogicalPlan node, ColumnLevelLineageBuilder builder); /** * Collect expressions for a given {@link LogicalPlan}. Column level lineage mechanism traverses * LogicalPlan on its node. This method will be called for each traversed node. Expression * dependency information should be put into builder. * * @param node * @param builder */ void collectExpressionDependencies(LogicalPlan node, ColumnLevelLineageBuilder builder); }  and making it available for Service Loader (implementation class name has to be put in a resource file META-INF/services/io.openlineage.spark3.agent.lifecycle.plan.column.CustomColumnLineageVisitor). "},{"title":"Future work‚Äã","type":1,"pageTitle":"Column Level Lineage","url":"/docs/integrations/spark/spark_column_lineage#future-work","content":"Current version of the mechanism allows finding input fields that were used to produce the output field but does not determine how were they used. This simplification allowed us to built column lineage feature at the minimum amount of code written. Given an incredible amount of Spark's functions, operators and expressions, our implementation needs just to know it was UnaryOperator, BinaryOperator, etc. without a requirement to understand the operation it performs. This approach still has some room for extensions like: Being able to find out if an output field is a simple copy of input one or some modification has been applied.Assume there exist a mechanism within an organisation to blur personal data. Be able to extract information if the output still contains personal data or it got blurred. "},{"title":"Quickstart with Jupyter","type":0,"sectionRef":"#","url":"/docs/integrations/spark/quickstart_local","content":"Quickstart with Jupyter Trying out the Spark integration is super easy if you already have Docker Desktop and git installed. Note: If you're on macOS Monterey (macOS 12) you'll have to release port 5000 before beginning by disabling the AirPlay Receiver. Check out the OpenLineage project into your workspace with: git clone https://github.com/OpenLineage/OpenLineage From the spark integration directory ($OPENLINEAGE_ROOT/integration/spark) execute docker-compose up This will start Marquez as an Openlineage client and Jupyter Spark notebook on localhost:8888. On startup, the notebook container logs will show a list of URLs including an access token, such as notebook_1 | To access the notebook, open this file in a browser: notebook_1 | file:///home/jovyan/.local/share/jupyter/runtime/nbserver-9-open.html notebook_1 | Or copy and paste one of these URLs: notebook_1 | http://abc12345d6e:8888/?token=XXXXXX notebook_1 | or http://127.0.0.1:8888/?token=XXXXXX Copy the URL with 127.0.0.1 as the hostname from your own log (the token will be different from mine) and paste it into your browser window. You should have a blank Jupyter notebook environment ready to go. Once your notebook environment is ready, click on the notebooks directory, then click on the New button to create a new Python 3 notebook. In the first cell in the window paste the following text: from pyspark.sql import SparkSession spark = (SparkSession.builder.master('local') .appName('sample_spark') .config('spark.extraListeners', 'io.openlineage.spark.agent.OpenLineageSparkListener') .config('spark.jars.packages', 'io.openlineage:openlineage-spark:0.12.0') .config('spark.openlineage.consoleTransport', 'true') .getOrCreate()) Once the Spark context is started, we adjust logging level to INFO with: spark.sparkContext.setLogLevel(&quot;INFO&quot;) and create some Spark table with: spark.createDataFrame([ {'a': 1, 'b': 2}, {'a': 3, 'b': 4} ]).write.mode(&quot;overwrite&quot;).saveAsTable(&quot;temp&quot;) The command shold output OpenLineage event in a form of log: 22/08/01 06:15:49 INFO ConsoleTransport: {&quot;eventType&quot;:&quot;START&quot;,&quot;eventTime&quot;:&quot;2022-08-01T06:15:49.671Z&quot;,&quot;run&quot;:{&quot;runId&quot;:&quot;204d9c56-6648-4d46-b6bd-f4623255d324&quot;,&quot;facets&quot;:{&quot;spark_unknown&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-2/OpenLineage.json#/$defs/RunFacet&quot;,&quot;inputs&quot;:[{&quot;description&quot;:{&quot;@class&quot;:&quot;org.apache.spark.sql.execution.LogicalRDD&quot;,&quot;id&quot;:1,&quot;streaming&quot;:false,&quot;traceEnabled&quot;:false,&quot;canonicalizedPlan&quot;:false},&quot;inputAttributes&quot;:[],&quot;outputAttributes&quot;:[{&quot;name&quot;:&quot;a&quot;,&quot;type&quot;:&quot;long&quot;,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;b&quot;,&quot;type&quot;:&quot;long&quot;,&quot;metadata&quot;:{}}]}]},&quot;spark.logicalPlan&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-2/OpenLineage.json#/$defs/RunFacet&quot;,&quot;plan&quot;:[{&quot;class&quot;:&quot;org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand&quot;,&quot;num-children&quot;:1,&quot;table&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.catalog.CatalogTable&quot;,&quot;identifier&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.TableIdentifier&quot;,&quot;table&quot;:&quot;temp&quot;},&quot;tableType&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.catalog.CatalogTableType&quot;,&quot;name&quot;:&quot;MANAGED&quot;},&quot;storage&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat&quot;,&quot;compressed&quot;:false,&quot;properties&quot;:null},&quot;schema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[]},&quot;provider&quot;:&quot;parquet&quot;,&quot;partitionColumnNames&quot;:[],&quot;owner&quot;:&quot;&quot;,&quot;createTime&quot;:1659334549656,&quot;lastAccessTime&quot;:-1,&quot;createVersion&quot;:&quot;&quot;,&quot;properties&quot;:null,&quot;unsupportedFeatures&quot;:[],&quot;tracksPartitionsInCatalog&quot;:false,&quot;schemaPreservesCase&quot;:true,&quot;ignoredProperties&quot;:null},&quot;mode&quot;:null,&quot;query&quot;:0,&quot;outputColumnNames&quot;:&quot;[a, b]&quot;},{&quot;class&quot;:&quot;org.apache.spark.sql.execution.LogicalRDD&quot;,&quot;num-children&quot;:0,&quot;output&quot;:[[{&quot;class&quot;:&quot;org.apache.spark.sql.catalyst.expressions.AttributeReference&quot;,&quot;num-children&quot;:0,&quot;name&quot;:&quot;a&quot;,&quot;dataType&quot;:&quot;long&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{},&quot;exprId&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.expressions.ExprId&quot;,&quot;id&quot;:6,&quot;jvmId&quot;:&quot;6a1324ac-917e-4e22-a0b9-84a5f80694ad&quot;},&quot;qualifier&quot;:[]}],[{&quot;class&quot;:&quot;org.apache.spark.sql.catalyst.expressions.AttributeReference&quot;,&quot;num-children&quot;:0,&quot;name&quot;:&quot;b&quot;,&quot;dataType&quot;:&quot;long&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{},&quot;exprId&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.expressions.ExprId&quot;,&quot;id&quot;:7,&quot;jvmId&quot;:&quot;6a1324ac-917e-4e22-a0b9-84a5f80694ad&quot;},&quot;qualifier&quot;:[]}]],&quot;rdd&quot;:null,&quot;outputPartitioning&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning&quot;,&quot;numPartitions&quot;:0},&quot;outputOrdering&quot;:[],&quot;isStreaming&quot;:false,&quot;session&quot;:null}]},&quot;spark_version&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-2/OpenLineage.json#/$defs/RunFacet&quot;,&quot;spark-version&quot;:&quot;3.1.2&quot;,&quot;openlineage-spark-version&quot;:&quot;0.12.0-SNAPSHOT&quot;}}},&quot;job&quot;:{&quot;namespace&quot;:&quot;default&quot;,&quot;name&quot;:&quot;sample_spark.execute_create_data_source_table_as_select_command&quot;,&quot;facets&quot;:{}},&quot;inputs&quot;:[],&quot;outputs&quot;:[{&quot;namespace&quot;:&quot;file&quot;,&quot;name&quot;:&quot;/home/jovyan/notebooks/spark-warehouse/temp&quot;,&quot;facets&quot;:{&quot;dataSource&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet&quot;,&quot;name&quot;:&quot;file&quot;,&quot;uri&quot;:&quot;file&quot;},&quot;schema&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;a&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;b&quot;,&quot;type&quot;:&quot;long&quot;}]},&quot;lifecycleStateChange&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet&quot;,&quot;lifecycleStateChange&quot;:&quot;CREATE&quot;}},&quot;outputFacets&quot;:{}}],&quot;producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-2/OpenLineage.json#/$defs/RunEvent&quot;} Generated JSON contains output dataset name and location {&quot;namespace&quot;:&quot;file&quot;,&quot;name&quot;:&quot;/home/jovyan/notebooks/spark-warehouse/temp&quot;, schema fields [{&quot;name&quot;:&quot;a&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;b&quot;,&quot;type&quot;:&quot;long&quot;}], etc. More comprehensive demo, that integrates Spark events with Marquez backend can be foundon our blog Tracing Data Lineage with OpenLineage and Apache Spark","keywords":""},{"title":"0.1.0 - 2021-8-13","type":0,"sectionRef":"#","url":"/docs/releases/0_1_0","content":"0.1.0 - 2021-8-13 OpenLineage is an Open Standard for lineage metadata collection designed to record metadata for a job in execution. The initial public release includes: An inital specification. The the inital version 1-0-0 of the OpenLineage specification defines the core model and facets.Integrations that collect lineage metadata as OpenLineage events: Apache Airflow with support for BigQuery, Great Expectations, Postgres, Redshift, SnowflakeApache Sparkdbt Clients that send OpenLineage events to an HTTP backend. Both java and python are initially supported.","keywords":""},{"title":"0.10.0 - 2022-6-24","type":0,"sectionRef":"#","url":"/docs/releases/0_10_0","content":"","keywords":""},{"title":"Added‚Äã","type":1,"pageTitle":"0.10.0 - 2022-6-24","url":"/docs/releases/0_10_0#added","content":"Add static code anlalysis tool mypy to run in CI for against all python modules (#802) @howardyooExtend SaveIntoDataSourceCommandVisitor to extract schema from LocalRelaiton and LogicalRdd in spark integration (#794) @pawel-big-lebowskiAdd InMemoryRelationInputDatasetBuilder for InMemory datasets to Spark integration (#818) @pawel-big-lebowskiAdd copyright to source files #755 @merobi-hubAdd SnowflakeOperatorAsync extractor support to Airflow integration #869 @merobi-hubAdd PMD analysis to proxy project (#889) @howardyoo "},{"title":"Changed‚Äã","type":1,"pageTitle":"0.10.0 - 2022-6-24","url":"/docs/releases/0_10_0#changed","content":"Skip FunctionRegistry.class serialization in Spark integration (#828) @mobuchowskiInstall new rust-based SQL parser by default in Airflow integration (#835) @mobuchowskiImprove overall pytest and integration tests for Airflow integration (#851,#858) @denimalpacaReduce OL event payload size by excluding local data and including output node in start events (#881) @collado-mikeSplit spark integration into submodules (#834, #890) @tnazarew @mobuchowski "},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.10.0 - 2022-6-24","url":"/docs/releases/0_10_0#fixed","content":"Conditionally import sqlalchemy lib for Great Expectations integration (#826) @pawel-big-lebowskiAdd check for missing class org.apache.spark.sql.catalyst.plans.logical.CreateV2Table in Spark integration (#866) @pawel-big-lebowskiFix static code analysis issues (#867,#874) @pawel-big-lebowski "},{"title":"0.11.0 - 2022-7-7","type":0,"sectionRef":"#","url":"/docs/releases/0_11_0","content":"","keywords":""},{"title":"Added‚Äã","type":1,"pageTitle":"0.11.0 - 2022-7-7","url":"/docs/releases/0_11_0#added","content":"HTTP option to override timeout and properly close connections in openlineage-java lib. #909 @mobuchowskiDynamic mapped tasks support to Airflow integration #906 @JDarDagranSqlExtractor to Airflow integration #907 @JDarDagranPMD to Java and Spark builds in CI #898 @merobi-hub "},{"title":"Changed‚Äã","type":1,"pageTitle":"0.11.0 - 2022-7-7","url":"/docs/releases/0_11_0#changed","content":"When testing extractors in the Airflow integration, set the extractor length assertion dynamic #882 @denimalpacaRender templates as start of integration tests for TaskListener in the Airflow integration #870 @mobuchowski  "},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.11.0 - 2022-7-7","url":"/docs/releases/0_11_0#fixed","content":"Dependencies bundled with openlineage-java lib. #855 @collado-mikePMD reported issues #891 @pawel-big-lebowskiSpark casting error and session catalog support for iceberg in Spark integration #856 @wslulciuc "},{"title":"0.12.0 - 2022-8-1","type":0,"sectionRef":"#","url":"/docs/releases/0_12_0","content":"","keywords":""},{"title":"Added‚Äã","type":1,"pageTitle":"0.12.0 - 2022-8-1","url":"/docs/releases/0_12_0#added","content":"Add Spark 3.3.0 support #950 @pawel-big-lebowskiAdd Apache Flink integration #951 @mobuchowskiAdd ability to extend column level lineage mechanism #922 @pawel-big-lebowskiAdd ErrorMessageRunFacet #897 @mobuchowskiAdd SQLCheckExtractors #717 @denimalpacaAdd RedshiftSQLExtractor &amp; RedshiftDataExtractor #930 @JDarDagranAdd dataset builder for AlterTableCommand #927 @tnazarew "},{"title":"Changed‚Äã","type":1,"pageTitle":"0.12.0 - 2022-8-1","url":"/docs/releases/0_12_0#changed","content":"Limit Delta events #905 @pawel-big-lebowskiAirflow integration: allow lineage metadata to flow through inlets and outlets #914 @fenil25 "},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.12.0 - 2022-8-1","url":"/docs/releases/0_12_0#fixed","content":"Limit size of serialized plan #917 @pawel-big-lebowskiFix noclassdef error #942 @pawel-big-lebowski "},{"title":"0.13.1 - 2022-8-25","type":0,"sectionRef":"#","url":"/docs/releases/0_13_1","content":"","keywords":""},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.13.1 - 2022-8-25","url":"/docs/releases/0_13_1#fixed","content":"Rename all parentRun occurrences to parent in Airflow integration 1037 @fm100 Changes the parentRun property name to parent in the Airflow integration to match the spec.Do not change task instance during on_running event 1028 @JDarDagran Fixes an issue in the Airflow integration with the on_running hook, which was changing the TaskInstance object along with the task attribute. "},{"title":"0.13.0 - 2022-8-22","type":0,"sectionRef":"#","url":"/docs/releases/0_13_0","content":"","keywords":""},{"title":"Added‚Äã","type":1,"pageTitle":"0.13.0 - 2022-8-22","url":"/docs/releases/0_13_0#added","content":"Add BigQuery check support #960 @denimalpaca Adds logic and support for proper dynamic class inheritance for BigQuery-style operators. (BigQuery's extractor needed additional logic to support the forthcoming BigQueryColumnCheckOperator and BigQueryTableCheckOperator.)Add RUNNING EventType in spec and Python client #972 @mzareba382 Introduces a RUNNING event state in the OpenLineage spec to indicate a running task and adds a RUNNING event type in the Python API.Use databases &amp; schemas in SQL Extractors #974 @JDarDagran Allows the Airflow integration to differentiate between databases and schemas. (There was no notion of databases and schemas when querying and parsing results from information_schema tables.)Implement Event forwarding feature via HTTP protocol #995 @howardyoo Adds HttpLineageStream to forward a given OpenLineage event to any HTTP endpoint.Introduce SymlinksDatasetFacet to spec #936 @pawel-big-lebowski Creates a new facet, the SymlinksDatasetFacet, to support the storing of alternative dataset names.Add Azure Cosmos Handler to Spark integration #983 @hmoazam Defines a new interface, the RelationHandler, to support Spark data sources that do not have TableCatalog, Identifier, or TableProperties set, as is the case with the Azure Cosmos DB Spark connector.Support OL Datasets in manual lineage inputs/outputs #1015 @conorbev Allows Airflow users to create OpenLineage Dataset classes directly in DAGs with no conversion necessary. (Manual lineage definition required users to create an airflow.lineage.entities.Table, which was then converted to an OpenLineage Dataset.) Create ownership facets #996 @julienledem Adds an ownership facet to both Dataset and Job in the OpenLineage spec to capture ownership of jobs and datasets. "},{"title":"Changed‚Äã","type":1,"pageTitle":"0.13.0 - 2022-8-22","url":"/docs/releases/0_13_0#changed","content":"Use RUNNING EventType in Flink integration for currently running jobs #985 @mzareba382 Makes use of the new RUNNING event type in the Flink integration, changing events sent by Flink jobs from OTHER to this new type.Convert task objects to JSON-encodable objects when creating custom Airflow version facets #1018 @fm100 Implements a to_json_encodable function in the Airflow integration to make task objects JSON-encodable. "},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.13.0 - 2022-8-22","url":"/docs/releases/0_13_0#fixed","content":"Add support for custom SQL queries in v3 Great Expectations API #1025 @collado-mike Fixes support for custom SQL statements in the Great Expectations provider. (The Great Expectations custom SQL datasource was not applied to the support for the V3 checkpoints API.) "},{"title":"0.14.0 - 2022-9-6","type":0,"sectionRef":"#","url":"/docs/releases/0_14_0","content":"","keywords":""},{"title":"Added‚Äã","type":1,"pageTitle":"0.14.0 - 2022-9-6","url":"/docs/releases/0_14_0#added","content":"Support ABFSS and Hadoop Logical Relation in Column-level lineage #1008 @wjohnson Introduces an extractDatasetIdentifier that uses similar logic to InsertIntoHadoopFsRelationVisitor to pull out the path on the HDFS compliant file system; tested on ABFSS and DBFS (Databricks FileSystem) to prove that lineage could be extracted using non-SQL commands.Add Kusto relation visitor #939 @hmoazam Implements a KustoRelationVisitor to support lineage for Azure Kusto's Spark connector.Add ColumnLevelLineage facet doc #1020 @julienledem Adds documentation for the Column-level lineage facet.Include symlinks dataset facet #935 @pawel-big-lebowski Includes the recently introduced SymlinkDatasetFacet in generated OpenLineage events.Add support for dbt 1.3 beta's metadata changes #1051 @mobuchowski Makes projects that are composed of only SQL models work on 1.3 beta (dbt 1.3 renamed the compiled_sql field to compiled_code to support Python models). Does not provide support for dbt's Python models.Support Flink 1.15 #1009 @mzareba382 Adds support for Flink 1.15.Add Redshift dialect to the SQL integration #1066 @mobuchowski Adds support for Redshift's SQL dialect in OpenLineage's SQL parser, including quirks such as the use of square brackets in JSON paths. (Note, this does not add support for all of Redshift's custom syntax.) "},{"title":"Changed‚Äã","type":1,"pageTitle":"0.14.0 - 2022-9-6","url":"/docs/releases/0_14_0#changed","content":"Make the timeout configurable in the Spark integration #1050 @tnazarew Makes timeout configurable by the user. (In some cases, the time needed to send events was longer than 5 seconds, which exceeded the timeout value.) "},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.14.0 - 2022-9-6","url":"/docs/releases/0_14_0#fixed","content":"Add a dialect parameter to Great Expectations SQL parser calls #1049 @collado-mike Specifies the dialect name from the SQL engine.Fix Delta 2.1.0 with Spark 3.3.0 #1065 @pawel-big-lebowski Allows delta support for Spark 3.3 and fixes potential issues. (The Openlineage integration for Spark 3.3 was turned on without delta support, as delta did not support Spark 3.3 at that time.) "},{"title":"0.14.1 - 2022-9-7","type":0,"sectionRef":"#","url":"/docs/releases/0_14_1","content":"","keywords":""},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.14.1 - 2022-9-7","url":"/docs/releases/0_14_1#fixed","content":"Fix Spark integration issues including error when no openlineage.timeout #1069 @pawel-big-lebowski OpenlineageSparkListener was failing when no openlineage.timeout was provided. "},{"title":"0.16.1 - 2022-11-3","type":0,"sectionRef":"#","url":"/docs/releases/0_16_1","content":"","keywords":""},{"title":"Added‚Äã","type":1,"pageTitle":"0.16.1 - 2022-11-3","url":"/docs/releases/0_16_1#added","content":"Airflow: add dag_run information to Airflow version run facet #1133 @fm100 Adds the Airflow DAG run ID to the taskInfo facet, making this additional information available to the integration.Airflow: add LoggingMixin to extractors #1149 @JDarDagran Adds a LoggingMixin class to the custom extractor to make the output consistent with general Airflow and OpenLineage logging settings.Airflow: add default extractor #1162 @mobuchowski Adds a DefaultExtractor to support the default implementation of OpenLineage for external operators without the need for custom extractors.Airflow: add on_complete argument in DefaultExtractor #1188 @JDarDagran Adds support for running another method on extract_on_complete.SQL: reorganize the library into multiple packages #1167 @StarostaGit @mobuchowski Splits the SQL library into a Rust implementation and foreign language bindings, easing the process of adding language interfaces. Also contains CI fix. "},{"title":"Changed‚Äã","type":1,"pageTitle":"0.16.1 - 2022-11-3","url":"/docs/releases/0_16_1#changed","content":"Airflow: move get_connection_uri as extractor's classmethod #1169 @JDarDagran The get_connection_uri method allowed for too many params, resulting in unnecessarily long URIs. This changes the logic to whitelisting per extractor.Airflow: change get_openlineage_facets_on_start/complete behavior #1201 @JDarDagran Splits up the method for greater legibility and easier maintenance. "},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.16.1 - 2022-11-3","url":"/docs/releases/0_16_1#fixed","content":"Airflow: always send SQL in SqlJobFacet as a string #1143 @mobuchowski Changes the data type of query from array to string to an fix error in the RedshiftSQLOperator. Airflow: include __extra__ case when filtering URI query params #1144 @JDarDagran Includes the conn.EXTRA_KEY in the get_connection_uri method to avoid exposing secrets in URIs via the __extra__ key. Airflow: enforce column casing in SQLCheckExtractors #1159 @denimalpaca Uses the parent extractor's _is_uppercase_names property to determine if the column should be upper cased in the SQLColumnCheckExtractor's _get_input_facets() method.Spark: prevent exception when no schema provided #1180 @pawel-big-lebowski Prevents evalution of column lineage when the schemFacet is null.Great Expectations: add V3 API compatibility #1194 @denimalpaca Fixes the Pandas datasource to make it V3 API-compatible. "},{"title":"Removed‚Äã","type":1,"pageTitle":"0.16.1 - 2022-11-3","url":"/docs/releases/0_16_1#removed","content":"Airflow: remove support for Airflow 1.10 #1128 @mobuchowski Removes the code structures and tests enabling support for Airflow 1.10. "},{"title":"0.17.0 - 2022-11-16","type":0,"sectionRef":"#","url":"/docs/releases/0_17_0","content":"","keywords":""},{"title":"Added‚Äã","type":1,"pageTitle":"0.17.0 - 2022-11-16","url":"/docs/releases/0_17_0#added","content":"Spark: support latest Spark 3.3.1 #1183 @pawel-big-lebowski Adds support for the latest version of Spark.Spark: add Kinesis Transport and support config Kinesis in Spark integration #1200 @yogayang Adds support for sending to Kinesis from the Spark integration. Spark: Disable specified facets #1271 @pawel-big-lebowski Adds the ability to disable specified facets from generated OpenLineage events.Python: add facets implementation to Python client #1233 @pawel-big-lebowski Adds missing facets to the Python client.SQL: add Rust parser interface #1172 @StarostaGit @mobuchowski Implements a Java interface in the Rust SQL parser, including a build script, native library loading mechanism, CI support and build fixes.Proxy: add helm chart for the proxy backed #1068 @wslulciuc Adds a helm chart for deploying the proxy backend on Kubernetes.Spec: include possible facets usage in spec #1249 @pawel-big-lebowski Extends the facets definition with a list of available facets.Website: publish YML version of spec to website #1300 @rossturk Adds configuration necessary to make the OpenLineage website auto-generate openAPI docs when the spec is published there.Docs: update language on nominating new committers #1270 @rossturk Updates the governance language to reflect the new policy on nominating committers. "},{"title":"Changed‚Äã","type":1,"pageTitle":"0.17.0 - 2022-11-16","url":"/docs/releases/0_17_0#changed","content":"Website: publish spec into new website repo location #1295 @rossturk Creates a new deploy key, adds it to CircleCI &amp; GitHub, and makes the necessary changes to the release.sh script.Airflow: change how pip installs packages in tox environments #1302 @JDarDagran Use deprecated resolver and constraints files provided by Airflow to avoid potential issues caused by pip's new resolver. "},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.17.0 - 2022-11-16","url":"/docs/releases/0_17_0#fixed","content":"Airflow: fix README for running integration test #1238 @sekikn Updates the README for consistency with supported Airflow versions.Airflow: add task_instance argument to get_openlineage_facets_on_complete #1269 @JDarDagran Adds the task_instance argument to DefaultExtractor.Java client: fix up all artifactory paths #1290 @harels Not all artifactory paths were changed in the build CI script in a previous PR.Python client: fix Mypy errors and adjust to PEP 484 #1264 @JDarDagran Adds a --no-namespace-packages argument to the Mypy command and adjusts code to PEP 484.Website: release all specs since last_spec_commit_id, not just HEAD~1 #1298 @rossturk The script now ships all specs that have changed since .last_spec_commit_id. "},{"title":"Removed‚Äã","type":1,"pageTitle":"0.17.0 - 2022-11-16","url":"/docs/releases/0_17_0#removed","content":"Deprecate HttpTransport.Builder in favor of HttpConfig #1287 @collado-mike Deprecates the Builder in favor of HttpConfig only and replaces the existing Builder implementation by delegating to the HttpConfig. "},{"title":"0.15.1 - 2022-10-5","type":0,"sectionRef":"#","url":"/docs/releases/0_15_1","content":"","keywords":""},{"title":"Added‚Äã","type":1,"pageTitle":"0.15.1 - 2022-10-5","url":"/docs/releases/0_15_1#added","content":"Airflow: improve development experience #1101 @JDarDagran Adds an interactive development environment to the Airflow integration and improves integration testing.Spark: add description for URL parameters in readme, change overwriteName to appName #1130 @tnazarew Adds more information about passing arguments with spark.openlineage.url and changes overwriteName to appName for clarity.Documentation: update issue templates for proposal &amp; add new integration template #1116 @rossturk Adds a YAML issue template for new integrations and fixes a bug in the proposal template. "},{"title":"Changed‚Äã","type":1,"pageTitle":"0.15.1 - 2022-10-5","url":"/docs/releases/0_15_1#changed","content":"Airflow: lazy load BigQuery client #1119 @mobuchowski Moves import of the BigQuery client from top level to local level to decrease DAG import time. "},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.15.1 - 2022-10-5","url":"/docs/releases/0_15_1#fixed","content":"Airflow: fix UUID generation conflict for Airflow DAGs with same name #1056 @collado-mike Adds a namespace to the UUID calculation to avoid conflicts caused by DAGs having the same name in different namespaces in Airflow deployments.Spark/BigQuery: fix issue with spark-bigquery-connector &gt;=0.25.0 #1111 @pawel-big-lebowski Makes the Spark integration compatible with the latest connector.Spark: fix column lineage #1069 @pawel-big-lebowski Fixes a null pointer exception error and an error when openlineage.timeout is not provided.Spark: set log level of Init OpenLineageContext to DEBUG #1064 @varuntestaz Prevents sensitive information from being logged unless debug mode is used.Java client: update version of SnakeYAML #1090 @TheSpeedding Bumps the SnakeYAML library version to include a key bug fix. dbt: remove requirement for OPENLINEAGE_URL to be set #1107 @mobuchowski Removes erroneous check for OPENLINEAGE_URL in the dbt integration.Python client: remove potentially cyclic import #1126 @mobuchowski Hides imports to remove potentially cyclic import.CI: build macos release package on medium resource class #1131 @mobuchowski Fixes failing build due to resource class being too large. "},{"title":"0.18.0 - 2022-12-8","type":0,"sectionRef":"#","url":"/docs/releases/0_18_0","content":"","keywords":""},{"title":"Added‚Äã","type":1,"pageTitle":"0.18.0 - 2022-12-8","url":"/docs/releases/0_18_0#added","content":"Airflow: support SQLExecuteQueryOperator #1379 @JDarDagran Changes the SQLExtractor and adds support for the dynamic assignment of extractors based on conn_type.Airflow: introduce a new extractor for SFTPOperator #1263 @sekikn Adds an extractor for tracing file transfers between local file systems.Airflow: add Sagemaker extractors #1136 @fhoda Creates extractors for SagemakeProcessingOperator and SagemakerTransformOperator.Airflow: add S3 extractor for Airflow operators #1166 @fhoda Creates an extractor for the S3CopyObject in the Airflow integration.Spec: add spec file for ExternalQueryRunFacet #1262 @howardyoo Adds a spec file to make this facet available for the Java client. Includes a README.Docs: add a TSC doc #1303 @merobi-hub Adds a document listing the members of the Technical Steering Committee. "},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.18.0 - 2022-12-8","url":"/docs/releases/0_18_0#fixed","content":"Spark: improve Databricks to send better events #1330 @pawel-big-lebowski Filters unwanted events and provides a meaningful job name.Spark-Bigquery: fix a few of the common errors #1377 @mobuchowski Fixes a few of the common issues with the Spark-Bigquery integration and adds an integration test and configures CI.Python: validate eventTime field in Python client #1355 @pawel-big-lebowskiValidates the eventTime of a RunEvent within the client library.Databricks: Handle Databricks Runtime 11.3 changes to DbFsUtils constructor #1351 @wjohnson Recaptures lost mount point information from the DatabricksEnvironmentFacetBuilder and environment-properties facet by looking at the number of parameters in the DbFsUtils constructor to determine the runtime version. "},{"title":"0.19.2 - 2023-1-4","type":0,"sectionRef":"#","url":"/docs/releases/0_19_2","content":"","keywords":""},{"title":"Added‚Äã","type":1,"pageTitle":"0.19.2 - 2023-1-4","url":"/docs/releases/0_19_2#added","content":"Airflow: add Trino extractor #1288 @sekikn Adds a Trino extractor to the Airflow integration.Airflow: add S3FileTransformOperator extractor #1450 @sekikn Adds an S3FileTransformOperator extractor to the Airflow integration.Airflow: add standardized run facet #1413 @JDarDagran Creates one standardized run facet for the Airflow integration.Airflow: add NominalTimeRunFacet and OwnershipJobFacet #1410 @JDarDagran Adds nominalEndTime and OwnershipJobFacet fields to the Airflow integration.dbt: add support for postgres datasources #1417 @julienledem Adds the previously unsupported postgres datasource type.Proxy: add client-side proxy (skeletal version) #1439 #1420 @fm100 Implements a skeletal version of a client-side proxy.Proxy: add CI job to publish Docker image #1086 @wslulciuc Includes a script to build and tag the image plus jobs to verify the build on every CI run and publish to Docker Hub.SQL: add ExtractionErrorRunFacet #1442 @mobuchowski Adds a facet to the spec to reflect internal processing errors, especially failed or incomplete parsing of SQL jobs.SQL: add column-level lineage to SQL parser #1432 #1461 @mobuchowski @StarostaGit Adds support for extracting column-level lineage from SQL statements in the parser, including adjustments to Rust-Python and Rust-Java interfaces and the Airflow integration's SQL extractor to make use of the feature. Also includes more tests, removal of the old parser, and removal of the common-build cache in CI (which was breaking the parser).Spark: pass config parameters to the OL client #1383 @tnazarew Adds a mechanism for making new lineage consumers transparent to the integration, easing the process of setting up new types of consumers. "},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.19.2 - 2023-1-4","url":"/docs/releases/0_19_2#fixed","content":"Airflow: fix collect_ignore, add flags to Pytest for cleaner output #1437 @JDarDagran Removes the extractors directory from the ignored list, improving unit testing.Spark &amp; Java client: fix README typos @versaurabh Fixes typos in the SPDX license headers. "},{"title":"0.2.0 - 2021-8-23","type":0,"sectionRef":"#","url":"/docs/releases/0_2_0","content":"","keywords":""},{"title":"Added‚Äã","type":1,"pageTitle":"0.2.0 - 2021-8-23","url":"/docs/releases/0_2_0#added","content":"Parse dbt command line arguments when invoking dbt-ol @mobuchowski. For example: $ dbt-ol run --project-dir path/to/dir Set UnknownFacet for spark (captures metadata about unvisited nodes from spark plan not yet supported) @OleksandrDvornik "},{"title":"Changed‚Äã","type":1,"pageTitle":"0.2.0 - 2021-8-23","url":"/docs/releases/0_2_0#changed","content":"Remove model from dbt job name @mobuchowskiDefault dbt job namespace to output dataset namespace @mobuchowskiRename openlineage.spark.* to io.openlineage.spark.* @OleksandrDvornik "},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.2.0 - 2021-8-23","url":"/docs/releases/0_2_0#fixed","content":"Remove instance references to extractors from DAG and avoid copying log property for serializability @collado-mike "},{"title":"0.2.1 - 2021-8-27","type":0,"sectionRef":"#","url":"/docs/releases/0_2_1","content":"","keywords":""},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.2.1 - 2021-8-27","url":"/docs/releases/0_2_1#fixed","content":"dbt: default --project-dir argument to current directory in dbt-ol script @mobuchowski "},{"title":"0.2.3 - 2021-10-7","type":0,"sectionRef":"#","url":"/docs/releases/0_2_3","content":"","keywords":""},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.2.3 - 2021-10-7","url":"/docs/releases/0_2_3#fixed","content":"dbt: add dbt v3 manifest support @mobuchowski "},{"title":"0.2.2 - 2021-9-8","type":0,"sectionRef":"#","url":"/docs/releases/0_2_2","content":"","keywords":""},{"title":"Added‚Äã","type":1,"pageTitle":"0.2.2 - 2021-9-8","url":"/docs/releases/0_2_2#added","content":"Implement OpenLineageValidationAction for Great Expectations @collado-mikefacet: add expectations assertions facet @mobuchowski "},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.2.2 - 2021-9-8","url":"/docs/releases/0_2_2#fixed","content":"airflow: pendulum formatting fix, add tests @mobuchowskidbt: do not emit events if run_result file was not updated @mobuchowski "},{"title":"0.20.4 - 2023-2-7","type":0,"sectionRef":"#","url":"/docs/releases/0_20_4","content":"","keywords":""},{"title":"Added‚Äã","type":1,"pageTitle":"0.20.4 - 2023-2-7","url":"/docs/releases/0_20_4#added","content":"Airflow: add new extractor for GCSToGCSOperator #1495 @sekikn Adds a new extractor for this operator.Flink: resolve topic names from regex, support 1.16.0 #1522 @pawel-big-lebowski Adds support for Flink 1.16.0 and makes the integration resolve topic names from Kafka topic patterns.Proxy: implement lineage event validator for client proxy #1469 @fm100 Implements logic in the proxy (which is still in development) for validating and handling lineage events. "},{"title":"Changed‚Äã","type":1,"pageTitle":"0.20.4 - 2023-2-7","url":"/docs/releases/0_20_4#changed","content":"CI: use ruff instead of flake8, isort, etc., for linting and formatting #1526 @mobuchowski Adopts the ruff package, which combines several linters and formatters into one fast binary. "},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.20.4 - 2023-2-7","url":"/docs/releases/0_20_4#fixed","content":"Airflow: make the Trino catalog non-mandatory #1572 @JDarDagran Makes the Trino catalog optional in the Trino extractor.Common: add explicit SQL dependency #1532 @mobuchowski Addresses a 0.19.2 breaking change to the GE integration by including the SQL dependency explicitly.DBT: adjust tqdm logging in dbt-ol #1549 @JdarDagran Adjusts tqdm to show the correct number of iterations and adds START events for parent runs.DBT: fix typo in log output #1493 @denimalpaca Fixes 'emittled' typo in log output.Great Expectations/Airflow: follow Snowflake dataset naming rules #1527 @mobuchowski Normalizes Snowflake dataset and datasource naming rules among DBT/Airflow/GE; canonizes old Snowflake account paths around making them all full-size with account, region and cloud names.Java and Python Clients: Kafka does not initialize properties if they are empty; check and notify about Confluent-Kafka requirement #1556 @mobuchowski Fixes the failure to initialize KafkaTransport in the Java client and adds an exception if the required confluent-kafka module is missing from the Python client.Spark: add square brackets for list-based Spark configs #1507 @Varunvaruns9 Adds a condition to treat configs with [] as lists. Note: [] will be required for list-based configs starting with 0.21.0.Spark: fix several Spark/BigQuery-related issues #1557 @mobuchowski Fixes the assumption that a version is always a number; adds support for HadoopMapReduceWriteConfigUtil; makes the integration access BigQueryUtil and getTableId using reflection, which supports all BigQuery versions; makes logs provide the full serialized LogicalPlan on debug.SQL: only report partial failures `#1479 @mobuchowski Changes the parser so it reports partial failures instead of failing the whole extraction. "},{"title":"0.20.6 - 2023-2-10","type":0,"sectionRef":"#","url":"/docs/releases/0_20_6","content":"","keywords":""},{"title":"Added‚Äã","type":1,"pageTitle":"0.20.6 - 2023-2-10","url":"/docs/releases/0_20_6#added","content":"Airflow: add new extractor for FTPFileTransmitOperator #1603 @sekikn Adds a new extractor for this Airflow operator serving legacy systems. "},{"title":"Changed‚Äã","type":1,"pageTitle":"0.20.6 - 2023-2-10","url":"/docs/releases/0_20_6#changed","content":"Airflow: make extractors for async operators work #1601 @JDarDagran Sends a deterministic Run UUID for Airflow runs. "},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.20.6 - 2023-2-10","url":"/docs/releases/0_20_6#fixed","content":"dbt: render actual profile only in profiles.yml #1599 @mobuchowski Adds an include_section argument for the Jinja render method to include only one profile if needed.dbt: make compiled_code optional #1595 @JDarDagran Makes compiled_code optional for manifest &gt; v7. "},{"title":"0.3.0 - 2021-12-3","type":0,"sectionRef":"#","url":"/docs/releases/0_3_0","content":"","keywords":""},{"title":"Added‚Äã","type":1,"pageTitle":"0.3.0 - 2021-12-3","url":"/docs/releases/0_3_0#added","content":"Spark3 support @OleksandrDvornik / @collado-mikeLineageBackend for Airflow 2 @mobuchowskiAdding custom spark version facet to spark integration @OleksandrDvornikAdding dbt version facet @mobuchowskiAdded support for Redshift profile @AlessandroLollo "},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.3.0 - 2021-12-3","url":"/docs/releases/0_3_0#fixed","content":"Sanitize JDBC URLs @OleksandrDvornikstrip openlineage url in python client @OleksandrDvornikdeploy spec if spec file changes @mobuchowski "},{"title":"0.21.1 - 2023-3-2","type":0,"sectionRef":"#","url":"/docs/releases/0_21_1","content":"","keywords":""},{"title":"Added‚Äã","type":1,"pageTitle":"0.21.1 - 2023-3-2","url":"/docs/releases/0_21_1#added","content":"Clients: add DEBUG logging of events to transports #1633 by @mobuchowski Ensures that the DEBUG loglevel on properly configured loggers will always log events, regardless of the chosen transport.Spark: add CustomEnvironmentFacetBuilder class #1545 by New contributor @Anirudh181001 Enables the capture of custom environment variables from Spark.Spark: introduce the new output visitors AlterTableAddPartitionCommandVisitor and AlterTableSetLocationCommandVisitor #1629 by New contributor @nataliezeller1 Adds visitors for extracting table names from the Spark commands AlterTableAddPartitionCommand and AlterTableSetLocationCommand. The intended use case is a custom transport for the OpenMetadata lineage API.Spark: add column lineage for JDBC relations #1636 by @tnazarew Adds column lineage information to JDBC events with data extracted from query by the SQL parser.SQL: add linux-aarch64 native library to Java SQL parser #1664 by @mobuchowski Adds a Linux-ARM version of the native library. The Java SQL parser interface had only Linux-x64 and MacOS universal binary variants previously. "},{"title":"Changed‚Äã","type":1,"pageTitle":"0.21.1 - 2023-3-2","url":"/docs/releases/0_21_1#changed","content":"Airflow: get table database in Athena extractor #1631 by New contributor @rinzool Changes the extractor to get a table's database from the table.schema field or the operator default if the field is None. "},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.21.1 - 2023-3-2","url":"/docs/releases/0_21_1#fixed","content":"dbt: add dbt seed to the list of dbt-ol events #1649 by New contributor @pohek321 Ensures that dbt-ol test no longer fails when run against an event seed.Spark: make column lineage extraction in Spark support caching #1634 by @pawel-big-lebowski Collect column lineage from Spark logical plans that contain cached datasets.Spark: add support for a deprecated config #1586 by @tnazarew Maps the deprecated spark.openlineage.url to spark.openlineage.transport.url.Spark: add error message in case of null in url #1590 by @tnazarew Improves error logging in the case of undefined URLs.Spark: collect complete event for really quick Spark jobs #1650 by @pawel-big-lebowski Improves the collecting of OpenLineage events on SQL complete in the case of quick operations.Spark: fix input/outputs for one node LogicalRelation plans #1668 by @pawel-big-lebowski For simple queries like select col1, col2 from my_db.my_table that do not write output, the Spark plan contained just a single node, which was wrongly treated as both an input and output dataset.SQL: fix file existence check in build script for openlineage-sql-java #1613 by @sekikn Ensures that the build script works if the library is compiled solely for Linux. "},{"title":"Removed‚Äã","type":1,"pageTitle":"0.21.1 - 2023-3-2","url":"/docs/releases/0_21_1#removed","content":"Airflow: remove JobIdMapping and update macros to better support Airflow version 2+ #1645 by @JDarDagran Updates macros to use OpenLineageAdapter's method to generate deterministic run UUIDs because using the JobIdMapping utility is incompatible with Airflow 2+. "},{"title":"0.3.1 - 2021-12-3","type":0,"sectionRef":"#","url":"/docs/releases/0_3_1","content":"","keywords":""},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.3.1 - 2021-12-3","url":"/docs/releases/0_3_1#fixed","content":"fix import in spark3 visitor @mobuchowski "},{"title":"0.4.0 - 2021-12-13","type":0,"sectionRef":"#","url":"/docs/releases/0_4_0","content":"","keywords":""},{"title":"Added‚Äã","type":1,"pageTitle":"0.4.0 - 2021-12-13","url":"/docs/releases/0_4_0#added","content":"Spark output metrics @OleksandrDvornikSeparated tests between Spark 2 &amp; 3 @pawel-big-lebowskiDatabricks install README and init scripts @wjohnsonIceberg integration with unit tests @pawel-big-lebowskiKafka read and write support @OleksandrDvornik / @collado-mikeArbitrary parameters supported in HTTP URL construction @wjohnsonIncreased visitor coverage for Spark commands @mobuchowski / @pawel-big-lebowski "},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.4.0 - 2021-12-13","url":"/docs/releases/0_4_0#fixed","content":"dbt: column descriptions are properly filled from metadata.json @mobuchowskidbt: allow parsing artifacts with version higher than officially supported @mobuchowskidbt: dbt build command is supported @mobuchowskidbt: fix crash when build command is used with seeds in dbt 1.0.0rc3 @mobuchowskispark: increase logical plan visitor coverage @mobuchowski spark: fix logical serialization recursion issue @OleksandrDvornikUse URL#getFile to fix build on Windows @mobuchowski "},{"title":"0.5.2 - 2022-2-10","type":0,"sectionRef":"#","url":"/docs/releases/0_5_2","content":"","keywords":""},{"title":"Added‚Äã","type":1,"pageTitle":"0.5.2 - 2022-2-10","url":"/docs/releases/0_5_2#added","content":"Proxy backend example using Kafka @wslulciucSupport Databricks Delta Catalog naming convention with DatabricksDeltaHandler @wjohnsonAdd javadoc as part of build task @mobuchowskiInclude TableStateChangeFacet in non V2 commands for Spark @mr-yusupovSupport for SqlDWRelation on Databricks' Azure Synapse/SQL DW Connector @wjohnsonImplement input visitors for v2 commands @pawel-big-lebowskiEnabled SparkListenerJobStart events to trigger open lineage events @collado-mike "},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.5.2 - 2022-2-10","url":"/docs/releases/0_5_2#fixed","content":"dbt: job namespaces for given dbt run match each other @mobuchowskiFix Breaking SnowflakeOperator Changes from OSS Airflow @denimalpacaMade corrections to account for DeltaDataSource handling @collado-mike "},{"title":"0.5.1 - 2022-1-18","type":0,"sectionRef":"#","url":"/docs/releases/0_5_1","content":"","keywords":""},{"title":"Added‚Äã","type":1,"pageTitle":"0.5.1 - 2022-1-18","url":"/docs/releases/0_5_1#added","content":"Support for dbt-spark adapter @mobuchowskiNew backend to proxy OpenLineage events to one or more event streams üéâ @mandy-chessell @wslulciucAdd Spark extensibility API with support for custom Dataset and custom facet builders @collado-mike "},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.5.1 - 2022-1-18","url":"/docs/releases/0_5_1#fixed","content":"airflow: fix import failures when dependencies for bigquery, dbt, great_expectations extractors are missing @lukaszlaszkoFixed openlineage-spark jar to correctly rename bundled dependencies @collado-mike "},{"title":"0.6.1 - 2022-3-7","type":0,"sectionRef":"#","url":"/docs/releases/0_6_1","content":"","keywords":""},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.6.1 - 2022-3-7","url":"/docs/releases/0_6_1#fixed","content":"Catch possible failures when emitting events and log them @mobuchowskidbt: jinja2 code using do extensions does not crash @mobuchowski "},{"title":"0.6.0 - 2022-3-4","type":0,"sectionRef":"#","url":"/docs/releases/0_6_0","content":"","keywords":""},{"title":"Added‚Äã","type":1,"pageTitle":"0.6.0 - 2022-3-4","url":"/docs/releases/0_6_0#added","content":"Extract source code of PythonOperator code similar to SQL facet @mobuchowskiAdd DatasetLifecycleStateDatasetFacet to spec @pawel-big-lebowskiAirflow: extract source code from BashOperator @mobuchowskiAdd generic facet to collect environmental properties (EnvironmentFacet) @harishsuneOpenLineage sensor for OpenLineage-Dagster integration @dalinkimJava-client: make generator generate enums as well @pawel-big-lebowskiAdded UnknownOperatorAttributeRunFacet to Airflow integration to record operators that don't produce lineage @collado-mike "},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.6.0 - 2022-3-4","url":"/docs/releases/0_6_0#fixed","content":"Airflow: increase import timeout in tests, fix exit from integration @mobuchowskiReduce logging level for import errors to info @rossturkRemove AWS secret keys and extraneous Snowflake parameters from connection uri @collado-mikeConvert to LifecycleStateChangeDatasetFacet @pawel-big-lebowski "},{"title":"0.6.2 - 2022-3-16","type":0,"sectionRef":"#","url":"/docs/releases/0_6_2","content":"","keywords":""},{"title":"Added‚Äã","type":1,"pageTitle":"0.6.2 - 2022-3-16","url":"/docs/releases/0_6_2#added","content":"CI: add integration tests for Airflow's SnowflakeOperator and dbt-snowflake @mobuchowskiIntroduce DatasetVersion facet in spec @pawel-big-lebowskiAirflow: add external query id facet @mobuchowski "},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.6.2 - 2022-3-16","url":"/docs/releases/0_6_2#fixed","content":"Complete Fix of Snowflake Extractor get_hook() Bug @denimalpacaUpdate artwork @rossturkAirflow tasks in a DAG now report a common ParentRunFacet @collado-mike "},{"title":"0.7.1 - 2022-4-19","type":0,"sectionRef":"#","url":"/docs/releases/0_7_1","content":"","keywords":""},{"title":"Added‚Äã","type":1,"pageTitle":"0.7.1 - 2022-4-19","url":"/docs/releases/0_7_1#added","content":"Python implements Transport interface - HTTP and Kafka transports are available (#530) @mobuchowskiAdd UnknownOperatorAttributeRunFacet and support in lineage backend (#547) @collado-mikeSupport Spark 3.2.1 (#607) @pawel-big-lebowskiAdd StorageDatasetFacet to spec (#620) @pawel-big-lebowskiAirflow: custom extractors lookup uses only get_operator_classnames method (#656) @mobuchowskiREADME.md created at OpenLineage/integrations for compatibility matrix (#663) @howardyoo "},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.7.1 - 2022-4-19","url":"/docs/releases/0_7_1#fixed","content":"Dagster: handle updated PipelineRun in OpenLineage sensor unit test (#624) @dominiquetiptonDelta improvements (#626) @collado-mikeFix SqlDwDatabricksVisitor for Spark2 (#630) @wjohnsonAirflow: remove redundant logging from GE import (#657) @mobuchowskiFix Shebang issue in Spark's wait-for-it.sh (#658) @mobuchowskiUpdate parent_run_id to be a uuid from the dag name and run_id (#664) @collado-mikeSpark: fix time zone inconsistency in testSerializeRunEvent (#681) @sekikn "},{"title":"0.8.2 - 2022-5-19","type":0,"sectionRef":"#","url":"/docs/releases/0_8_2","content":"","keywords":""},{"title":"Added‚Äã","type":1,"pageTitle":"0.8.2 - 2022-5-19","url":"/docs/releases/0_8_2#added","content":"openlineage-airflow now supports getting credentials from Airflows secrets backend (#723) @mobuchowskiopenlineage-spark now supports Azure Databricks Credential Passthrough (#595) @wjohnsonopenlineage-spark detects datasets wrapped by ExternalRDDs (#746) @collado-mike "},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.8.2 - 2022-5-19","url":"/docs/releases/0_8_2#fixed","content":"PostgresOperator fails to retrieve host and conn during extraction (#705) @sekiknSQL parser accepts lists of sql statements (#734) @mobuchowskiMissing schema when writing to Delta tables in Databricks (#748) @collado-mike "},{"title":"0.8.1 - 2022-4-29","type":0,"sectionRef":"#","url":"/docs/releases/0_8_1","content":"","keywords":""},{"title":"Added‚Äã","type":1,"pageTitle":"0.8.1 - 2022-4-29","url":"/docs/releases/0_8_1#added","content":"Airflow integration uses new TaskInstance listener API for Airflow 2.3+ (#508) @mobuchowskiSupport for HiveTableRelation as input source in Spark integration (#683) @collado-mikeAdd HTTP and Kafka Client to openlineage-java lib (#480) @wslulciuc, @mobuchowskiNew SQL parser, used by Postgres, Snowflake, Great Expectations integrations (#644) @mobuchowski "},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.8.1 - 2022-4-29","url":"/docs/releases/0_8_1#fixed","content":"GreatExpectations: Fixed bug when invoking GreatExpectations using v3 API (#683) @collado-mike "},{"title":"0.9.0 - 2022-6-3","type":0,"sectionRef":"#","url":"/docs/releases/0_9_0","content":"","keywords":""},{"title":"Added‚Äã","type":1,"pageTitle":"0.9.0 - 2022-6-3","url":"/docs/releases/0_9_0#added","content":"Add static code anlalysis tool mypy to run in CI for against all python modules (#802) @howardyooExtend SaveIntoDataSourceCommandVisitor to extract schema from LocalRelaiton and LogicalRdd in spark integration (#794) @pawel-big-lebowskiAdd InMemoryRelationInputDatasetBuilder for InMemory datasets to Spark integration (#818) @pawel-big-lebowskiAdd copyright to source files #755 @merobi-hubAdd SnowflakeOperatorAsync extractor support to Airflow integration #869 @merobi-hubAdd PMD analysis to proxy project (#889) @howardyoo "},{"title":"Changed‚Äã","type":1,"pageTitle":"0.9.0 - 2022-6-3","url":"/docs/releases/0_9_0#changed","content":"Skip FunctionRegistry.class serialization in Spark integration (#828) @mobuchowskiInstall new rust-based SQL parser by default in Airflow integration (#835) @mobuchowskiImprove overall pytest and integration tests for Airflow integration (#851,#858) @denimalpacaReduce OL event payload size by excluding local data and including output node in start events (#881) @collado-mikeSplit spark integration into submodules (#834, #890) @tnazarew @mobuchowski "},{"title":"Fixed‚Äã","type":1,"pageTitle":"0.9.0 - 2022-6-3","url":"/docs/releases/0_9_0#fixed","content":"Conditionally import sqlalchemy lib for Great Expectations integration (#826) @pawel-big-lebowskiAdd check for missing class org.apache.spark.sql.catalyst.plans.logical.CreateV2Table in Spark integration (#866) @pawel-big-lebowskiFix static code analysis issues (#867,#874) @pawel-big-lebowski "},{"title":"Facets & Extensibility","type":0,"sectionRef":"#","url":"/docs/spec/facets/","content":"Facets &amp; Extensibility Facets provide context to the OpenLineage events. Generally, an OpenLineage event contains the type of the event, who created it, and when the event happened. In addition to the basic information related to the event, it provides facets for more details in four general categories: job: What kind of activity ranrun: How it raninputs: What was used during its runoutputs: What was the outcome of the run Here is an example of the four facets in action. Notice the element facets under each of the four categories of the OpenLineage event: { &quot;eventType&quot;: &quot;START&quot;, &quot;eventTime&quot;: &quot;2020-12-28T19:52:00.001+10:00&quot;, &quot;run&quot;: { &quot;runId&quot;: &quot;d46e465b-d358-4d32-83d4-df660ff614dd&quot;, &quot;facets&quot;: { &quot;parent&quot;: { &quot;job&quot;: { &quot;name&quot;: &quot;dbt-execution-parent-job&quot;, &quot;namespace&quot;: &quot;dbt-namespace&quot; }, &quot;run&quot;: { &quot;runId&quot;: &quot;f99310b4-3c3c-1a1a-2b2b-c1b95c24ff11&quot; } } } }, &quot;job&quot;: { &quot;namespace&quot;: &quot;workshop&quot;, &quot;name&quot;: &quot;process_taxes&quot;, &quot;facets&quot;: { &quot;sql&quot;: { &quot;query&quot;: &quot;insert into taxes_out select id, name, is_active from taxes_in&quot; } } }, &quot;inputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://workshop-db:None&quot;, &quot;name&quot;: &quot;workshop.public.taxes-in&quot;, &quot;facets&quot;: { &quot;schema&quot;: { &quot;fields&quot;: [ { &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;int&quot;, &quot;description&quot;: &quot;Customer's identifier&quot; }, { &quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;Customer's name&quot; }, { &quot;name&quot;: &quot;is_active&quot;, &quot;type&quot;: &quot;boolean&quot;, &quot;description&quot;: &quot;Has customer completed activation process&quot; } ] } } }], &quot;outputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://workshop-db:None&quot;, &quot;name&quot;: &quot;workshop.public.taxes-out&quot;, &quot;facets&quot;: { &quot;schema&quot;: { &quot;fields&quot;: [ { &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;int&quot;, &quot;description&quot;: &quot;Customer's identifier&quot; }, { &quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;Customer's name&quot; }, { &quot;name&quot;: &quot;is_active&quot;, &quot;type&quot;: &quot;boolean&quot;, &quot;description&quot;: &quot;Has customer completed activation process&quot; } ] } } }], &quot;producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client&quot; } For more information of what kind of facets are available as part of OpenLineage spec, please refer to the sub sections Run Facets, Job Facets, and Dataset Facets of this document.","keywords":""},{"title":"Dataset Facets","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/","content":"Dataset Facets Dataset Facets are generally consisted of common facet that is used both in inputs and outputs of the OpenLineage event. There are facets that exist specifically for input or output datasets. { ... &quot;inputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://workshop-db:None&quot;, &quot;name&quot;: &quot;workshop.public.taxes-in&quot;, &quot;facets&quot;: { # This is where the common dataset facets are located }, &quot;inputFacets&quot;: { # This is where the input dataset facets are located } }], &quot;outputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://workshop-db:None&quot;, &quot;name&quot;: &quot;workshop.public.taxes-out&quot;, &quot;facets&quot;: { # This is where the common dataset facets are located }, &quot;outputFacets&quot;: { # This is where the output dataset facets are located } }], ... } In the above Example, Notice that there is a distinction of facets that are common for both input and output dataset, and input or output specific datasets. As for the common datasets, they all reside under the facets property. However, input or output specific facets are located either in inputFacets or outputFacets property.","keywords":""},{"title":"Column Level Lineage Dataset Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/column_lineage_facet","content":"Column Level Lineage Dataset Facet Column level lineage provides fine grained information on datasets' dependencies. Not only we know the dependency exist, but we are also able to understand which input columns are used to produce output columns. This allows answering questions like Which root input columns are used to construct column x? For example, a Job might executes the following query: INSERT INTO top_delivery_times ( order_id, order_placed_on, order_delivered_on, order_delivery_time ) SELECT order_id, order_placed_on, order_delivered_on, DATEDIFF(minute, order_placed_on, order_delivered_on) AS order_delivery_time, FROM delivery_7_days ORDER BY order_delivery_time DESC LIMIT 1; This would establish the following relationships between the delivery_7_days and top_delivery_times tables: An OpenLinage run state update that represent this query using column-level lineage facets might look like: { &quot;eventType&quot;: &quot;START&quot;, &quot;eventTime&quot;: &quot;2020-02-22T22:42:42.000Z&quot;, &quot;run&quot;: ..., &quot;job&quot;: ..., &quot;inputs&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot; } ], &quot;outputs&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.top_delivery_times&quot;, &quot;facets&quot;: { &quot;columnLineage&quot;: { &quot;_producer&quot;: &quot;https://github.com/MarquezProject/marquez/blob/main/docker/metadata.json&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-1/ColumnLineageDatasetFacet.json&quot;, &quot;fields&quot;: { &quot;order_id&quot;: { &quot;inputFields&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot;, &quot;field&quot;: &quot;order_id&quot; } ] }, &quot;order_placed_on&quot;: { &quot;inputFields&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot;, &quot;field&quot;: &quot;order_placed_on&quot; } ] }, &quot;order_delivered_on&quot;: { &quot;inputFields&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot;, &quot;field&quot;: &quot;order_delivered_on&quot; } ] }, &quot;order_delivery_time&quot;: { &quot;inputFields&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot;, &quot;field&quot;: &quot;order_placed_on&quot; }, { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot;, &quot;field&quot;: &quot;order_delivered_on&quot; } ] } } } } } ], ... } The facet specification can be found here.","keywords":""},{"title":"Data Quality Assertions Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/data_quality_assertions","content":"Data Quality Assertions Facet Example: { ... &quot;inputs&quot;: { &quot;facets&quot;: { &quot;dataQualityAssertions&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/DataQualityAssertionsDatasetFacet.json&quot;, &quot;assertions&quot;: [ { &quot;assertion&quot;: &quot;not_null&quot;, &quot;success&quot;: true, &quot;column&quot;: &quot;user_name&quot; }, { &quot;assertion&quot;: &quot;is_string&quot;, &quot;success&quot;: true, &quot;column&quot;: &quot;user_name&quot; } ] } } } ... } The facet specification can be found here.","keywords":""},{"title":"Custom Facets","type":0,"sectionRef":"#","url":"/docs/spec/facets/custom-facets","content":"","keywords":""},{"title":"Example of creating your first custom facet‚Äã","type":1,"pageTitle":"Custom Facets","url":"/docs/spec/facets/custom-facets#example-of-creating-your-first-custom-facet","content":"Let's look at this sample OpenLineage client code written in python, that defines and uses a custom facet called my-facet. #!/usr/bin/env python3 from openlineage.client.run import ( RunEvent, RunState, Run, Job, Dataset, OutputDataset, InputDataset, ) from openlineage.client.client import OpenLineageClient, OpenLineageClientOptions from openlineage.client.facet import ( BaseFacet, SqlJobFacet, SchemaDatasetFacet, SchemaField, SourceCodeLocationJobFacet, NominalTimeRunFacet, ) import uuid from datetime import datetime, timezone, timedelta from typing import List import attr from random import random import logging, os logging.basicConfig(level=logging.DEBUG) PRODUCER = f&quot;https://github.com/openlineage-user&quot; namespace = &quot;python_client&quot; url = &quot;http://localhost:5000&quot; api_key = &quot;1234567890ckcu028rzu5l&quot; client = OpenLineageClient( url=url, # optional api key in case the backend requires it options=OpenLineageClientOptions(api_key=api_key), ) # generates job facet def job(job_name, sql, location): facets = { &quot;sql&quot;: SqlJobFacet(sql) } if location != None: facets.update( {&quot;sourceCodeLocation&quot;: SourceCodeLocationJobFacet(&quot;git&quot;, location)} ) return Job(namespace=namespace, name=job_name, facets=facets) @attr.s class MyFacet(BaseFacet): name: str = attr.ib() age: str = attr.ib() email: str = attr.ib() _additional_skip_redact: List[str] = ['name', 'age', 'email'] def __init__(self, name, age, email): super().__init__() self.name = name self.age = age self.email = email # geneartes run racet def run(run_id, hour, name, age, email): return Run( runId=run_id, facets={ &quot;nominalTime&quot;: NominalTimeRunFacet( nominalStartTime=f&quot;2022-04-14T{twoDigits(hour)}:12:00Z&quot; ), &quot;my_facet&quot;: MyFacet(name, age, email) }, ) # generates dataset def dataset(name, schema=None, ns=namespace): if schema == None: facets = {} else: facets = {&quot;schema&quot;: schema} return Dataset(namespace, name, facets) # generates output dataset def outputDataset(dataset, stats): output_facets = {&quot;stats&quot;: stats, &quot;outputStatistics&quot;: stats} return OutputDataset(dataset.namespace, dataset.name, dataset.facets, output_facets) # generates input dataset def inputDataset(dataset, dq): input_facets = { &quot;dataQuality&quot;: dq, } return InputDataset(dataset.namespace, dataset.name, dataset.facets, input_facets) def twoDigits(n): if n &lt; 10: result = f&quot;0{n}&quot; elif n &lt; 100: result = f&quot;{n}&quot; else: raise f&quot;error: {n}&quot; return result now = datetime.now(timezone.utc) # generates run Event def runEvents(job_name, sql, inputs, outputs, hour, min, location, duration): run_id = str(uuid.uuid4()) myjob = job(job_name, sql, location) myrun = run(run_id, hour, 'user_1', 25, 'user_1@email.com') st = now + timedelta(hours=hour, minutes=min, seconds=20 + round(random() * 10)) end = st + timedelta(minutes=duration, seconds=20 + round(random() * 10)) started_at = st.isoformat() ended_at = end.isoformat() return ( RunEvent( eventType=RunState.START, eventTime=started_at, run=myrun, job=myjob, producer=PRODUCER, inputs=inputs, outputs=outputs, ), RunEvent( eventType=RunState.COMPLETE, eventTime=ended_at, run=myrun, job=myjob, producer=PRODUCER, inputs=inputs, outputs=outputs, ), ) # add run event to the events list def addRunEvents( events, job_name, sql, inputs, outputs, hour, minutes, location=None, duration=2 ): (start, complete) = runEvents( job_name, sql, inputs, outputs, hour, minutes, location, duration ) events.append(start) events.append(complete) events = [] # create dataset data for i in range(0, 5): user_counts = dataset(&quot;tmp_demo.user_counts&quot;) user_history = dataset( &quot;temp_demo.user_history&quot;, SchemaDatasetFacet( fields=[ SchemaField(name=&quot;id&quot;, type=&quot;BIGINT&quot;, description=&quot;the user id&quot;), SchemaField( name=&quot;email_domain&quot;, type=&quot;VARCHAR&quot;, description=&quot;the user id&quot; ), SchemaField(name=&quot;status&quot;, type=&quot;BIGINT&quot;, description=&quot;the user id&quot;), SchemaField( name=&quot;created_at&quot;, type=&quot;DATETIME&quot;, description=&quot;date and time of creation of the user&quot;, ), SchemaField( name=&quot;updated_at&quot;, type=&quot;DATETIME&quot;, description=&quot;the last time this row was updated&quot;, ), SchemaField( name=&quot;fetch_time_utc&quot;, type=&quot;DATETIME&quot;, description=&quot;the time the data was fetched&quot;, ), SchemaField( name=&quot;load_filename&quot;, type=&quot;VARCHAR&quot;, description=&quot;the original file this data was ingested from&quot;, ), SchemaField( name=&quot;load_filerow&quot;, type=&quot;INT&quot;, description=&quot;the row number in the original file&quot;, ), SchemaField( name=&quot;load_timestamp&quot;, type=&quot;DATETIME&quot;, description=&quot;the time the data was ingested&quot;, ), ] ), &quot;snowflake://&quot;, ) create_user_counts_sql = &quot;&quot;&quot;CREATE OR REPLACE TABLE TMP_DEMO.USER_COUNTS AS ( SELECT DATE_TRUNC(DAY, created_at) date, COUNT(id) as user_count FROM TMP_DEMO.USER_HISTORY GROUP BY date )&quot;&quot;&quot; # location of the source code location = &quot;https://github.com/some/airflow/dags/example/user_trends.py&quot; # run simulating Airflow DAG with snowflake operator addRunEvents( events, &quot;create_user_counts&quot;, create_user_counts_sql, [user_history], [user_counts], i, 11, location, ) for event in events: from openlineage.client.serde import Serde # print(Serde.to_json(event)) # time.sleep(1) client.emit(event)  As you can see in the source code, there is a class called MyFacet which extends from the BaseFacet of OpenLineage, having three attributes of name, age, and email. @attr.s class MyFacet(BaseFacet): name: str = attr.ib() age: str = attr.ib() email: str = attr.ib() _additional_skip_redact: List[str] = ['name', 'age', 'email'] def __init__(self, name, age, email): super().__init__() self.name = name self.age = age self.email = email  And, when the application is generating a Run data, you can see the instantiation of MyFacet, having the name my_facet. def run(run_id, hour, name, age, email): return Run( runId=run_id, facets={ &quot;nominalTime&quot;: NominalTimeRunFacet( nominalStartTime=f&quot;2022-04-14T{twoDigits(hour)}:12:00Z&quot; ), &quot;my_facet&quot;: MyFacet(name, age, email) }, )  When you run this application with python (and please make sure you have installed openlineage-python using pip before running it), you will see a series of JSON output that represents the OpenLineage events being submitted. Here is one example. { &quot;eventTime&quot;: &quot;2022-12-09T09:17:28.239394+00:00&quot;, &quot;eventType&quot;: &quot;COMPLETE&quot;, &quot;inputs&quot;: [ { &quot;facets&quot;: { &quot;schema&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.18.0/client/python&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/SchemaDatasetFacet&quot;, &quot;fields&quot;: [ { &quot;description&quot;: &quot;the user id&quot;, &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;BIGINT&quot; }, { &quot;description&quot;: &quot;the user id&quot;, &quot;name&quot;: &quot;email_domain&quot;, &quot;type&quot;: &quot;VARCHAR&quot; }, { &quot;description&quot;: &quot;the user id&quot;, &quot;name&quot;: &quot;status&quot;, &quot;type&quot;: &quot;BIGINT&quot; }, { &quot;description&quot;: &quot;date and time of creation of the user&quot;, &quot;name&quot;: &quot;created_at&quot;, &quot;type&quot;: &quot;DATETIME&quot; }, { &quot;description&quot;: &quot;the last time this row was updated&quot;, &quot;name&quot;: &quot;updated_at&quot;, &quot;type&quot;: &quot;DATETIME&quot; }, { &quot;description&quot;: &quot;the time the data was fetched&quot;, &quot;name&quot;: &quot;fetch_time_utc&quot;, &quot;type&quot;: &quot;DATETIME&quot; }, { &quot;description&quot;: &quot;the original file this data was ingested from&quot;, &quot;name&quot;: &quot;load_filename&quot;, &quot;type&quot;: &quot;VARCHAR&quot; }, { &quot;description&quot;: &quot;the row number in the original file&quot;, &quot;name&quot;: &quot;load_filerow&quot;, &quot;type&quot;: &quot;INT&quot; }, { &quot;description&quot;: &quot;the time the data was ingested&quot;, &quot;name&quot;: &quot;load_timestamp&quot;, &quot;type&quot;: &quot;DATETIME&quot; } ] } }, &quot;name&quot;: &quot;temp_demo.user_history&quot;, &quot;namespace&quot;: &quot;python_client&quot; } ], &quot;job&quot;: { &quot;facets&quot;: { &quot;sourceCodeLocation&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.18.0/client/python&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/SourceCodeLocationJobFacet&quot;, &quot;type&quot;: &quot;git&quot;, &quot;url&quot;: &quot;https://github.com/some/airflow/dags/example/user_trends.py&quot; }, &quot;sql&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.18.0/client/python&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/SqlJobFacet&quot;, &quot;query&quot;: &quot;CREATE OR REPLACE TABLE TMP_DEMO.USER_COUNTS AS (\\n\\t\\t\\tSELECT DATE_TRUNC(DAY, created_at) date, COUNT(id) as user_count\\n\\t\\t\\tFROM TMP_DEMO.USER_HISTORY\\n\\t\\t\\tGROUP BY date\\n\\t\\t\\t)&quot; } }, &quot;name&quot;: &quot;create_user_counts&quot;, &quot;namespace&quot;: &quot;python_client&quot; }, &quot;outputs&quot;: [ { &quot;facets&quot;: {}, &quot;name&quot;: &quot;tmp_demo.user_counts&quot;, &quot;namespace&quot;: &quot;python_client&quot; } ], &quot;producer&quot;: &quot;https://github.com/openlineage-user&quot;, &quot;run&quot;: { &quot;facets&quot;: { &quot;my_facet&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.18.0/client/python&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/BaseFacet&quot;, &quot;age&quot;: 25, &quot;email&quot;: &quot;user_1@email.com&quot;, &quot;name&quot;: &quot;user_1&quot; }, &quot;nominalTime&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.18.0/client/python&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/NominalTimeRunFacet&quot;, &quot;nominalStartTime&quot;: &quot;2022-04-14T04:12:00Z&quot; } }, &quot;runId&quot;: &quot;7886a902-8fec-422f-9ee4-818489e59f5f&quot; } }  Notice the facet information my_facet that has is now part of the OpenLineage event.  ... &quot;run&quot;: { &quot;facets&quot;: { &quot;my_facet&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.18.0/client/python&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/BaseFacet&quot;, &quot;age&quot;: 25, &quot;email&quot;: &quot;user_1@email.com&quot;, &quot;name&quot;: &quot;user_1&quot; }, ...  OpenLineage backend should be able to store this information when submitted, and later, when you access the Lineage, you should be able to view the facet information that you submitted, along with your custom facet that you made. Below is the screen shot of one of the OpenLineage backend called Marquez, that shows th custom facet that the application has submitted.  You might have noticed the schema URL is actually that of BaseFacet. By default, if the facet class did not specify its own schema URL, that value would be that of BaseFacet. From the view of OpenLineage specification, this is legal. However, if you have your own JSON spec defined, and has it publically accessible, you can specify it by overriding the _get_schema function as such: @attr.s class MyFacet(BaseFacet): name: str = attr.ib() age: str = attr.ib() email: str = attr.ib() _additional_skip_redact: List[str] = ['name', 'age', 'email'] def __init__(self, name, age, email): super().__init__() self.name = name self.age = age self.email = email @staticmethod def _get_schema() -&gt; str: return &quot;https://somewhere/schemas/myfacet.json#/definitions/MyFacet&quot;  And the _schemaURL of the OpenLineage event would now reflect the change as such:  &quot;run&quot;: { &quot;facets&quot;: { &quot;my_facet&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.18.0/client/python&quot;, &quot;_schemaURL&quot;: &quot;https://somewhere/schemas/myfacet.json#/definitions/MyFacet&quot;, &quot;age&quot;: 25, &quot;email&quot;: &quot;user_1@email.com&quot;, &quot;name&quot;: &quot;user_1&quot; },  "},{"title":"Datasource Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/data_source","content":"Datasource Facet Example: { ... &quot;inputs&quot;: { &quot;facets&quot;: { &quot;dataSource&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/DatasourceDatasetFacet.json&quot;, &quot;name&quot;: &quot;datasource_one&quot;, &quot;url&quot;: &quot;https://some.location.com/datsource/one&quot; } } } ... } The facet specification can be found here.","keywords":""},{"title":"Lifecycle State Change Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/lifecycle_state_change","content":"Lifecycle State Change Facet Example: { ... &quot;outputs&quot;: { &quot;facets&quot;: { &quot;lifecycleStateChange&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/LifecycleStateChangeDatasetFacet.json&quot;, &quot;lifecycleStateChange&quot;: &quot;CREATE&quot; } } } ... } { ... &quot;outputs&quot;: { &quot;facets&quot;: { &quot;lifecycleStateChange&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/LifecycleStateChangeDatasetFacet.json&quot;, &quot;lifecycleStateChange&quot;: &quot;RENAME&quot;, &quot;previousIdentifier&quot;: { &quot;namespace&quot;: &quot;example_namespace&quot;, &quot;name&quot;: &quot;example_table_1&quot; } } } } ... } The facet specification can be found here.","keywords":""},{"title":"Data Quality Metrics Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/input-dataset-facets/data_quality_metrics","content":"Data Quality Metrics Facet Example: { ... &quot;inputs&quot;: { &quot;inputFacets&quot;: { &quot;dataQualityMetrics&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/DataQualityMetricsInputDatasetFacet.json&quot;, &quot;rowCount&quot;: 123, &quot;bytes&quot;: 35602, &quot;columnMetrics&quot;: { &quot;column_one&quot;: { &quot;nullCount&quot;: 132, &quot;distincCount&quot;: 11, &quot;sum&quot;: 500, &quot;count&quot;: 234, &quot;min&quot;: 111, &quot;max&quot;: 3234, &quot;quantiles&quot;: { &quot;0.1&quot;: 12, &quot;0.5&quot;: 22, &quot;1&quot;: 123, &quot;2&quot;: 11 } }, &quot;column_two&quot;: { &quot;nullCount&quot;: 132, &quot;distincCount&quot;: 11, &quot;sum&quot;: 500, &quot;count&quot;: 234, &quot;min&quot;: 111, &quot;max&quot;: 3234, &quot;quantiles&quot;: { &quot;0.1&quot;: 12, &quot;0.5&quot;: 22, &quot;1&quot;: 123, &quot;2&quot;: 11 } }, &quot;column_three&quot;: { &quot;nullCount&quot;: 132, &quot;distincCount&quot;: 11, &quot;sum&quot;: 500, &quot;count&quot;: 234, &quot;min&quot;: 111, &quot;max&quot;: 3234, &quot;quantiles&quot;: { &quot;0.1&quot;: 12, &quot;0.5&quot;: 22, &quot;1&quot;: 123, &quot;2&quot;: 11 } } } } } } ... } The facet specification can be found here.","keywords":""},{"title":"Output Statistics Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/output-dataset-facets/output_statistics","content":"Output Statistics Facet Example: { ... &quot;inputs&quot;: { &quot;outputFacets&quot;: { &quot;outputStatistics&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/OutputStatisticsOutputDatasetFacet.json&quot;, &quot;rowCount&quot;: 123, &quot;size&quot;: 35602 } } } ... } The facet specification can be found here.","keywords":""},{"title":"Ownership Dataset Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/ownership","content":"Ownership Dataset Facet Example: { ... &quot;inputs&quot;: { &quot;facets&quot;: { &quot;ownership&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/OwnershipDatasetFacet.json&quot;, &quot;owners&quot;: [ { &quot;name&quot;: &quot;maintainer_one&quot;, &quot;type&quot;: &quot;MAINTAINER&quot; } ] } } } ... } The facet specification can be found here.","keywords":""},{"title":"Schema Dataset Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/schema","content":"Schema Dataset Facet The schema dataset facet contains the schema of a particular dataset. Besides a name, it provides an optional type and description of each field. Example: { ... &quot;inputs&quot;: { &quot;facets&quot;: { &quot;schema&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/SchemaDatasetFacet.json&quot;, &quot;fields&quot;: [ { &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;int&quot;, &quot;description&quot;: &quot;Customer's identifier&quot; }, { &quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;Customer's name&quot; }, { &quot;name&quot;: &quot;is_active&quot;, &quot;type&quot;: &quot;boolean&quot;, &quot;description&quot;: &quot;Has customer completed activation process&quot; } ] } } } ... } The facet specification can be found here.","keywords":""},{"title":"Storage Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/storage","content":"Storage Facet Example: { ... &quot;inputs&quot;: { &quot;facets&quot;: { &quot;storage&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/StorageDatasetFacet.json&quot;, &quot;storageLayer&quot;: &quot;iceberg&quot;, &quot;fileFormat&quot;: &quot;csv&quot; } } } ... } The facet specification can be found here.","keywords":""},{"title":"Symlinks Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/symlinks","content":"Symlinks Facet Example: { ... &quot;inputs&quot;: { &quot;facets&quot;: { &quot;symlinks&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/SymlinksDatasetFacet.json&quot;, &quot;identifiers&quot;: [ &quot;namespace&quot;: &quot;example_namespace&quot;, &quot;name&quot;: &quot;example_dataset_1&quot;, &quot;type&quot;: &quot;table&quot; ] } } } ... } The facet specification can be found here.","keywords":""},{"title":"Job Facets","type":0,"sectionRef":"#","url":"/docs/spec/facets/job-facets/","content":"Job Facets Job Facets apply to a distinct instance of a job: an abstract process that consumes, executes, and produces datasets (defined as its inputs and outputs). It is identified by a unique name within a namespace. The Job evolves over time and this change is captured during the job runs.","keywords":""},{"title":"Version Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/version_facet","content":"Version Facet Example: { ... &quot;inputs&quot;: { &quot;facets&quot;: { &quot;version&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/DatasetVersionDatasetFacet.json&quot;, &quot;datasetVersion&quot;: &quot;1&quot; } } } ... } The facet specification can be found here.","keywords":""},{"title":"Ownership Job Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/job-facets/ownership","content":"Ownership Job Facet The facet that contains the information regarding users or group who owns this particular job. Example: { ... &quot;job&quot;: { &quot;facets&quot;: { &quot;ownership&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/OwnershipJobFacet.json&quot;, &quot;owners&quot;: [ { &quot;name&quot;: &quot;maintainer_one&quot;, &quot;type&quot;: &quot;MAINTAINER&quot; } ] } } } ... } The facet specification can be found here","keywords":""},{"title":"Documentation Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/job-facets/documentation","content":"Documentation Facet Contains the documentation or description of the job. Example: { ... &quot;job&quot;: { &quot;facets&quot;: { &quot;documentation&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/main/spec/facets/DocumentationJobFacet.json&quot;, &quot;description&quot;: &quot;This is the documentation of something.&quot; } } } ... } The facet specification can be found here","keywords":""},{"title":"Source Code Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/job-facets/source-code","content":"Source Code Facet The source code of a particular job (e.g. Python script) Example: { ... &quot;job&quot;: { &quot;facets&quot;: { &quot;sourceCode&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/main/spec/facets/SourceCodeJobFacet.json&quot;, &quot;language&quot;: &quot;python&quot;, &quot;sourceCode&quot;: &quot;print('hello, OpenLineage!')&quot; } } } ... } The facet specification can be found here","keywords":""},{"title":"SQL Job Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/job-facets/sql","content":"SQL Job Facet The SQL Job Facet contains a SQL query that was used in a particular job. Example: { ... &quot;job&quot;: { &quot;facets&quot;: { &quot;sql&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/main/spec/facets/SQLJobFacet.json&quot;, &quot;query&quot;: &quot;select id, name from schema.table where id = 1&quot; } } } ... } The facet specification can be found here","keywords":""},{"title":"Source Code Location Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/job-facets/source-code-location","content":"Source Code Location Facet The facet that indicates where the source code is located. Example: { ... &quot;job&quot;: { &quot;facets&quot;: { &quot;sourceCodeLocation&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/main/spec/facets/SourceCodeLocationJobFacet.json&quot;, &quot;type&quot;: &quot;git|svn&quot;, &quot;url&quot;: &quot;https://github.com/MarquezProject/marquez-airflow-quickstart/blob/693e35482bc2e526ced2b5f9f76ef83dec6ec691/dags/hello.py&quot;, &quot;repoUrl&quot;: &quot;git@github.com:{org}/{repo}.git or https://github.com/{org}/{repo}.git|svn://&lt;your_ip&gt;/&lt;repository_name&gt;&quot;, &quot;path&quot;: &quot;path/to/my/dags&quot;, &quot;version&quot;: &quot;git: the git sha | Svn: the revision number&quot;, &quot;tag&quot;: &quot;example&quot;, &quot;branch&quot; &quot;main&quot; } } } ... } The facet specification can be found here","keywords":""},{"title":"Run Facets","type":0,"sectionRef":"#","url":"/docs/spec/facets/run-facets/","content":"Run Facets Run Facets apply to a specific instance of a particular running job. Every run will have a uniquely identifiable run ID that is usually in UUID format, that can later be tracked.","keywords":""},{"title":"External Query Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/run-facets/external_query","content":"External Query Facet The facet that describes the identification of the query that the run is related to which was executed by external systems. Even though the query itself is not contained, using this facet, the user should be able to access the query and its details. Example: { ... &quot;run&quot;: { &quot;facets&quot;: { &quot;externalQuery&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/main/spec/facets/ExternalQueryRunFacet.json&quot;, &quot;externalQueryId&quot;: &quot;my-project-1234:US.bquijob_123x456_123y123z123c&quot;, &quot;source&quot;: &quot;bigquery&quot; } } } ... } The facet specification can be found here","keywords":""},{"title":"Error Message Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/run-facets/error_message","content":"Error Message Facet The facet to contain information about the failures during the run of the job. A typical payload would be the message, stack trace, etc. Example: { ... &quot;run&quot;: { &quot;facets&quot;: { &quot;errorMessage&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/main/spec/facets/ErrorMessageRunFacet.json&quot;, &quot;message&quot;: &quot;org.apache.spark.sql.AnalysisException: Table or view not found: wrong_table_name; line 1 pos 14&quot;, &quot;programmingLanguage&quot;: &quot;JAVA&quot;, &quot;stackTrace&quot;: &quot;Exception in thread \\&quot;main\\&quot; java.lang.RuntimeException: A test exception\\nat io.openlineage.SomeClass.method(SomeClass.java:13)\\nat io.openlineage.SomeClass.anotherMethod(SomeClass.java:9)&quot; } } } ... } The facet specification can be found here","keywords":""},{"title":"Nominal Time Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/run-facets/nominal_time","content":"Nominal Time Facet The facet to describe the nominal start and end time of the run. The nominal usually means the time the job run was expected to run (like a scheduled time), and the actual time can be different. Example: { ... &quot;run&quot;: { &quot;facets&quot;: { &quot;nominalTime&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/main/spec/facets/SQLJobFacet.json&quot;, &quot;nominalStartTime&quot;: &quot;2020-12-17T03:00:00.000Z&quot;, &quot;nominalEndTime&quot;: &quot;2020-12-17T03:05:00.000Z&quot; } } } ... } The facet specification can be found here","keywords":""},{"title":"Parent Run Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/run-facets/parent_run","content":"Parent Run Facet Commonly, scheduler systems like Apache Airflow will trigger processes on remote systems, such as on Apache Spark or Apache Beam jobs. Those systems might have their own OpenLineage integration and report their own job runs and dataset inputs/outputs. The ParentRunFacet allows those downstream jobs to report which jobs spawned them to preserve job hierarchy. To do that, the scheduler system should have a way to pass its own job and run id to the child job. Example: { ... &quot;run&quot;: { &quot;facets&quot;: { &quot;parent&quot;: { &quot;job&quot;: { &quot;name&quot;: &quot;the-execution-parent-job&quot;, &quot;namespace&quot;: &quot;the-namespace&quot; }, &quot;run&quot;: { &quot;runId&quot;: &quot;f99310b4-3c3c-1a1a-2b2b-c1b95c24ff11&quot; } } } } ... } The facet specification can be found here.","keywords":""},{"title":"Naming Conventions","type":0,"sectionRef":"#","url":"/docs/spec/naming","content":"","keywords":""},{"title":"Dataset Naming‚Äã","type":1,"pageTitle":"Naming Conventions","url":"/docs/spec/naming#dataset-naming","content":"A dataset, or table, is organized according to a producer, namespace, database and (optionally) schema. Producer\tFormula\tExample URIPostgres\tproducer + host + port + database + table\tpostgres://db.foo.com:6543/metrics.sales.orders MySQL\tproducer + host + port + database + table\tmysql://db.foo.com:6543/metrics.orders S3\tproducer + bucket + path\ts3://sales-metrics/orders.csv GCS\tproducer + bucket + path\tgcs://sales-metrics/orders.csv HDFS\tproducer + host + port + path\thdfs://stg.foo.com:3000/salesorders.csv BigQuery\tproducer + project + dataset + table\tbigquery:metrics.sales.orders Redshift\tproducer + host + port + database + schema + table\tredshift://examplecluster.XXXXXXXXXXXX.us-west-2.redshift.amazonaws.com:5439/metrics.sales.orders Athena\tproducer + host + catalog + database + table\tawsathena://athena.us-west-2.amazonaws.com/metrics.sales.orders Azure Synapse\tproducer + host + port + database + schema + table\tsqlserver://XXXXXXXXXXXX.sql.azuresynapse.net:1433;database=SQLPool1/sales.orders Azure Cosmos DB\tproducer + host + database + 'colls' + table\tazurecosmos://XXXXXXXXXXXX.documents.azure.com/dbs/metrics/colls/orders "},{"title":"Job Naming‚Äã","type":1,"pageTitle":"Naming Conventions","url":"/docs/spec/naming#job-naming","content":"A Job is a recurring data transformation with inputs and outputs. Each execution is captured as a Run with corresponding metadata. A Run event identifies the Job it instances by providing the job‚Äôs unique identifier. The Job identifier is composed of a Namespace and Name. The job name is unique within its namespace. Producer\tFormula\tExampleAirflow\tnamespace + DAG + task\tairflow-staging.orders_etl.count_orders SQL\tnamespace + name\tgx.validate_datasets "},{"title":"Run Naming‚Äã","type":1,"pageTitle":"Naming Conventions","url":"/docs/spec/naming#run-naming","content":"Runs are named using client-generated UUIDs. The OpenLineage client is responsible for generating them and maintaining them throughout the duration of the runcycle. from openlineage.client.run import Run run = Run(str(uuid4()))  "},{"title":"Why Naming Matters‚Äã","type":1,"pageTitle":"Naming Conventions","url":"/docs/spec/naming#why-naming-matters","content":"Naming enables focused insight into data flows, even when datasets and workflows are distributed across an organization. This focus enabled by naming is key to the production of useful lineage.  "},{"title":"Additional Resources‚Äã","type":1,"pageTitle":"Naming Conventions","url":"/docs/spec/naming#additional-resources","content":"The OpenLineage Naming SpecWhat's in a Namespace Blog Post "},{"title":"Producers","type":0,"sectionRef":"#","url":"/docs/spec/producers","content":"Producers info This page could use some extra detail! You're welcome to contribute using the Edit link at the bottom. The _producer value is included in an OpenLineage request as a way to know how the metadata was generated. It is a URI that links to a source code SHA or the location where a package can be found. For example, this field is populated by many of the common integrations. For example, the dbt integration will set this value to https://github.com/OpenLineage/OpenLineage/tree/{__version__}/integration/dbt and the Python client will set it to https://github.com/OpenLineage/OpenLineage/tree/{__version__}/client/python.","keywords":""},{"title":"Object Model","type":0,"sectionRef":"#","url":"/docs/spec/object-model","content":"","keywords":""},{"title":"Run State Update‚Äã","type":1,"pageTitle":"Object Model","url":"/docs/spec/object-model#run-state-update","content":"A Run State Update is prepared and sent when something important occurs within your pipeline, and each one can be thought of as a distinct observation. This commonly happens when a Job starts or finishes. The run state itself refers to a stage within the run cycle of the current run. Usually, the first Run State for a Job would be START and the last would be COMPLETE. A run cycle is likely to have at least two Run State Updates, and perhaps more. Each one will also have timestamp of when this particular state change happened.  Each Run State Update can include detail about the Job, the Run, and the input and output Datasets involved in the run. Subsequent updates are additive: input Datasets, for example, can be specified along with START, along with COMPLETE, or both. This accommodates situations where information is only available at certain times. Each of these three core entities can also be extended through the use of facets, some of which are documented in the relevant sections below. "},{"title":"Job‚Äã","type":1,"pageTitle":"Object Model","url":"/docs/spec/object-model#job","content":"A Job is a process that consumes or produces Datasets. This is abstract, and can map to different things in different operational contexts. For example, a job could be a task in a workflow orchestration system. It could also be a model, a query, or a checkpoint. Depending on the system under observation, a Job can represent a small or large amount of work. A Job is the part of the object model that represents a discrete bit of defined work. If, for example, you have cron running a Python script that executes a CREATE TABLE x AS SELECT * FROM y query every day, the Python script is the Job. Jobs are identified by a unique name within a namespace. They are expected to evolve over time and their changes can be captured through Run State Updates. "},{"title":"Job Facets‚Äã","type":1,"pageTitle":"Object Model","url":"/docs/spec/object-model#job-facets","content":"Facets that can be used to augment the metadata of a Job include: sourceCodeLocation: Captures the source code location and version (e.g., the git SHA) of the job. sourceCode: Captures the language (e.g. python) and complete source code of the job. Using this source code, users can gain useful information about what the job does. For more details, please refer to the Job Facets. "},{"title":"Run‚Äã","type":1,"pageTitle":"Object Model","url":"/docs/spec/object-model#run","content":"A Run is an instance of a Job that represents one of its occurrences in time. Each run will have a uniquely identifiable runId that is generated by the client in UUID format. The client is responsible for maintaining the runId between different Run State Updates in the same Run. Runs can be used to observe changes in Jobs between their instances. If, for example, you have cron running a Python script that repeats a query every day, this should resuilt in a separate Run for each day. "},{"title":"Run Facets‚Äã","type":1,"pageTitle":"Object Model","url":"/docs/spec/object-model#run-facets","content":"Facets that can be used to augment the metadata of a Run include: nominalTime: Captures the time this run is scheduled for. This is typically used for scheduled jobs. The job has a nominally scheduled time that will be different from the actual time it ran. parent: Captures the parent Job and Run, for instances where this Run was spawned from a parent Run. For example in the case of Airflow, there's a Run that represents the DAG itself that is the parent of the individual Runs that represent the tasks it spawns. Similarly when a SparkOperator starts a Spark job, this creates a separate run that refers to the task run as its parent. errorMessage: Captures potential error messages - and optionally stack traces - with which the run failed. sql: Captures the SQL query, if this job runs one. For more details, please refer to the Run Facets. "},{"title":"Dataset‚Äã","type":1,"pageTitle":"Object Model","url":"/docs/spec/object-model#dataset","content":"A Dataset is an abstract representation of data. This can refer to a small amount or large amount of data, as long as it's discrete. For databases, this should be a table. For cloud storage, this is often an object in a bucket. This can represent a directory of a filesystem. It has a unique name within a namespace derived from its physical location (i.e., db.host.database.schema.table). The combined namespace and name for a Dataset should be enough to uniquely identify it within a data ecosystem. Typically, a Dataset changes when a job writing to it completes. Similarly to the Job and Run distinction, metadata that is more static from Run to Run is captured in a DatasetFacet - for example, the schema that does not change every run). What changes every Run is captured as an InputFacet or an OutputFacet - for example, a time partition indicating the subset of the data set that was read or written). A Dataset is the part of the object model that represents a discrete collection of data. If, for example, you have cron running a Python script that executes a CREATE TABLE x AS SELECT * FROM y query every day, the x and y tables are Datasets. "},{"title":"Dataset Facets‚Äã","type":1,"pageTitle":"Object Model","url":"/docs/spec/object-model#dataset-facets","content":"Facets that can be used to augment the metadata of a Dataset include: schema: Captures the schema of the dataset dataSource: Captures the database instance containing this Dataset (e.g., database schema, object store bucket) lifecycleStateChange: Captures the lifecycle states of the Dataset (e.g., alter, create, drop, overwrite, rename, truncate) version: Captures the dataset version when versioning is defined by the data store (e.g.. Iceberg snapshot ID) Input Datasets have the following facets: dataQualityMetrics: Captures dataset-level and column-level data quality metrics (row count, byte size, null count, distinct count, average, min, max, quantiles) dataQualityAssertions: Captures the result of running data tests on dataset or its columns Output Datasets have the following facets: outputStatistics: Captures the size of the output written to a dataset (e.g., row count and byte size) For more details, please refer to the Dataset Facets. "},{"title":"The Run Cycle","type":0,"sectionRef":"#","url":"/docs/spec/run-cycle","content":"","keywords":""},{"title":"Run States‚Äã","type":1,"pageTitle":"The Run Cycle","url":"/docs/spec/run-cycle#run-states","content":"There are five run states currently defined in the OpenLineage spec: START to indicate the beginning of a Job RUNNING to provide additional information about a running Job COMPLETE to signify that execution of the Job has concluded ABORT to signify that the Job has been stopped abnormally FAIL to signify that the Job has failed OTHER to provide additional metadata outside of the standard run cycle - e.g., on a run that has already completed "},{"title":"Typical Scenarios‚Äã","type":1,"pageTitle":"The Run Cycle","url":"/docs/spec/run-cycle#typical-scenarios","content":"A batch Job - e.g., an Airflow task or a dbt model - will typically be represented as a START event followed by a COMPLETE event. Occasionally, an ABORT or FAIL event will be sent when a job does not complete successfully.  A long-running Job - e.g., a microservice or a stream - will typically be represented by a START event followed by a series of RUNNING events that report changes in the run or emit performace metrics. Occasionally, a COMPLETE, ABORT, or FAIL event will occur, often followed by a START event as the job is reinitiated.  "},{"title":"Working with Schemas","type":0,"sectionRef":"#","url":"/docs/spec/schemas","content":"","keywords":""},{"title":"Create a new issue with label spec‚Äã","type":1,"pageTitle":"Working with Schemas","url":"/docs/spec/schemas#create-a-new-issue-with-label-spec","content":"Before you decide to make any changes, it is best advised that you first label your issue with spec. This will indicate the the issue is related to any changes in the current OpenLineage spec. "},{"title":"Make changes to the spec's version‚Äã","type":1,"pageTitle":"Working with Schemas","url":"/docs/spec/schemas#make-changes-to-the-specs-version","content":"Whenever there is a change to existing spec file (JSON), you need to bump up the version of the existing current spec, so that the changes can go through the code generation and gradle build. Consider the following spec file, where you will see the URL in $id that shows what is the current spec version the file currently is. { &quot;$schema&quot;: &quot;https://json-schema.org/draft/2020-12/schema&quot;, &quot;$id&quot;: &quot;https://openlineage.io/spec/facets/1-0-1/ColumnLineageDatasetFacet.json&quot;, &quot;$defs&quot;: {  In this example, bumping up the version to the new value, should be changed from 1-0-1 to 1-0-2. { &quot;$schema&quot;: &quot;https://json-schema.org/draft/2020-12/schema&quot;, &quot;$id&quot;: &quot;https://openlineage.io/spec/facets/1-0-2/ColumnLineageDatasetFacet.json&quot;, &quot;$defs&quot;: {  If you do not bump the version to higher number, the code generation of Java client will fail. "},{"title":"Python client's codes need to be manually updated‚Äã","type":1,"pageTitle":"Working with Schemas","url":"/docs/spec/schemas#python-clients-codes-need-to-be-manually-updated","content":"Java client's build process does involve code generation that automatically produces OpenLineage classes derived from the spec files, so you do not need to do anything in terms of coding the client. However, python client libraries does not depend on the spec files to be generated, so you have to make sure to add changes to the python code in order for it to know and use the changes. As for the facets, they are implemented here, so generally, you need to apply necessary changes to it. As for the general structure of OpenLineage's run events, it can be found here. "},{"title":"Add test cases‚Äã","type":1,"pageTitle":"Working with Schemas","url":"/docs/spec/schemas#add-test-cases","content":"Make sure to add changes to the unit tests for python and java to make sure the unit test can be performed against your new SPEC changes. Refer to existing test codes to add yours in. "},{"title":"Test the SPEC change using code generation and integration tests‚Äã","type":1,"pageTitle":"Working with Schemas","url":"/docs/spec/schemas#test-the-spec-change-using-code-generation-and-integration-tests","content":"When you have modified the SPEC file(s), always make sure to perform code generation and unit tests by going into client/java and running ./gradlew generateCode and ./gradlew test. As for python, cd into client/python and run pytest. Note: Some of the tests may fail due to the fact that they require external systems like kafka. You can ignore those errors. "}]