[{"title":"Introducing OpenLineage 0.1.0","type":0,"sectionRef":"#","url":"/blog/0.1-release","content":"We are pleased to announce the initial release of OpenLineage. This release includes the core specification, data model, clients, and integrations with common data tools. We are pleased to announce the initial release of OpenLineage. This is the culmination of a broad community effort, and establishes a common framework for data lineage collection and analysis. We want to thank all the contributors as well all the projects and companies involved in the design (in alphabetical order): Airflow, Astronomer, Datakin, Data Mesh, dbt, Egeria, GetInData, Great Expectations, Iceberg (and others that I am probably forgetting). This release includes: The initial 1-0-0 release of the OpenLineage specificationA core lineage model of Jobs, Runs and Datasets Core facetsData Quality Metrics and statisticsDataset schemaSource code locationSQL Clients that send OpenLineage events to an HTTP backend JavaPython Integrations that collect lineage metadata as OpenLineage events Apache Airflow with support for BigQuery, Great Expectations, Postgres, Redshift, SnowflakeApache Sparkdbt This is only the beginning. We invite everyone interested to consult and contribute to the roadmap. The roadmap currently contains, among other things: adding support for Kafka, BI dashboards, and column level lineage...but you can influence it by participating! Follow the repo to stay updated. And, as always, you can join the conversation on Slack.","keywords":""},{"title":"Announcing OpenLineage 1.0","type":0,"sectionRef":"#","url":"/blog/1.0-release","content":"","keywords":""},{"title":"An Evolving Spec​","type":1,"pageTitle":"Announcing OpenLineage 1.0","url":"/blog/1.0-release#an-evolving-spec","content":"Now that we’re at v1.0, static lineage has made its way to OpenLineage! Up till now, OpenLineage has focused on “runtime” lineage - i.e., metadata generated when jobs are run. Capturing information as transformations of datasets occur enables precise descriptions of those transformations. The 1.0 release reflects the fact that there is demand for &quot;design-time&quot; lineage. The concept behind this is that even when datasets are not being touched yet, lineage metadata about them can still be useful and valuable. Although operational lineage covers many use cases, some scenarios call for lineage about jobs that have not run - and might never do so. Also, in many cases, a combination of both static and runtime approaches provides the best operational results. For example, imagine that a dataset exists in a data warehouse and dashboarding tool for which a pipeline has always been broken. Static lineage will show not only that the dataset exists but also that the pipeline for it has never run or always fails. "},{"title":"Implementing Static Lineage​","type":1,"pageTitle":"Announcing OpenLineage 1.0","url":"/blog/1.0-release#implementing-static-lineage","content":"For an overview of the implementation, read the release preview by Michael Robinson. The first part of the implementation was authored by Paweł Leszczyński in OpenLineage 0.29.2, which included two new event types along with support for them in the Python client. Additional work, contributed by Julien Le Dem and Jakub Dardziński, involved: adding facet deletion to handle the case in which a user adds and deletes a dataset in the same request (0.30.1)removing references to facets from the core spec that broke compatibility with the Json schema specification (1.0.0). On the Marquez side, adding support for static lineage is ongoing. Marquez 0.37.0 includes support in the API for decoding static events via a new EventTypeResolver. "},{"title":"Supporting New Use Cases​","type":1,"pageTitle":"Announcing OpenLineage 1.0","url":"/blog/1.0-release#supporting-new-use-cases","content":"With the release of 1.0, we now support use cases like: bootstrapping of a lineage graph with prospective runs for auditingcapturing dataset ownership changes outside of runsconsuming facets from external systemscreating dataset symlinks more easily Static lineage promises to fill the blind spots that dynamic lineage alone could not reach, offering a macroscopic view of how data flows and is accessed throughout an entire organization. "},{"title":"Additional Resources​","type":1,"pageTitle":"Announcing OpenLineage 1.0","url":"/blog/1.0-release#additional-resources","content":"Getting Started GuideOpenLineage 1.0 ReleaseJsonSchema SpecificationOpenAPI Specification for HTTP-based Implementation "},{"title":"The OpenLineage Airflow Provider is Here","type":0,"sectionRef":"#","url":"/blog/airflow-provider","content":"","keywords":""},{"title":"Critical Improvements​","type":1,"pageTitle":"The OpenLineage Airflow Provider is Here","url":"/blog/airflow-provider#critical-improvements","content":"Before 2.7.0, OpenLineage metadata was only available via a plugin implementation maintained in the OpenLineage project. In other words, the integration was an external package getting lineage from the outside. Being external to Airflow, the integration had to use extractors to get lineage – special classes created for all supported operators. In order to function, these locally maintained extractors had to understand operators’ internals and know where to look for data. While being the best possible approach under the circumstances, this solution was hardly ideal. On the one hand, it was brittle because it depended on both operators’ and Airflow’s internals. On the other, it required extra work to maintain compatibility with new versions of providers and Airflow itself. We had to keep up with changes to not only operators but also Airflow – which is not exactly a small, slowly-moving project. Improvements coming with the provider are not limited to fixes, however. The OpenLineage Provider promises to enable some long-sought-after enhancements, including support for one of the most-used Airflow operators – more about which below. "},{"title":"High-level Design​","type":1,"pageTitle":"The OpenLineage Airflow Provider is Here","url":"/blog/airflow-provider#high-level-design","content":"The provider approach solves these maintenance and reliability issues by moving the extraction logic, along with unit tests, to each provider. Although a lot of up-front work has gone into creating the provider, full implementation of this solution has actually been distributed (and necessarily remains a work in progress). No longer self-contained, the integration is now part of the operator contract and belongs to every provider that supports OpenLineage. Relocating the extraction logic in this way makes the integration more robust by ensuring the stability of the lineage contract in each operator. Another benefit of the approach: adding lineage coverage to custom operators is now easier. "},{"title":"Implementation​","type":1,"pageTitle":"The OpenLineage Airflow Provider is Here","url":"/blog/airflow-provider#implementation","content":"The OpenLineage Provider has been implemented in Airflow by reimplementing the openlineage-airflow package from the OpenLineage project in the apache-airflow-providers-openlineage provider in the base Airflow Docker image, where it can be easily enabled by configuration. Furthermore, lineage extraction logic that was included in Extractors in that package is now implemented in operators living in their provider package along with unit tests, eliminating the need for Extractors in most cases. For this purpose, a new optional API for Operators (get_openlineage_facets_on_{start(), complete(ti), failure(ti)}, documented here) can be used. Example Operator​ The Google Cloud Provider in Airflow is one of the providers to which extraction logic has been added. The get_openlineage_facets_on_complete() function in the gcs_to_gcs operator shows how easy adding OpenLineage support to an operator can be.  def get_openlineage_facets_on_complete(self, task_instance): &quot;&quot;&quot; Implementing _on_complete because the execute method does preprocessing on internals. This means we won't have to normalize self.source_object and self.source_objects, destination bucket and so on. &quot;&quot;&quot; from openlineage.client.run import Dataset from airflow.providers.openlineage.extractors import OperatorLineage return OperatorLineage( inputs=[ Dataset(namespace=f&quot;gs://{self.source_bucket}&quot;, name=source) for source in sorted(self.resolved_source_objects) ], outputs=[ Dataset(namespace=f&quot;gs://{self.destination_bucket}&quot;, name=target) for target in sorted(self.resolved_target_objects) ], )  In this case, the operator itself presents us with the source and target buckets, and objects which will be copied. Implementing OpenLineage support requires only properly initializing the name and namespace of the object according to the naming schema Implementing the Provider in Custom Operators​ The OpenLineage Provider in Airflow makes implementing support for custom operators easy. In fact, now there is nothing stopping you from adding OpenLineage support to your own custom operator. The provider detects OpenLineage methods and calls them when appropriate – before task execution, after success, or after complete. Also, you don’t have to add all three – the failure method falls back to the complete method if it’s not present, and the complete method to the start method. "},{"title":"Future Development​","type":1,"pageTitle":"The OpenLineage Airflow Provider is Here","url":"/blog/airflow-provider#future-development","content":"The OpenLineage Provider makes possible several sought-after enhancements, including: Integration with XCom datasets (Airflow AIP-48)Coverage of PythonOperator, the most-used operator in Airflow, including Task Flow supportSupport for Hooks, which would track their own lineage to be collected by the PythonOperator and presented as its own lineage "},{"title":"Supported Operators​","type":1,"pageTitle":"The OpenLineage Airflow Provider is Here","url":"/blog/airflow-provider#supported-operators","content":"The OpenLineage Provider currently supports the following operators, with support for additional operators coming soon: Apache KafkaAWS SageMakerGCSCommon-SQL, including support for multiple databases like Postgres and MySQLMS AzureSnowflake We welcome contributions and feedback on operator support and will be happy to help anyone get started adding extraction logic to an existing or custom operator. "},{"title":"Additional Resources​","type":1,"pageTitle":"The OpenLineage Airflow Provider is Here","url":"/blog/airflow-provider#additional-resources","content":"If you are interested in participating in the effort to add support for more operators, reach out to us on Slack. For background on the architecture and implementation plan, read the proposal. For guides on getting started with OpenLineage, read the docs. "},{"title":"Meet Us in Toronto on September 18th!","type":0,"sectionRef":"#","url":"/blog/airflow-summit-meetup","content":"","keywords":""},{"title":"Time, Place & Format​","type":1,"pageTitle":"Meet Us in Toronto on September 18th!","url":"/blog/airflow-summit-meetup#time-place--format","content":"Date: September 18th, 2023 Format: In-person Time: 5:00-8:00 PM ET Address: Canarts Media Studio, 600 Bay Street, Unit 410, Toronto, ON M5G 1M6 Phone: 416-805-2286 Getting There​ Canarts Media Studio is a 3-minute walk away from the Marriott Downtown CF at Eaton Centre and accessible via two subway lines, a streetcar line and a bus route. Subway: Dundas subway stationSubway: St Patrick subway stationStreetcar: Dundas - 505 eastbound and westboundBus: Bay - 19 southbound and northbound Getting In​ Take the elevator to the fourth floor. Canarts is in #410. Stuck outside or in the lobby? Post a message in Slack, and someone will come down. "},{"title":"Hope to see you there!​","type":1,"pageTitle":"Meet Us in Toronto on September 18th!","url":"/blog/airflow-summit-meetup#hope-to-see-you-there","content":""},{"title":"Meet Us at Data Council Austin","type":0,"sectionRef":"#","url":"/blog/data-council-meetup","content":"","keywords":""},{"title":"Meetup Details​","type":1,"pageTitle":"Meet Us at Data Council Austin","url":"/blog/data-council-meetup#meetup-details","content":"Date: March 30, 2023Time: 12:15-1:30 pm CST Place: AT&amp;T Hotel and Conference Center, UT Austin, Room 103 "},{"title":"Video - OpenLineage at Data Agility Day","type":0,"sectionRef":"#","url":"/blog/data-agility-day","content":"At Data Agility Day 2021, Julien Le Dem and Kevin Mellott outlined their approach to data lineage and discussed various approaches to implementing it in the real world. OpenLineage made an appearance at Data Agility Day 2021, when contributors Julien Le Dem and Kevin Mellott took the virtual stage for a casual conversation about data lineage. The result was both informative and enjoyable. If you couldn't make the event this year, that's okay! The video is now available, and it's almost as good as being there in person. Julien Le Dem is the creator and lead engineer of OpenLineage. Kevin Mellott implemented the Enterprise Data Platform at Northwestern Mutual, and recently shared a post detailing his team’s experiences. The video is also available at the Data Agility Day site, where you can keep an eye out for future events.","keywords":""},{"title":"The Current State of Column-level Lineage","type":0,"sectionRef":"#","url":"/blog/column-lineage","content":"","keywords":""},{"title":"Overview & background​","type":1,"pageTitle":"The Current State of Column-level Lineage","url":"/blog/column-lineage#overview--background","content":"Long one of our most requested new features, column-level lineage was added to the Spark integration with the release of OpenLineage 0.9.0. Project committer Paweł Leszczyński (@pawel-big-lebowski) authored the relevant pull requests (#645, #698, #738 and #772). In its current form, column-level lineage in OpenLineage is limited to the Spark integration and not yet visible in the Marquez UI. But this is only the first step in a broader, ongoing project to implement the feature across the project, and we’d love your help. Column-level lineage is a worthy pursuit. It dramatically extends the reach of OpenLineage’s metadata capture, providing finely grained information about datasets' dependencies. As Paweł and project lead Julien Le Dem (@julienledem) wrote in the initial proposal, “Not only can we know that a dependency exists, but we are also able to understand which input columns are used to produce output columns. This allows [for] answering questions like ‘Which root input columns are used to construct column x?’” Another reason to pursue column-level lineage: the demands of regulatory compliance. Bodies such as the GDPR, HIPAA, CCPA, BCBS and PCI have instituted requirements for data accuracy and integrity that compel companies and organizations to obtain deeper insight into their datasets and pipelines. "},{"title":"Why start with the Spark integration?​","type":1,"pageTitle":"The Current State of Column-level Lineage","url":"/blog/column-lineage#why-start-with-the-spark-integration","content":"As Julien and Paweł's proposal suggests, the Spark integration was a logical starting point for adding column-level lineage. This is so because the integration relies on implementing visitors that traverse a LogicalPlan and extract meaningful information when encountered. These data include outputs and inputs with their schemas (which we were already identifying, in fact). The LogicalPlan also exposes the expressions that derive the output columns from the input columns. They can be inspected to derive column-level lineage. Traversing the LogicalPlan allows for the capturing of all the dependencies required to build column-level lineage. "},{"title":"A new facet in the spec​","type":1,"pageTitle":"The Current State of Column-level Lineage","url":"/blog/column-lineage#a-new-facet-in-the-spec","content":"In the process of implementing column-level lineage, Paweł and Julien contributed a new facet schema, ColumnLineageDatasetFacet, to the OpenLineage spec. This facet uses fields to relay data points about dependencies. These are properties of items in the InputField property of the facet (namespace, name and field), as well as two human-readable string fields (transformationDescription, transformationType) for conveying information about dataset transformations. The last field, transformationType, may be especially useful for those whose companies or organizations need to track the usage of sensitive personal information. An example of a columnLineage facet in the outputs array of a lineage event: { &quot;namespace&quot;: &quot;{namespace of the outputdataset}&quot;, &quot;name&quot;: &quot;{name of the output dataset}&quot;, &quot;facets&quot;: { &quot;schema&quot;: { &quot;fields&quot;: [ { &quot;name&quot;: &quot;{first column of the output dataset}&quot;, &quot;type&quot;: &quot;{its type}&quot;}, { &quot;name&quot;: &quot;{second column of the output dataset}&quot;, &quot;type&quot;: &quot;{its type}&quot;}, ... ] }, &quot;columnLineage&quot;: { &quot;{first column of the output dataset}&quot;: { &quot;inputFields&quot;: [ { &quot;namespace&quot;: &quot;{input dataset namespace}&quot;, name: &quot;{input dataset name}&quot;, &quot;field&quot;: &quot;{input dataset column name}&quot;}, ... other inputs ], &quot;transformationDescription&quot;: &quot;identical&quot;, &quot;transformationType&quot;: &quot;IDENTITY&quot; }, &quot;{second column of the output dataset}&quot;: ..., ... } } }  "},{"title":"How it works​","type":1,"pageTitle":"The Current State of Column-level Lineage","url":"/blog/column-lineage#how-it-works","content":"As we’ve seen, column-level lineage is being collected via the new columnLineage dataset facet. For each output, this facet contains a list of the output's fields along with the input fields used to create it. The input fields are identified by a namespace, name and field. But how is OpenLineage obtaining the data about dependencies that the facet relays? In PR #698, Paweł describes the mechanism this way: The core mechanism first gets an output schema and logical plan as inputs.Then, the OutputFieldsCollector class traverses the plan to gather the outputs. Outputs can be extracted from Aggregate or Project, and each output field has an ExprId (expression ID) that is attached from the plan.Next, the InputFieldsCollector class is used to collect inputs that can be extracted from DataSourceV2Relation, DataSourceV2ScanRelation, HiveTableRelation or LogicalRelation. Each input field takes its ExprId from the plan, and each input is identified by a DatasetIdentifier, which means it contains the name and namespace of a dataset and an input field.Finally, the FieldDependenciesCollector traverses the plan to identify dependencies between different ExprIds. Dependencies map parent expressions to children expressions. This is used to identify the inputs used to evaluate certain outputs. "},{"title":"What’s next?​","type":1,"pageTitle":"The Current State of Column-level Lineage","url":"/blog/column-lineage#whats-next","content":"Work on extending column-level lineage in the project is ongoing. For example, project committer Will Johnson (@wjohnson) has opened a PR (#963) to add support for common dataframe operations not covered due to the initial focus on Spark. As Will writes in the PR, Currently, the Column Lineage Input Field Collectors work mainly for Spark SQL operations and Data Source V2. This leaves out normal dataframe operations like inserting into HDFS without the use of a Hive table. Column Lineage should support this scenario as many users will want to see column lineage for operations outside of SQL and Hive Metastore backed tables. Also, Paweł has written enhancements that will enable column-level lineage in the case of altered table and column names and allow one to extend column-level lineage without contributing to OpenLineage (to avoid exposing proprietary code, for example). Meanwhile, over in Marquez, Julien has contributed a proposal to add a column-level endpoint to the project that would leverage OpenLineage’s ColumnLineageDatasetFacet. This approach would add column lineage to an existing endpoint by embedding the columnLineage facet in the data section of the DATASET nodes. "},{"title":"How can I contribute?​","type":1,"pageTitle":"The Current State of Column-level Lineage","url":"/blog/column-lineage#how-can-i-contribute","content":"We welcome contributions to this ongoing effort at implementing column-level lineage in OpenLineage! If you’re interested in contributing, one of our existing integrations might be a good place to start. OpenLineage’s growing list of integrations includes Airflow, dbt, Dagster and Flink. Sounds fun? Check out our new contributor guide to get started. "},{"title":"Happening Soon - Our First Meetup!","type":0,"sectionRef":"#","url":"/blog/data-lineage-meetup","content":"","keywords":""},{"title":"Time, Place & Format​","type":1,"pageTitle":"Happening Soon - Our First Meetup!","url":"/blog/data-lineage-meetup#time-place--format","content":"Date: March 9, 2023 Format: In-person Time: 6-8 pm ET Address: CIC, 225 Dyer Street, Providence, RI, US 02903  Getting There​ Air: the nearest airport is T.F. Green/PVD. Boston Logan is also within 1.5-2 hours' driving distance. Rail: Amtrak serves PVD, which is within walking distance of CIC. Road: garages and lots are a short walk away from the venue, and metered street parking is also available nearby. Richmond Garage South Street Landing garage Clifford parking lot  Getting In​ Check in with the CIC concierge inside the north entrance. The concierge will direct you to the Hope Island Room on the 3rd floor. Arriving Early?​ Come to the coffee bar in Plant City at 334 South Water Street, Providence RI 02903, which is a short walk from CIC. Other out-of-towners will be meeting up there between 3 and 6 pm. "},{"title":"Hope to see you there!​","type":1,"pageTitle":"Happening Soon - Our First Meetup!","url":"/blog/data-lineage-meetup#hope-to-see-you-there","content":""},{"title":"Backfilling Airflow DAGs using Marquez","type":0,"sectionRef":"#","url":"/blog/backfilling-airflow-dags-using-marquez","content":"","keywords":""},{"title":"1. Brief Intro to Backfilling Airflow DAGs​","type":1,"pageTitle":"Backfilling Airflow DAGs using Marquez","url":"/blog/backfilling-airflow-dags-using-marquez#1-brief-intro-to-backfilling-airflow-dags","content":"Airflow supports backfilling DAG runs for a historical time window given a start and end date. Let's say our example.etl_orders_7_days DAG started failing on 2021-06-06, and we wanted to reprocess the daily table partitions for that week (assuming all partitions have been backfilled upstream). In order to run the backfill for example.etl_orders_7_days, using the Airflow CLI, you open up a terminal and execute the following backfill command: # Backfill weekly food orders $ airflow dags backfill \\ --start-date 2021-06-06 \\ --end-date 2021-06-06 \\ example.etl_orders_7_days  Now, the backfill was fairly straightforward but they're not always trivial. That is, we still have the following open questions: How quickly can data quality issues be identified and explored?What alerting rules should be in place to notify downstream DAGs of possible upstream processing issues or failures?What effects (if any) would upstream DAGs have on downstream DAGs if dataset consumption was delayed? Next, we'll demonstrate how lineage metadata managed with Marquez can help answer some of these questions (and more!) by maintaining inter-DAG dependencies and cataloging historical runs of DAGs. "},{"title":"2. Exploring Lineage Metadata using Marquez​","type":1,"pageTitle":"Backfilling Airflow DAGs using Marquez","url":"/blog/backfilling-airflow-dags-using-marquez#2-exploring-lineage-metadata-using-marquez","content":"Note: To seed the Marquez HTTP API server with the sample lineage metadata used in this blog post, see the Write Sample Lineage Metadata to Marquez section in Marquez's quickstart guide. 2.1 COLLECT DAG LINEAGE METADATA​  Figure 1: DAG lineage metadata. Marquez is an open source metadata service for the collection, aggregation, and visualization of a data ecosystem’s metadata. Marquez has integration support for Airflow with minimal configuration. Using the marquez-airflow library, DAG lineage metadata will be collected automatically during DAG execution using the OpenLineage standard, then stored in Marquez’s centralized data model. To learn more about how lineage metadata is stored and versioned in Marquez, see the Data Model section in Marquez's quickstart guide. The Airflow integration gives us two important benefits: DAG Metadata: Each DAG has a code version, inputs and outputs, run args, and run state transitions. Keeping a global historical log of DAG runs linked to code will quickly highlight upstream dependencies errors and minimize downstream impact.Lineage Metadata: Each DAG may have one or more upstream dependency. Keeping track of inter-DAG dependencies will allow for teams within an organization to safely depend on one another’s datasets, while also understanding which DAGs will be impacted downstream of a DAG failure. In this blog, we won't go into how to enable lineage metadata collection for Airflow DAGs. But, we encourage you to take a look at Marquez's Airflow example to learn how to troubleshoot DAG failures using Marquez. 2.2 GET LINEAGE METADATA VIA REST API​ In Marquez, each dataset and job has its own globally unique node ID that can be used to query the lineage graph. The LineageAPI returns a set of nodes consisting of edges. An edge is directed and has a defined origin and destination. A lineage graph may contain the following node types: dataset:&lt;namespace&gt;:&lt;dataset&gt;, job:&lt;namespace&gt;:&lt;job&gt;. So, let's start by querying the lineage graph for our example.etl_orders_7_days DAG using the node ID job:food_delivery:example.etl_orders_7_days. You'll notice in the returned lineage graph that the DAG input datasets are public.categories, public.orders, and public.menus with public.orders_7_days as the output dataset: REQUEST​ $ curl -X GET &quot;http://localhost:5000/api/v1-beta/lineage?nodeId=job:food_delivery:example.etl_orders_7_days&quot;  RESPONSE​ 200 OK { &quot;graph&quot;: [{ &quot;id&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;, &quot;type&quot;: &quot;JOB&quot;, &quot;data&quot;: { &quot;type&quot;: &quot;BATCH&quot;, &quot;id&quot;: { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;example.etl_orders_7_days&quot; }, &quot;name&quot;: &quot;example.etl_orders_7_days&quot;, &quot;createdAt&quot;: &quot;2021-06-06T14:50:13.931946Z&quot;, &quot;updatedAt&quot;: &quot;2021-06-06T14:57:54.037399Z&quot;, &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;inputs&quot;: [ {&quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.categories&quot;}, {&quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.menu_items&quot;}, {&quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.orders&quot;}, {&quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.menus&quot;} ], &quot;outputs&quot;: [ {&quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.orders_7_days&quot;} ], &quot;location&quot;: &quot;https://github.com/example/jobs/blob/2294bc15eb49071f38425dc927e48655530a2f2e/etl_orders_7_days.py&quot;, &quot;context&quot;: { &quot;sql&quot;: &quot;INSERT INTO orders_7_days (order_id, placed_on, discount_id, menu_id, restaurant_id, menu_item_id, category_id)\\n SELECT o.id AS order_id, o.placed_on, o.discount_id, m.id AS menu_id, m.restaurant_id, mi.id AS menu_item_id, c.id AS category_id\\n FROM orders AS o\\n INNER JOIN menu_items AS mi\\n ON menu_items.id = o.menu_item_id\\n INNER JOIN categories AS c\\n ON c.id = mi.category_id\\n INNER JOIN menu AS m\\n ON m.id = c.menu_id\\n WHERE o.placed_on &gt;= NOW() - interval '7 days';&quot; }, &quot;description&quot;: &quot;Loads newly placed orders weekly.&quot;, &quot;latestRun&quot;: { &quot;id&quot;: &quot;5c7f0dc4-d3c1-4f16-9ac3-dc86c5da37cc&quot;, &quot;createdAt&quot;: &quot;2021-06-06T14:50:36.853459Z&quot;, &quot;updatedAt&quot;: &quot;2021-06-06T14:57:54.037399Z&quot;, &quot;nominalStartTime&quot;: &quot;2021-06-06T14:54:00Z&quot;, &quot;nominalEndTime&quot;: &quot;2021-06-06T14:57:00Z&quot;, &quot;state&quot;: &quot;FAILED&quot;, &quot;startedAt&quot;: &quot;2021-06-06T14:54:14.037399Z&quot;, &quot;endedAt&quot;: &quot;2021-06-06T14:57:54.037399Z&quot;, &quot;durationMs&quot;: 220000, &quot;args&quot;: {}, &quot;location&quot;: &quot;https://github.com/example/jobs/blob/2294bc15eb49071f38425dc927e48655530a2f2e/etl_orders_7_days.py&quot;, &quot;context&quot;: { &quot;sql&quot;: &quot;INSERT INTO orders_7_days (order_id, placed_on, discount_id, menu_id, restaurant_id, menu_item_id, category_id)\\n SELECT o.id AS order_id, o.placed_on, o.discount_id, m.id AS menu_id, m.restaurant_id, mi.id AS menu_item_id, c.id AS category_id\\n FROM orders AS o\\n INNER JOIN menu_items AS mi\\n ON menu_items.id = o.menu_item_id\\n INNER JOIN categories AS c\\n ON c.id = mi.category_id\\n INNER JOIN menu AS m\\n ON m.id = c.menu_id\\n WHERE o.placed_on &gt;= NOW() - interval '7 days';&quot; }, &quot;facets&quot;: {} } }, &quot;inEdges&quot;: [ {&quot;origin&quot;: &quot;dataset:food_delivery:public.categories&quot;, &quot;destination&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;}, &quot;destination&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;}, {&quot;origin&quot;: &quot;dataset:food_delivery:public.orders&quot;, &quot;destination&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;}, {&quot;origin&quot;: &quot;dataset:food_delivery:public.menus&quot;, &quot;destination&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;} ], &quot;outEdges&quot;: [ {&quot;origin&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;, &quot;destination&quot;: &quot;dataset:food_delivery:public.orders_7_days&quot;} ] } }, ...] }  "},{"title":"3. Using Lineage Metadata to Backfill Airflow DAGs​","type":1,"pageTitle":"Backfilling Airflow DAGs using Marquez","url":"/blog/backfilling-airflow-dags-using-marquez#3-using-lineage-metadata-to-backfill-airflow-dags","content":"3.1 BACKFILLING​  Figure 2: Backfilled daily table partitions. To run a backfill for example.etl_orders_7_days using the DAG lineage metadata stored in Marquez, we'll need to query the lineage graph for the upstream DAG where the error originated. Now, let's assume the example.etl_orders DAG upstream of example.etl_orders_7_days failed to write some of the daily table partitions needed for the weekly food order trends report (see Figure 2). To fix the weekly trends report, we'll first need to backfill the missing daily table partitions public.orders_2021_06_04, public.orders_2021_06_05, and public.orders_2021_06_06: # Backfill daily food orders $ airflow dags backfill \\ --start-date 2021-06-04 \\ --end-date 2021-06-06 \\ example.etl_orders   Figure 3: Airflow inter-DAG dependencies. Then, using the script backfill.sh defined below, we can easily backfill all DAGs downstream of example.etl_orders: backfill.sh​ Note: Make sure you have jq installed before running backfill.sh. #!/bin/bash # # Backfill DAGs automatically using lineage metadata stored in Marquez. # # Usage: $ ./backfill.sh &lt;start-date&gt; &lt;end-date&gt; &lt;dag-id&gt; set -e # Backfills DAGs downstream of the given node ID, recursively. backfill_downstream_of() { node_id=&quot;${1}&quot; # Get out edges for node ID out_edges=($(echo $lineage_graph \\ | jq -r --arg NODE_ID &quot;${node_id}&quot; '.graph[] | select(.id==$NODE_ID) | .outEdges[].destination')) for out_edge in &quot;${out_edges[@]}&quot;; do # Run backfill if out edge is a job node (i.e. &lt;dataset&gt; =&gt; &lt;job&gt;) if [[ &quot;${out_edge}&quot; = job:* ]]; then dag_id=&quot;${out_edge##*:}&quot; echo &quot;backfilling ${dag_id}...&quot; airflow backfill --start_date &quot;${start_date}&quot; --end_date &quot;${start_date}&quot; &quot;${dag_id}&quot; fi # Follow out edges downstream, recursively backfill_downstream_of &quot;${out_edge}&quot; done } start_date=&quot;${1}&quot; end_date=&quot;${2}&quot; dag_id=&quot;${3}&quot; # (1) Build job node ID (format: 'job:&lt;namespace&gt;:&lt;job&gt;') node_id=&quot;job:food_delivery:${dag_id}&quot; # (2) Get lineage graph lineage_graph=$(curl -s -X GET &quot;http://localhost:5000/api/v1-beta/lineage?nodeId=${node_id}&quot;) # (3) Run backfill backfill_downstream_of &quot;${node_id}&quot;  When you run the script backfill.sh, it will output all backfilled DAGs to the console: $ ./backfill.sh 2021-06-06 2021-06-06 example.etl_orders backfilling example.etl_orders_7_days... backfilling example.etl_delivery_7_days... backfilling example.delivery_times_7_days...  "},{"title":"4. Conclusion​","type":1,"pageTitle":"Backfilling Airflow DAGs using Marquez","url":"/blog/backfilling-airflow-dags-using-marquez#4-conclusion","content":"In this blog post, we showed how easy it can be to automate backfilling DAGs downstream of a data quality issue using lineage metadata stored in Marquez. With only two steps, we were able to backfill missing daily table partitions, then automatically re-run failed DAGs downstream of the upstream DAG where the error originated. But, what measures can we put in place to detect low-quality data issues faster, therefore avoiding backfills altogether? Since Marquez collects DAG run metadata that can be viewed using the Runs API, building automated processes that periodically check DAG run states and quickly notifying teams of upstream data quality issue (or missed SLAs) in a timely fashion is just one possible preventive measure. We encourge you to explore Marquez's opinionated Metadata API and define your own automated process(es) for analyzing lineage metadata! If you need help or have any questions, you can always join our Slack channel or reach out to us on Twitter. "},{"title":"2023 Ecosystem Survey","type":0,"sectionRef":"#","url":"/blog/ecosystem-survey","content":"","keywords":""},{"title":"An Important Measure and Milestone​","type":1,"pageTitle":"2023 Ecosystem Survey","url":"/blog/ecosystem-survey#an-important-measure-and-milestone","content":"This Ecosystem Survey, our first ever, is an effort to gauge how well we are serving our partners and users and to gain a better understanding of your needs. We plan to conduct a survey like this on an annual basis. The questions focus on users': views on how we should prioritize supporting various tooling categories (e.g., orchestrators),views on how we should prioritize supporting specific tools in each category,organizations and roles,use cases,motivations for choosing the project,experience of this community,alternatives you explored,and more (but not much!). Note: it might appear longer than it is due to the large number of optional questions. Not all questions apply to all use cases. Thank you in advance for taking the time to help us chart the course of OpenLineage! We look forward to sharing the results. "},{"title":"Extending OpenLineage with Facets","type":0,"sectionRef":"#","url":"/blog/extending-with-facets","content":"","keywords":""},{"title":"Open Source​","type":1,"pageTitle":"Extending OpenLineage with Facets","url":"/blog/extending-with-facets#open-source","content":"Standardizing an API through open source collaboration can be challenging. On one end, you need to get input and feedback from the people who will use the API in different contexts. On the other, you want to avoid getting stuck in disagreements arising from the different and sometimes incompatible viewpoints that inevitably drive these discussions. Thankfully, there are mechanisms to help organize and decouple those disagreements and drive discussions towards conclusion. A community driven open source project works very differently from a product you buy off the shelf. At the very moment you start using it - maybe starting by reading the doc - you become part of the community and start sharing a little bit of ownership. As with any software, you might encounter problems... but in this case, you immediately become part of the solution. In a healthy community, how much of the solution you become is entirely up to you. Maybe you spotted a typo and reported it. Maybe you opened a pull request to fix it. You might propose an improvement, or even build one yourself. All of those contributions, no matter how small, make the project better for everyone. That very powerful flywheel motion gathers momentum and drives very successful open source projects. One of the success factors of such an open source project is how much it can minimize the friction for new community members who want to contribute. The easier it is to contribute, the faster the project will acquire momentum. It’s not about getting other people’s input, it’s about giving them a share of ownership and encouraging them to drive the areas where they can most effectively contribute. In a multi-faceted domain like data lineage, enabling others to lead discussions is critical. "},{"title":"Making progress​","type":1,"pageTitle":"Extending OpenLineage with Facets","url":"/blog/extending-with-facets#making-progress","content":"In this context, we need mechanisms to converge often and make incremental progress. You definitely want to avoid having a big monolithic spec that takes a long time to reach consensus on - if you ever do. A discussion around a large ultra-spec that combines specifications from multiple related domains will lose steam. We need to keep conversations focused on the topics that individual contributors care about. It is critical to subdivide the specification in concrete and granular decision points where consistent and significant progress can be made. Not everyone will care about all the aspects of the specification, and that is fine. We need to make sure contributors can easily focus on the aspects they do care about. This need for a very granular decision making process, one where we can make progress independently on different aspects of the spec, leads naturally into decomposition of the specification into smaller independent subsets. This will keep conversations focused and moving. It also decouples workstreams where consensus can be reached from those that are more contentious. For example the contributors interested in data quality might be different from the ones interested in column-level lineage or query performance. "},{"title":"Embracing different points of view​","type":1,"pageTitle":"Extending OpenLineage with Facets","url":"/blog/extending-with-facets#embracing-different-points-of-view","content":"Depending on their perspective, contributors may have very different opinions on how to model a certain aspect of data. Or they may have different use-cases in mind. Instead of pitting different view-points against each other and forcing alignment on every point, it is sometimes beneficial to allow them to be expressed separately. For example, when you ask a data practitioner &quot;what is data lineage?&quot; they may have very different definitions for it. Some care about how a specific metric is derived from the raw data, and need column level lineage. Some will care about compliance with privacy regulations and need relevant metadata to locate sensitive data and trace its movement.Some will care about the reliability of data delivery and need data freshness and quality metrics - in particular, how they change over time in correlation with changes in the system. All those are valid view points that deserve to be captured appropriately and can be defined independently in a framework that allows them to cohabitate. Mechanics OpenLineage is purposefully providing a faceted model around a minimalistic core spec to enable this granular decision making, minimize friction in contributing, and favor community-driven improvements. The core spec focuses on high-level modeling of jobs, runs, datasets, and their relation. Each OpenLineage event refers to a run of a job and its input and output datasets. A job is a recurring transformation that reads from datasets and writes to datasets. It has a unique name that identifies it across runs.A run identifies an individual execution of a job. It might be an incremental or full batch process. It could also be a streaming job.A dataset could be a table in a warehouse or a folder in a blob store. It is consumed or written to by jobs. Facets are pieces of metadata that can be attached to those core entities. Facets have their own schema and capture various aspects of those entities. "},{"title":"Facets are individual atomic specs​","type":1,"pageTitle":"Extending OpenLineage with Facets","url":"/blog/extending-with-facets#facets-are-individual-atomic-specs","content":"Like the core model, facets are defined by a JSONSchema. They are a self-contained definition of one aspect of a job, a dataset, or a run at the time the event happened. They make the model extensible. The notion of facets is powerful because it makes it easy to add more information to the model - you just define a new facet. There’s a clear compatibility model when introducing a new facet, since fields that are defined at the same time are grouped together. For example, there’s a facet to capture the schema of a dataset. There’s a facet to capture the version of a job in source control. There’s a facet to capture the parameters of a run. Facets are optional and may not apply to every instance of an entity. "},{"title":"Facets enable specialization of models​","type":1,"pageTitle":"Extending OpenLineage with Facets","url":"/blog/extending-with-facets#facets-enable-specialization-of-models","content":"The core entities are fairly generic. A dataset might be a table in a warehouse or a topic in a Kafka broker. A job might be a SQL query or a machine learning training job. This generic high level model of lineage can be specialized by adding facets for that particular type of entity. At-rest data might be versioned, enabling transactions at the run level. Streaming data might capture the offsets and partitions where a streaming job started reading. Datasets might have a schema like a warehouse table, or not (for example, in the case of a machine learning model). By capturing a generic representation of lineage and allowing progressive specialization of those entities, this approach offers a lot of flexibility. "},{"title":"Facets allow expressing different point of views​","type":1,"pageTitle":"Extending OpenLineage with Facets","url":"/blog/extending-with-facets#facets-allow-expressing-different-point-of-views","content":"There can be divergent points of view on how to model a certain aspect of metadata. Facets allow these models to cohabitate in a common framework. One example of this is capturing the physical plan of a query execution. Each data warehouse might have its own unique way of describing execution plans. It is very valuable to be able to capture both a precise (but maybe too specific) model as well as a generic (but possibly imprecise or lossy) representation. They can be captured as two different facets. This also gives us opportunities to define several competing models and use the resulting information to collaborate on a more unified and generic representation. This emergent modeling is actually extremely useful in an open source setting, and as a way to make incremental progress. "},{"title":"Custom facets make the model decentralized​","type":1,"pageTitle":"Extending OpenLineage with Facets","url":"/blog/extending-with-facets#custom-facets-make-the-model-decentralized","content":"Most importantly, the OpenLineage spec allows custom facets that are defined elsewhere, completely outside of the spec. This allows others to extend the spec as-needed without having to coordinate with anyone or ask any permission from a governing body. They can make their own opinionated definition of an aspect of metadata. All that is required is that they publish a JSONSchema that describes their facets, prefixed by a unique namespace. This lowers the barrier to experimentation and encourages incremental progress by making the experimentation of others visible. The facets that become broadly useful can eventually be represented in the core spec. Contribute! As a community, we’ve done our best to minimize friction when experimenting with or contributing to OpenLineage. We’re looking forward to seeing you join us as we make data lineage transparent across the data ecosystem. "},{"title":"Pursuing Lineage from Airflow using Custom Extractors","type":0,"sectionRef":"#","url":"/blog/extractors","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Pursuing Lineage from Airflow using Custom Extractors","url":"/blog/extractors#overview","content":"Airflow is built around operators, each having a different function and requiring a different approach to lineage. The OpenLineage Airflow integration detects which Airflow operators your DAG is using and extracts lineage data from them using extractors. The community has already authored a number of extractors to support Airflow’s Great Expectations, BigQuery, Python, Postgres, SQL and Bash operators (and more – you can find all the extractors here.) Nevertheless, in the course of pursuing lineage, you may find yourself needing to write custom extractors. Some teams use custom extractors to automate repeatable work – using the same code from PythonOperator across a project, for example. Another common use case is that a team needs to use an operator for which a pre-built extractor does not yet exist. Airflow has literally hundreds of operators. Built-in support for custom extractors makes OpenLineage a flexible, highly adaptable solution for pipelines that use Airflow for orchestration. "},{"title":"How it works​","type":1,"pageTitle":"Pursuing Lineage from Airflow using Custom Extractors","url":"/blog/extractors#how-it-works","content":"As we explain in the OpenLineage docs, custom extractors must be derived from the BaseExtractor class (import it from openlineage.airflow.extractors.base). Extractors have methods they can implement: extract, extract_on_complete and get_operator_classnames. Either extract or extract_on_complete is required. The get_operator_classnames method, also required, is for providing a list of operators from which your extractor can get lineage. For example: @classmethod def get_operator_classnames(cls) -&gt; List[str]: return ['PostgresOperator']  If the name of the operator matches one of the names on the list, the extractor will be instantiated – using the operator passed to the extractor as a parameter and stored in the self.operator property – and both the extract and extract_on_complete methods will be called. They both return information used by the OpenLineage integration to emit OpenLineage events. The difference is that extract is called before the operator's execute method to generate a START event, while extract_on_complete is called afterward to generate a COMPLETE event. The latter has access to any additional information that the operator leaves behind following execution. A good example of this is the SnowflakeOperator, which sets query_ids after execution. Both methods return a TaskMetadata structure: @attr.s class TaskMetadata: name: str = attr.ib() # deprecated inputs: List[Dataset] = attr.ib(factory=list) outputs: List[Dataset] = attr.ib(factory=list) run_facets: Dict[str, BaseFacet] = attr.ib(factory=dict) job_facets: Dict[str, BaseFacet] = attr.ib(factory=dict)  The inputs and outputs are lists of plain OpenLineage datasets. The run_facets and job_facets are dictionaries of optional JobFacets and RunFacets that accompany a job. For example, you might want to attach a SqlJobFacet if your operator is executing SQL. Note: in order for a custom extractor to work, it must be registered first, so the OpenLineage integration can import it. You can read about how to use environment variables to do this here. "},{"title":"Example: the RedshiftDataExtractor​","type":1,"pageTitle":"Pursuing Lineage from Airflow using Custom Extractors","url":"/blog/extractors#example-the-redshiftdataextractor","content":"In the RedshiftDataExtractor, the extract_on_complete method parses SQL, obtains task stats using the get_facets method of the RedshiftDataDatasetsProvider class, and returns a TaskMetadata instance. We can see usage of a SQL statement, and the connection is provided by an actual operator. def extract_on_complete(self, task_instance) -&gt; Optional[TaskMetadata]: log.debug(f&quot;extract_on_complete({task_instance})&quot;) job_facets = {&quot;sql&quot;: SqlJobFacet(self.operator.sql)} log.debug(f&quot;Sending SQL to parser: {self.operator.sql}&quot;) sql_meta: Optional[SqlMeta] = parse(self.operator.sql, self.default_schema) log.debug(f&quot;Got meta {sql_meta}&quot;) try: redshift_job_id = self._get_xcom_redshift_job_id(task_instance) if redshift_job_id is None: raise Exception( &quot;Xcom could not resolve Redshift job id. Job may have failed.&quot; ) except Exception as e: log.error(f&quot;Cannot retrieve job details from {e}&quot;, exc_info=True) return TaskMetadata( name=get_job_name(task=self.operator), run_facets={}, job_facets=job_facets, ) client = self.operator.hook.conn redshift_details = [ &quot;database&quot;, &quot;cluster_identifier&quot;, &quot;db_user&quot;, &quot;secret_arn&quot;, &quot;region&quot;, ] connection_details = { detail: getattr(self.operator, detail) for detail in redshift_details } stats = RedshiftDataDatasetsProvider( client=client, connection_details=connection_details ).get_facets( job_id=redshift_job_id, inputs=sql_meta.in_tables if sql_meta else [], outputs=sql_meta.out_tables if sql_meta else [], ) return TaskMetadata( name=get_job_name(task=self.operator), inputs=[ds.to_openlineage_dataset() for ds in stats.inputs], outputs=[ds.to_openlineage_dataset() for ds in stats.output], run_facets=stats.run_facets, job_facets={&quot;sql&quot;: SqlJobFacet(self.operator.sql)}, )  "},{"title":"Common issues​","type":1,"pageTitle":"Pursuing Lineage from Airflow using Custom Extractors","url":"/blog/extractors#common-issues","content":"There are two common issues associated with custom extractors. First, when the wrong path is provided to OPENLINEAGE_EXTRACTORS, the extractor isn’t imported and OpenLineage events aren’t emitted. The path needs to be exactly the same as the one you are using in your code. Also, make sure that the extractor code is available to import from Airflow’s Python interpreter. Second, imports from Airflow can be unnoticeably cyclical. This is due to the fact that OpenLineage code gets instantiated when the Airflow worker itself starts, in contrast to DAG code. OpenLineage extraction can fail as a result. To avoid this issue, make sure that all imports from Airflow are local – in the extract or extract_on_complete methods. If you need imports for type checking, guard them behind typing.TYPE_CHECKING. "},{"title":"How to get started​","type":1,"pageTitle":"Pursuing Lineage from Airflow using Custom Extractors","url":"/blog/extractors#how-to-get-started","content":"Check out the existing extractors here. Read the docs about the Airflow integration, including tips on registering and debugging your custom extractor, here. "},{"title":"How to contribute​","type":1,"pageTitle":"Pursuing Lineage from Airflow using Custom Extractors","url":"/blog/extractors#how-to-contribute","content":"We welcome your contributions! One of our existing integrations might be a good place to start. OpenLineage’s growing list of partners includes Airflow, dbt, Dagster and Flink. Sounds fun? Check out our new contributor guide to get started. "},{"title":"OpenLineage Advances to Incubation Stage with the LFAI & Data","type":0,"sectionRef":"#","url":"/blog/incubation-stage-lfai","content":"","keywords":""},{"title":"What It Means​","type":1,"pageTitle":"OpenLineage Advances to Incubation Stage with the LFAI & Data","url":"/blog/incubation-stage-lfai#what-it-means","content":"Now that we’ve cleared this hurdle, we have access to additional services from the foundation, including assistance with creative work, marketing and communication support, and event-planning assistance. Graduation from the program, which will earn us a voting seat on the TAC, is on the horizon. Stay tuned for updates on our progress with the foundation. "},{"title":"About the LFAI & Data​","type":1,"pageTitle":"OpenLineage Advances to Incubation Stage with the LFAI & Data","url":"/blog/incubation-stage-lfai#about-the-lfai--data","content":"LF AI &amp; Data is an umbrella foundation of the Linux Foundation that supports open source innovation in artificial intelligence (AI) and data. LF AI &amp; Data was created to support open source AI and data, and to create a sustainable open source AI and data ecosystem that makes it easy to create AI and data products and services using open source technologies. They foster collaboration under a neutral environment with an open governance in support of the harmonization and acceleration of open source technical projects. For more info about the foundation and other LFAI &amp; Data projects, visit their website. "},{"title":"What's Next​","type":1,"pageTitle":"OpenLineage Advances to Incubation Stage with the LFAI & Data","url":"/blog/incubation-stage-lfai#whats-next","content":"The next step for the project is Graduation, which we expect to happen early this summer. Requirements for Graduation include 1000 stars on GitHub and the OpenSSF Gold Badge. Watch this space for updates on our progress. "},{"title":"OpenLineage joins the LF AI & Data Foundation","type":0,"sectionRef":"#","url":"/blog/joining-lfai","content":"Becoming a LF AI &amp; Data project ensures that OpenLineage can never belong to a company, or even a group of developers; it belongs to us all. I am pleased to share that the OpenLineage project is joining the LF AI &amp; Data foundation as a Sandbox Project! This is an important step towards the development of an open ecosystem for lineage metadata collection. The LF AI &amp; Data Foundation provides a vendor-neutral governance structure that can help the project grow broad industry collaboration. Even more importantly, becoming a LF AI &amp; Data project ensures that OpenLineage can never belong to a company, or even a group of developers; it belongs to us all. The license can’t be changed to protect the business interests of a subset of the community. That’s important, because in order to succeed we need a whole lot of software projects - open source and proprietary - to adopt this standard and allow their users to begin collecting lineage metadata. In the full announcement, Ibrahim Haddad, Executive Director of LF AI &amp; Data, writes: “We look forward to working with the OpenLineage project to grow the project’s footprint in the ecosystem, expand its community of adopters and contributors, and to foster the creation of collaboration opportunities with our members and other related projects.”","keywords":""},{"title":"Using Marquez to Visualize dbt Models","type":0,"sectionRef":"#","url":"/blog/dbt-with-marquez","content":"Each time dbt runs, it generates a trove of metadata about datasets and the work it performs with them. In this post, I’d like to show you how to harvest this metadata and put it to good use. The first time I built a data warehouse was in a completely different era, even though it wasn’t all that long ago. It was a few dozen tables + a collection of loader scripts and an ETL tool. If I’m honest, calling the whole thing a “data warehouse” is a bit grandiose, but it worked. At the time, my defining question was “how can I make all of my most important data available for study without spending more than it’s worth?” Because my database capacity wasn’t infinite, I couldn’t keep all of my data forever. The jobs I wrote would pull data from operational data stores, perform a bunch of slicing and aggregation, and load summary data into the warehouse. They shoveled bits every night from one server to another, performing calculations in between - and that meant they had to run on a beefy server with close proximity to my data. Skip forward to the current day and here I am, building and running models from a cafe over pretty shaky wifi. My, how things have changed. Cloud data warehouses like Google BigQuery, Amazon Redshift, and Snowflake have created a new economic and technological possibility: we can now pretty much just load everything - including our entire operational data stores - into a single warehouse. Once everything is in one place, data can be sliced up and analyzed much more quickly. This is where dbt shines, at making transformations within a cloud data warehouse easy. And we all know what happens when you make something easy: it finds a way to happen a lot. People are doing more complex transformations than ever before, and the need for lineage context is becoming greater than ever. Fortunately, each time dbt runs it generates a trove of metadata about datasets and the work it performs with them. In this post, I’d like to show you how to harvest this metadata and put it to good use. Our Example For our example, let’s choose the kind of experiment that I might run in my day-to-day life. I’m the head of marketing at Datakin, which means the metrics I’m most interested in are usually about some sort of human behavior. I ask questions like: Does [x] technology space matter, and to whom? Is it waxing or waning?Are there adjacent ecosystems we should be collaborating with?Who are the influencers in this space? Who are the major contributors?What challenges are users facing? What does successful adoption look like? There are a lot of ways to try to answer these questions. None of them are any more reliable than human behavior itself, and every resulting metric requires analysis and judgment. But there are still some pretty fun things to discover. And what better data source to mine to understand technical audiences than Stack Overflow? So let’s see what we can learn from the Stack Overflow public data set in BigQuery. But not the whole thing; it is very large, so let’s study just a part of it. I created a sample dbt project that contains a handful of models to study all of the questions and answers we can find about the topic of ELT. These models: Create slices of the key Stack Overflow tables, pulling them into a separate BigQuery project. These slices only contain the rows that are related to questions tagged with “elt”. That way, we can query them tortuously all day long without scanning through gobs of partitions and running up our bill.Augment these slices by performing some helpful calculations - in this case, the number of upvotes/downvotes per question.Populate two summary tables for consumption by a BI system of some sort: a daily summary table that can be used to study trends and a user summary table that can be used to learn about the most influential contributors. This is exactly the kind of experiment I have run multiple times over the years, across numerous stacks. It’s usually pretty messy. But this time, after running all of these models, we will be rewarded with a gorgeous Marquez lineage graph. We’ll be able to see how everything fits together. Setting Everything Up First, if you haven’t already, run through the excellent dbt tutorial. It will show you how to create a BigQuery project, provision a service account, download a JSON key, and set up your local dbt environment. The rest of this example assumes that you have created a BigQuery project where our models can be run, and you know how to properly configure dbt to connect to it. Next, let’s start a local Marquez instance to store our lineage metadata. Make sure you have Docker running, and then: git clone https://github.com/MarquezProject/marquez.git &amp;&amp; cd marquez ./docker/up.sh Check to make sure Marquez is up by visiting http://localhost:3000. You should see an empty Marquez instance with a message saying there isn’t any data. Also, you should be able to see the server output from your requests in the terminal window where Marquez is running. Keep this window open until we’re done. Now, let’s open a new terminal window/pane and clone the GitHub project containing our models: git clone https://github.com/rossturk/stackostudy.git &amp;&amp; cd stackostudy Next we need to install dbt and its integration with OpenLineage. I like to do this in a Python virtual environment because I make mistakes - as we all do - and I enjoy knowing that I can burn everything down and start over quickly if I need to. Virtual environments make this easy. To create one and install everything we need, run the following commands: python -m venv virtualenv source virtualenv/bin/activate pip install dbt dbt-openlineage dbt learns how to connect to your BigQuery project by looking for a matching profile in ~/.dbt/profiles.yml. Create or edit this file so it contains a section with your BigQuery connection details. You will need to point to the location of a file containing the JSON key for your service account. If you aren’t sure, you can follow this section in the dbt documentation. My profiles.yml looked like this when I was done: stackostudy: target: dev outputs: dev: type: bigquery method: service-account keyfile: /Users/rturk/.dbt/dbt-example.json project: dbt-example dataset: stackostudy threads: 1 timeout_seconds: 300 location: US priority: interactive Run dbt debug to make sure that you have everything configured correctly. % dbt debug Running with dbt=0.20.1 dbt version: 0.20.1 python version: 3.8.12 python path: /opt/homebrew/Cellar/dbt/0.20.1_1/libexec/bin/python3 os info: macOS-11.5.2-arm64-arm-64bit Using profiles.yml file at /Users/rturk/.dbt/profiles.yml Using dbt_project.yml file at /Users/rturk/projects/stackostudy/dbt_project.yml Configuration: profiles.yml file [OK found and valid] dbt_project.yml file [OK found and valid] Required dependencies: - git [OK found] Connection: method: service-account database: stacko-study schema: stackostudy location: US priority: interactive timeout_seconds: 300 maximum_bytes_billed: None Connection test: OK connection ok A Few Important Details There are a couple of considerations to make when designing dbt models for use with OpenLineage. By following these conventions, you can help OpenLineage collect the most complete metadata possible. First, when working with datasets outside of your dbt project, define them in a schema YAML file inside the models/ directory: version: 2 sources: - name: stackoverflow database: bigquery-public-data schema: stackoverflow tables: - name: posts_questions - name: posts_answers - name: users - name: votes This contains the name of the external dataset - in this case, bigquery-public-datasets, and lists the tables that are used by the models in this project. It doesn’t matter what the file is named, as long as it ends with .yml and is inside the models/ directory, so I called mine schema.yml 🤷‍♂️ If you hardcode dataset and table names into your queries instead, dbt will likely run successfully but dataset metadata will be incompletely collected. When writing queries, be sure to use the {{ ref() }} and {{ source() }} jinja functions when referring to data sources. The {{ ref() }} function can be used to refer to tables within the same model, and the {{ source() }} function refers to tables we have defined in schema.yml. That way, dbt will properly keep track of the relationships between datasets. For example, to select from both an external dataset and one in this model: select * from {{ source('stackoverflow', 'posts_answers') }} where parent_id in (select id from {{ ref('filtered_questions') }} ) Performing a Run Okay! We are ready to perform a run. Before we do, though, there’s one last step we need to take. Run dbt docs generate. This will cause dbt to create a target/catalog.json file containing the schemas of each dataset referred to in the models. This file will be parsed by the dbt OpenLineage integration and sent to our Marquez server. If it doesn’t exist, a lineage graph will still be generated but schema details won’t be available in Marquez. dbt docs generate Running with dbt=0.20.1 Found 8 models, 0 tests, 0 snapshots, 0 analyses, 164 macros, 0 operations, 0 seed files, 4 sources, 0 exposures 12:15:10 | Concurrency: 1 threads (target='dev') 12:15:10 | 12:15:10 | Done. 12:15:10 | Building catalog 12:15:26 | Catalog written to /Users/rturk/projects/stackostudy/target/catalog.json The OpenLineage integration for dbt is implemented as a wrapper, dbt-ol. This wrapper runs dbt and, after it completes, analyzes the target/catalog.json, target/run_results.json and target/manifest.json files. It sends corresponding OpenLineage events to the endpoint specified in the OPENLINEAGE_URL environment variable. To run the models: % OPENLINEAGE_URL=http://localhost:5000 dbt-ol run Running with dbt=0.20.1 Found 8 models, 0 tests, 0 snapshots, 0 analyses, 164 macros, 0 operations, 0 seed files, 4 sources, 0 exposures 12:35:41 | Concurrency: 1 threads (target='dev') 12:35:41 | 12:35:41 | 1 of 8 START incremental model stackostudy.filtered_questions........ [RUN] 12:35:46 | 1 of 8 OK created incremental model stackostudy.filtered_questions... [MERGE (0.0 rows, 34.6 GB processed) in 4.52s] 12:35:46 | 2 of 8 START incremental model stackostudy.filtered_answers.......... [RUN] 12:35:51 | 2 of 8 OK created incremental model stackostudy.filtered_answers..... [MERGE (0.0 rows, 26.8 GB processed) in 5.22s] 12:35:51 | 3 of 8 START incremental model stackostudy.filtered_votes............ [RUN] 12:36:05 | 3 of 8 OK created incremental model stackostudy.filtered_votes....... [MERGE (0.0 rows, 6.5 GB processed) in 14.58s] 12:36:05 | 4 of 8 START incremental model stackostudy.filtered_users............ [RUN] 12:36:21 | 4 of 8 OK created incremental model stackostudy.filtered_users....... [MERGE (0.0 rows, 2.5 GB processed) in 16.09s] 12:36:21 | 5 of 8 START view model stackostudy.summary_daily.................... [RUN] 12:36:23 | 5 of 8 OK created view model stackostudy.summary_daily............... [OK in 1.01s] 12:36:23 | 6 of 8 START view model stackostudy.answer_stats..................... [RUN] 12:36:23 | 6 of 8 OK created view model stackostudy.answer_stats................ [OK in 0.96s] 12:36:23 | 7 of 8 START view model stackostudy.question_stats................... [RUN] 12:36:24 | 7 of 8 OK created view model stackostudy.question_stats.............. [OK in 0.88s] 12:36:24 | 8 of 8 START view model stackostudy.user_stats....................... [RUN] 12:36:26 | 8 of 8 OK created view model stackostudy.user_stats.................. [OK in 1.21s] 12:36:26 | 12:36:26 | Finished running 4 incremental models, 4 view models in 45.39s. Completed successfully Done. PASS=8 WARN=0 ERROR=0 SKIP=0 TOTAL=8 Emitted 16 openlineage events Note the output showing the number of OpenLineage events emitted to Marquez. Reviewing the Output If everything ran successfully you should be able to see a list of jobs when you navigate to http://localhost:3000. Upon clicking a job, you will see a lineage graph that looks similar to this: Our set of models, previously represented by SQL inside text files, has become more easily digestible. The dependencies between datasets are now completely obvious. Data engineers can throw away their remaining whiteboards, hooray! There’s something satisfying about seeing models represented in two-dimensional space. But more importantly, this integration allows us to capture the state of a dbt pipeline as it runs. Using a long-running instance of Marquez (or another OpenLineage-compatible metadata repository) this information can be studied as it changes over time. To see how the OpenLineage dbt integration works, visit its GitHub repository.","keywords":""},{"title":"Expecting Great Quality with OpenLineage Facets","type":0,"sectionRef":"#","url":"/blog/dataquality_expectations_facet","content":"","keywords":""},{"title":"The Parable of Bad Data​","type":1,"pageTitle":"Expecting Great Quality with OpenLineage Facets","url":"/blog/dataquality_expectations_facet#the-parable-of-bad-data","content":"Several years ago, I worked as a developer on the experimentation team at Amazon, which owned the code libraries and data processing systems that supported experimentation on the amazon.com website (among other systems). Developers used our libraries and a microservice we maintained to “trigger” an experiment for a customer- that is, the customer was randomized into either control or treatment and the resulting assignment was recorded in the logs, which my team consumed in the analysis of the data later on. One of the interesting parts of my job was helping our users diagnose problems with their experiment results. A classic example was a Kindle developer who was prototyping a new feature for book pages that would make the site more engaging for Kindle owners- perhaps a “look inside” feature, or maybe some better recommendations. A customer would come to the website and the developer’s code would determine if the customer belonged in Control or Treatment. If Control, the assignment was logged and no feature was shown- the site looked to the customer as it always had. But if the assignment was Treatment, the code would check the customer’s account to determine if they owned a Kindle device and, if yes, the assignment was logged and the customer saw the fancy new feature on the books page. The experiment showed the developer’s feature would be wildly successful- an increase of over $10 in Kindle book purchases per customer on average over the course of the 2 weeks the experiment ran- projected to be tens of billions of dollars in annual revenue due to this one feature! With data in hand, the developer requested tons of resources to build the feature up to production standards. After three months and two dozen people’s labor, the feature was ready to ship. The developers deployed their new service and the incredible feature was unleashed. For days afterward, everyone watched the metrics dashboards waiting for that hockey stick uptick in the revenue graphs 📈. But it never materialized! The graph was flat. No change at all! Weeks went by. Nothing. How could the experiment results be so far from reality? Of course, if you’ve ever run A/B tests, you probably already recognized the developer’s mistake. In their randomization logic, customers who were assigned control were logged and forgotten, while customers who were assigned treatment were logged only after validating that they owned a Kindle device. It turned out the total number of customers who came to amazon.com was far greater than the number of customers who owned a Kindle device. And if you divide the total sum of Kindle book sales by all of the amazon.com customers, regardless of whether they own a Kindle device, that average will come out quite a lot lower than if you calculate the average Kindle book revenue from only customers who own Kindles.  In reality, this story never happened. Why? Because we knew the adage- Bad Data is Worse than No Data. In the story, people took data of poor quality and used it to justify bad decisions. In our system, we checked the quality of the data and, if we detected assignment imbalances, we simply invalidated the experiment and hid the results. Over the years, I can’t count the number of times our users asked us to just give them partial results or just exclude certain segments or to let them know if things were “trending” the right way. Our policy was firm- if we couldn’t trust the quality of the data, the results were meaningless and we would not surface them in our system. "},{"title":"Data-Driven Depends On Data-Quality​","type":1,"pageTitle":"Expecting Great Quality with OpenLineage Facets","url":"/blog/dataquality_expectations_facet#data-driven-depends-on-data-quality","content":"Today, most businesses consider themselves data-driven. The stereotype of the maverick CEO leading with his or her gut is mostly antiquated, with a handful of exceptions. And yet, even though people know intellectually that data is only useful if it is correct, we tend to stop digging once we find some data that confirms our pre-existing assumptions. We justify bad decisions by claiming that they are “data-based” without ever validating the quality of our sources. Where did that data come from? How old is it? Is the logic that generated it correct? Is it derived from some other dataset? What is the quality of that dataset? Thankfully, data quality validation is becoming more and more common in data engineering organizations. In part, this is due to the prevalence of new tools and their integration with common workflow engines which we already use to schedule the jobs that generate and process our data. One such tool that has been gaining in popularity is called Great Expectations, a Python-based framework for defining assertions about data sets which easily integrates with existing workflow tools, such as Airflow. In software development, testing the behavior of our code with unit and integration tests has been common practice for years. Similarly, using Great Expectations, a data engineer can assert that a dataset has a row count that falls within an expected range, that column values are not null, or that values match a specified regular expression. One can even create custom expectations, such as validating that the number of records in treatment is roughly the same as the number of records in control (this post is not intended to be an in-depth tutorial on setting up Great Expectations; if you want to read more on its capabilities and to get started, I recommend the going through the Quick Start tutorial). "},{"title":"A Sample Assertion Suite​","type":1,"pageTitle":"Expecting Great Quality with OpenLineage Facets","url":"/blog/dataquality_expectations_facet#a-sample-assertion-suite","content":"As a simple example, imagine a table of new customers that you need to import into your Data Warehouse. Before importing, we want to check the data quality of this new batch of customers. One example suite of assertions we could test is below: { &quot;data_asset_type&quot;: &quot;Dataset&quot;, &quot;expectation_suite_name&quot;: &quot;customers_suite&quot;, &quot;expectations&quot;: [ { &quot;expectation_type&quot;: &quot;expect_table_row_count_to_be_between&quot;, &quot;kwargs&quot;: { &quot;max_value&quot;: 1000, &quot;min_value&quot;: 100 }, &quot;meta&quot;: {} }, { &quot;expectation_type&quot;: &quot;expect_table_column_count_to_equal&quot;, &quot;kwargs&quot;: { &quot;value&quot;: 8 }, &quot;meta&quot;: {} }, { &quot;expectation_type&quot;: &quot;expect_table_columns_to_match_ordered_list&quot;, &quot;kwargs&quot;: { &quot;column_list&quot;: [ &quot;id&quot;, &quot;created_at&quot;, &quot;updated_at&quot;, &quot;name&quot;, &quot;email&quot;, &quot;address&quot;, &quot;phone&quot;, &quot;city_id&quot; ] }, &quot;meta&quot;: {} }, { &quot;expectation_type&quot;: &quot;expect_column_values_to_be_unique&quot;, &quot;kwargs&quot;: { &quot;column&quot;: &quot;email&quot; }, &quot;meta&quot;: {} } ], &quot;meta&quot;: { // ... } }  This sample suite contains 4 data quality assertions- that the dataset contains between 100 and 1000 rows, that the table contains exactly 8 columns, that they match the explicit list of column names we expect, and that the email column contains only distinct values. "},{"title":"Adding Data Quality Checks to an Airflow pipeline​","type":1,"pageTitle":"Expecting Great Quality with OpenLineage Facets","url":"/blog/dataquality_expectations_facet#adding-data-quality-checks-to-an-airflow-pipeline","content":"With a suite of assertions in hand, we can update our Airflow DAG to only import data into our Data Warehouse if it matches our expectations. A simple DAG might look like this from airflow import DAG from airflow.contrib.operators.bigquery_operator import BigQueryOperator from airflow.utils.dates import days_ago from great_expectations_provider.operators.great_expectations import GreatExpectationsOperator dag = DAG( 'etl_customers', schedule_interval='@daily', catchup=False, default_args=default_args, description='Loads newly registered customers daily.' ) t1 = BigQueryOperator( task_id='if_not_exists', sql=''' CREATE TABLE IF NOT EXISTS food_delivery.customers ( id INT64, created_at TIME, updated_at TIME, name STRING, email STRING, address STRING, phone STRING, city_id INT64 ) ''', use_legacy_sql=False, dag=dag ) t2 = GreatExpectationsOperator( expectation_suite_name='customers_suite', batch_kwargs={ 'table': 'tmp_customers', 'datasource': 'food_delivery_db' }, dag=dag task_id='customers_expectation', ) t3 = BigQueryOperator( task_id='etl', sql=''' SELECT id, created_at, updated_at, name, email, address, phone, city_id FROM food_delivery.tmp_customers ''', destination_dataset_table='airflow_marquez.food_delivery.customers', use_legacy_sql=False, dag=dag ) t1 &gt;&gt; t2 &gt;&gt; t3  This is great! Our DAG creates our target table in BigQuery (if it doesn’t already exist), checks the quality of thetmp_customers table by running the customers_suite defined earlier, then imports only if all of the data quality checks pass. And thus ended all data quality problems forever. Just kidding. Because reality is never so straightforward. In reality, the recommendations team wanted to start generating recommendations for new customers without waiting until the next day, so they built a data pipeline to start consuming from the tmp_customers table directly. And the supply chain folks wanted to start detecting what recipes are gaining popularity so they can predict what supplies will need to be restocked sooner, so they started reading from the bothtmp_orders table and the tmp_customers table before they’re available in the DW. Oh, and the scheduling team needs an idea of the geography of the various customers that are ordering and what the distances are between restaurants and customers so they can get the right number of drivers in the right neighborhoods and of course the marketing team wants to use all of this data to make predictions about how much to spend on the right search engine and social media ads and they absolutely cannot wait until tomorrow at 8AM to update their models. "},{"title":"Tracing Data Quality With OpenLineage Facets​","type":1,"pageTitle":"Expecting Great Quality with OpenLineage Facets","url":"/blog/dataquality_expectations_facet#tracing-data-quality-with-openlineage-facets","content":"Users are never satisfied with the way things are supposed to work. There’s always a reason to work around gatekeepers- oftentimes, very good reasons that have real business impact- and data engineering is full of creative and resourceful people who absolutely will find a way to get at that data. Even at Amazon, the experiment data was available in the click stream logs, so resourceful users could (and sometimes did) calculate their own experiment results if they really wanted to. So it’s important not just to have data quality checks, but to trace the impact of that data throughout an organization. The OpenLineage standard uses Facets to augment the core data model with useful information about the jobs, runs, and datasets reported on. One interesting detail about facets is that they can be attached to an entity after the fact. In the Marquez reference implementation, a dataset version is created every time a job run writes to or otherwise modifies a dataset. Output facets, such as the new record count or the number of bytes written, are attached directly to the dataset version when the job run completes. But consuming jobs can also attach facets to the version of the dataset that exists at the start time of the job’s execution. In the OpenLineage Airflow integration, Great Expectations tasks, such as the one in our example DAG above, are evaluated after they run and the expectation results (as well as some other data quality metrics) are collected into aDataQuality Metrics Input Dataset Facet, which is reported to the server along with the rest of the lineage metadata. In Marquez, we recognize the version of the dataset that was read by the job run and the data quality metadata is permanently associated with that dataset version. The impact of this is that any job that reads that data, whether it happens before or after the dataset quality assertion, can be linked to the data quality facet recorded (provided that the dataset version doesn’t change between the data quality check and the read job). This integration is extremely straightforward to get working. If you already have the Marquez Airflow DAG running in your Airflow workflows, there’s nothing to do! Great Expectations tasks are already being detected and the metrics and assertion statuses are already being reported to your configured instance of Marquez. If you’ve never integrated Marquez with your Airflow setup, add a couple of environment variablesand change one line of code: - from airflow import DAG + from marquez_airflow import DAG from airflow.contrib.operators.bigquery_operator import BigQueryOperator from airflow.utils.dates import days_ago from great_expectations_provider.operators.great_expectations import GreatExpectationsOperator  I’ve previously written about how to determine the version of the dataset that was read by a particular job run. With Great Expectations now integrated into my Airflow DAG, I want to see what the data quality metrics are for the latest version of the customers dataset that was processed by my ETL job. I’ll hit my datakin demo instance: $ curl &quot;https://demo.datakin.com/api/v1/namespaces/food_delivery/jobs/etl.etl_delivery_7_days&quot; | jq | less { &quot;id&quot;: { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;etl.etl_delivery_7_days&quot; }, &quot;type&quot;: &quot;BATCH&quot;, &quot;name&quot;: &quot;etl.etl_delivery_7_days&quot;, &quot;createdAt&quot;: &quot;2021-07-23T19:32:03.401782Z&quot;, &quot;updatedAt&quot;: &quot;2021-08-06T05:11:03.604573Z&quot;, &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;inputs&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.customers&quot; }, //... ], &quot;latestRun&quot;: { &quot;id&quot;: &quot;1043e596-ccb8-4bfb-8fc2-7ee066253248&quot;, &quot;jobVersion&quot;: { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;etl.etl_delivery_7_days&quot;, &quot;version&quot;: &quot;bc6c294b-b0eb-3160-a06d-1ff9ba3a4e1c&quot; }, &quot;inputVersions&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.customers&quot;, &quot;version&quot;: &quot;4c33f292-40a9-304d-b43f-c7ffb2256e7f&quot; }, // ... ], // ... } }  With the input version of the public.customers dataset, I can query Marquez for all the metadata we have about that specific version of the dataset. $ curl &quot;https://demo.datakin.com/api/v1/namespaces/food_delivery/datasets/public.customers/versions/4c33f292-40a9-304d-b43f-c7ffb2256e7f&quot; | jq | less { &quot;id&quot;: { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.customers&quot; }, &quot;type&quot;: &quot;DB_TABLE&quot;, &quot;name&quot;: &quot;public.customers&quot;, &quot;physicalName&quot;: &quot;public.customers&quot;, &quot;createdAt&quot;: &quot;2021-08-06T05:02:59.189118Z&quot;, &quot;version&quot;: &quot;4c33f292-40a9-304d-b43f-c7ffb2256e7f&quot;, &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;sourceName&quot;: &quot;analytics_db&quot;, &quot;fields&quot;: [ { &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;INTEGER&quot;, &quot;tags&quot;: [], &quot;description&quot;: &quot;The unique ID of the customer.&quot; }, // ... ], &quot;facets&quot;: { &quot;stats&quot;: { &quot;size&quot;: 53362712, &quot;rowCount&quot;: 4969 }, &quot;dataSource&quot;: { &quot;uri&quot;: &quot;jdbc:postgresql://localhost:3306/deliveries&quot;, &quot;name&quot;: &quot;analytics_db&quot; }, &quot;description&quot;: &quot;A table for customers.&quot;, &quot;dataQuality&quot;: { &quot;bytes&quot;: 53362712, &quot;rowCount&quot;: 4969, &quot;columnMetrics&quot;: { &quot;id&quot;: { &quot;nullCount&quot;: 0, &quot;distinctCount&quot;: 4969 }, &quot;name&quot;: { &quot;nullCount&quot;: 0, &quot;distinctCount&quot;: 4969 }, &quot;email&quot;: { &quot;nullCount&quot;: 0, &quot;distinctCount&quot;: 4969 } } }, &quot;greatExpectations_assertions&quot;: { &quot;assertions&quot;: [ { &quot;success&quot;: true, &quot;expectationType&quot;: &quot;expect_table_row_count_to_be_between&quot; }, { &quot;success&quot;: true, &quot;expectationType&quot;: &quot;expect_column_to_exist&quot; }, { &quot;success&quot;: true, &quot;columnId&quot;: &quot;id&quot;, &quot;expectationType&quot;: &quot;expect_column_values_to_be_unique&quot; }, { &quot;success&quot;: true, &quot;columnId&quot;: &quot;id&quot;, &quot;expectationType&quot;: &quot;expect_column_values_to_not_be_null&quot; }, { &quot;success&quot;: true, &quot;columnId&quot;: &quot;created_at&quot;, &quot;expectationType&quot;: &quot;expect_column_values_to_not_be_null&quot; }, //.... ] } } }  Note the facets field contains several properties- stats, dataSource, description, dataQualityand greatExpectations_assertions. Each of those describes some attribute about the dataset version. Some of the facets are attached at write-time, some are attached later- when the dataset is read. In our Datakin demo, we have a lot more assertions than what I included in the sample suite above and that can be seen in this response. In addition to counting rows and columns, we also validate that id columns are unique and non-null, timestamps fall within specified ranges (did you know that if you accidentally write a timestamp too far in the future, certain JDBC drivers will overflow the Calendar instance they use for converting timezones?), and emails match expected regular expressions. With the ability to attach data quality facets to dataset versions and the ability to trace the specific versions of datasets read by and written to by specific job runs, I can trust whether the data I’m looking at is good data or bad data. And if my data quality checks fail, I can find out whether I need to contact somebody over in marketing or recommendations to backfill their pipelines once the issue has been corrected.  Whether your business is an e-commerce shop that wants to improve its customer experience or a music streaming service that wants to make better listening recommendations or an autonomous vehicle company trying to improve the car’s ability to detect double parked vehicles, the quality of your data is paramount to making good decisions. Quality testing tools are out there and, chances are, they already work with the pipeline workflow tool you’re using today. And with OpenLineage support, you can be confident in the quality of the data at every stage in your pipeline. "},{"title":"OpenLineage Support for Streaming to Feature at Kafka Summit","type":0,"sectionRef":"#","url":"/blog/kafka-summit-talk","content":"At this year's Kafka Summit in London, two project committers, Paweł Leszczyński and Maciej Obuchowski, will give a talk entitledOpenLineage for Stream Processing on March 19th at 2:00 PM GMT. As the abstract available on the summit websitesays, the talk will cover some of the 'many useful features completed or begun' recently related to stream processing, including: a seamless OpenLineage &amp; Apache Flink integration, support for streaming jobs in Marquez, progress on a built-in lineage API within the Flink codebase. As the abstract goes on to say, Cross-platform lineage allows for a holistic overview of data flow and its dependencies within organizations, including stream processing. This talk will provide an overview of the most recent developments in the OpenLineage Flink integration and share what’s in store for this important collaboration. This talk is a must-attend for those wishing to stay up-to-date on lineage developments in the stream processing world. Register and attend this interesting talk if you can. And keep an eye out for an announcement about a recording if and when one becomes available. Thanks, Maciej and Paweł, for spreading the word about these exciting developments in the project.","keywords":""},{"title":"Join us in London on January 31st","type":0,"sectionRef":"#","url":"/blog/london-confluent-meetup","content":"","keywords":""},{"title":"Time, Place & Format​","type":1,"pageTitle":"Join us in London on January 31st","url":"/blog/london-confluent-meetup#time-place--format","content":"Date: January 31, 2024 Format: In-person Time: 6:00-8:00 pm GMT Address: Confluent, 1 Bedford Street, London WC2E 9HD, United Kingdom "},{"title":"Metaphor's Integration with OpenLineage Enhances Data Governance and Collaboration","type":0,"sectionRef":"#","url":"/blog/metaphor-integration","content":"","keywords":""},{"title":"Embracing OpenLineage​","type":1,"pageTitle":"Metaphor's Integration with OpenLineage Enhances Data Governance and Collaboration","url":"/blog/metaphor-integration#embracing-openlineage","content":"Metaphor’s journey towards data governance excellence led to the adoption of OpenLineage, a standardized metadata model that enables seamless integration with various data systems. The motivation behind this decision is threefold: Streamlined Metadata Collection: OpenLineage provides a standardized framework for collecting and managing metadata, simplifying the process of gathering lineage information from diverse data processing systems.Interoperability: OpenLineage boasts a range of clients for popular data processing frameworks like Apache Spark and Apache Airflow. By adopting OpenLineage, we gain the ability to seamlessly integrate with these systems, ensuring data lineage tracking across the entire data ecosystem.Enhanced Data Understanding: Integration with OpenLineage enhances Metaphor's core mission of making data understandable and actionable. It empowers users to gain deeper insights into data assets by tracking their lineage, from source to consumption.  The Metaphor team recognized the immense potential of OpenLineage and quickly decided to adopt OpenLineage as a crucial component of their platform. "},{"title":"Integration with OpenLineage​","type":1,"pageTitle":"Metaphor's Integration with OpenLineage Enhances Data Governance and Collaboration","url":"/blog/metaphor-integration#integration-with-openlineage","content":"To integrate with OpenLineage, we developed our own OpenLineage-compatible REST endpoint. This endpoint allows Metaphor to consume metadata events emitted by OpenLineage clients, including the ones for Apache Spark and Airflow. The integration process involves transforming this metadata into Metaphor’s own data models, which are then associated with the corresponding data assets within the platform, creating a cohesive global knowledge graph. The user interface of Metaphor plays a pivotal role in making this integration accessible. Users can easily drill down into data jobs and data lineages, facilitating their work and enriching their understanding of data assets.  "},{"title":"Realizing the Benefits​","type":1,"pageTitle":"Metaphor's Integration with OpenLineage Enhances Data Governance and Collaboration","url":"/blog/metaphor-integration#realizing-the-benefits","content":"We have rolled out the OpenLineage integration to production environments. Early adopters have already begun to reap the benefits of Spark and Airflow lineage graphs, especially the column-level lineage. Users can now gain deeper insights into their data assets, making it easier to make informed decisions, optimize data usage, and maximize ROI. In conclusion, the integration of OpenLineage with Metaphor represents a significant step forward in the world of data management and governance. It combines the power of standardized metadata with an intuitive social platform, offering organizations a comprehensive solution for their data needs. As more organizations embrace this integration, we can expect to see even greater advancements in data governance and collaboration, unlocking the full potential of their data assets. "},{"title":"Additional Resources​","type":1,"pageTitle":"Metaphor's Integration with OpenLineage Enhances Data Governance and Collaboration","url":"/blog/metaphor-integration#additional-resources","content":"Metaphor Data: https://metaphor.io/Try Metaphor: https://metaphor.io/try Metaphor Documentation: https://docs.metaphor.io/docsMetaphor OSS Connectors: https://github.com/MetaphorData/connectors "},{"title":"At Manta, OpenLineage Opens Doors to New Insights","type":0,"sectionRef":"#","url":"/blog/manta-integration","content":"Adopting OpenLineage as part of our portfolio allows MANTA to bring detailed run-time lineage to our customers. Here at MANTA, we are very excited to be working closely with OpenLineage and, more importantly, with the OpenLineage Community. As a leader in lineage analysis, we see first-hand the complexity required to achieve effective lineage, and the benefits of having an accepted standard for the sharing of operational lineage metadata. OpenLineage moves everything in the direction of enhanced interoperability, and helps to ensure that enterprises have maximum flexibility for current and future tool selection. Adopting OpenLineage as part of our portfolio allows MANTA to bring detailed run-time lineage to our customers, many of whom are enterprise organizations and need this level of granularity. This is especially important for new technologies such as Apache Airflow, whose integration with OpenLineage continues to evolve. Apache Airflow, as an example, is increasingly being utilized by our customers as part of their process orchestration portfolio; as such, these companies need lineage coverage for these operations. Having a recognized industry standard for lineage capture and reporting is an enabler for enhanced metadata management and governance. OpenLineage helps to ensure increased consistency in pipeline analysis, especially as more and more solutions appear in the Cloud, and in the general marketplace, for the transformation, enrichment, and overall movement of information through new and future dataflows. Vendors like MANTA will continue to offer creative and purposeful solutions that answer key questions and meet the end-to-end requirements of the business. For selected technologies, OpenLineage enables us to do this faster and simpler. Eighteen months ago, we started our investigation into OpenLineage. After working with various customers earlier this year, we decided to double down on our investment and get more involved with the OpenLineage Community. Throughout our journey, we’ve found this growing community to be welcoming, helpful, and collaborative. MANTA is pleased to contribute however we can to this important open source project. Are you ready to join? For more information about MANTA's data lineage solution, visit our website. To learn more about contributing to OpenLineage, check out the project's new contributor guide.","keywords":""},{"title":"Meet Us in NYC Later This Month!","type":0,"sectionRef":"#","url":"/blog/nyc-meetup","content":"","keywords":""},{"title":"Time, Place & Format​","type":1,"pageTitle":"Meet Us in NYC Later This Month!","url":"/blog/nyc-meetup#time-place--format","content":"Date: April 26th, 2023 Format: In-person Time: 5:30-8:30 pm ET Address: Astronomer, 636 6th Avenue, 3rd Floor, New York, NY 10011 Getting There​ The Astronomer NY offices are in the Flatiron District at the intersection of 6th Avenue and 19th Street, one block east of the 18th Street Metro station. The entrance to the building is on 19th Street. Getting In​ If you arrive before 6 pm, simply come on up to the third floor! Otherwise, post a message in Slack to let us know you're here, and someone will let you in. "},{"title":"Hope to see you there!​","type":1,"pageTitle":"Meet Us in NYC Later This Month!","url":"/blog/nyc-meetup#hope-to-see-you-there","content":""},{"title":"OpenLineage Support in Egeria","type":0,"sectionRef":"#","url":"/blog/openlineage-egeria","content":"","keywords":""},{"title":"OpenLineage Support in Egeria​","type":1,"pageTitle":"OpenLineage Support in Egeria","url":"/blog/openlineage-egeria#openlineage-support-in-egeria","content":"Egeria is a sister open source project to OpenLineage in the LF AI and Data Foundation. Egeria provides Open Metadata and Governance standard types and integration technology to exchange metadata between different technologies. It stitches together different standards to create a complete landscape of metadata about an organization’s digital operations. OpenLineage is very welcome to the Egeria team since it defines a standard for dynamic lineage capture. This means Egeria can capture open lineage events to detect new assets and activity around them, link this new knowledge into the existing metadata and distribute it to the open metadata ecosystem. Egeria also executes governance processes for maintaining both metadata and the data sources it describes. Since it is running processes, it also makes sense that Egeria produces open lineage for its processes. The diagram below is a big animal picture showing the different features relating to open lineage that Egeria offers. With Egeria’s plug-and-play architecture you can pick and choose which pieces you need.  The numbers on the diagram refer to the notes below. Egeria can capture open lineage events directly through HTTP or via the proxy backend.OpenLineage metadata is correlated and matched to existing metadata captured through a variety of mechanisms from direct metadata extraction from the hosting data platforms, to updates through dev ops pipelines to metadata discovery analytic tools.Egeria can publish OpenLineage events. These include the OpenLineage events it received (potentially augmented with additional facets), or events generated from its own governance processes. Published OpenLineage events can go to Egeria’s OpenLineage file-based log store for later processing or to any application that supports the OpenLineage API (Marquez, for example -- another project from LF AI and Data).The metadata extracted from OpenLineage events can be distributed to the open metadata ecosystem using standard approaches. This means it can be picked up by connected data science, governance and lineage tools.Governance processes linked to the open metadata ecosystem can use OpenLineage events to validate that their originating processes are operating as frequently and as accurately as expected. More information on Egeria’s open lineage support can be found here. The Egeria community would like to thank the OpenLineage community for their great support while we created this integration. We look forward to continuing to work together as both our projects mature. "},{"title":"How Northwestern Mutual Simplified Data Observability with OpenLineage & Marquez","type":0,"sectionRef":"#","url":"/blog/openlineage-at-northwestern-mutual","content":"","keywords":""},{"title":"Embrace cloud managed services​","type":1,"pageTitle":"How Northwestern Mutual Simplified Data Observability with OpenLineage & Marquez","url":"/blog/openlineage-at-northwestern-mutual#embrace-cloud-managed-services","content":"Many of the foundational needs of an Enterprise Data Platform can be accomplished using a cloud-first mindset. While we may not all agree which cloud provider is best, we can all agree that the level of scale and sophistication accomplished around things like storage, compute, and redundancy are going to be MUCH greater when relying on a cloud provider than when rolling your own solution. "},{"title":"We are software engineers​","type":1,"pageTitle":"How Northwestern Mutual Simplified Data Observability with OpenLineage & Marquez","url":"/blog/openlineage-at-northwestern-mutual#we-are-software-engineers","content":"The Data Mesh evolution has reminded the industry that centralized data teams do not scale or empower anybody. With this principle in mind, our platform teams embraced full automation from the beginning and designed for self-service workflows. We do not want to become the bottleneck to insights; rather, we want to enable data owners to manage and share their datasets throughout the company. We want to empower data engineers with transformation and machine learning capabilities, so that they can author pipelines and deliver insights. "},{"title":"Aim for simplicity through consistency​","type":1,"pageTitle":"How Northwestern Mutual Simplified Data Observability with OpenLineage & Marquez","url":"/blog/openlineage-at-northwestern-mutual#aim-for-simplicity-through-consistency","content":"Traditionally, data platforms have gathered and constructed technical metadata based on events of the past. For example, there are many crawlers that will examine various database systems and build a catalog to make those datasets “discoverable.” Logs from various jobs can be parsed in extremely specific ways to identify datasets consumed and produced by a given pipeline to infer data lineage. We viewed these traditional methods as a massive impediment to activating DataOps, due to differing technology solutions and the historical-based approach of their designs. Our platform aimed to achieve dynamic decisions based on what is happening as it is happening. We also recognize and appreciate the complexity of this portion of the platform and did not find it wise to build from the ground up. Especially with the industry momentum towards real-time data observability, why add another custom solution to the stack? With such an evolving technical landscape, it was important for us to avoid vendor lock to allow us flexibility in future decisions. NM hearts OL When we first learned of the OpenLineage specification, we were very intrigued and hopeful. An open specification focused on observing real-time events AND unifying tools and frameworks?!? Fast forward nine months, and we cannot believe how much capability we have developed around data observability in such a brief time. Let me back up a little... Marquez is a metadata management framework that implements the OpenLineage specification. It transforms your data runtime events into a searchable catalog of technical metadata. It was a perfect fit to the skills of our Platform Data Engineers - it is written in Java, runs in Kubernetes, and integrates well with our backend services via web-based APIs. We were able to quickly deploy this framework into our own environment, which provided us with several immediate wins. "},{"title":"Flexible framework​","type":1,"pageTitle":"How Northwestern Mutual Simplified Data Observability with OpenLineage & Marquez","url":"/blog/openlineage-at-northwestern-mutual#flexible-framework","content":"Since it is aligned with the OpenLineage framework, Marquez can process messages from ANY data producer that is publishing compliant events. The Marquez and OpenLineage communities have been doing an excellent job maturing the integration library, which allows you to tackle this challenge at the infrastructure level. This is the ultimate easy button approach and our own ideal state; configure an environment on behalf of your user base and sit back while it automatically detects and logs the activity within! In the cases when an integration either does not exist or you need to address a more custom workflow, you can construct and emit your own OpenLineage event messages. Marquez will still be able to process and store custom OpenLineage events, provided they meet the requirements of the open standard. For example, our teams have been able to programmatically construct OpenLineage messages within code that pulls data from various on-premises database servers and publishes it into our Data Platform. Using the OpenLineage specification, we extract the actual table schema from the source system as part of the Dataset entity and log the executing SQL query as part of the Job entity. This code was simplistic and allowed us to meet our immediate needs around observing data movement and recording those event details. "},{"title":"Alignment with enterprise​","type":1,"pageTitle":"How Northwestern Mutual Simplified Data Observability with OpenLineage & Marquez","url":"/blog/openlineage-at-northwestern-mutual#alignment-with-enterprise","content":"Marquez already supported Kubernetes when we got involved, which provided us with many different deployment options. Our first contributions to the project were made to mature the Helm chart and to enhance security around the base images and Kubernetes secrets usage. These changes allowed us to fully automate our deployments using GitOps and incorporate internal security measures involving container vulnerability management. The flexibility offered by the Marquez deployment architecture and our ability to customize its details allowed us to activate new production use cases in about a month. We were happy with this timeline, given the series of security checkpoints that were validated and the wealth of functionality we had just unlocked. "},{"title":"Collaborative working group​","type":1,"pageTitle":"How Northwestern Mutual Simplified Data Observability with OpenLineage & Marquez","url":"/blog/openlineage-at-northwestern-mutual#collaborative-working-group","content":"Both the Marquez and OpenLineage communities have been extremely welcoming, and that has been a huge factor in our success at Northwestern Mutual. Our feedback and ideas have been encouraged and heard, which is evidenced by evolving project roadmaps and accepted developer contributions. We have learned quite a bit from the community members and feel fortunate to be a part of this group. Monthly community meetings are informative yet have an amazingly informal feel to them. Where are we headed? The Unified Data Platform at Northwestern Mutual relies on the OpenLineage standard to formulate technical metadata within our various platform services. Publishing these events into Marquez has provided us with an effortless way to understand our running jobs. We can easily trace a downstream dataset to the job that produced it, as well as examine individual runs of that job or any preceding ones. Gaining the ability to observe lineage throughout our platform has been huge, and we are just getting started. Our teams are working to apply standard OpenLineage integrations into our environment and introduce data quality facets into our events. We have also been establishing operational workflows using job run information, to allow our DataOps team to monitor durations and measure against SLAs. "},{"title":"Microsoft Purview Accelerates Lineage Extraction from Azure Databricks","type":0,"sectionRef":"#","url":"/blog/openlineage-microsoft-purview","content":"","keywords":""},{"title":"Summary​","type":1,"pageTitle":"Microsoft Purview Accelerates Lineage Extraction from Azure Databricks","url":"/blog/openlineage-microsoft-purview#summary","content":"Microsoft Purview provides a comprehensive platform to populate native and custom data lineage metadata from on-prem, OSS, SaaS, and multi-cloud data systems. The Azure Databricks to Microsoft Purview Solution Accelerator takes advantage of the robust Spark integration inside OpenLineage and translates OpenLineage events into Microsoft Purview native assets supporting: Azure Data Lake Gen 2Azure Blob StorageAzure SQLAzure Synapse SQL Pools Customers of Azure Databricks and Microsoft Purview can try the solution today by following the demo instructions or connector only instructions. "},{"title":"What is Microsoft Purview?​","type":1,"pageTitle":"Microsoft Purview Accelerates Lineage Extraction from Azure Databricks","url":"/blog/openlineage-microsoft-purview#what-is-microsoft-purview","content":"Microsoft Purview provides an ambient data governance solution that helps you unify and manage your data wherever it exists – on-premises, in the cloud, or on a software-as-a-service (SaaS) platform. With Microsoft Purview, you can: create a holistic, up-to-date map of your data landscape with automated data discovery, sensitive data classification, and end-to-end data lineage.enable data curators to manage and secure your data estate.empower data consumers to find valuable, trustworthy data. Figure 1. Microsoft Purview is an ambient data governance platform for an enterprise. Microsoft Purview automates data discovery by providing data scanning and classification as a service for assets across your data estate. Microsoft Purview integrates metadata and descriptions of discovered data assets into a holistic map of your data ecosystem. Layered on this map are purpose-built apps that create environments for data discovery, policy management, and insights into your data landscape. "},{"title":"Data Lineage in Microsoft Purview​","type":1,"pageTitle":"Microsoft Purview Accelerates Lineage Extraction from Azure Databricks","url":"/blog/openlineage-microsoft-purview#data-lineage-in-microsoft-purview","content":"Organizations need data to conduct business, and they need trustworthy data to perform analysis and make key decisions. Data lineage and provenance provide insights into data pedigree, which relates to operational information, runtime analysis, historical lineage, and ownership information. Users rely on pedigree when taking insights from data. Critical scenarios involving root cause analysis, impact analysis, quality control, compliance, and audit tracing are served by data lineage and provenance. Data Lineage in Microsoft Purview is a core platform capability that populates the Microsoft Purview Data Map with data movement and transformations across systems. With the backing of Apache Atlas 2.2, lineage is captured as it flows in the enterprise and stitched without gaps irrespective of its source. Data lineage in Microsoft Purview enables data analysts and data stewards to conduct root cause analysis, troubleshoot, and perform impact analysis of data moving upstream and downstream in data estates. With a combined platform and interactive lineage visualization tool, data investigations related to quality, trust, and compliance can be self-served in a few clicks rather than requested from a third party. Microsoft Purview has native data lineage support for 20+ sources, many of which are integrated at engine runtimes. For example, data lineage is pushed from Azure Data Factory when pipelines are run. This deep integration allows Microsoft Purview to capture operational metadata such as job start/end times, the number of rows impacted, job run status and more. In addition to native support, the open APIs can be used to integrate with enterprise systems to support custom lineage.  Figure 2. Native data lineage visualization in Microsoft Purview. "},{"title":"OpenLineage + Microsoft​","type":1,"pageTitle":"Microsoft Purview Accelerates Lineage Extraction from Azure Databricks","url":"/blog/openlineage-microsoft-purview#openlineage--microsoft","content":"This integration came about because Microsoft Purview sought a lineage solution for Azure Databricks users, ideally one that would support all Azure data repository types, from Azure Blob Storage to Azure SQL. The team that took on this challenge was the Early Access Engineering (EAE) team, a group of data experts at Microsoft who forge competitive differentiation and value by using groundbreaking technology and features before they become available to the general enterprise landscape. A History of Contributions to OpenLineage​ The EAE team at Microsoft has a long history of contributions to open source projects in general and to OpenLineage in particular. In December of 2021, Will Johnson contributed a PR to OpenLineage to add support for arbitrary parameters in the OpenLineage URL. This change supported key-based authentication via URL and eased the process of sending metadata from OpenLineage to repositories other than Marquez, OpenLineage’s sister project. This in turn supported additional integrations and collaboration and has helped to increase adoption of the OpenLineage standard. Over the course of seven months, the Microsoft team contributed eight pull requests to enable: better support for the Azure Blob File System (Azure Data Lake Gen 2).use of an Azure Function as the lineage endpoint.lineage extraction for Azure Synapse as a data source.extraction of Databricks environment properties such as notebook paths and job ids. Contributing open source integrations to OpenLineage benefits not only Microsoft Purview but also the data landscape as a whole. Collaborations like this one help increase adoption of the OpenLineage standard across the industry, which gets us closer to the single standard we need for consistently powerful and reliable lineage across the wide diversity of tooling in today’s data pipelines. At Microsoft, this kind of work is not unique to the EAE team. Across the company, cross-functional, community-driven teams foster innovation through open source collaboration. "},{"title":"Why Contribute to OpenLineage?​","type":1,"pageTitle":"Microsoft Purview Accelerates Lineage Extraction from Azure Databricks","url":"/blog/openlineage-microsoft-purview#why-contribute-to-openlineage","content":"Most enterprise data environments are convoluted, with data systems spread across on-prem, multi-cloud, SaaS, and open-source platforms. The data moves between a variety of storage, processing, analytical, and SaaS data systems. Azure Databricks is one such data system in an enterprise with a lakehouse platform in the cloud that combines data warehouses and data lakes to offer an open and unified platform for data and AI. Microsoft Purview customers have long asked for the ability to populate and govern Azure Databricks assets in the Microsoft Purview DataMap. With OpenLineage, we are bringing runtime Data Lineage capture from Azure Databricks Spark workloads to Microsoft Purview. In addition, by contributing to OpenLineage, Microsoft can offer users of OpenLineage on other platforms the ability to represent metadata models of Microsoft data sources accurately in their lineage graphs. For example, users of Spark on any other platform can represent the metadata models of Microsoft data sources more accurately. Lastly, OpenLineage benefits from Microsoft’s contributions as they will add hundreds if not thousands of new users to the OpenLineage standard. This will spur more contributions by the OpenLineage community as more users request that new implementations and features be added to the specification. "},{"title":"About the Solution​","type":1,"pageTitle":"Microsoft Purview Accelerates Lineage Extraction from Azure Databricks","url":"/blog/openlineage-microsoft-purview#about-the-solution","content":"Figure 3. The flow of metadata from Azure Databricks to Microsoft Purview using OpenLineage. An Azure administrator deploys an Azure Function (serverless C# application) and an Event Hub (to store OpenLineage events) by running a deployment script.An administrator configures a Databricks cluster as per the OpenLineage install instructions along with the Azure Function key and OpenLineage host pointing to the Azure Function.The OpenLineage Spark jar extracts the necessary inputs and outputs and emits them to the Azure Function.The Azure Functions transform the OpenLineage payload and push lineage to Microsoft Purview through the Apache Atlas REST APIs.Databricks Lineage is then visible inside Microsoft Purview! "},{"title":"Getting Started with Microsoft Purview​","type":1,"pageTitle":"Microsoft Purview Accelerates Lineage Extraction from Azure Databricks","url":"/blog/openlineage-microsoft-purview#getting-started-with-microsoft-purview","content":"Quickly and easily create a Microsoft Purview account to explore the new features. Try out the Azure Databricks to Microsoft Purview Solution Accelerator. Learn how to deploy the solution. "},{"title":"What the Future Holds​","type":1,"pageTitle":"Microsoft Purview Accelerates Lineage Extraction from Azure Databricks","url":"/blog/openlineage-microsoft-purview#what-the-future-holds","content":"Microsoft plans to continue contributing to OpenLineage to ensure that users can extract lineage from additional Azure data sources such as Azure Data Explorer (Kusto), Azure Cosmos DB, and Azure Event Hubs, and that OpenLineage continues to perform well on Azure Databricks. In addition, Microsoft plans to keep up-to-date with advancements made by the OpenLineage community, such as the exciting recent contribution of column-level lineage to the project. "},{"title":"Acknowledging the Contributors​","type":1,"pageTitle":"Microsoft Purview Accelerates Lineage Extraction from Azure Databricks","url":"/blog/openlineage-microsoft-purview#acknowledging-the-contributors","content":"The OpenLineage Spark integration is the product of hard work by teams inside and outside Microsoft. Contributors from the Microsoft Early Access Engineering team include: Mark Taylor, Principal Technical Specialist (@marktayl1)Will Johnson, Global Black Belt - Big Data, Analytics, and ML Specialist (@wjohnson)Rodrigo Monteiro, Global Black Belt - Big Data, Analytics (@rodrigomonteiro-gbb)Travis Hilbert, Technical Specialist (@travishilbert)Matt Savarino, Sr. Technical Specialist (@mattsavarino) Outside Microsoft, contributors to the OpenLineage Spark integration are based at a range of internationally distributed companies and organizations. Additional contributors to the integration include: Michael Collado, Staff Software Engineer, Astronomer (@collado-mike)Oleksandr Dvornik, Senior Java Developer, UBS (@OleksandrDvornik)Paweł Leszczyński, Data Engineer, GetInData (@pawel-big-lebowski)Tomasz Nazarewicz, Data Engineer, GetInData (@tnazarew)Maciej Obuchowski, Software Engineer, GetInData (@mobuchowski) Kengo Seki, PMC Member and Committer, Apache Software Foundation (@sekikn)Ziyoiddin Yusupov, Senior Software Engineer, UBS (@mr-yusupov) Try the Azure Databricks to Microsoft Purview Solution Accelerator today! "},{"title":"Exploring Lineage History via the Marquez API","type":0,"sectionRef":"#","url":"/blog/explore-lineage-api","content":"","keywords":""},{"title":"Getting Started​","type":1,"pageTitle":"Exploring Lineage History via the Marquez API","url":"/blog/explore-lineage-api#getting-started","content":"To get started, we need a running instance of Marquez with a little bit of seed data. For these exercises, we'll assume you have a terminal with the following programs installed dockergitcurljqless (optional) Download and install any dependencies you don't already have. You'll need the docker daemon running (see the docs for your platform to get that started). Then check out the Marquez repository from Github and run the docker image locally: git clone https://github.com/MarquezProject/marquez cd marquez ./docker/up.sh --seed  This script uses docker-compose to spin up a self-contained installation of Marquez, including a local database container, web frontend, and service instance. Additionally, it populates a set of sample data that's useful for exploring the API. You'll know when the seed job is done when you see the following line in the output logs seed-marquez-with-metadata exited with code 0  Once the seed job is done, we can begin exploring the API. "},{"title":"The Jobs​","type":1,"pageTitle":"Exploring Lineage History via the Marquez API","url":"/blog/explore-lineage-api#the-jobs","content":"In a separate terminal window, type the following command curl &quot;http://localhost:5000/api/v1/namespaces/food_delivery/jobs/&quot; | jq | less  The output returned should look something like the following { &quot;jobs&quot;: [ { &quot;id&quot;: { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;example.delivery_times_7_days&quot; }, &quot;type&quot;: &quot;BATCH&quot;, &quot;name&quot;: &quot;example.delivery_times_7_days&quot;, &quot;createdAt&quot;: &quot;2021-06-24T21:50:39.229759Z&quot;, &quot;updatedAt&quot;: &quot;2021-06-24T22:05:45.321952Z&quot;, &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;inputs&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot; } ], &quot;outputs&quot;: [], &quot;location&quot;: &quot;https://github.com/example/jobs/blob/2294bc15eb49071f38425dc927e48655530a2f2e/delivery_times_7_days.py&quot;, &quot;context&quot;: { &quot;sql&quot;: &quot;INSERT INTO top_delivery_times (order_id, order_placed_on, order_dispatched_on, order_delivered_on, order_delivery_time,\\n customer_email, restaurant_id, driver_id)\\n SELECT order_id, order_placed_on, order_delivered_on, DATEDIFF(minute, order_placed_on, order_delivered_on) AS order_delivery_time,\\n customer_email, restaurant_id, driver_id\\n FROM delivery_7_days\\nGROUP BY restaurant_id\\nORDER BY order_delivery_time DESC\\n LIMIT 1;&quot; }, &quot;description&quot;: &quot;Determine weekly top delivery times by restaurant.&quot;, &quot;latestRun&quot;: { &quot;id&quot;: &quot;f4fada30-dfcc-400c-9391-2d7a506b9139&quot;, &quot;createdAt&quot;: &quot;2021-06-24T21:50:59.509739Z&quot;, &quot;updatedAt&quot;: &quot;2021-06-24T22:05:45.321952Z&quot;, &quot;nominalStartTime&quot;: &quot;2021-06-24T22:02:00Z&quot;, &quot;nominalEndTime&quot;: &quot;2021-06-24T22:05:00Z&quot;, &quot;state&quot;: &quot;FAILED&quot;, &quot;startedAt&quot;: &quot;2021-06-24T22:02:39.321952Z&quot;, &quot;endedAt&quot;: &quot;2021-06-24T22:05:45.321952Z&quot;, &quot;durationMs&quot;: 186000, &quot;args&quot;: {}, &quot;jobVersion&quot;: { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;example.delivery_times_7_days&quot;, &quot;version&quot;: &quot;e9eafa5b-e334-358d-a3b4-61c8d3de75f3&quot; }, &quot;inputVersions&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot;, &quot;version&quot;: &quot;a40ec54f-b8e1-35f7-b868-58b27383b5ff&quot; } ], &quot;outputVersions&quot;: [], &quot;context&quot;: { &quot;sql&quot;: &quot;INSERT INTO top_delivery_times (order_id, order_placed_on, order_dispatched_on, order_delivered_on, order_delivery_time,\\n customer_email, restaurant_id, driver_id)\\n SELECT order_id, order_placed_on, order_delivered_on, DATEDIFF(minute, order_placed_on, order_delivered_on) AS order_delivery_time,\\n customer_email, restaurant_id, driver_id\\n FROM delivery_7_days\\nGROUP BY restaurant_id\\nORDER BY order_delivery_time DESC\\n LIMIT 1;&quot; }, &quot;facets&quot;: {} }, &quot;facets&quot;: {} }, ... ] }  For brevity, I only included a single job- in this case, a job called example.delivery_times_7_daysin the food_delivery namespace (which we specified in the curl command). Your output will include many more jobs. There are a few things in the job output worth noting. The first is the id of the job:  &quot;id&quot;: { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;example.delivery_times_7_days&quot; },  There is no version information in the id, as this API refers to the unversioned job information. The job itself is mutable, in the sense that each time you query the API, the content of the job may change as new versions are created. The response includes the set of input and output datasets, as well as the current job source location:  &quot;inputs&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot; } ], &quot;outputs&quot;: [], &quot;location&quot;: &quot;https://github.com/example/jobs/blob/2294bc15eb49071f38425dc927e48655530a2f2e/delivery_times_7_days.py&quot;,  If a new version of the job is created, any or all of these fields can change. "},{"title":"The Job Run​","type":1,"pageTitle":"Exploring Lineage History via the Marquez API","url":"/blog/explore-lineage-api#the-job-run","content":"The next thing to notice is the latestRun field. This includes information about the latest Run of this job:  &quot;latestRun&quot;: { &quot;id&quot;: &quot;f4fada30-dfcc-400c-9391-2d7a506b9139&quot;, &quot;createdAt&quot;: &quot;2021-06-24T21:50:59.509739Z&quot;, &quot;updatedAt&quot;: &quot;2021-06-24T22:05:45.321952Z&quot;, &quot;nominalStartTime&quot;: &quot;2021-06-24T22:02:00Z&quot;, &quot;nominalEndTime&quot;: &quot;2021-06-24T22:05:00Z&quot;, &quot;state&quot;: &quot;FAILED&quot;, &quot;startedAt&quot;: &quot;2021-06-24T22:02:39.321952Z&quot;, &quot;endedAt&quot;: &quot;2021-06-24T22:05:45.321952Z&quot;, &quot;durationMs&quot;: 186000, &quot;args&quot;: {}, &quot;jobVersion&quot;: { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;example.delivery_times_7_days&quot;, &quot;version&quot;: &quot;e9eafa5b-e334-358d-a3b4-61c8d3de75f3&quot; }, &quot;inputVersions&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot;, &quot;version&quot;: &quot;a40ec54f-b8e1-35f7-b868-58b27383b5ff&quot; } ], &quot;outputVersions&quot;: [], &quot;context&quot;: { &quot;sql&quot;: &quot;INSERT INTO top_delivery_times (order_id, order_placed_on, order_dispatched_on, order_delivered_on, order_delivery_time,\\n customer_email, restaurant_id, driver_id)\\n SELECT order_id, order_placed_on, order_delivered_on, DATEDIFF(minute, order_placed_on, order_delivered_on) AS order_delivery_time,\\n customer_email, restaurant_id, driver_id\\n FROM delivery_7_days\\nGROUP BY restaurant_id\\nORDER BY order_delivery_time DESC\\n LIMIT 1;&quot; }, &quot;facets&quot;: {} },  Here, we see explicit version information in the jobVersion, the inputVersions, and theoutputVersions fields. This is included because every Run is tied to exactly one immutable version of a job and one immutable version of each input dataset and each output dataset (it's worth noting that a Run can be tied to one version of a dataset as its input and another version of the same dataset as its output- a SQL MERGE statement is one common use case supported by this). The other important field to notice in the Run structure is the state  &quot;state&quot;: &quot;FAILED&quot;,  Uh-oh. Looks like the last time this job ran, it failed. "},{"title":"Tracing Failures​","type":1,"pageTitle":"Exploring Lineage History via the Marquez API","url":"/blog/explore-lineage-api#tracing-failures","content":"The first question we have when diagnosing a failure is Is this the first time it's failed? Or has it been broken a while? Let's use the API to find out. Checking previous runs is easily accomplished by hitting the job's runsAPI. Job runs are returned in descending order by start time, so the latest runs should be at the top. Since we only want to check whether (and which) previous runs failed, we can use the following command: curl &quot;http://localhost:5000/api/v1/namespaces/food_delivery/jobs/example.delivery_times_7_days/runs&quot; | \\ jq '.runs | map({&quot;id&quot;: .id, &quot;state&quot;: .state})' | less  I get the following output: [ { &quot;id&quot;: &quot;cb436906-1c66-4ce4-b7ac-ceebfd1babf8&quot;, &quot;state&quot;: &quot;FAILED&quot; }, { &quot;id&quot;: &quot;34bd4d60-82a6-4cac-ad76-815e6d95a93c&quot;, &quot;state&quot;: &quot;COMPLETED&quot; }, { &quot;id&quot;: &quot;352c67c3-c8d7-4b3a-b7da-8532aa9b8335&quot;, &quot;state&quot;: &quot;COMPLETED&quot; }, { &quot;id&quot;: &quot;0c62b1cc-2e43-44d0-9443-0a1d9768fece&quot;, &quot;state&quot;: &quot;COMPLETED&quot; }, { &quot;id&quot;: &quot;5900de19-12f7-4a6e-8118-8e0792d98f65&quot;, &quot;state&quot;: &quot;COMPLETED&quot; }, ... ]  This is an incomplete list of jobs, but it's obvious from this sampling that this is the first job failure in the recent execution history. What we want to see now is what changed between the last successful run and this one. We'll need to grab the id fields of each of the runs we want to compare. The run ids in the seed data are randomly generated, so they'll be different if you're following along. Grab the run ids with the following shell commands: FAILED_RUN_ID=$(curl &quot;http://localhost:5000/api/v1/namespaces/food_delivery/jobs/example.delivery_times_7_days/runs&quot; | jq -r '.runs[0].id') SUCCESSFUL_RUN_ID=$(curl &quot;http://localhost:5000/api/v1/namespaces/food_delivery/jobs/example.delivery_times_7_days/runs&quot; | jq -r '.runs[1].id')  To get a specific run, we call the /jobs/runs API. Since each Run ID is required to be unique, the API doesn't require a namespace or a job name. We can get the failed job run with curl &quot;http://localhost:5000/api/v1/jobs/runs/$FAILED_RUN_ID&quot; | jq | less  The output is the same as the latestRun field of the JobVersions API. Recall the output of that API includes these three important fields: the jobVersion, the inputVersionsand the outputVersions.  &quot;jobVersion&quot;: { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;example.delivery_times_7_days&quot;, &quot;version&quot;: &quot;e9eafa5b-e334-358d-a3b4-61c8d3de75f3&quot; }, &quot;inputVersions&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot;, &quot;version&quot;: &quot;a40ec54f-b8e1-35f7-b868-58b27383b5ff&quot; } ], &quot;outputVersions&quot;: [],  These fields give us what we need to trace the lineage of the specific job runs we want to compare. "},{"title":"Job Versions​","type":1,"pageTitle":"Exploring Lineage History via the Marquez API","url":"/blog/explore-lineage-api#job-versions","content":"The first thing to look at is the jobVersion. Nearly 100% of the time, a job failure can be traced to a code change. Let's compare the job version of the failed run with the job version of the successful one: diff &lt;(curl -s &quot;http://localhost:5000/api/v1/jobs/runs/$FAILED_RUN_ID&quot; | jq -r '.jobVersion.version') \\ &lt;(curl -s &quot;http://localhost:5000/api/v1/jobs/runs/$SUCCESSFUL_RUN_ID&quot; | jq -r '.jobVersion.version') 1c1 &lt; e9eafa5b-e334-358d-a3b4-61c8d3de75f3 --- &gt; 92d801c0-021e-3c3d-ba18-c9e8504b143d  Right away, we see there is a difference. A number of factors contribute to the job versioning logic in Marquez: The source code locationThe job contextThe list of input datasetsThe list of output datasets The version generation code is a deterministic function of these four inputs, so if any of them change, the version will change. Let's find out what changed between the two job versions. To do the diff, we ought to get rid of anything we expect to differ ahead of time: the version, the createdAtand updatedAt timestamps, and the latestRun. The version field is also nested within the job version's id field, so we'll omit that too. FAILED_JOB_VERSION=$(curl -s &quot;http://localhost:5000/api/v1/jobs/runs/$FAILED_RUN_ID&quot; | jq -r '.jobVersion.version') SUCCESSFUL_JOB_VERSION=$(curl -s &quot;http://localhost:5000/api/v1/jobs/runs/$SUCCESSFUL_RUN_ID&quot; | jq -r '.jobVersion.version') diff &lt;(curl -s &quot;http://localhost:5000/api/v1/namespaces/food_delivery/jobs/example.delivery_times_7_days/versions/$FAILED_JOB_VERSION&quot; | \\ jq 'del(.[&quot;id&quot;, &quot;version&quot;, &quot;createdAt&quot;, &quot;updatedAt&quot;, &quot;latestRun&quot;])') \\ &lt;(curl -s &quot;http://localhost:5000/api/v1/namespaces/food_delivery/jobs/example.delivery_times_7_days/versions/$SUCCESSFUL_JOB_VERSION&quot; | \\ jq 'del(.[&quot;id&quot;, &quot;version&quot;, &quot;createdAt&quot;, &quot;updatedAt&quot;, &quot;latestRun&quot;])') 14c14,23 &lt; &quot;outputs&quot;: [] --- &gt; &quot;outputs&quot;: [ &gt; { &gt; &quot;namespace&quot;: &quot;food_delivery&quot;, &gt; &quot;name&quot;: &quot;public.top_delivery_times&quot; &gt; }, &gt; { &gt; &quot;namespace&quot;: &quot;food_delivery&quot;, &gt; &quot;name&quot;: &quot;public.discounts&quot; &gt; } &gt; ]  Oh, interesting! The two job versions only differ because of the output datasets. This is an interesting point that should be addressed in the Marquez API- the version generation is constructed when the run completes, even if the job run failed. Sometimes this has no impact on the versioning, as the output datasets can be determined before the job run executes. But sometimes we see impacts like this where a job run failed before we had a chance to discover the output datasets. "},{"title":"Tracing Upstream Lineage​","type":1,"pageTitle":"Exploring Lineage History via the Marquez API","url":"/blog/explore-lineage-api#tracing-upstream-lineage","content":"So what gives? The job code didn't actually change! So what caused the failure? Here's where the lineage tracking becomes useful. Recall again, the run output gave us 3 interesting fields: the jobVersion, the inputVersions, and the outputVersions. We already know that the outputVersions is empty because the latest failed run didn't have a chance to determine the outputs. But we can take a look at the input datasets. "},{"title":"Dataset Versions​","type":1,"pageTitle":"Exploring Lineage History via the Marquez API","url":"/blog/explore-lineage-api#dataset-versions","content":"diff &lt;(curl -s &quot;http://localhost:5000/api/v1/jobs/runs/$FAILED_RUN_ID&quot; | jq -r '.inputVersions') \\ &lt;(curl -s &quot;http://localhost:5000/api/v1/jobs/runs/$SUCCESSFUL_RUN_ID&quot; | jq -r '.inputVersions') 5c5 &lt; &quot;version&quot;: &quot;a40ec54f-b8e1-35f7-b868-58b27383b5ff&quot; --- &gt; &quot;version&quot;: &quot;5e439f1f-1a44-3700-961f-60c79c75a1ec&quot;  Dataset versions work differently from job versions. They don't only change when the structure changes. Every time a job run modifies or writes to a dataset, the dataset version changes. Unless a job schedule is more frequent than its upstream job's schedule (e.g., an hourly job consuming a daily generated dataset), it is expected that each job run consumes a different version of a dataset. To find out if there is a significant difference, we have to compare the two versions with the dataset's versions API. We know there's only a single input dataset, so we'll keep this simple, but you could also write a loop to check multiple input datasets if needed. In this post, we omit the structure of the datasetVersion, but you can explore it yourself with the following: FAILED_DATASET_VERSION=$(curl -s &quot;http://localhost:5000/api/v1/jobs/runs/$FAILED_RUN_ID&quot; | jq -r '.inputVersions[0].version') curl -s &quot;http://localhost:5000/api/v1/namespaces/food_delivery/datasets/public.delivery_7_days/versions/$FAILED_DATASET_VERSION&quot; | jq | less  As with the job versions, we'll omit some of the data we expect to be different in order to produce a useful diff: FAILED_DATASET_VERSION=$(curl -s &quot;http://localhost:5000/api/v1/jobs/runs/$FAILED_RUN_ID&quot; | jq -r '.inputVersions[0].version') SUCCESSFUL_DATASET_VERSION=$(curl -s &quot;http://localhost:5000/api/v1/jobs/runs/$SUCCESSFUL_RUN_ID&quot; | jq -r '.inputVersions[0].version') diff &lt;(curl -s &quot;http://localhost:5000/api/v1/namespaces/food_delivery/datasets/public.delivery_7_days/versions/$FAILED_DATASET_VERSION&quot; | \\ jq 'del(.[&quot;id&quot;, &quot;version&quot;, &quot;createdAt&quot;, &quot;createdByRun&quot;])') \\ &lt;(curl -s &quot;http://localhost:5000/api/v1/namespaces/food_delivery/datasets/public.delivery_7_days/versions/$SUCCESSFUL_DATASET_VERSION&quot; | \\ jq 'del(.[&quot;id&quot;, &quot;version&quot;, &quot;createdAt&quot;, &quot;createdByRun&quot;])') 58c58 &lt; &quot;type&quot;: &quot;VARCHAR&quot;, --- &gt; &quot;type&quot;: &quot;INTEGER&quot;,  Hey! Somehow one of the fields was converted from a an INT to a VARCHAR! One of the helpful fields in the version API is the createdByRun, which is similar to the jobVersion's latestRun. It provides the job run that last altered the dataset, creating the new version. We can quickly compare the job versions of the runs that created these two dataset versions: diff &lt;(curl -s &quot;http://localhost:5000/api/v1/namespaces/food_delivery/datasets/public.delivery_7_days/versions/$FAILED_DATASET_VERSION&quot; | \\ jq '.createdByRun.jobVersion') \\ &lt;(curl -s &quot;http://localhost:5000/api/v1/namespaces/food_delivery/datasets/public.delivery_7_days/versions/$SUCCESSFUL_DATASET_VERSION&quot; | \\ jq '.createdByRun.jobVersion') 4c4 &lt; &quot;version&quot;: &quot;c222a72e-92cc-3bb6-b3b7-c174cbc76387&quot; --- &gt; &quot;version&quot;: &quot;76c375bf-58ac-3d19-b94f-424fe2784601&quot;  And we can do a quick comparison of the two job versions. Since the job name is different, we'll let jq generate the endpoints for us diff &lt;(curl -s $(curl -s &quot;http://localhost:5000/api/v1/namespaces/food_delivery/datasets/public.delivery_7_days/versions/$FAILED_DATASET_VERSION&quot; | \\ jq -r '.createdByRun.jobVersion | &quot;http://localhost:5000/api/v1/namespaces/&quot; + .namespace + &quot;/jobs/&quot; + .name + &quot;/versions/&quot; + .version') | \\ jq 'del(.[&quot;id&quot;, &quot;version&quot;, &quot;createdAt&quot;, &quot;updatedAt&quot;, &quot;latestRun&quot;])') \\ &lt;(curl -s $(curl -s &quot;http://localhost:5000/api/v1/namespaces/food_delivery/datasets/public.delivery_7_days/versions/$SUCCESSFUL_DATASET_VERSION&quot; | \\ jq -r '.createdByRun.jobVersion | &quot;http://localhost:5000/api/v1/namespaces/&quot; + .namespace + &quot;/jobs/&quot; + .name + &quot;/versions/&quot; + .version') | \\ jq 'del(.[&quot;id&quot;, &quot;version&quot;, &quot;createdAt&quot;, &quot;updatedAt&quot;, &quot;latestRun&quot;])') 4c4 &lt; &quot;location&quot;: &quot;https://github.com/example/jobs/blob/c87f2a40553cfa4ae7178083a068bf1d0c6ca3a8/etl_delivery_7_days.py&quot;, --- &gt; &quot;location&quot;: &quot;https://github.com/example/jobs/blob/4d0b5d374261fdaf60a1fc588dd8f0d124b0e87f/etl_delivery_7_days.py&quot;,  And there it is. Because nearly 100% of the time, a job failure can be traced to a code change. In this example, the job immediately upstream decided to change the output schema of its dataset. In reality, it's not always so straightforward. Sometimes the upstream job is just a passthrough- maybe it applies some filters to a subset of the columns and writes out whatever schema it's given. In that case, the job immediately upstream would have succeeded without a change in the job version. Or the code change in the upstream job could be innocuous. Maybe someone added a comment or fixed an unrelated bug. We might do some follow up and discover we have to continue our search upstream. But the Marquez API actually gives us that ability. Using the /lineage API, we can even explore the downsteam impact of changes. So if you owned the etl_delivery_7_days job and wanted to see what the impact of changing the varchar to an int was on running jobs, the following jq recursive script will let you walk the downstream jobs and show the state of the last run: # For readability, the jq filter is in a file broken into multiple lines cat recurse.jq .graph as $graph | .graph[] | select(.id == &quot;job:food_delivery:example.etl_delivery_7_days&quot;) | recurse(.outEdges[] | .destination as $nodeId | $graph[] | select(.id == $nodeId)) | select(.type == &quot;JOB&quot;) | {&quot;id&quot;: .id, &quot;state&quot;: .data.latestRun.state} curl -s &quot;http://localhost:5000/api/v1-beta/lineage?nodeId=job:food_delivery:example.etl_delivery_7_days&quot; | jq -f recurse.jq less { &quot;id&quot;: &quot;job:food_delivery:example.etl_delivery_7_days&quot;, &quot;state&quot;: &quot;COMPLETED&quot; } { &quot;id&quot;: &quot;job:food_delivery:example.delivery_times_7_days&quot;, &quot;state&quot;: &quot;FAILED&quot; }  In this post, we did everything manually with bash (because the shell is your most powerful tool when debugging a live outage you've never encountered before; and let's be honest- how many outages aren'tsomething you've never encountered before), but this could easily have been done in Java or Go or Python. The openapi specin the Marquez repo can be used to generate a client in whatever language you want to write your ops tool in. So build some tooling and help your next debugging session run a little more smoothly. But wait! What about the times when the job isn't failing, but the data is wrong! Ah, the data quality checks! This is where the extensibility of the OpenLineage model comes to our rescue with a field in the responses that we completely glossed over  &quot;facets&quot;: {}  But I think that's a topic for another post. "},{"title":"Join us in New York on June 22nd","type":0,"sectionRef":"#","url":"/blog/nyc-collibra-meetup","content":"","keywords":""},{"title":"Time, Place & Format​","type":1,"pageTitle":"Join us in New York on June 22nd","url":"/blog/nyc-collibra-meetup#time-place--format","content":"Date: June 22nd, 2023 Format: In-person Time: 6:00-8:00 pm ET Address: 61 Broadway, New York, NY 10006 Getting In​ Check in will be in the lobby with ID. Then, take the elevator to the 31st floor. "},{"title":"Hope to see you there!​","type":1,"pageTitle":"Join us in New York on June 22nd","url":"/blog/nyc-collibra-meetup#hope-to-see-you-there","content":""},{"title":"Data Lineage with Snowflake","type":0,"sectionRef":"#","url":"/blog/openlineage-snowflake","content":"","keywords":""},{"title":"Introduction​","type":1,"pageTitle":"Data Lineage with Snowflake","url":"/blog/openlineage-snowflake#introduction","content":"We are excited to reveal a new way to gather lineage metadata directly from Snowflake: the OpenLineage Adapter. This integration offers Snowflake’s enterprise users a powerful tool for analyzing and diagnosing issues with their data pipelines. This new integration will add new diagnostic capability to one of the world’s largest data platforms. Snowflake’s Data Cloud currently empowers more than 5,900 companies, including 241 of the Fortune 500 as of January 2022, to unite siloed data, securely share data, and execute diverse analytic workloads across their organizations. Legacy platforms struggled to provide a single, secure, and universally accessible platform for organizations to warehouse and analyze their data, but Snowflake’s Data Cloud provides a global ecosystem where customers, providers, and partners can finally break down data silos and derive value from rapidly growing data sets in secure, compliant, and governed ways. "},{"title":"Background​","type":1,"pageTitle":"Data Lineage with Snowflake","url":"/blog/openlineage-snowflake#background","content":"An open source LF AI &amp; Data Foundation sandbox project, OpenLineage provides an open standard for metadata and lineage collection that instruments jobs as they are running. OpenLineage not only automates the process of generating lineage and metadata about datasets, jobs, and runs in a data flow, but also does this in real time behind the scenes. With OpenLineage’s open standard and extensible backend, users can easily identify the root causes of slow or failing jobs and issues with data quality in their ecosystems without parsing queries. The magic of OpenLineage is its standard API for capturing lineage events. Any number of tools – from schedulers to SQL engines – can send metadata from this endpoint to a compatible tool such as Marquez for visualization and further analysis of a pipeline. Historically, the process of producing lineage and collecting metadata has been laborious and error-prone. Extracting data from query logs via parsing, for example, required one to reimplement database parsing logic, which added complexity and introduced opportunities for user error. In addition, the lineage collected was incomplete. One could learn about the view that was queried but not about the underlying tables in the pipeline, much less about the upstream and downstream dependencies of the datasets. OpenLineage, by contrast, exploits what the database already knows and does to maintain an up-to-date, end-to-end graph of a pipeline – and makes the graph available via an API. OpenLineage and Snowflake play nicely because the latter is unusual among cloud data platforms for offering lineage information out of the box in a view (ACCESS_HISTORY). The integration of OpenLineage builds on this foundation to offer automated generation of lineage and metadata. The value proposition of Snowflake + OpenLineage lies in the combination of an open standard tool, which supports multiple data systems to provide lineage in a single format, to Snowflake’s existing production of lineage information on an enterprise scale. The integration gives customers the ability to consume enterprise-wide table lineage and process lineage together in a consolidated OpenLineage format. "},{"title":"Approach​","type":1,"pageTitle":"Data Lineage with Snowflake","url":"/blog/openlineage-snowflake#approach","content":"The process of integrating OpenLineage benefited from an existing query logging tool already available to Snowflake enterprise customers: the ACCESS_HISTORY view. As its name suggests, this feature, designed initially for governance use cases, offers users a detailed view of read operations conducted on Snowflake objects (e.g., tables, views, and columns) on an on-demand basis in response to SQL queries. (Write operations are viewable as a preview feature.) As developed primarily by former Snowflake intern Aly Hirani with support from Datakin Senior Engineer Minkyu Park, the OpenLineage integration makes Access History the basis of automated production of lineage and metadata. But rather than produce a view for querying, OpenLineage produces a holistic lineage graph. To create the graph, the integration takes the data used to populate the Access History view and sends it to the OpenLineage backend as a standard OpenLineage event. Events in OpenLineage are JSON objects that employ a consistent naming strategy for database entities and enrich those entities with facets: { &quot;eventType&quot;: &quot;COMPLETE&quot;, &quot;eventTime&quot;: &quot;2020-12-28T20:52:00.001+10:00&quot;, &quot;run&quot;: { &quot;runId&quot;: &quot;d46e465b-d358-4d32-83d4-df660ff614dd&quot; }, &quot;job&quot;: { &quot;namespace&quot;: &quot;my-namespace&quot;, &quot;name&quot;: &quot;my-job&quot; }, &quot;outputs&quot;: [{ &quot;namespace&quot;: &quot;my-namespace&quot;, &quot;name&quot;: &quot;my-output&quot;, &quot;facets&quot;: { &quot;schema&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client&quot;, &quot;_schemaURL&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/spec/OpenLineage.json#/definitions/SchemaDatasetFacet&quot;, &quot;fields&quot;: [ { &quot;name&quot;: &quot;a&quot;, &quot;type&quot;: &quot;VARCHAR&quot;}, { &quot;name&quot;: &quot;b&quot;, &quot;type&quot;: &quot;VARCHAR&quot;} ] } } }], &quot;producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client&quot; }  "},{"title":"A DAG-based Solution​","type":1,"pageTitle":"Data Lineage with Snowflake","url":"/blog/openlineage-snowflake#a-dag-based-solution","content":"Automating lineage production from the Access History view required a two-DAG solution. Minkyu had initially planned to use one DAG to scan the view and produce the lineage graph, but the timing of the logs used for the view precluded the production of lineage data with a single DAG. The solution Minkyu found was a separate DAG with a schedule for scanning the Access History view on a regular interval. def send_ol_events(): client = OpenLineageClient.from_environment() with connect(user=SNOWFLAKE_USER, password=SNOWFLAKE_PASSWORD, account=SNOWFLAKE_ACCOUNT, database='OPENLINEAGE', schema='PUBLIC') as conn: with conn.cursor() as cursor: ol_view = 'OPENLINEAGE_ACCESS_HISTORY' ol_event_time_tag = 'OL_LATEST_EVENT_TIME' var_query = f''' set current_organization='{SNOWFLAKE_ACCOUNT}'; ''' cursor.execute(var_query) ol_query = f''' SELECT * FROM {ol_view} WHERE EVENT:eventTime &gt; system$get_tag('{ol_event_time_tag}', '{ol_view}', 'table') ORDER BY EVENT:eventTime ASC; ''' cursor.execute(ol_query) ol_events = [json.loads(ol_event[0]) for ol_event in cursor.fetchall()] for ol_event in ol_events: client.emit(ol_event) if len(ol_events) &gt; 0: latest_event_time = ol_events[-1]['eventTime'] cursor.execute(f''' ALTER VIEW {ol_view} SET TAG {ol_event_time_tag} = '{latest_event_time}'; ''') default_args = { 'owner': 'openlineage', 'depends_on_past': False, 'start_date': days_ago(1), 'email_on_failure': False, 'email_on_retry': False, 'email': ['demo@openlineage.io'], 'snowflake_conn_id': 'openlineage_snowflake' } with DAG('etl_openlineage', schedule_interval='@hourly', catchup=False, default_args=default_args, description='Send OL events every minutes', tags=[&quot;extract&quot;]) as dag: t1 = PythonOperator(task_id='ol_event', python_callable=send_ol_events)  "},{"title":"Getting Started with an Example​","type":1,"pageTitle":"Data Lineage with Snowflake","url":"/blog/openlineage-snowflake#getting-started-with-an-example","content":"This example uses Airflow to run a collection of Snowflake queries for a fictional food delivery service. Lineage data for these queries will be recorded within Snowflake ACCESS_HISTORY and, using the OpenLineage Access History View, emitted to an OpenLineage backend. This is done using a series of DAGs in dags/etl that each use SnowflakeOperator to run queries, along with a DAG in dags/lineage that uses PythonOperator to send generated OpenLineage events to the configured backend. "},{"title":"Prerequisites​","type":1,"pageTitle":"Data Lineage with Snowflake","url":"/blog/openlineage-snowflake#prerequisites","content":"Installing Marquez​ First, checkout the Marquez repository: % git clone https://github.com/MarquezProject/marquez.git % cd marquez  Then, run Marquez in detached mode: % docker/up.sh -d %  Preparing Snowflake​ First, check out the OpenLineage Access History View repository: % git clone https://github.com/Snowflake-Labs/OpenLineage-AccessHistory-Setup.git % cd OpenLineage-AccessHistory-Setup  The OPENLINEAGE database and FOOD_DELIVERY schema in Snowflake need to be created. This can be done using the SnowSQL command-line tool, or by pasting the queries into a new Snowflake Worksheet. This example uses SnowSQL. % snowsql -u &lt;snowflake-user&gt; -a &lt;snowflake-account&gt; SnowSQL&gt; CREATE DATABASE OPENLINEAGE; SnowSQL&gt; CREATE SCHEMA OPENLINEAGE.FOOD_DELIVERY;  The view defined in open_lineage_access_history.sql also needs to be created. This view represents the entries in ACCESS_HISTORY as specially-constructed JSON objects containing RunEvents that can be emitted to an OpenLineage backend. To create it, use SnowSQL to set the current_organization session variable and execute the SQL file. SnowSQL&gt; SET current_organization='&lt;snowflake-organization&gt;'; SnowSQL&gt; USE SCHEMA OPENLINEAGE.PUBLIC; SnowSQL&gt; !source open_lineage_access_history.sql  Finally, our lineage extraction DAG relies upon a tag on the view to keep track of which lineage events have been processed. This tag needs to be initialized: SnowSQL&gt; CREATE TAG OL_LATEST_EVENT_TIME; SnowSQL&gt; ALTER VIEW OPENLINEAGE.PUBLIC.OPENLINEAGE_ACCESS_HISTORY SET TAG OL_LATEST_EVENT_TIME = '1970-01-01T00:00:00.000'; SnowSQL&gt; !quit %  "},{"title":"Preparing the Environment​","type":1,"pageTitle":"Data Lineage with Snowflake","url":"/blog/openlineage-snowflake#preparing-the-environment","content":"The following environment variables need to be set in order for the query DAGs to connect to Snowflake, and so that the extraction DAG can send lineage events to your OpenLineage backend: SNOWFLAKE_USERSNOWFLAKE_PASSWORDSNOWFLAKE_ACCOUNTOPENLINEAGE_URLAIRFLOW_CONN_OPENLINEAGE_SNOWFLAKE To do this, copy the .env-example file to .env, and edit it to provide the appropriate values for your environment. The variables in this file will be set for each service in the Airflow deployment. % cd examples/airflow % cp .env-example .env % vi .env  "},{"title":"Preparing Airflow​","type":1,"pageTitle":"Data Lineage with Snowflake","url":"/blog/openlineage-snowflake#preparing-airflow","content":"Once the environment is prepared, initialize Airflow with docker-compose: % docker-compose up airflow-init  This will take several minutes. When it has finished, bring up the Airflow services: % docker-compose up  This will also take several minutes. Eventually, the webserver will be up at http://localhost:8080. Log in using the default credentials (airflow/airflow) and navigate to the DAGs page. When you see 12 DAGs in the list, you can be confident that Airflow has completed its initialization of the example. "},{"title":"Running the Example​","type":1,"pageTitle":"Data Lineage with Snowflake","url":"/blog/openlineage-snowflake#running-the-example","content":"Each of the DAGs is paused by default. Enable each one, skipping the etl_openlineage DAG for now. They may not all run successfully on the first try, since they have interdependencies that this example leaves unmanaged.  After each DAG has completed at least one successful run, enable etl_openlineage. Wait for it to complete its run. "},{"title":"Result​","type":1,"pageTitle":"Data Lineage with Snowflake","url":"/blog/openlineage-snowflake#result","content":"Navigate to your Marquez deployment and view the resulting lineage graph:  "},{"title":"Potential Improvements​","type":1,"pageTitle":"Data Lineage with Snowflake","url":"/blog/openlineage-snowflake#potential-improvements","content":"This new integration paves the way for an exciting set of potential future capabilities. These include support for Object_Dependencies and the addition of Granular Lineage (column-level lineage). We are interested in feedback from users, which will help the team at Snowflake and the members of the OpenLineage community prioritize future work. "},{"title":"Conclusion​","type":1,"pageTitle":"Data Lineage with Snowflake","url":"/blog/openlineage-snowflake#conclusion","content":"Snowflake’s integration of the OpenLineage standard promises to dramatically improve enterprise users’ ability to diagnose issues with quality and performance in their pipelines. This project is cause for optimism about future collaboration with OpenLineage. The fit between Snowflake’s enterprise product and OpenLineage is already fairly seamless. Further collaboration would likely yield additional features and, by extension, more value for Snowflake’s customers. Also, the fact that OpenLineage is an open standard offers opportunities for fruitful integrations with other partners. Supporters of OpenLineage already include Spark, Airflow, and dbt, and the list is growing. For more information or to contribute to OpenLineage, reach out on twitter or Slack, and check out the repositories on Github. "},{"title":"How OpenLineage takes inspiration from OpenTelemetry","type":0,"sectionRef":"#","url":"/blog/openlineage-takes-inspiration-from-opentelemetry","content":"","keywords":""},{"title":"Data vs Services​","type":1,"pageTitle":"How OpenLineage takes inspiration from OpenTelemetry","url":"/blog/openlineage-takes-inspiration-from-opentelemetry#data-vs-services","content":"The data world and the service world have many similarities but also a few crucial differences. Let’s start by drawing the similarities: The contract for services is the API, in the data world the contract is the dataset schema. Properly tracked Data lineage is as powerful as distributed request tracing for services.Data Quality checks are the data pipelines equivalent of services’ circuit breakers.The Data catalog is data’s service discoveryData quality metrics are similar to service metrics and both can define SLOs. \tData\tServicesContract\tDataset schema\tService API Tracking Dependencies\tData lineage\tDistributed traces Preventing cascading failures\tData Quality checks\tCircuit breakers Discovery\tData catalog\tService Discovery SLOs\tFreshness, data quality\tAvailability, latency "},{"title":"Observability in the Services world​","type":1,"pageTitle":"How OpenLineage takes inspiration from OpenTelemetry","url":"/blog/openlineage-takes-inspiration-from-opentelemetry#observability-in-the-services-world","content":"In many ways, observability is a lot more mature in the services world than it is in the data world. Service telemetry data is usually described as traces, metrics and logs that allow us to observe how services behave and interact with each other. Recognizing how telemetry data is connected across service layers is key to understanding and mitigating complex distributed failures in today’s environments. OpenTelemetry is the standard that allows collection of telemetry data in a uniform, vendor-agnostic way across services and databases. For example, it enables the understanding of dependencies between microservices, facilitating investigation into how a single failure might impact services several layers removed. The creation of OpenTelemetry removed the need for every monitoring, tracing analysis and log indexing platform to create unique integrations to collect that information from the environment. "},{"title":"Observability in the Data world​","type":1,"pageTitle":"How OpenLineage takes inspiration from OpenTelemetry","url":"/blog/openlineage-takes-inspiration-from-opentelemetry#observability-in-the-data-world","content":"The data world is organized in a slightly different manner. Services strive for high availability and expose a contract to be requested from. Data pipelines consume datasets and produce datasets. They could be executed as batch processes or using streaming engines but their fundamental contract is to consume data from given inputs and produce data to given outputs. The contract is now the schema of the dataset and an expectation of a rate of update. In this world observability cares about a few things: Is the data being delivered? We might be happy with data being delivered at an hourly or daily rate but we want to know if the job responsible for this is failing and won’t be updating it at all. As a consequence all the datasets depending on this will also not be updated. Correlated SLA misses likes this must be identified to avoid many teams investigating the same problem independently.Is the data being delivered on time? Batch processing for example is relatively forgiving and can still deliver outputs according to a time SLA even when it failed and had to retry or was slower than usual because of disruptions in its environment. However critical data will need to be delivered according to pre-defined SLA. We want to be able to understand where in the data pipeline dependencies, a bottleneck caused a delay and resolve the issue.Is the data correct? Now the worst thing that can happen is not a data pipeline failing. This case is relatively easy to recover from. Once the issue is fixed and the pipeline restarted, it will typically catch up and get back to normal operation after the delay induced by the failure. The worst case scenario for a data engineer or data scientist, is the pipeline carrying through and producing bad data. This usually propagates to downstream consumers and all over the place. Recovering requires understanding that the data is incorrect (usually using a data quality library like Great Expectations or Deequ), identifying the upstream dataset where the problem originated, identifying the downstream datasets where the problem propagated, and restating all those datasets to the correct result.Auditing what happened: Another common need whether it’s for compliance or governance reasons is being able to know if specific sensitive datasets are used according to a defined set of rules. This can be used to protect user privacy, comply with financial regulation, or ensure that key metrics are derived from trusted data sources. The key common element in all those use cases is understanding dependencies through data lineage, just like services care about understanding dependencies through service traces. "},{"title":"Differences Between the Data world and the service world​","type":1,"pageTitle":"How OpenLineage takes inspiration from OpenTelemetry","url":"/blog/openlineage-takes-inspiration-from-opentelemetry#differences-between-the-data-world-and-the-service-world","content":"If the data world has exactly the same needs for observability than the service world, there are key differences between the two that create the need for a different API to instrument data pipelines. "},{"title":"Overall dependency structure:​","type":1,"pageTitle":"How OpenLineage takes inspiration from OpenTelemetry","url":"/blog/openlineage-takes-inspiration-from-opentelemetry#overall-dependency-structure","content":"Services dependencies can vary at the request level. Different requests to the same service may trigger very different downstream requests to other services. Service logic may create very different dependency patterns depending on input, timing and context. Services depend on other services that can be persistent or stateless.Data pipelines tend to be expressed in terms of a transformation from a defined set of input datasets to one or several output datasets. Their input/output structure tends to be a lot more stable and not vary much from record to record in the dataset. It’s effectively a bigraph: jobs consume datasets and datasets are produced by jobs. "},{"title":"Push vs Pull:​","type":1,"pageTitle":"How OpenLineage takes inspiration from OpenTelemetry","url":"/blog/openlineage-takes-inspiration-from-opentelemetry#push-vs-pull","content":"Services send or push requests to downstream services. Whether it’s synchronous or asynchronous, they can add a traceid to their request that will be propagated downstream. An upstream request spawns one or more downstream requests in a tree structure.Data pipelines pull data from the datasets they consume from. They aggregate and join datasets together. The structure of dependencies is typically a Directed Acyclic Graph at the dataset level instead of a tree at the request level. This means that the granularity of updates does not match one to one in a lot of cases. The frequency of updates can be different from one pipeline to the next and does not neatly align with a trace flowing down the dependencies. "},{"title":"Granularity of data updates​","type":1,"pageTitle":"How OpenLineage takes inspiration from OpenTelemetry","url":"/blog/openlineage-takes-inspiration-from-opentelemetry#granularity-of-data-updates","content":"Services treat one request at a time and tend to optimize for latency of the request being processed.Data pipelines consume entire datasets and tend to prioritize throughput over latency. The result output can be combining many records from various inputs. When a service request spawns multiple requests downstream a data pipeline tends to do the opposite at the record level while producing multiple derived datasets. "},{"title":"Parallels between OpenLineage and OpenTelemetry​","type":1,"pageTitle":"How OpenLineage takes inspiration from OpenTelemetry","url":"/blog/openlineage-takes-inspiration-from-opentelemetry#parallels-between-openlineage-and-opentelemetry","content":""},{"title":"An API first​","type":1,"pageTitle":"How OpenLineage takes inspiration from OpenTelemetry","url":"/blog/openlineage-takes-inspiration-from-opentelemetry#an-api-first","content":"Like OpenTelemetry is an API to collect traces, logs and metrics, OpenLineage is first an API to collect lineage. It is agnostic to the backend collecting the data and aspires to be integrated in every data processing engine. Data lineage is the equivalent of traces for services. It keeps track of dependencies between datasets and data pipelines and how they change over time. "},{"title":"Broad language support​","type":1,"pageTitle":"How OpenLineage takes inspiration from OpenTelemetry","url":"/blog/openlineage-takes-inspiration-from-opentelemetry#broad-language-support","content":"Like OpenTelemetry, OpenLineage has broad language support through the definition of its API in the standard JSONSchema representation. It also has dedicated clients to simplify using its semantics in the languages most commonly used for data processing (Java and Python). "},{"title":"Backend agnostic​","type":1,"pageTitle":"How OpenLineage takes inspiration from OpenTelemetry","url":"/blog/openlineage-takes-inspiration-from-opentelemetry#backend-agnostic","content":"Like OpenTelemetry, OpenLineage allows the implementation of multiple backends to consume lineage events for a variety of use cases. For example Marquez is an Open Source reference implementation that keeps track of all the changes in your environment and will help you understand what happened if something goes wrong. Other metadata projects like Egeria, DataHub, Atlas or Amundsen can also benefit from OpenLineage. Egeria in particular is committed to support OpenLineage as a Metadata bus. Like OpenTelemetry, anybody can consume and leverage OpenLineage events. "},{"title":"Integrates with popular frameworks and libraries​","type":1,"pageTitle":"How OpenLineage takes inspiration from OpenTelemetry","url":"/blog/openlineage-takes-inspiration-from-opentelemetry#integrates-with-popular-frameworks-and-libraries","content":"Like OpenTelemetry, OpenLineage aspires to be integrated in every data processing tool in the ecosystem. It also provides integration with popular tools that are not integrated yet. For example today you can cover Apache Spark, BigQuery, Snowflake, Redshift, Apache Airflow and others. "},{"title":"OpenLineage specific capabilities​","type":1,"pageTitle":"How OpenLineage takes inspiration from OpenTelemetry","url":"/blog/openlineage-takes-inspiration-from-opentelemetry#openlineage-specific-capabilities","content":"In addition to those, OpenLineage is extensible to cover various aspects of metadata that are specific to the data world. OpenLineage defines a notion of facet that lets attach well defined pieces of metadata to the OpenLineage entities (Jobs, Runs and Datasets). Facets can be either part of the core spec or be defined as custom facets by third parties. This flexible mechanism lets define independent specs for dataset schema, query profiles or data quality metrics for example. But this is a topic for another post. "},{"title":"How Operators and Extractors Work Under-the-Hook","type":0,"sectionRef":"#","url":"/blog/operators-and-extractors-technical-deep-dive","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"How Operators and Extractors Work Under-the-Hook","url":"/blog/operators-and-extractors-technical-deep-dive#overview","content":"Airflow Operators and OpenLineage Extractors have a specific, if quirky, way of working together. Recently, the way they work together has seen a bit of an overhaul, and the new SQL Check Extractors added a new and unique way that the extractors work and interact with operators. In this blog, we'll demystify these relationships. Note: This blog post describes the relationships of the operators and extractors only for Airflow&gt;=2.3 and OpenLineage&gt;=0.12.0. "},{"title":"Integration​","type":1,"pageTitle":"How Operators and Extractors Work Under-the-Hook","url":"/blog/operators-and-extractors-technical-deep-dive#integration","content":""},{"title":"The Operator and the Extractor​","type":1,"pageTitle":"How Operators and Extractors Work Under-the-Hook","url":"/blog/operators-and-extractors-technical-deep-dive#the-operator-and-the-extractor","content":"Some quick definitions are in order before we continue. The Airflow Operator defines a task, which is the unit of work in Airflow. All operators inherit from the BaseOperator, and in addition to taking the arguments of the BaseOperator, they can take arguments specific to the kind of task they are going to perform, such as a specific conn_id to connect to a datasource or a dictionary of checks to perform on that datasource. The OpenLineage Extractor is somewhat analogous to the Airflow Operator: it is a unit of work in OpenLineage, which takes the relevant input and output data from an operator, creates OpenLineage data facets, and sends those facets to be displayed in Marquez or Datakin. Each extractor maps to a specific set of operators via the get_operator_classnames() class method. The extractors all inherit from a BaseExtractor, which defines a few abstract methods, importantly execute() and execute_on_complete(). Briefly, the two other major OpenLineage constructs in this story are the ExtractorManager, which is responsible for identifying the correct extractor to use, and the Listener, which is the connecting piece between OpenLineage and Airflow. "},{"title":"Interplay​","type":1,"pageTitle":"How Operators and Extractors Work Under-the-Hook","url":"/blog/operators-and-extractors-technical-deep-dive#interplay","content":"Next, we'll walk through what happens when an Airflow instance with OpenLineage support runs a DAG, and how that operator data makes it to the Marquez or Datakin UI. First, a DAG is born. When the DAG is run, the scheduler runs the operators in order by calling their execute() method. This is the first time the OpenLineage Listener responds. Triggered by the execute() event, it calls the Manager, which identifies the correct extractor based on the task_id. Then the Extractor's execute() method is run, potentially returning lineage data in the form of a metadata object. When the operator is done--either succeeded or failed--, the Listener calls the Manager again, and this time the Manager triggers the Extractor's execute_on_complete() method, which may also return metadata based on the result of the task. The metadata object is then sent to Marquez or Datakin, where the data is displayed. "},{"title":"SQL Check Operators/Extractors​","type":1,"pageTitle":"How Operators and Extractors Work Under-the-Hook","url":"/blog/operators-and-extractors-technical-deep-dive#sql-check-operatorsextractors","content":"The SQLCheckOperators and SQLCheckExtractors work slightly differently than the interplay outlined above. The biggest difference is that the SQLCheckExtractors all inherit from a BaseSqlCheckExtractor, which in turn dynamically inherits from the appropriate extractor at run time. The appropriate extractor is always some existing SQL database extractor. The BaseSqlCheckExtractor itself only implements the extract_on_complete() method by determining whether the super class’s extract() or extract_on_complete() method should be run to gather metadata. The _get_input_facets() methods are all implemented by the particular check extractors, and are called in the super class’s extract() or extract_on_complete() method. The dynamic inheritance is done by defining the SQLCheckExtractors inside the get_check_extractors() function that takes a class as a parameter and passes that class to the constructor of the BaseSqlCheckExtractor. When a SQLCheckOperator is run, and the Manager searches for the correct extractor to use, it calls instantiate_abstract_extractors() with the given task instance. In this function, the task instance is used to find the correct extractor that will be the superclass of the BaseSqlCheckOperator. To do this, the function uses a set of hard-coded operator names whose extractors will dynamically inherit from the found superclass. Currently, this list is the set of SQLCheckOperators, which corresponds to a dictionary of extractor keys and conn_type values. The given task’s class name is checked against the set of operator names, and if it is in the set, a loop compares the existing extractor’s corresponding conn_type from the aforementioned dictionary to the given task instance’s conn_type retrieved from Airflow connections. If there’s a match, the get_check_extractors() method is called with the matched extractor, instantiating all the operators with the correct superclass. The SQLCheckExtractors only rely on the extract_on_complete() method, as the values needed from the operators, i.e. the results of the query and the success or failures of the checks, are only available after the operator completes. "},{"title":"Meet Us in San Francisco on June 27th!","type":0,"sectionRef":"#","url":"/blog/sf-meetup","content":"","keywords":""},{"title":"Time, Place & Format​","type":1,"pageTitle":"Meet Us in San Francisco on June 27th!","url":"/blog/sf-meetup#time-place--format","content":"Date: June 27th, 2023 Format: In-person Time: 5:30-8:30 pm PT Address: Astronomer, 8 California Street, 7th Floor, San Francisco, CA 94111 Getting There​ The Astronomer SF office is in the Financial District at the corner of California and Drumm Streets, catty-cornered from the Market/Drumm Street cable car stop. Getting In​ An Astronomer team member in the lobby will direct you to the Astronomer offices on the seventh floor. "},{"title":"Hope to see you there!​","type":1,"pageTitle":"Meet Us in San Francisco on June 27th!","url":"/blog/sf-meetup#hope-to-see-you-there","content":""},{"title":"Tracing Data Lineage with OpenLineage and Apache Spark","type":0,"sectionRef":"#","url":"/blog/openlineage-spark","content":"","keywords":""},{"title":"The Age of Data Democratization​","type":1,"pageTitle":"Tracing Data Lineage with OpenLineage and Apache Spark","url":"/blog/openlineage-spark#the-age-of-data-democratization","content":"In 2015, Apache Spark seemed to be taking over the world. Many of us had spent the prior few years moving our large datasets out of the Data Warehouse into &quot;Data Lakes&quot;- repositories of structured and unstructured data in distributed file systems or object stores, like HDFS or S3. This enabled us to build analytic systems that could handle traditional, table-structured data alongside flexible, unstructured JSON blobs, giving us access to more data and allowing us to move much faster than we’d previously been able to. The problem was that taking the data out of Data Warehouses meant that the people who really needed access to the data, analysts and statisticians, could no longer use the tools they were comfortable with in order to read that data. Where previously, SQL and Python were all that was needed to start exploring and analyzing a dataset, now people needed to write Java or use specialized scripting languages, like Pig, to get at the data. Systems that did support SQL, such as Hive, were unbearably slow for any but the most basic operations. In many places, the statisticians were dependent on software engineers to build custom tools for access, meaning the bottleneck had moved from the systems that needed to store and process the data to the humans who were supposed to tell us what systems to build. Then along came Apache Spark, which gave back to analysts the ability to use their beloved Python (and eventually SQL) tools to process raw data in object stores without the dependency on software engineers. While others were attracted to its ability to perform multiple operations on data without the I/O overhead of alternatives, like Pig or Hive, data scientists were thrilled to start piping that data through their NumPy and Pandas scripts. "},{"title":"A Colossal Mess​","type":1,"pageTitle":"Tracing Data Lineage with OpenLineage and Apache Spark","url":"/blog/openlineage-spark#a-colossal-mess","content":"Of course, the natural consequence of this data democratization is that it becomes difficult to keep track of who is using the data and for what purpose. Hidden dependencies and Hyrum’s Law suddenly meant that changes to the data schema would inadvertently break downstream processes or that stale, deprecated datasets were still being consumed, and that corrupted datasets would leak into unknown processes making recovery difficult or even impossible.  The goal of OpenLineage is to reduce issues and speed up recovery by exposing those hidden dependencies and informing both producers and consumers of data about the state of that data and the potential blast radius of changes and software bugs. Naturally, support for Apache Spark seemed like a good idea and, while the Spark 2.4 branch has been supported for some time, the recent OpenLineage 0.3 release has explicit support for Spark 3.1. 🎉 "},{"title":"Getting Started​","type":1,"pageTitle":"Tracing Data Lineage with OpenLineage and Apache Spark","url":"/blog/openlineage-spark#getting-started","content":"Our approach to integrating with Spark is not super novel nor is it complicated to integrate into your own system. Spark has had a SparkListener interface since before the 1.x days. If you're a heavy Spark user, it's likely you're already familiar with it and how it's used in Spark applications. OpenLineage integrates with Spark by implementing that interface and collecting information about jobs that are executed inside a Spark application. To activate the listener, add the following properties to your Spark configuration: spark.jars.packages io.openlineage:openlineage-spark:0.3.+ spark.extraListeners io.openlineage.spark.agent.OpenLineageSparkListener  This can be added to your cluster’s spark-defaults.conf file, in which case it will record lineage for every job executed on the cluster, or added to specific jobs on submission via the spark-submit command. Once the listener is activated, it needs to know where to report lineage events, as well as the namespace of your jobs. Add the following additional configuration lines to your spark-defaults.conf file or your Spark submission script: spark.openlineage.transport.url {your.openlineage.url} spark.openlineage.transport.type 'http' spark.openlineage.namespace {your.openlineage.namespace}  "},{"title":"The Demo​","type":1,"pageTitle":"Tracing Data Lineage with OpenLineage and Apache Spark","url":"/blog/openlineage-spark#the-demo","content":"Trying out the Spark integration is super easy if you already have Docker Desktop and git installed. To follow along with this demo, you’ll also need a Google Cloud account and a Service Account JSON key file for an account that has access to BigQuery and read/write access to your GCS bucket. I added mine to a file called bq-spark-demo.json. Note: If you're on macOS Monterey (macOS 12) you'll have to release port 5000 before beginning by disabling the AirPlay Receiver. Check out the OpenLineage project into your workspace with: git clone https://github.com/OpenLineage/OpenLineage  Then cd into the integration/spark directory. Run mkdir -p docker/notebooks/gcs and copy your service account credentials file into that directory. Then run: docker-compose up  This launches a Jupyter notebook with Spark already installed as well as a Marquez API endpoint to report lineage. Once the notebook server is up and running, you should see something like the following text in the logs: notebook_1 | [I 21:43:39.014 NotebookApp] Jupyter Notebook 6.4.4 is running at: notebook_1 | [I 21:43:39.014 NotebookApp] http://082cb836f1ec:8888/?token=507af3cf9c22f627f6c5211d6861fe0804d9f7b19a93ca48 notebook_1 | [I 21:43:39.014 NotebookApp] or http://127.0.0.1:8888/?token=507af3cf9c22f627f6c5211d6861fe0804d9f7b19a93ca48 notebook_1 | [I 21:43:39.015 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).  Copy the URL with 127.0.0.1 as the hostname from your own log (the token will be different from mine) and paste it into your browser window. You should have a blank Jupyter notebook environment ready to go.  Once your notebook environment is ready, click on the notebooks directory, then click on the New button to create a new Python 3 notebook.  In the first cell in the window paste the following text. Update the GCP project and bucket names and the service account credentials file, then run the code: from pyspark.sql import SparkSession import urllib.request # download dependencies for BigQuery and GCS gc_jars = ['https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.1.1/gcs-connector-hadoop3-2.1.1-shaded.jar', 'https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/bigquery-connector/hadoop3-1.2.0/bigquery-connector-hadoop3-1.2.0-shaded.jar', 'https://repo1.maven.org/maven2/com/google/cloud/spark/spark-bigquery-with-dependencies_2.12/0.22.2/spark-bigquery-with-dependencies_2.12-0.22.2.jar'] files = [urllib.request.urlretrieve(url)[0] for url in gc_jars] # Set these to your own project and bucket project_id = 'bq-openlineage-spark-demo' gcs_bucket = 'bq-openlineage-spark-demo-bucket' credentials_file = '/home/jovyan/notebooks/gcs/bq-spark-demo.json' spark = (SparkSession.builder.master('local').appName('openlineage_spark_test') .config('spark.jars', &quot;,&quot;.join(files)) # Install and set up the OpenLineage listener .config('spark.jars.packages', 'io.openlineage:openlineage-spark:0.3.+') .config('spark.extraListeners', 'io.openlineage.spark.agent.OpenLineageSparkListener') .config('spark.openlineage.transport.url', 'http://marquez-api:5000') .config('spark.openlineage.transport.type', 'http') .config('spark.openlineage.namespace', 'spark_integration') # Configure the Google credentials and project id .config('spark.executorEnv.GCS_PROJECT_ID', project_id) .config('spark.executorEnv.GOOGLE_APPLICATION_CREDENTIALS', '/home/jovyan/notebooks/gcs/bq-spark-demo.json') .config('spark.hadoop.google.cloud.auth.service.account.enable', 'true') .config('spark.hadoop.google.cloud.auth.service.account.json.keyfile', credentials_file) .config('spark.hadoop.fs.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem') .config('spark.hadoop.fs.AbstractFileSystem.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS') .config(&quot;spark.hadoop.fs.gs.project.id&quot;, project_id) .getOrCreate())  Most of this is boilerplate- we need the BigQuery and GCS libraries installed in the notebook environment, then we need to set the configuration parameters to tell the libraries what GCP project we want to use and how to authenticate with Google. The parameters specific to OpenLineage are the four we already covered- spark.jars.packages,spark.extraListeners, spark.openlineage.host, spark.openlineage.namespace. Here, we’ve configured the host to be the marquez-api container started by Docker. Google has a wealth of information available as public datasets in BigQuery. If you’re ever bored one Saturday night, browse the datasets available- you’ll find census data, crime data, liquor sales, and even a black hole database. For the demo, I thought I’d browse some of the Covid19 related datasets they have. Specifically, there’s a dataset that reports the likelihood of people in a given county to wear masks (broken up into five categories: always, frequently,sometimes, rarely, and never). There’s also a giant dataset called covid19_open_data that contains things like vaccination rates, current totals of confirmed cases, hospitalizations, deaths, population breakdowns, and policies on mask-wearing, contact tracing, and vaccination-mandates. Both datasets contain the county FIPS code for US counties, meaning we can join the two datasets and start exploring. Create a new cell in the notebook and paste the following code: from pyspark.sql.functions import expr, col mask_use = spark.read.format('bigquery') \\ .option('parentProject', project_id) \\ .option('table', 'bigquery-public-data:covid19_nyt.mask_use_by_county') \\ .load() \\ .select(expr(&quot;always + frequently&quot;).alias(&quot;frequent&quot;), expr(&quot;never + rarely&quot;).alias(&quot;rare&quot;), &quot;county_fips_code&quot;) opendata = spark.read.format('bigquery') \\ .option('parentProject', project_id) \\ .option('table', 'bigquery-public-data.covid19_open_data.covid19_open_data') \\ .load() \\ .filter(&quot;country_name == 'United States of America'&quot;) \\ .filter(&quot;date == '2021-10-31'&quot;) \\ .select(&quot;location_key&quot;, expr('cumulative_deceased/(population/100000)').alias('deaths_per_100k'), expr('cumulative_persons_fully_vaccinated/(population - population_age_00_09)').alias('vaccination_rate'), col('subregion2_code').alias('county_fips_code')) joined = mask_use.join(opendata, 'county_fips_code') joined.write.mode('overwrite').parquet(f'gs://{gcs_bucket}/demodata/covid_deaths_and_mask_usage/')  Again, this is standard Spark DataFrame usage. The particulars are completely irrelevant to the OpenLineage data collection- we don’t need to call any new APIs or change our code in any way. Here, I’ve filtered the covid19_open_data table to include only U.S. data and to include the data for Halloween 2021. That dataset has a large number of columns, but for my own purposes, I’m only interested in a few of them. I calculate deaths_per_100kusing the existing cumulative_deceased and population columns and I calculate the vaccination_rate using the total population, subtracting the 0-9 year olds, since they weren’t eligible for vaccination at the time. For the mask_use_by_county data, I don't really care about the difference between rarely and never, so I combine them into a single number. I do the same for frequently and always. I join the few columns I want from the two datasets and store the combined result in GCS. Add one more cell to the notebook and paste the following: spark.read.parquet(f'gs://{gcs_bucket}/demodata/covid_deaths_and_mask_usage/').count()  The notebook will likely spit out a warning and a stacktrace (it should probably be a debug statement), then give you a total of 3142 records. So far, so good. Now what? If this was a data science blog, we might start generating some scatter plots or doing a linear regression to determine whether frequent mask usage was a predictor of high death rates or vaccination rates. But since we're really focused on lineage collection, I'll leave the rest of the analysis up to those with the time and inclination to dig further. Instead, let's switch to exploring the lineage records we just created. The docker-compose.yml file that ships with the OpenLineage repo includes only the Jupyter notebook and the Marquez API. For exploring visually, we’ll also want to start up the Marquez web project. Without terminating the existing docker containers, run the following command in a new terminal: docker run --network spark_default -p 3000:3000 -e MARQUEZ_HOST=marquez-api -e MARQUEZ_PORT=5000 --link marquez-api:marquez-api marquezproject/marquez-web:0.19.1  Now open a new browser tab and navigate to http://localhost:3000. You should see a screen like the following:  Note the spark_integration namespace was found for us and automatically chosen, since there are no other namespaces available. We can see three jobs listed on the jobs page of the UI. They all start with openlineage_spark_test, which is the appName we passed to the SparkSession we were building in the first cell of the notebook. Each query execution or RDD action is represented as a distinct job and the name of the action is appended to the application name to form the name of the job. Clicking on the openlineage_spark_test.execute_insert_into_hadoop_fs_relation_command node, we can see the lineage graph for our notebook:  The graph shows the openlineage_spark_test.execute_insert_into_hadoop_fs_relation_command job reads from two input datasets, bigquery-public-data.covid19_nyt.mask_use_by_countyand bigquery-public-data.covid19_open_data.covid19_open_data, and writes to a third dataset,/demodata/covid_deaths_and_mask_usage. The namespace is missing from that third dataset- the fully qualified name isgs://&lt;your_bucket&gt;/demodata/covid_deaths_and_mask_usage. Before clicking on the datasets, though, the bottom bar shows some really interesting data that was collected from the Spark job. Dragging the bar up expands the view so we can get a better look at that data.  Two facets that are always collected from Spark jobs are the spark_version and the spark.logicalPlan. The first simply reports what version of Spark was executing, as well as the version of the openlineage-spark library. This is helpful information to collect when trying to debug a job run. The second facet is the serialized optimized LogicalPlan Spark reports when the job runs. Spark’s query optimization can have dramatic effects on the execution time and efficiency of the query job. Tracking how query plans change over time can significantly aid in debugging slow queries or OutOfMemory errors in production. Clicking on the first BigQuery dataset gives us information about the data we read:  Here, we can see the schema of the dataset as well as the datasource — namely BigQuery. We can get similar information about the dataset written to in GCS:  As in the BigQuery dataset, we can see the output schema and the datasource — here, the gs:// scheme and the name of the bucket we wrote to. In addition to the schema, we also see a stats facet, reporting the number of output records and bytes as -1. A somewhat recent change to the OpenLineage schema resulted in output facets being recorded in a new field- one that Marquez is not yet reading from. The old, deprecated facet reports the output stats incorrectly. An upcoming bugfix should correct the stats view so that we can see the number of rows written as well as the number of output bytes (the statistics are actually recorded correctly- the API simply needs to start returning the correct values). You may have noticed the VERSIONS tab on the bottom bar. We can click it, but since the job has only ever run once, we’ll only see one version. Clicking on the version, we’ll see the same schema and statistics facets, but specific to the version we clicked.  In production, this dataset would have many versions, as each time the job runs a new version of the dataset is created. This allows us to track changes to the statistics and schema over time, again aiding in debugging slow jobs (suddenly, we started writing 50% more records!) or data quality issues (suddenly, we’re only writing half as many records as normal!) and job failures (somebody changed the output schema and downstream jobs are failing!). The final job in the UI is a HashAggregate job- this represents the count() method we called at the end to show the number of records in the dataset. Rather than a count(), this could easily be a toPandas() call or some other job that reads and processes that data- perhaps storing output back into GCS or updating a Postgres database or publishing a new model, etc. Regardless of where the output gets stored, the OpenLineage integration allows you to see the entire lineage graph, unifying datasets in object stores, relational databases, and more traditional data warehouses.  The Spark integration is still a work in progress, but users are already getting insights into their graphs of datasets stored in object stores like S3, GCS, and Azure Blob Storage, as well as BigQuery and relational databases like Postgres. Now with Spark 3.1 supported, we can gain visibility into more environments, like Databricks, EMR, and Dataproc clusters. Data lineage gives visibility to the (hopefully) high quality, (hopefully) regularly updated datasets that everyone depends on, maybe without even realizing it. Spark helped usher in a welcome age of data democratization. Now data observability can help ensure we’re making the best possible use of the data available. "},{"title":"The Python Client -- the Foundation of OpenLineage Integrations","type":0,"sectionRef":"#","url":"/blog/python-client","content":"","keywords":""},{"title":"Introduction​","type":1,"pageTitle":"The Python Client -- the Foundation of OpenLineage Integrations","url":"/blog/python-client#introduction","content":"Thanks to the OpenLineage community’s active work on integrations, the pursuit of lineage is getting more efficient and effective all the time. And our growing list of partners and adapters makes OpenLineage plenty powerful out of the box. At the same time, the nature of the data engineering field means that lineage capture is an ongoing process – simply put, the work of lineage is never done. Hence, as lineage capture becomes integral to your pipelines, situations can arise that require new custom integrations. Enter the Python client, one of two built-in clients included in the project (the other being the Java client). The OpenLineage spec is defined using JSON schema, but we have created these clients so that new integrations do not have to reinvent the wheel. OpenLineage’s Python client enables the creation of lineage metadata events with Python code. The core data structures currently offered by the client include the RunEvent, RunState, Run, Job, Dataset, and Transport classes. These either configure or collect data for the emission of lineage events. In the history of the project, the client has been useful in helping us avoid unnecessary duplication of code. It is also integral to OpenLineage’s existing integrations, serving as the basis of the Airflow and dbt integrations, for example. It could also act as the foundation of your own custom integration should you need to write one. (Another use case: tests for a new Airflow extractor.) For this reason, an existing integration can serve as a useful example of how to use the client to write a new integration (and, hopefully, contribute it back to the project!). What follows is an overview of the Python client and the dbt integration, which uses the Python client. You’ll see how the client receives metadata from dbt to make it available to a consumer of OpenLineage such as Microsoft Purview, Amundsen, Astronomer, Egeria or Marquez. "},{"title":"Python Client Data Structures​","type":1,"pageTitle":"The Python Client -- the Foundation of OpenLineage Integrations","url":"/blog/python-client#python-client-data-structures","content":"The core structures of the client organize metadata about the foundational objects of the OpenLineage spec: runs, jobs and datasets. A dataset is a class consisting of a name, namespace and, optionally, facets array: @attr.s class Dataset: namespace: str = attr.ib() name: str - attr.ib() Facets: Dict = attr.ib(factory=dict)  The same goes for a job: @attr.s class Job: namespace: str = attr.ib() name: str - attr.ib() Facets: Dict = attr.ib(factory=dict)  A RunEvent sends the time, state, job, run, producer, input and output information needed to construct an OpenLineage job run event: @attr.s class RunEvent: eventType: RunState = attr.ib(validator=attr.validators.in_(RunState)) eventTime: str = attr.ib() run: Run = attr.ib() job: Job = attr.ib() producer: str = attr.ib() inputs: Optional[List[Dataset]] = attr.ib(factory=list) outputs: Optional[List[Dataset]] = attr.ib(factory=list)  "},{"title":"The OpenLineage-dbt Integration​","type":1,"pageTitle":"The Python Client -- the Foundation of OpenLineage Integrations","url":"/blog/python-client#the-openlineage-dbt-integration","content":"At a high level, the dbt integration uses the Python client to push metadata to the OpenLineage backend. The metadata it makes available comprises the run lifecycle, including any dataset inputs and outputs accessed during a job run. In the dbt-ol script, the integration uses the project’s ParentRunMetadata and DbtArtifactProcessor classes, both of which can be found in the OpenLineage common integration, to parse metadata from the dbt manifest and run_result in order to produce OpenLineage events: from openlineage.common.provider.dbt import DbtArtifactProcessor, ParentRunMetadata #… if parent_id: parent_namespace, parent_job_name, parent_run_id = parent_id.split(‘/’) parent_run_metadata = ParentRunMetadata( run_id=parent_run_id, job_name=parent_job_name, job_namespace=parent_namespace ) processor = DbtArtifactProcessor( producer=PRODUCER, target=target, job_namespace=job_namespace, project_dir=project_dir, profile_name=profile_name, logger=logger )  The integration uses a wrapper for dbt runs because start and complete events are not emitted until execution concludes: dbt_run_event = dbt_run_event_start( job_name=f“dbt-run-{processor.project[‘name’]}”, job_namespace=job_namespace, parent_run_metadata=parent_run_metadata ) dbt_run_metadata = ParentRunMetadata( run_id=dbt_run_event.run.runId, job_name=dbt_run_event.job.name, job_namespace=dbt_run_event.job.namespace ) processor.dbt_run_metadata = dbt_run_metadata  After executing dbt, the script parses the metadata using the processor and emits a run event: events = processor.parse().events() client.emit(dbt_run_event_end( run_id=dbt_run_metadata.run_id, job_namespace=dbt_run_metadata.job_namespace, job_name=dbt_run_metadata.job_name, parent_run_metadata=parent_run_metadata )) logger.info(f&quot;Emitted {len(events) + 2} openlineage events&quot;)  "},{"title":"Additional Resources​","type":1,"pageTitle":"The Python Client -- the Foundation of OpenLineage Integrations","url":"/blog/python-client#additional-resources","content":"Check out the source code here: https://github.com/OpenLineage/OpenLineage/tree/main/client/python. Interested in contributing to the project? Read our guide for new contributors: https://github.com/OpenLineage/OpenLineage/blob/main/CONTRIBUTING.md. Join us on Slack: http://bit.ly/OpenLineageSlack. Attend a community meeting: https://bit.ly/OLwikitsc. "},{"title":"OpenLineage 1.0, Featuring Static Lineage, is Coming Soon","type":0,"sectionRef":"#","url":"/blog/static-lineage","content":"","keywords":""},{"title":"Static, AKA \"Design,\" Lineage is Coming Soon​","type":1,"pageTitle":"OpenLineage 1.0, Featuring Static Lineage, is Coming Soon","url":"/blog/static-lineage#static-aka-design-lineage-is-coming-soon","content":"OpenLineage 1.0, which is expected early in August, will add support for static lineage to the project. An initiative to add the provision of static lineage, sometimes also called &quot;design&quot; or &quot;design-time&quot; lineage, to OpenLineage came out of conversations with community members at Collibra, Manta and Marquez. Data catalogs like those offered by Collibra and Manta will benefit from static lineage support, but so will other users. In one way, this addition represents an exciting new chapter in the history of the project. In another, it represents a return to our roots. Before OpenLineage focused on operational lineage, it supported a form of static lineage. What is Static Lineage?​ OpenLineage has traditionally supported only operational, or &quot;runtime,&quot; lineage -- metadata emitted when jobs run. In other words, OpenLineage has been engineered to capture information as transformations of datasets are happening, enabling precise descriptions of those transformations. As part of this process, OpenLineage has nonetheless also captured some static metadata -- specifically, information about jobs (such as the current version of the code) and datasets (such as the schema). What was called for was a way to collect such static metadata outside of the run context. What Use Cases are Served by Static Lineage?​ Use cases include: bootstrapping of a lineage graph with prospective runs for auditingcapturing dataset ownership changes consuming facets from external systemscreating dataset symlinks more easily Implementation Details​ In order to add static lineage to the spec, two new event types were proposed: DatasetEvent and JobEvent. These new events will add facets at a point in time that will apply to an entity until a new version of the same facet is produced. The first step in implementing static lineage was completed with the release of OpenLineage 0.29.2, which included two types in the spec for &quot;runless&quot; metadata: a DatasetEvent and JobEvent (along with support for the new types in the Python client). The next steps will include changing the event lifecycle (from running to complete, failed, or aborted) to handle events of the new types, and adding facet deletion to handle the case in which a user adds and deletes a dataset in the same request. Adding support for static lineage in Marquez is also ongoing, and we are excited about the progress there. Marquez 0.37.0 includes support in the API for decoding static events via a new EventTypeResolver. More Information​ For more details including the code changes, see: the static lineage proposal by Julien Le Dem, Maciej Obuchowski, Benji Lampel and Ross Turk the initial pull request by Paweł Leszczyński "},{"title":"Meet Us in San Francisco on August 30th!","type":0,"sectionRef":"#","url":"/blog/sf-meetup-2","content":"","keywords":""},{"title":"Time, Place & Format​","type":1,"pageTitle":"Meet Us in San Francisco on August 30th!","url":"/blog/sf-meetup-2#time-place--format","content":"Date: August 30th, 2023 Format: In-person Time: 5:30-8:30 pm PT Address: Astronomer, 8 California Street, 7th Floor, San Francisco, CA 94111 Getting There​ The Astronomer SF office is in the Financial District at the corner of California and Drumm Streets, catty-cornered from the Market/Drumm Street cable car stop. Getting In​ An Astronomer team member in the lobby will direct you to the Astronomer offices on the seventh floor. "},{"title":"Hope to see you there!​","type":1,"pageTitle":"Meet Us in San Francisco on August 30th!","url":"/blog/sf-meetup-2#hope-to-see-you-there","content":""},{"title":"Meet Us in Warsaw on November 29th!","type":0,"sectionRef":"#","url":"/blog/warsaw-meetup","content":"","keywords":""},{"title":"Time, Place & Format​","type":1,"pageTitle":"Meet Us in Warsaw on November 29th!","url":"/blog/warsaw-meetup#time-place--format","content":"Date: November 29th, 2023 Format: Hybrid Time: 17:30-20:30 CET Address: Google Warsaw, The Hub, Rondo Daszyńskiego 2c, 00-843 Warsaw, Poland "},{"title":"Joining Virtually​","type":1,"pageTitle":"Meet Us in Warsaw on November 29th!","url":"/blog/warsaw-meetup#joining-virtually","content":"A Zoom link will be provided to attendees in the days leading up to the event. "},{"title":"Hope to see you there!​","type":1,"pageTitle":"Meet Us in Warsaw on November 29th!","url":"/blog/warsaw-meetup#hope-to-see-you-there","content":""},{"title":"Why an Open Standard for Lineage Metadata?","type":0,"sectionRef":"#","url":"/blog/why-open-standard","content":"","keywords":""},{"title":"Background​","type":1,"pageTitle":"Why an Open Standard for Lineage Metadata?","url":"/blog/why-open-standard#background","content":"It’s called OpenLineage for a reason – it’s an open-source spec for the collection of lineage metadata. This is a large part of its appeal. If you ask our users why they chose OpenLineage, they are likely to cite, in addition to its simplicity and desirable integrations, the fact that it provides an open spec for lineage. (But please note that lineage metadata is not the only kind of metadata OpenLineage supports, event time and run state being two additional forms of metadata provided by the core spec, and facets provide even more. OpenLineage is easily extensible and offers more than just lineage out of the box!) An open spec for lineage metadata will be more likely to succeed because it will foster collaboration and hasten wide adoption across the data ecosystem. Advantages of a collaborative, open approach include faster innovation, reduced duplication of effort, and better interoperability between systems. In fact, to our thinking, an open standard is the only way to approach the constantly moving target of 100% coverage of tooling in the fast-moving data space. We also believe that the pursuit of total coverage is worth the short-term challenges involved in getting buy-in across the industry. "},{"title":"Bluetooth: Evidence that Open Standards Work​","type":1,"pageTitle":"Why an Open Standard for Lineage Metadata?","url":"/blog/why-open-standard#bluetooth-evidence-that-open-standards-work","content":"Ross Turk (@rossturk), one of the early evangelists for OpenLineage, has often cited the example of Bluetooth, a spec, when making the case for OpenLineage. The example is a salutary one. According to Bluetooth, 5.4 billion Bluetooth-equipped devices will ship this year. That’s a lot of headsets and waterproof speakers, among the many other things that use Bluetooth, but why did the standard become the dominant spec for short-range wireless connectivity? One possible explanation stands out: it started as an open standard. The Bluetooth standard has been in development since the late 1990s, when Nokia, Ericsson and Intel began work on it. They knew that only an open standard would make wireless connectivity across devices and industries a reality, but they collaborated on the spec because neither company was the leader of its market segment. Unable to use market dominance to impose a standard, they joined forces instead. When the Bluetooth SIG (Special Interest Group) launched in 1998, it had five members: Ericsson, IBM, Toshiba, Nokia and Intel. Today, membership stands at over 38,000 companies. "},{"title":"OK, but What’s in It for Me?​","type":1,"pageTitle":"Why an Open Standard for Lineage Metadata?","url":"/blog/why-open-standard#ok-but-whats-in-it-for-me","content":"Playing devil’s advocate, it’s one thing to argue that companies straddling multiple industries and dealing with complex hardware-related challenges can benefit from open standards. It’s perhaps another to argue that companies in the data space can benefit from open standards – especially when the competition will, too. If one’s focus is only on short-term gains and losses, then this concern has some merit. A truly open standard is open to all, meaning partners and competitors alike reap the benefits. (Even in the short term, there are ways to differentiate, however.) If one takes a broader view, though, it becomes clear that lineage metadata is only truly valuable to anyone if it offers end-to-end and fully agnostic pipeline visibility. The best way to get total coverage that is reliable and persistent is to get the participation of the metadata producers themselves. This reality means that, absent a dominant open standard, one’s own stakeholders – from internal engineering teams to customers to external partners – will feel the pain of incomplete coverage. This will have long-term implications for productivity, product quality, user experience and, ultimately, profitability. Fair enough, but won’t a shared standard dilute member companies’ value propositions? Not if their products are adequately differentiated. Competition through differentiated products, combined with collaboration on a shared standard, is the solution. "},{"title":"What’s in It for the Ecosystem?​","type":1,"pageTitle":"Why an Open Standard for Lineage Metadata?","url":"/blog/why-open-standard#whats-in-it-for-the-ecosystem","content":"The data ecosystem is evolving continuously, with new tools being added daily. Given this constant rate of change, a spec that is open – and, therefore, more likely to become technology-agnostic – offers the fastest route to comprehensive and up-to-date pipeline observability. The speed with which the ecosystem is evolving has meant that, ironically enough, some legacy systems, particularly in the Big Data space, have remained viable for many years. An open standard is better positioned not only to support new tools as they emerge but also to maintain support for legacy systems. In short, the way to ensure that a standard is tool-agnostic and resilient is to make it a community effort owned by all. "},{"title":"How Can I Get Involved?​","type":1,"pageTitle":"Why an Open Standard for Lineage Metadata?","url":"/blog/why-open-standard#how-can-i-get-involved","content":"Anyone can contribute to OpenLineage by forking the GitHub repository and opening a pull request. For more information about getting started as a contributor, read the new contributor guide. Prefer to get your feet wet first? Try our quickstart guide. "},{"title":"What's in a Namespace?","type":0,"sectionRef":"#","url":"/blog/whats-in-a-namespace","content":"","keywords":""},{"title":"Background​","type":1,"pageTitle":"What's in a Namespace?","url":"/blog/whats-in-a-namespace#background","content":"With all due respect to Shakespeare's Juliet, in the OpenLineage spec at least, names in general -- and namespaces in particular -- are everything. OK, that’s an exaggeration, but not by much. The function of namespaces is to provide unique IDs for everything in the lineage graph so that jobs and datasets can be rendered as nodes. This means namespaces make stitching input and output datasets together as pipelines possible – which is to say they effectively make lineage possible. In the broader context of the spec, namespaces reflect the importance of naming and naming conventions to the way OpenLineage constructs lineage.  In creating pipelines organized according to data sources (in the case of datasets) or schedulers (in the case of jobs), namespaces enable focused insight into data flows, even when datasets and workflows are distributed across an organization. This focus enabled by namespaces is key to the production of useful lineage. If everything lived in a single namespace, every lineage graph would show everything that happened in an ecosystem – and be too cluttered to be useful. "},{"title":"Namespaces in the Spec​","type":1,"pageTitle":"What's in a Namespace?","url":"/blog/whats-in-a-namespace#namespaces-in-the-spec","content":"A look at the spec provides additional detail about namespaces. In the spec, namespaces are at the top of the hierarchy, which means that they have priority over datasets, jobs, and the graphs that connect them. Namespaces contain graphs, in fact, along with just about everything else in a datasource or scheduler’s domain. Ultimately, this reflects the spec’s bias towards tracking dataset and job transformations. To wit: the same code applied to different input datasets results in different jobs (not different runs of the same job). If those jobs share a scheduler, they will also share a namespace – but not a graph, which makes tracking the transformations much easier. Similarly, if different input datasets share a datasource, they will also share a namespace (but not a graph). As you can see, the track switching can get a little complicated, but the namespace abstraction has some clear advantages. "},{"title":"Namespaces in the Wild​","type":1,"pageTitle":"What's in a Namespace?","url":"/blog/whats-in-a-namespace#namespaces-in-the-wild","content":"Consider the common scenario in which multiple teams in an organization maintain pipelines that access the same datasets. Now, imagine trying to collect and display lineage from the organization’s ecosystem without having a way to distinguish between the different pipelines that use the same datasets. The ambiguous metadata would make any graph so cluttered as to be practically meaningless. Suffice it to say, without a strategy for naming at that macro level of the ecosystem, creating a meaningful graph and tracking transformations is much more difficult. Namespaces also ensure that lineage is meaningful irrespective of the various sources of a job’s metadata. A scope above the dataset and run makes heterogeneous, holistic lineage possible in the case of datasets and jobs. We define the unique name strategy per resource to ensure it is followed uniformly independently from who is producing metadata and we can connect lineage from various sources.  In sum, namespaces make operational lineage possible – which is, while maybe not everything, close to it. "},{"title":"Consulting the Marquez API​","type":1,"pageTitle":"What's in a Namespace?","url":"/blog/whats-in-a-namespace#consulting-the-marquez-api","content":"Thanks to the Marquez API reference, we know that a namespaces endpoint is available that we can query for all namespaces. If you use curl to do so, here’s what you’ll get after building Marquez from source with seed data (using ./docker/up.sh --build --seed): ➜ marquez git:(main) ✗ curl -v http://localhost:5000/api/v1/namespaces | jq { &quot;namespaces&quot;: [ { &quot;name&quot;: &quot;default&quot;, &quot;createdAt&quot;: &quot;2022-12-07T15:02:24.135154Z&quot;, &quot;updatedAt&quot;: &quot;2022-12-07T15:02:24.135154Z&quot;, &quot;ownerName&quot;: &quot;anonymous&quot;, &quot;description&quot;: &quot;The default global namespace for dataset, job, and run metadata not belonging to a user-specified namespace.&quot;, &quot;isHidden&quot;: false }, { &quot;name&quot;: &quot;food_delivery&quot;, &quot;createdAt&quot;: &quot;2020-02-22T22:42:42Z&quot;, &quot;updatedAt&quot;: &quot;2020-02-22T22:42:42Z&quot;, &quot;ownerName&quot;: &quot;anonymous&quot;, &quot;description&quot;: null, &quot;isHidden&quot;: false } ] }  The namespaces endpoint returns all the available namespaces, which is helpful because, as we’ll see, so much of the information available from the API requires a namespace. For this reason alone, you might say the namespace is the “root” of the object model. Say you want to retrieve one or more datasets from the API. First, you’ll need a namespace: ➜ marquez git:(main) ✗ curl -v http://localhost:5000/api/v1/namespaces/food_delivery/datasets/public.drivers | jq { &quot;id&quot;: { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.drivers&quot; }, &quot;type&quot;: &quot;DB_TABLE&quot;, &quot;name&quot;: &quot;public.drivers&quot;, &quot;physicalName&quot;: &quot;public.drivers&quot;, &quot;createdAt&quot;: &quot;2020-02-22T22:42:42Z&quot;, &quot;updatedAt&quot;: &quot;2020-02-22T22:42:42Z&quot;, &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;sourceName&quot;: &quot;default&quot;, &quot;fields&quot;: [ { &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;INTEGER&quot;, &quot;tags&quot;: [], &quot;description&quot;: &quot;The unique ID of the driver.&quot; }, …  Say you want to retrieve a job. You’ll need a namespace: ➜ marquez git:(main) ✗ curl -v http://localhost:5000/api/v1/namespaces/food_delivery/jobs/etl_order_status | jq { &quot;id&quot;: { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;etl_order_status&quot; }, &quot;type&quot;: &quot;BATCH&quot;, &quot;name&quot;: &quot;etl_order_status&quot;, &quot;simpleName&quot;: &quot;etl_order_status&quot;, &quot;parentJobName&quot;: null, &quot;createdAt&quot;: &quot;2020-02-22T22:42:42Z&quot;, &quot;updatedAt&quot;: &quot;2020-02-22T22:44:52Z&quot;, &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;inputs&quot;: [], &quot;outputs&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.order_status&quot; } ], …  Runs? You’ll need a namespace for those: ➜ marquez git:(main) ✗ curl -v http://localhost:5000/api/v1/namespaces/food_delivery/jobs/etl_order_status/runs | jq { &quot;runs&quot;: [ { &quot;id&quot;: &quot;b7098939-87f0-4207-878f-dfd8e8804d8a&quot;, &quot;createdAt&quot;: &quot;2020-02-22T22:42:42Z&quot;, &quot;updatedAt&quot;: &quot;2020-02-22T22:44:52Z&quot;, &quot;nominalStartTime&quot;: null, &quot;nominalEndTime&quot;: null, &quot;state&quot;: &quot;COMPLETED&quot;, &quot;startedAt&quot;: &quot;2020-02-22T22:42:42Z&quot;, &quot;endedAt&quot;: &quot;2020-02-22T22:44:52Z&quot;, &quot;durationMs&quot;: 130000, &quot;args&quot;: {}, &quot;jobVersion&quot;: { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;etl_order_status&quot;, &quot;version&quot;: &quot;44ca508b-43cc-392f-bbd2-9ca77d501afa&quot; }, &quot;inputVersions&quot;: [], &quot;outputVersions&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.order_status&quot;, &quot;version&quot;: &quot;0c16298c-cbe2-3547-8429-309917290570&quot; } ], &quot;context&quot;: { &quot;sql&quot;: &quot;INSERT INTO order_status (id, transitioned_at, status, order_id, customer_id, restaurant_id, driver_id)\\n SELECT id, transitioned_at, status, order_id, customer_id, restaurant_id, driver_id\\n FROM tmp_order_status;&quot; }, &quot;facets&quot;: {} } ] }  As the API reveals, namespaces really are key in the spec: they organize and unlock most of the insights OpenLineage has to offer. "},{"title":"Dataset Namespaces​","type":1,"pageTitle":"What's in a Namespace?","url":"/blog/whats-in-a-namespace#dataset-namespaces","content":"Having explored the thinking behind namespaces and their role in the spec, we can get into how they organize datasets and jobs. Let’s start with datasets, the simpler of the two cases due to the more straightforward way datasets’ namespaces are constructed. In short, a dataset’s namespace is always tied to its datasource. As the spec says, the namespace for a dataset is the unique name for its datasource. Data sources vary, however, so the specific construction of dataset namespaces also varies across datasource types. (But they tend to map to databases.) In the case of Postgres, Mysql, and Trino, for example, we derive the namespace of a dataset from a combination of the scheme, host, and port of the service instance: * Namespace: postgres://{host}:{port} of the service instance. * Scheme = postgres * Authority = {host}:{port} * Namespace: mysql://{host}:{port} of the service instance. * Scheme = mysql * Authority = {host}:{port} * Namespace: trino://{host}:{port} of the service instance. * Scheme = trino * Authority = {host}:{port}  Redshift requires a different strategy. It’s possible to interact with Redshift via SQL and an API, so a Redshift namespace is composed of a cluster identifier, region name and port – the only common unique ID available to both methods: * Namespace: redshift://{cluster_identifier}.{region_name}:{port} of the cluster instance. * Scheme = redshift * Authority = {cluster_identifier}:{port}  Snowflake and Amazon’s serverless Athena warehouse service, which do not require a port, are even simpler: * Namespace: awsathena://athena.{region_name}.amazonaws.com of the service instance. * Scheme = awsathena * Authority = athena.{region_name}.amazonaws.com * Namespace: snowflake://{account name} * Scheme = snowflake * Authority = {account name}  And so on. As you can see, the provenance of dataset namespaces is pretty straightforward: it’s the data source. That said, sometimes deriving the data source of a dataset is not a simple operation. Some datasets can be identified two different ways, for example. A Spark dataset can be written using a Hive metastore and table name but read using the physical location of the data. Before we added the SymlinksDatasetFacet, this naming conflict could break the lineage graph. Symlinks both provide alternative dataset names as a workaround in such cases and contain extra information about the datasets. "},{"title":"Job Namespaces​","type":1,"pageTitle":"What's in a Namespace?","url":"/blog/whats-in-a-namespace#job-namespaces","content":"We’ve seen that for datasets the namespace is determined by the data source. But jobs are a different animal, so their namespaces are also different. How is a job’s namespace derived? As in the case of datasets, the unique naming of jobs is essential, and a job’s unique name consists of a namespace and name. Unlike datasets, jobs descend from schedulers, not data sources. Also unlike datasets, jobs are reducible: a job is composed of executions, or runs (as you can see from the “Historial de Ejecuciones” tab – if you were to take the new language switcher for a spin and select Spanish, that is!).  Consulting the spec, we find more detail about the naming of jobs: Jobs have a name that is unique to them in their namespace by construction. The Namespace is the root of the naming hierarchy. The job name is constructed to identify the job within that namespace. Example: * Airflow: * Namespace: the namespace is assigned to the airflow instance. Ex: airflow-staging, airflow-prod * Job: each task in a DAG is a job. name: {dag name}.{task name} * Spark: * Namespace: the namespace is provided as a configuration parameter as in airflow. If there's a parent job, we use the same namespace, otherwise it is provided by configuration. * Spark app job name: the spark.app.name * Spark action job name: {spark.app.name}.{node.name}  So, while for datasets it’s all about the datasource, for jobs it’s all about the scheduler. And the ParentRun facet makes tracking job namespaces possible. { &quot;run&quot;: { &quot;runId&quot;: &quot;run_uuid&quot; }, &quot;job&quot;: { &quot;namespace&quot;: &quot;job_namespace&quot;, &quot;name&quot;: &quot;job_name&quot; } }  For all events, if a parent job exists, the facet’s namespace value is used to assign a namespace. Otherwise, one is provided by configuration. "},{"title":"What's the Point?​","type":1,"pageTitle":"What's in a Namespace?","url":"/blog/whats-in-a-namespace#whats-the-point","content":"But why bother with dataset namespaces in the first place? One answer to this question gets to what is, for some users, a primary value proposition of OpenLineage. A common use case for lineage collection involves tracking dataset access and transformation across jobs and teams in an organization – for monitoring the use of PII, for example. Tags are supported by OpenLineage and can be used to meet this need, but, depending on how an organization’s ecosystem is constructed, namespaces can also help with this common governance use case. Let’s explore a simple example constructed using the Python client. Imagine that a library’s website has two components, a catalog and a blog, and that both features access the same user and profile tables, both of which contain PII.  In the spec, a dataset is unique only within a namespace – not across multiple namespaces – so a number of different graphs are possible depending on how the datasets are produced and accessed across an organization. For example, if for some reason the users and profiles tables shared two different data sources, the tables would belong to two different namespaces (let’s call them catalog_project and blog_project). While not a typical scenario, this would result in two different, uncluttered graphs of the multiple flows making use of the shared datasets:   The reason for the simplicity? The users and profiles tables belong to both the catalog_project and blog_project namespaces. A more typical scenario might involve single versions of the tables being produced by one data source, which would assign them to a single OpenLineage namespace. Ironically, a simpler approach like this results in a more complicated visualization. Notice that the graph remains the same regardless of the namespace selected:   One advantage of this architecture is that it results in graphs making clear that the datasets containing PII are shared by the two jobs. Depending on an organization’s needs, developers might also find it more convenient to be able to see both jobs and their shared datasets in the same graph. A third scenario might involve the isolation of PII by the use of a dedicated database and, by extension, a dedicated namespace (e.g., user_data). In the resulting visualization, the job views above remain the same, but the shared datasets containing PII are now collected in the user_data namespace on the datasets page of the Marquez UI:  Namespaces offer organizations a range of insights into how their teams are accessing and transforming sensitive data. "},{"title":"How to Learn More​","type":1,"pageTitle":"What's in a Namespace?","url":"/blog/whats-in-a-namespace#how-to-learn-more","content":"If you’re new to OpenLineage and want to check out namespaces in action, a good entry point is the Getting Started guide. There you can learn about the core model, collect run-level metadata using Marquez as the HTTP backend, and explore lineage in the Marquez UI. Helpful resources for learning more about the namespaces include the spec, where Naming.md is the Rosetta stone for namespace construction and naming conventions in the project. We also welcome contributions to the project. One of the existing integrations might be a good place to start. Our growing list of partners includes Airflow, dbt, Dagster and Flink. Sound fun? Check out the new contributor guide to get started. "},{"title":"Acknowledgments​","type":1,"pageTitle":"What's in a Namespace?","url":"/blog/whats-in-a-namespace#acknowledgments","content":"Ross Turk (@rossturk) and Paweł Leszczyński (@pawel-big-lebowski) contributed valuable feedback and suggestions. Any faults are the author's own. "},{"title":"About OpenLineage","type":0,"sectionRef":"#","url":"/docs/","content":"","keywords":""},{"title":"Design​","type":1,"pageTitle":"About OpenLineage","url":"/docs/#design","content":"OpenLineage is an Open Standard for lineage metadata collection designed to record metadata for a job in execution. The standard defines a generic model of dataset, job, and run entities uniquely identified using consistent naming strategies. The core model is highly extensible via facets. A facet is user-defined metadata and enables entity enrichment. We encourage you to familiarize yourself with the core model below:  "},{"title":"How OpenLineage Benefits the Ecosystem​","type":1,"pageTitle":"About OpenLineage","url":"/docs/#how-openlineage-benefits-the-ecosystem","content":"Below, we illustrate the challenges of collecting lineage metadata from multiple sources, schedulers and/or data processing frameworks. We then outline the design benefits of defining an Open Standard for lineage metadata collection. BEFORE:​  Each project has to instrument its own custom metadata collection integration, therefore duplicating efforts.Integrations are external and can break with new versions of the underlying scheduler and/or data processing framework, requiring projects to ensure backwards compatibility. WITH OPENLINEAGE:​  Integration efforts are shared across projects.Integrations can be pushed to the underlying scheduler and/or data processing framework; no longer does one need to play catch up and ensure compatibility! "},{"title":"Scope​","type":1,"pageTitle":"About OpenLineage","url":"/docs/#scope","content":"OpenLineage defines the metadata for running jobs and their corresponding events. A configurable backend allows the user to choose what protocol to send the events to. "},{"title":"Core model​","type":1,"pageTitle":"About OpenLineage","url":"/docs/#core-model","content":" A facet is an atomic piece of metadata attached to one of the core entities. See the spec for more details. "},{"title":"Spec​","type":1,"pageTitle":"About OpenLineage","url":"/docs/#spec","content":"The specification is defined using OpenAPI and allows extension through custom facets. "},{"title":"Integrations​","type":1,"pageTitle":"About OpenLineage","url":"/docs/#integrations","content":"The OpenLineage repository contains integrations with several systems. Apache AirflowApache FlinkApache SparkDagsterdbtSQL "},{"title":"Related projects​","type":1,"pageTitle":"About OpenLineage","url":"/docs/#related-projects","content":"Marquez: Marquez is an LF AI &amp; DATA project to collect, aggregate, and visualize a data ecosystem's metadata. It is the reference implementation of the OpenLineage API. OpenLineage collection implementation Egeria: Egeria Open Metadata and Governance. A metadata bus. "},{"title":"Setup a development environment","type":0,"sectionRef":"#","url":"/docs/development/developing/java/setup","content":"Setup a development environment info This page needs your contribution! Please contribute new examples using the edit link at the bottom.","keywords":""},{"title":"Developing With OpenLineage","type":0,"sectionRef":"#","url":"/docs/development/developing/","content":"","keywords":""},{"title":"Clients​","type":1,"pageTitle":"Developing With OpenLineage","url":"/docs/development/developing/#clients","content":"For Python and Java, we've created clients that you can use to properly create and emit OpenLineage events to HTTP, Kafka, and other consumers. "},{"title":"API Documentation​","type":1,"pageTitle":"Developing With OpenLineage","url":"/docs/development/developing/#api-documentation","content":"OpenAPI documentationJava Doc "},{"title":"Common Library (Python)​","type":1,"pageTitle":"Developing With OpenLineage","url":"/docs/development/developing/#common-library-python","content":"Getting lineage from systems like BigQuery or Redshift isn't necessarily tied to orchestrator or processing engine you're using. For this reason, we've extracted that functionality from our Airflow library and packaged it for separate use. "},{"title":"Environment Variables​","type":1,"pageTitle":"Developing With OpenLineage","url":"/docs/development/developing/#environment-variables","content":"The following environment variables are available commonly for both Java and Python languages. Name\tDescription\tSinceOPENLINEAGE_API_KEY\tThe optional API key to be set on each lineage request. This will be set as a Bearer token in case authentication is required. OPENLINEAGE_CONFIG\tThe optional path to locate the configuration file. The configuration file is in YAML format. Example: openlineage.yml OPENLINEAGE_DISABLED\tWhen set to true, will prevent OpenLineage from emitting events to the receiving backend\t0.9.0 OPENLINEAGE_URL\tThe URL for the HTTP transport of where to emit lineage events to. If not yet, no lineage data will be emitted, and event data (JSON) will be written to standard output. Example: http://localhost:8080\t "},{"title":"SQL parser​","type":1,"pageTitle":"Developing With OpenLineage","url":"/docs/development/developing/#sql-parser","content":"We've created SQL parser that allows you to extract lineage from SQL statements. The parser is implemented in Rust; however, it's also available as a Python library. You can take a look at its code on GitHub. "},{"title":"Setup a development environment","type":0,"sectionRef":"#","url":"/docs/development/developing/python/setup","content":"","keywords":""},{"title":"Docker Compose development environment​","type":1,"pageTitle":"Setup a development environment","url":"/docs/development/developing/python/setup#docker-compose-development-environment","content":"There is also possibility to create local Docker-based development environment that has OpenLineage libraries setup along with Airflow and some helpful services. To do that you should run run-dev-airflow.sh script located here. The script uses the same Docker Compose files as integration tests. Two main differences are: it runs in non-blocking wayit mounts OpenLineage Python packages as editable and mounted to Airflow containers. This allows to change code and test it live without need to rebuild whole environment. When using above script, you can add the -i flag or --attach-integration flag. This can be helpful when you need to run arbitrary integration tests during development. For example, the following command run in the integration container... python -m pytest test_integration.py::test_integration[great_expectations_validation-requests/great_expectations.json]  ...runs a single test which you can repeat after changes in code. "},{"title":"Logging","type":0,"sectionRef":"#","url":"/docs/development/developing/java/troubleshooting/logging","content":"","keywords":""},{"title":"Maven​","type":1,"pageTitle":"Logging","url":"/docs/development/developing/java/troubleshooting/logging#maven","content":"pom.xml  &lt;dependencies&gt; ... &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-api&lt;/artifactId&gt; &lt;version&gt;2.7&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.7&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-slf4j-impl&lt;/artifactId&gt; &lt;version&gt;2.7&lt;/version&gt; &lt;/dependency&gt; ... &lt;/dependencies&gt;  "},{"title":"Gradle​","type":1,"pageTitle":"Logging","url":"/docs/development/developing/java/troubleshooting/logging#gradle","content":"build.gradle dependencies { ... implementation &quot;org.apache.logging.log4j:log4j-api:2.7&quot; implementation &quot;org.apache.logging.log4j:log4j-core:2.7&quot; implementation &quot;org.apache.logging.log4j:log4j-slf4j-impl:2.7&quot; ... }  You also need to create a log4j configuration file, log4j2.properties on the classpath. Here is the sample log configuration. # Set to debug or trace if log4j initialization is failing status = warn # Name of the configuration name = ConsoleLogConfigDemo # Console appender configuration appender.console.type = Console appender.console.name = consoleLogger appender.console.layout.type = PatternLayout appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n # Root logger level rootLogger.level = debug # Root logger referring to console appender rootLogger.appenderRef.stdout.ref = consoleLogger  Re-compiling and running the ol.test.OpenLineageClientTest again will produce the following outputs: 2022-12-07 08:57:24 INFO OpenLineageClientTest:33 - Running OpenLineage Client Test... 2022-12-07 08:57:25 DEBUG HttpTransport:96 - POST http://localhost:5000/api/v1/lineage: {&quot;eventType&quot;:&quot;START&quot;,&quot;eventTime&quot;:&quot;2022-12-07T14:57:25.072781Z&quot;,&quot;run&quot;:{&quot;runId&quot;:&quot;0142c998-3416-49e7-92aa-d025c4c93697&quot;,&quot;facets&quot;:{&quot;nominalTime&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/NominalTimeRunFacet.json#/$defs/NominalTimeRunFacet&quot;,&quot;nominalStartTime&quot;:&quot;2022-12-07T14:57:25.072781Z&quot;,&quot;nominalEndTime&quot;:&quot;2022-12-07T14:57:25.072781Z&quot;}}},&quot;job&quot;:{&quot;namespace&quot;:&quot;namespace&quot;,&quot;name&quot;:&quot;jobName&quot;,&quot;facets&quot;:{}},&quot;inputs&quot;:[{&quot;namespace&quot;:&quot;ins&quot;,&quot;name&quot;:&quot;input&quot;,&quot;facets&quot;:{&quot;version&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasetVersionDatasetFacet.json#/$defs/DatasetVersionDatasetFacet&quot;,&quot;datasetVersion&quot;:&quot;input-version&quot;}},&quot;inputFacets&quot;:{&quot;dataQualityMetrics&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DataQualityMetricsInputDatasetFacet.json#/$defs/DataQualityMetricsInputDatasetFacet&quot;,&quot;rowCount&quot;:10,&quot;bytes&quot;:20,&quot;columnMetrics&quot;:{&quot;mycol&quot;:{&quot;nullCount&quot;:1,&quot;distinctCount&quot;:10,&quot;sum&quot;:3000.0,&quot;count&quot;:10.0,&quot;min&quot;:5.0,&quot;max&quot;:30.0,&quot;quantiles&quot;:{&quot;25&quot;:52.0}}}}}}],&quot;outputs&quot;:[{&quot;namespace&quot;:&quot;ons&quot;,&quot;name&quot;:&quot;output&quot;,&quot;facets&quot;:{&quot;version&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasetVersionDatasetFacet.json#/$defs/DatasetVersionDatasetFacet&quot;,&quot;datasetVersion&quot;:&quot;output-version&quot;}},&quot;outputFacets&quot;:{&quot;outputStatistics&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/OutputStatisticsOutputDatasetFacet.json#/$defs/OutputStatisticsOutputDatasetFacet&quot;,&quot;rowCount&quot;:10,&quot;size&quot;:20}}}],&quot;producer&quot;:&quot;producer&quot;,&quot;schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-4/OpenLineage.json#/$defs/RunEvent&quot;} 2022-12-07 08:57:25 DEBUG HttpTransport:96 - POST http://localhost:5000/api/v1/lineage: {&quot;eventType&quot;:&quot;COMPLETE&quot;,&quot;eventTime&quot;:&quot;2022-12-07T14:57:25.42041Z&quot;,&quot;run&quot;:{&quot;runId&quot;:&quot;0142c998-3416-49e7-92aa-d025c4c93697&quot;,&quot;facets&quot;:{&quot;nominalTime&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/NominalTimeRunFacet.json#/$defs/NominalTimeRunFacet&quot;,&quot;nominalStartTime&quot;:&quot;2022-12-07T14:57:25.42041Z&quot;,&quot;nominalEndTime&quot;:&quot;2022-12-07T14:57:25.42041Z&quot;}}},&quot;job&quot;:{&quot;namespace&quot;:&quot;namespace&quot;,&quot;name&quot;:&quot;jobName&quot;,&quot;facets&quot;:{}},&quot;inputs&quot;:[{&quot;namespace&quot;:&quot;ins&quot;,&quot;name&quot;:&quot;input&quot;,&quot;facets&quot;:{&quot;version&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasetVersionDatasetFacet.json#/$defs/DatasetVersionDatasetFacet&quot;,&quot;datasetVersion&quot;:&quot;input-version&quot;}},&quot;inputFacets&quot;:{&quot;dataQualityMetrics&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DataQualityMetricsInputDatasetFacet.json#/$defs/DataQualityMetricsInputDatasetFacet&quot;,&quot;rowCount&quot;:10,&quot;bytes&quot;:20,&quot;columnMetrics&quot;:{&quot;mycol&quot;:{&quot;nullCount&quot;:1,&quot;distinctCount&quot;:10,&quot;sum&quot;:3000.0,&quot;count&quot;:10.0,&quot;min&quot;:5.0,&quot;max&quot;:30.0,&quot;quantiles&quot;:{&quot;25&quot;:52.0}}}}}}],&quot;outputs&quot;:[{&quot;namespace&quot;:&quot;ons&quot;,&quot;name&quot;:&quot;output&quot;,&quot;facets&quot;:{&quot;version&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasetVersionDatasetFacet.json#/$defs/DatasetVersionDatasetFacet&quot;,&quot;datasetVersion&quot;:&quot;output-version&quot;}},&quot;outputFacets&quot;:{&quot;outputStatistics&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/OutputStatisticsOutputDatasetFacet.json#/$defs/OutputStatisticsOutputDatasetFacet&quot;,&quot;rowCount&quot;:10,&quot;size&quot;:20}}}],&quot;producer&quot;:&quot;producer&quot;,&quot;schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-4/OpenLineage.json#/$defs/RunEvent&quot;}  Logs will also produce meaningful error messages when something does not work correctly. For example, if the backend server does not exist, you would get the following messages in your console output: 2022-12-07 09:15:16 INFO OpenLineageClientTest:33 - Running OpenLineage Client Test... 2022-12-07 09:15:16 DEBUG HttpTransport:96 - POST http://localhost:5000/api/v1/lineage: {&quot;eventType&quot;:&quot;START&quot;,&quot;eventTime&quot;:&quot;2022-12-07T15:15:16.668979Z&quot;,&quot;run&quot;:{&quot;runId&quot;:&quot;69861937-55ba-43f5-ab5e-fe78ef6a283d&quot;,&quot;facets&quot;:{&quot;nominalTime&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/NominalTimeRunFacet.json#/$defs/NominalTimeRunFacet&quot;,&quot;nominalStartTime&quot;:&quot;2022-12-07T15:15:16.668979Z&quot;,&quot;nominalEndTime&quot;:&quot;2022-12-07T15:15:16.668979Z&quot;}}},&quot;job&quot;:{&quot;namespace&quot;:&quot;namespace&quot;,&quot;name&quot;:&quot;jobName&quot;,&quot;facets&quot;:{}},&quot;inputs&quot;:[{&quot;namespace&quot;:&quot;ins&quot;,&quot;name&quot;:&quot;input&quot;,&quot;facets&quot;:{&quot;version&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasetVersionDatasetFacet.json#/$defs/DatasetVersionDatasetFacet&quot;,&quot;datasetVersion&quot;:&quot;input-version&quot;}},&quot;inputFacets&quot;:{&quot;dataQualityMetrics&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DataQualityMetricsInputDatasetFacet.json#/$defs/DataQualityMetricsInputDatasetFacet&quot;,&quot;rowCount&quot;:10,&quot;bytes&quot;:20,&quot;columnMetrics&quot;:{&quot;mycol&quot;:{&quot;nullCount&quot;:1,&quot;distinctCount&quot;:10,&quot;sum&quot;:3000.0,&quot;count&quot;:10.0,&quot;min&quot;:5.0,&quot;max&quot;:30.0,&quot;quantiles&quot;:{&quot;25&quot;:52.0}}}}}}],&quot;outputs&quot;:[{&quot;namespace&quot;:&quot;ons&quot;,&quot;name&quot;:&quot;output&quot;,&quot;facets&quot;:{&quot;version&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasetVersionDatasetFacet.json#/$defs/DatasetVersionDatasetFacet&quot;,&quot;datasetVersion&quot;:&quot;output-version&quot;}},&quot;outputFacets&quot;:{&quot;outputStatistics&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/OutputStatisticsOutputDatasetFacet.json#/$defs/OutputStatisticsOutputDatasetFacet&quot;,&quot;rowCount&quot;:10,&quot;size&quot;:20}}}],&quot;producer&quot;:&quot;producer&quot;,&quot;schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-4/OpenLineage.json#/$defs/RunEvent&quot;} io.openlineage.client.OpenLineageClientException: org.apache.http.conn.HttpHostConnectException: Connect to localhost:5000 [localhost/127.0.0.1, localhost/0:0:0:0:0:0:0:1] failed: Connection refused at io.openlineage.client.transports.HttpTransport.emit(HttpTransport.java:113) at io.openlineage.client.OpenLineageClient.emit(OpenLineageClient.java:42) at ol.test.OpenLineageClientTest.main(OpenLineageClientTest.java:48) Caused by: org.apache.http.conn.HttpHostConnectException: Connect to localhost:5000 [localhost/127.0.0.1, localhost/0:0:0:0:0:0:0:1] failed: Connection refused at org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:156) at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:376) at org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393) at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236) at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186) at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:89) at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110) at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:108) at io.openlineage.client.transports.HttpTransport.emit(HttpTransport.java:108) ... 2 more Caused by: java.net.ConnectException: Connection refused at java.base/sun.nio.ch.Net.pollConnect(Native Method) at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672) at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:542) at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:585) at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327) at java.base/java.net.Socket.connect(Socket.java:666) at org.apache.http.conn.socket.PlainConnectionSocketFactory.connectSocket(PlainConnectionSocketFactory.java:75) at org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142) ... 12 more  If you wish to output loggigng message to a file, you can modify the basic configuration by adding a file appender configuration as follows: # Set to debug or trace if log4j initialization is failing status = warn # Name of the configuration name = ConsoleLogConfigDemo # Console appender configuration appender.console.type = Console appender.console.name = consoleLogger appender.console.layout.type = PatternLayout appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n # File appender configuration appender.file.type = File appender.file.name = fileLogger appender.file.fileName = app.log appender.file.layout.type = PatternLayout appender.file.layout.pattern = %d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n # Root logger level rootLogger.level = debug # Root logger referring to console appender rootLogger.appenderRef.stdout.ref = consoleLogger rootLogger.appenderRef.file.ref = fileLogger  And the logs will be saved to a file app.log. Outputting logs using log4j2 is just one way of doing it, so below are some additional resources of undersatnding how Java logging works, and other ways to output the logs. "},{"title":"Further readings​","type":1,"pageTitle":"Logging","url":"/docs/development/developing/java/troubleshooting/logging#further-readings","content":"https://www.baeldung.com/java-logging-introhttps://www.baeldung.com/slf4j-with-log4j2-logback#Log4j2https://mkyong.com/logging/log4j2-properties-example/ "},{"title":"Airflow","type":0,"sectionRef":"#","url":"/docs/development/developing/python/tests/airflow","content":"","keywords":""},{"title":"Unit tests​","type":1,"pageTitle":"Airflow","url":"/docs/development/developing/python/tests/airflow#unit-tests","content":"In order to make running unit tests against multiple Airflow versions easier there is possibility to use tox. To run unit tests against all configured Airflow versions just run: tox  You can also list existing environments with: tox -l  that should list: py3-airflow-2.1.4 py3-airflow-2.2.4 py3-airflow-2.3.4 py3-airflow-2.4.3 py3-airflow.2.5.0  Then you can run tests in chosen environment, e.g.: tox -e py3-airflow-2.3.4  setup.cfg contains tox-related configuration. By default tox command runs: flake8 lintingpytest command Additionally, outside of tox you should run mypy static code analysis. You can do that with: python -m mypy openlineage  "},{"title":"Integration tests​","type":1,"pageTitle":"Airflow","url":"/docs/development/developing/python/tests/airflow#integration-tests","content":"Integration tests are located in tests/integration/tests directory. They require running Docker containers to provision local test environment: Airflow components (worker, scheduler), databases (PostgreSQL, MySQL) and OpenLineage events consumer. How to run​ Integration tests require usage of docker compose. There are scripts prepared to make build images and run tests easier. AIRFLOW_IMAGE=&lt;name-of-airflow-image&gt; ./tests/integration/docker/up.sh  e.g. AIRFLOW_IMAGE=apache/airflow:2.3.4-python3.7 ./tests/integration/docker/up.sh  What tests are ran​ The actual setup is to run all defined Airflow DAGs, collect OpenLineage events and check if they meet requirements. The test you should pay most attention to is test_integration. It compares produced events to expected JSON structures recursively, with a respect if fields are not missing. Some of the tests are skipped if database connection specific environment variables are not set. The example is set of SNOWFLAKE_PASSWORD and SNOWFLAKE_ACCOUNT_ID variables. View stored OpenLineage events​ OpenLineage events produced from Airflow runs are stored locally in ./tests/integration/tests/events directory. The files are not overwritten, rather new events are appended to existing files. Example how to add new integration test​ Let's take following CustomOperator for which we should add CustomExtractor and test it. First we create DAG in integration tests DAGs folder: airflow/tests/integration/tests/airflow/dags. from airflow.models import BaseOperator from airflow.utils.dates import days_ago from airflow import DAG default_args = { 'depends_on_past': False, 'start_date': days_ago(7) } dag = DAG( 'custom_extractor', schedule_interval='@once', default_args=default_args ) class CustomOperator(BaseOperator): def execute(self, context: Any): for i in range(10): print(i) t1 = CustomOperator( task_id='custom_extractor', dag=dag )  In the same folder we create custom_extractor.py: from typing import Union, Optional, List from openlineage.client.run import Dataset from openlineage.airflow.extractors import TaskMetadata from openlineage.airflow.extractors.base import BaseExtractor class CustomExtractor(BaseExtractor): @classmethod def get_operator_classnames(cls) -&gt; List[str]: return ['CustomOperator'] def extract(self) -&gt; Union[Optional[TaskMetadata], List[TaskMetadata]]: return TaskMetadata( &quot;test&quot;, inputs=[ Dataset( namespace=&quot;test&quot;, name=&quot;dataset&quot;, facets={} ) ] )  Typically we want to compare produced metadata against expected. In order to do that we create JSON file custom_extractor.json in airflow/tests/integration/requests:  [{ &quot;eventType&quot;: &quot;START&quot;, &quot;inputs&quot;: [{ &quot;facets&quot;: {}, &quot;name&quot;: &quot;dataset&quot;, &quot;namespace&quot;: &quot;test&quot; }], &quot;job&quot;: { &quot;facets&quot;: { &quot;documentation&quot;: { &quot;description&quot;: &quot;Test dag.&quot; } }, &quot;name&quot;: &quot;custom_extractor.custom_extractor&quot;, &quot;namespace&quot;: &quot;food_delivery&quot; }, &quot;run&quot;: { &quot;facets&quot;: { &quot;airflow_runArgs&quot;: { &quot;externalTrigger&quot;: false }, &quot;parent&quot;: { &quot;job&quot;: { &quot;name&quot;: &quot;custom_extractor&quot;, &quot;namespace&quot;: &quot;food_delivery&quot; } } } } }, { &quot;eventType&quot;: &quot;COMPLETE&quot;, &quot;inputs&quot;: [{ &quot;facets&quot;: {}, &quot;name&quot;: &quot;dataset&quot;, &quot;namespace&quot;: &quot;test&quot; }], &quot;job&quot;: { &quot;facets&quot;: {}, &quot;name&quot;: &quot;custom_extractor.custom_extractor&quot;, &quot;namespace&quot;: &quot;food_delivery&quot; } } ]  and add parameter for test_integration in airflow/tests/integration/test_integration.py: (&quot;source_code_dag&quot;, &quot;requests/source_code.json&quot;), + (&quot;custom_extractor&quot;, &quot;requests/custom_extractor.json&quot;), (&quot;unknown_operator_dag&quot;, &quot;requests/unknown_operator.json&quot;),  That should setup a check for existence of both START and COMPLETE events, custom input facet and correct job facet. Full example can be found in source code available in integration tests directory. "},{"title":"Java","type":0,"sectionRef":"#","url":"/docs/client/java","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Java","url":"/docs/client/java#overview","content":"The OpenLineage Java client enables the creation of lineage metadata events with Java code. The core data structures currently offered by the client are the RunEvent, RunState, Run, Job, Dataset, and Transport classes, along with various Facets that can come under run, job, and dataset. There are various transport classes that the library provides that carry the lineage events into various target endpoints (e.g. HTTP). You can also use the Java client to create your own custom integrations. "},{"title":"Installation​","type":1,"pageTitle":"Java","url":"/docs/client/java#installation","content":"Java client is provided as library that can either be imported into your Java project using Maven or Gradle. Maven: &lt;dependency&gt; &lt;groupId&gt;io.openlineage&lt;/groupId&gt; &lt;artifactId&gt;openlineage-java&lt;/artifactId&gt; &lt;version&gt;0.12.0&lt;/version&gt; &lt;/dependency&gt;  or Gradle: implementation 'io.openlineage:openlineage-java:0.12.0'  For more information on the available versions of the openlineage-java, please refer to the maven repository. "},{"title":"Configuration​","type":1,"pageTitle":"Java","url":"/docs/client/java#configuration","content":"Use the following options to configure the client: An openlineage.yml in the user's current working directoryAn openlineage.yml under .openlineage/ in the user's home directory (ex: ~/.openlineage/openlineage.yml)Environment variables Note: By default, the client will give you sane defaults, but you can easily override them. "},{"title":"Environment Variables​","type":1,"pageTitle":"Java","url":"/docs/client/java#environment-variables","content":"The list of available environment varaibles can be found here. YAML transport: type: &lt;type&gt; # ... transport specific configuration  Here is an example of using HTTP transport for your client: transport: type: http url: http://localhost:5000  Note: For a full list of supported transports, see transports. "},{"title":"Transports​","type":1,"pageTitle":"Java","url":"/docs/client/java#transports","content":"The Transport abstraction defines an emit() method for OpenLineage.RunEvent. There are three built-in transports: ConsoleTransport, HttpTransport, and KafkaTransport. ConsoleTransport​ in YAML: transport: type: CONSOLE  You can also specify the ConsoleTransport when building a new client instance. OpenLineageClient client = OpenLineageClient.builder() .transport( new ConsoleTransport() .build();  HttpTransport​ in YAML: transport: type: HTTP url: http://localhost:5000 auth: type: api_key api_key: f38d2189-c603-4b46-bdea-e573a3b5a7d5  You can override the default configuration of the HttpTransport by specifying the URL and API key when creating a new client: OpenLineageClient client = OpenLineageClient.builder() .transport( HttpTransport.builder() .uri(&quot;http://localhost:5000&quot;) .apiKey(&quot;f38d2189-c603-4b46-bdea-e573a3b5a7d5&quot;) .build()) .build();  To configure the client with query params appended on each HTTP request, use: Map&lt;String, String&gt; queryParamsToAppend = Map.of( &quot;param0&quot;,&quot;value0&quot;, &quot;param1&quot;, &quot;value1&quot; ); // Connect to http://localhost:5000 OpenLineageClient client = OpenLineageClient.builder() .transport( HttpTransport.builder() .uri(&quot;http://localhost:5000&quot;, queryParamsToAppend) .apiKey(&quot;f38d2189-c603-4b46-bdea-e573a3b5a7d5&quot;) .build()) .build(); // Define a simple OpenLineage START or COMPLETE event OpenLineage.RunEvent startOrCompleteRun = ... // Emit OpenLineage event to http://localhost:5000/api/v1/lineage?param0=value0&amp;param1=value1 client.emit(startOrCompleteRun);  Alternatively, use the following environment variables to configure the HttpTransport: OPENLINEAGE_URL: the URL for the HTTP transport (default: http://localhost:8080)OPENLINEAGE_API_KEY: the API key to be set on each HTTP request Not everything will be supported while using this method. KafkaTransport​ in YAML: transport: type: Kafka topicName: openlineage.events # Kafka properties (see: http://kafka.apache.org/0100/documentation.html#producerconfigs) properties: bootstrap.servers: localhost:9092,another.host:9092 acks: all retries: 3 key.serializer: org.apache.kafka.common.serialization.StringSerializer value.serializer: org.apache.kafka.common.serialization.StringSerializer  KafkaTransport depends on you to provide artifact org.apache.kafka:kafka-clients:3.1.0 or compatible on your classpath. "},{"title":"KinesisTransport​","type":1,"pageTitle":"Java","url":"/docs/client/java#kinesistransport","content":"If transport.type is set to kinesis, then the below parameters would be read and used when building KinesisProducer. Also, KinesisTransport depends on you to provide artifact com.amazonaws:amazon-kinesis-producer:0.14.0 or compatible on your classpath. transport: type: kinesis streamName: some-stream-name region: us-east-2 topicName: openlineage.events properties: # some kinesis properties  "},{"title":"Error Handling via Transport​","type":1,"pageTitle":"Java","url":"/docs/client/java#error-handling-via-transport","content":"// Connect to http://localhost:5000 OpenLineageClient client = OpenLineageClient.builder() .transport( HttpTransport.builder() .uri(&quot;http://localhost:5000&quot;) .apiKey(&quot;f38d2189-c603-4b46-bdea-e573a3b5a7d5&quot;) .build()) .registerErrorHandler(new EmitErrorHandler() { @Override public void handleError(Throwable throwable) { // Handle emit error here } }).build();  "},{"title":"Defining Your Own Transport​","type":1,"pageTitle":"Java","url":"/docs/client/java#defining-your-own-transport","content":"OpenLineageClient client = OpenLineageClient.builder() .transport( new MyTransport() { @Override public void emit(OpenLineage.RunEvent runEvent) { // Add emit logic here } }).build();  "},{"title":"Usage​","type":1,"pageTitle":"Java","url":"/docs/client/java#usage","content":""},{"title":"1. Simple OpenLineage Client Test for Console Transport​","type":1,"pageTitle":"Java","url":"/docs/client/java#1-simple-openlineage-client-test-for-console-transport","content":"First, let's explore how we can create OpenLineage client instance, but not using any actual transport to emit the data yet, except only to our Console. This would be a good exercise to run tests and check the data payloads.  OpenLineageClient client = OpenLineageClient.builder() .transport(new ConsoleTransport()).build();  Also, we will then get a sample payload to produce a RunEvent:  // create one start event for testing RunEvent event = buildEvent(EventType.START);  Lastly, we will emit this event using the client that we instantiated\\:  // emit the event client.emit(event);  Here is the full source code of the test client application: package ol.test; import io.openlineage.client.OpenLineage; import io.openlineage.client.OpenLineageClient; import io.openlineage.client.OpenLineage.RunEvent; import io.openlineage.client.OpenLineage.InputDataset; import io.openlineage.client.OpenLineage.Job; import io.openlineage.client.OpenLineage.JobFacets; import io.openlineage.client.OpenLineage.OutputDataset; import io.openlineage.client.OpenLineage.Run; import io.openlineage.client.OpenLineage.RunFacets; import io.openlineage.client.OpenLineage.RunEvent.EventType; import io.openlineage.client.transports.ConsoleTransport; import java.net.URI; import java.time.ZoneId; import java.time.ZonedDateTime; import java.util.Arrays; import java.util.List; import java.util.UUID; /** * My first openlinage client code */ public class OpenLineageClientTest { public static void main( String[] args ) { try { OpenLineageClient client = OpenLineageClient.builder() .transport(new ConsoleTransport()).build(); // create one start event for testing RunEvent event = buildEvent(EventType.START); // emit the event client.emit(event); } catch (Exception e) { e.printStackTrace(); } } // sample code to build event public static RunEvent buildEvent(EventType eventType) { ZonedDateTime now = ZonedDateTime.now(ZoneId.of(&quot;UTC&quot;)); URI producer = URI.create(&quot;producer&quot;); OpenLineage ol = new OpenLineage(producer); UUID runId = UUID.randomUUID(); // run facets RunFacets runFacets = ol.newRunFacetsBuilder() .nominalTime( ol.newNominalTimeRunFacetBuilder() .nominalStartTime(now) .nominalEndTime(now) .build()) .build(); // a run is composed of run id, and run facets Run run = ol.newRunBuilder().runId(runId).facets(runFacets).build(); // job facets JobFacets jobFacets = ol.newJobFacetsBuilder().build(); // job String name = &quot;jobName&quot;; String namespace = &quot;namespace&quot;; Job job = ol.newJobBuilder().namespace(namespace).name(name).facets(jobFacets).build(); // input dataset List&lt;InputDataset&gt; inputs = Arrays.asList( ol.newInputDatasetBuilder() .namespace(&quot;ins&quot;) .name(&quot;input&quot;) .facets( ol.newDatasetFacetsBuilder() .version(ol.newDatasetVersionDatasetFacet(&quot;input-version&quot;)) .build()) .inputFacets( ol.newInputDatasetInputFacetsBuilder() .dataQualityMetrics( ol.newDataQualityMetricsInputDatasetFacetBuilder() .rowCount(10L) .bytes(20L) .columnMetrics( ol.newDataQualityMetricsInputDatasetFacetColumnMetricsBuilder() .put( &quot;mycol&quot;, ol.newDataQualityMetricsInputDatasetFacetColumnMetricsAdditionalBuilder() .count(10D) .distinctCount(10L) .max(30D) .min(5D) .nullCount(1L) .sum(3000D) .quantiles( ol.newDataQualityMetricsInputDatasetFacetColumnMetricsAdditionalQuantilesBuilder() .put(&quot;25&quot;, 52D) .build()) .build()) .build()) .build()) .build()) .build()); // output dataset List&lt;OutputDataset&gt; outputs = Arrays.asList( ol.newOutputDatasetBuilder() .namespace(&quot;ons&quot;) .name(&quot;output&quot;) .facets( ol.newDatasetFacetsBuilder() .version(ol.newDatasetVersionDatasetFacet(&quot;output-version&quot;)) .build()) .outputFacets( ol.newOutputDatasetOutputFacetsBuilder() .outputStatistics(ol.newOutputStatisticsOutputDatasetFacet(10L, 20L)) .build()) .build()); // run state udpate which encapsulates all - with START event in this case RunEvent runStateUpdate = ol.newRunEventBuilder() .eventType(OpenLineage.RunEvent.EventType.START) .eventTime(now) .run(run) .job(job) .inputs(inputs) .outputs(outputs) .build(); return runStateUpdate; } }  The result of running this will result in the following output from your Java application: [main] INFO io.openlineage.client.transports.ConsoleTransport - {&quot;eventType&quot;:&quot;START&quot;,&quot;eventTime&quot;:&quot;2022-08-05T15:11:24.858414Z&quot;,&quot;run&quot;:{&quot;runId&quot;:&quot;bb46bbc4-fb1a-495a-ad3b-8d837f566749&quot;,&quot;facets&quot;:{&quot;nominalTime&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/NominalTimeRunFacet.json#/$defs/NominalTimeRunFacet&quot;,&quot;nominalStartTime&quot;:&quot;2022-08-05T15:11:24.858414Z&quot;,&quot;nominalEndTime&quot;:&quot;2022-08-05T15:11:24.858414Z&quot;}}},&quot;job&quot;:{&quot;namespace&quot;:&quot;namespace&quot;,&quot;name&quot;:&quot;jobName&quot;,&quot;facets&quot;:{}},&quot;inputs&quot;:[{&quot;namespace&quot;:&quot;ins&quot;,&quot;name&quot;:&quot;input&quot;,&quot;facets&quot;:{&quot;version&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasetVersionDatasetFacet.json#/$defs/DatasetVersionDatasetFacet&quot;,&quot;datasetVersion&quot;:&quot;input-version&quot;}},&quot;inputFacets&quot;:{&quot;dataQualityMetrics&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DataQualityMetricsInputDatasetFacet.json#/$defs/DataQualityMetricsInputDatasetFacet&quot;,&quot;rowCount&quot;:10,&quot;bytes&quot;:20,&quot;columnMetrics&quot;:{&quot;mycol&quot;:{&quot;nullCount&quot;:1,&quot;distinctCount&quot;:10,&quot;sum&quot;:3000.0,&quot;count&quot;:10.0,&quot;min&quot;:5.0,&quot;max&quot;:30.0,&quot;quantiles&quot;:{&quot;25&quot;:52.0}}}}}}],&quot;outputs&quot;:[{&quot;namespace&quot;:&quot;ons&quot;,&quot;name&quot;:&quot;output&quot;,&quot;facets&quot;:{&quot;version&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasetVersionDatasetFacet.json#/$defs/DatasetVersionDatasetFacet&quot;,&quot;datasetVersion&quot;:&quot;output-version&quot;}},&quot;outputFacets&quot;:{&quot;outputStatistics&quot;:{&quot;_producer&quot;:&quot;producer&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/OutputStatisticsOutputDatasetFacet.json#/$defs/OutputStatisticsOutputDatasetFacet&quot;,&quot;rowCount&quot;:10,&quot;size&quot;:20}}}],&quot;producer&quot;:&quot;producer&quot;,&quot;schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-2/OpenLineage.json#/$defs/RunEvent&quot;}  "},{"title":"2. Simple OpenLineage Client Test for Http Transport​","type":1,"pageTitle":"Java","url":"/docs/client/java#2-simple-openlineage-client-test-for-http-transport","content":"Now, using the same code base, we will change how the client application works by switching the Console transport into Http Transport as shown below. This code will now be able to send the OpenLineage events into a compatible backends such as Marquez. Before making this change and running it, make sure you have an instance of Marquez running on your local environment. Setting up and running Marquez can be found here. OpenLineageClient client = OpenLineageClient.builder() .transport( HttpTransport.builder() .uri(&quot;http://localhost:5000&quot;) .build()) .build();  If we ran the same application, you will now see the event data not emitted in the output console, but rather via the HTTP transport to the marquez backend that was running.  Notice that the Status of this job run will be in RUNNING state, as it will be in that state until it receives an end event that will close off its gaps. That is how the OpenLineage events would work. Now, let's change the previous example to have lineage event doing a complete cycle of START -&gt; COMPLETE: package ol.test; import io.openlineage.client.OpenLineage; import io.openlineage.client.OpenLineageClient; import io.openlineage.client.OpenLineage.RunEvent; import io.openlineage.client.OpenLineage.InputDataset; import io.openlineage.client.OpenLineage.Job; import io.openlineage.client.OpenLineage.JobFacets; import io.openlineage.client.OpenLineage.OutputDataset; import io.openlineage.client.OpenLineage.Run; import io.openlineage.client.OpenLineage.RunFacets; import io.openlineage.client.OpenLineage.RunEvent.EventType; import io.openlineage.client.transports.HttpTransport; import java.net.URI; import java.time.ZoneId; import java.time.ZonedDateTime; import java.util.Arrays; import java.util.List; import java.util.UUID; /** * My first openlinage client code */ public class OpenLineageClientTest { public static void main( String[] args ) { try { OpenLineageClient client = OpenLineageClient.builder() .transport( HttpTransport.builder() .uri(&quot;http://localhost:5000&quot;) .build()) .build(); // create one start event for testing RunEvent event = buildEvent(EventType.START, null); // emit the event client.emit(event); // another event to COMPLETE the run event = buildEvent(EventType.COMPLETE, event.getRun().getRunId()); // emit the second COMPLETE event client.emit(event); } catch (Exception e) { e.printStackTrace(); } } // sample code to build event public static RunEvent buildEvent(EventType eventType, UUID runId) { ZonedDateTime now = ZonedDateTime.now(ZoneId.of(&quot;UTC&quot;)); URI producer = URI.create(&quot;producer&quot;); OpenLineage ol = new OpenLineage(producer); if (runId == null) { runId = UUID.randomUUID(); } // run facets RunFacets runFacets = ol.newRunFacetsBuilder() .nominalTime( ol.newNominalTimeRunFacetBuilder() .nominalStartTime(now) .nominalEndTime(now) .build()) .build(); // a run is composed of run id, and run facets Run run = ol.newRunBuilder().runId(runId).facets(runFacets).build(); // job facets JobFacets jobFacets = ol.newJobFacetsBuilder().build(); // job String name = &quot;jobName&quot;; String namespace = &quot;namespace&quot;; Job job = ol.newJobBuilder().namespace(namespace).name(name).facets(jobFacets).build(); // input dataset List&lt;InputDataset&gt; inputs = Arrays.asList( ol.newInputDatasetBuilder() .namespace(&quot;ins&quot;) .name(&quot;input&quot;) .facets( ol.newDatasetFacetsBuilder() .version(ol.newDatasetVersionDatasetFacet(&quot;input-version&quot;)) .build()) .inputFacets( ol.newInputDatasetInputFacetsBuilder() .dataQualityMetrics( ol.newDataQualityMetricsInputDatasetFacetBuilder() .rowCount(10L) .bytes(20L) .columnMetrics( ol.newDataQualityMetricsInputDatasetFacetColumnMetricsBuilder() .put( &quot;mycol&quot;, ol.newDataQualityMetricsInputDatasetFacetColumnMetricsAdditionalBuilder() .count(10D) .distinctCount(10L) .max(30D) .min(5D) .nullCount(1L) .sum(3000D) .quantiles( ol.newDataQualityMetricsInputDatasetFacetColumnMetricsAdditionalQuantilesBuilder() .put(&quot;25&quot;, 52D) .build()) .build()) .build()) .build()) .build()) .build()); // output dataset List&lt;OutputDataset&gt; outputs = Arrays.asList( ol.newOutputDatasetBuilder() .namespace(&quot;ons&quot;) .name(&quot;output&quot;) .facets( ol.newDatasetFacetsBuilder() .version(ol.newDatasetVersionDatasetFacet(&quot;output-version&quot;)) .build()) .outputFacets( ol.newOutputDatasetOutputFacetsBuilder() .outputStatistics(ol.newOutputStatisticsOutputDatasetFacet(10L, 20L)) .build()) .build()); // run state udpate which encapsulates all - with START event in this case RunEvent runStateUpdate = ol.newRunEventBuilder() .eventType(eventType) .eventTime(now) .run(run) .job(job) .inputs(inputs) .outputs(outputs) .build(); return runStateUpdate; } }  Now, when you run this application, the Marquez would have an output that would looke like this:  "},{"title":"Client","type":0,"sectionRef":"#","url":"/docs/development/developing/python/tests/client","content":"Client info This page needs your contribution! Please contribute new examples using the edit link at the bottom. There are unit tests available for OpenLineage Python client. You can run them with a simple pytest command with directory set to client base path.","keywords":""},{"title":"Python","type":0,"sectionRef":"#","url":"/docs/client/python","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Python","url":"/docs/client/python#overview","content":"The Python client is the basis of existing OpenLineage integrations such as Airflow and dbt. The client enables the creation of lineage metadata events with Python code. The core data structures currently offered by the client are the RunEvent, RunState, Run, Job, Dataset, and Transport classes. These either configure or collect data for the emission of lineage events. You can use the client to create your own custom integrations. "},{"title":"Installation​","type":1,"pageTitle":"Python","url":"/docs/client/python#installation","content":"Download the package using pip with pip install openlineage-python  To install the package from source, use python setup.py install  "},{"title":"Configuration​","type":1,"pageTitle":"Python","url":"/docs/client/python#configuration","content":"We recommend configuring the client with an openlineage.yml file that tells the client how to connect to an OpenLineage backend. You can make this file available to the client three ways: Set an environment variable to a file path: OPENLINEAGE_CONFIG=path/to/openlineage.yml.Put the file in the working directory.Put the file in $HOME/.openlineage. In openlineage.yml, use a standard transport interface to specify the transport type (http, console, kafka, file, or custom) and authorization parameters: transport: type: &quot;http&quot; url: &quot;https://backend:5000&quot; auth: type: &quot;api_key&quot; api_key: &quot;f048521b-dfe8-47cd-9c65-0cb07d57591e&quot;  The type property (required) is a fully qualified class name that can be imported. "},{"title":"Environment Variables​","type":1,"pageTitle":"Python","url":"/docs/client/python#environment-variables","content":"The list of available environment varaibles can be found here. "},{"title":"Built-in Transport Types​","type":1,"pageTitle":"Python","url":"/docs/client/python#built-in-transport-types","content":"HTTP​ type - string (required)url - string (required)endpoint - string specifying the endpoint to which events are sent. Default: api/v1/lineage (optional)timeout - float specifying a timeout value when sending an event. Default: 5 seconds. (optional)verify - boolean specifying whether the client should verify TLS certificates from the backend. Default: true. (optional)auth - dictionary specifying authentication options. Requires the type property. (optional) type - string specifying the &quot;api_key&quot; or the fully qualified class name of your TokenProvider. (required if auth is provided)api_key - string setting the Authentication HTTP header as the Bearer. (required if api_key is set) Example: transport: type: http url: https://backend:5000 endpoint: events/receive auth: type: api_key api_key: f048521b-dfe8-47cd-9c65-0cb07d57591e  Console​ type - string (required) Example: transport: type: console  Kafka​ Kafka transport requires confluent-kafka package to be additionally installed. It can be installed also by specifying kafka client extension: pip install openlineage-python[kafka] type - string (required)config - string containing a Kafka producer config (required)topic - string specifying the topic (required)flush - boolean specifying whether Kafka should flush after each event. Default: true. (optional) There's a caveat for using KafkaTransport with Airflow integration. In this integration, a Kafka producer needs to be created for each OpenLineage event. It happens due to the Airflow execution and plugin model, which requires us to send messages from worker processes. These are created dynamically for each task execution. Example: transport: type: kafka config: bootstrap.servers: mybroker acks: all retries: 3 topic: my_topic flush: true  File​ log_file_path - string specifying the path of the file (if append is true, a file path is expected, otherwise a file prefix is expected). (required)append - boolean . If set to True, each event will be appended to a single file (log_file_path); otherwise, all events will be written separately in distinct files suffixed by a timestring. Default: false. (optional) Example: transport: type: file log_file_path: ol_events_ append: false  "},{"title":"Custom Transport Type​","type":1,"pageTitle":"Python","url":"/docs/client/python#custom-transport-type","content":"To implement a custom transport, follow the instructions in transport.py. "},{"title":"Getting Started​","type":1,"pageTitle":"Python","url":"/docs/client/python#getting-started","content":"To try out the client, follow the steps below to install and explore OpenLineage, Marquez (the reference implementation of OpenLineage), and the client itself. Then, the instructions will show you how to use these tools to add a run event and datasets to an existing namespace. "},{"title":"Prerequisites​","type":1,"pageTitle":"Python","url":"/docs/client/python#prerequisites","content":"Docker 17.05+Docker Compose 1.29.1+Git (preinstalled on most versions of MacOS; verify your version with git version)4 GB of available memory (the minimum for Docker — more is strongly recommended) "},{"title":"Install OpenLineage and Marquez​","type":1,"pageTitle":"Python","url":"/docs/client/python#install-openlineage-and-marquez","content":"Clone the Marquez Github repository: git clone https://github.com/MarquezProject/marquez.git  "},{"title":"Install the Python client​","type":1,"pageTitle":"Python","url":"/docs/client/python#install-the-python-client","content":"pip install openlineage-python  "},{"title":"Start Docker and Marquez​","type":1,"pageTitle":"Python","url":"/docs/client/python#start-docker-and-marquez","content":"Start Docker Desktop Run Marquez with preloaded data: cd marquez ./docker/up.sh --seed  Marquez should be up and running at http://localhost:3000. Take a moment to explore Marquez to get a sense of how metadata is displayed in the UI. Namespaces – the global contexts for runs and datasets – can be found in the top right corner, and icons for jobs and runs can be found in a tray along the left side. Next, configure OpenLineage and add a script to your project that will generate a new job and new datasets within an existing namespace (here we’re using the food_delivery namespace that got passed to Marquez with the –seed argument we used earlier). Create a directory for your script: .. mkdir python_scripts &amp;&amp; cd python_scripts  In the python_scripts directory, create a Python script (we used the name generate_events.py for ours) and an openlineage.yml file. In openlineage.yml, define a transport type and URL to tell OpenLineage where and how to send metadata: transport: type: http url: http://localhost:5000  In generate_events.py, import the Python client and the methods needed to create a job and datasets. Also required (to create a run): the datetime and uuid packages: from openlineage.client.run import RunEvent, RunState, Run, Job, Dataset from openlineage.client import OpenLineageClient from datetime import datetime from uuid import uuid4  Then, in the same file, initialize the Python client: client = OpenLineageClient.from_environment()  It is also possible to specify parameters such as URL for client to connect to, without using environment variables or openlineage.yaml file, by directly setting it up when instantiating OpenLineageClient: client = OpenLineageClient(url=&quot;http://localhost:5000&quot;)  For more details about options to setup OpenLineageClient such as API tokens or HTTP transport settings, please refer to the following example Specify the producer of the new lineage metadata with a string: producer = “OpenLineage.io/website/blog”  Now you can create some basic dataset objects. These require a namespace and name: inventory = Dataset(namespace=“food_delivery”, name=“public.inventory”) menus = Dataset(namespace=“food_delivery”, name=“public.menus_1”) orders = Dataset(namespace=“food_delivery”, name=“public.orders_1”)  You can also create a job object (we’ve borrowed this one from the existing food_delivery namespace): job = Job(namespace=“food_delivery”, name=“example.order_data”)  To create a run object you’ll need to specify a unique ID: run = Run(str(uuid4()))  a START run event: client.emit( RunEvent( RunState.START, datetime.now().isoformat(), run, job, producer ) )  and, finally, a COMPLETE run event: client.emit( RunEvent( RunState.COMPLETE, datetime.now().isoformat(), run, job, producer, inputs=[inventory], outputs=[menus, orders], ) )  Now you have a complete script for creating datasets and a run event! Execute it in the terminal to send the metadata to Marquez: python3 generate_scripts.py  Marquez will update itself automatically, so the new job and datasets should now be visible in the UI. Clicking on the jobs icon (the icon with the three interlocking gears), will make the example.order_data job appear in the list of jobs:  When you click on the job, you will see a new map displaying the job, input and outputs we created with our script:  "},{"title":"Full Example Source Code​","type":1,"pageTitle":"Python","url":"/docs/client/python#full-example-source-code","content":"#!/usr/bin/env python3 from openlineage.client.run import ( RunEvent, RunState, Run, Job, Dataset, OutputDataset, InputDataset, ) from openlineage.client.client import OpenLineageClient, OpenLineageClientOptions from openlineage.client.facet import ( SqlJobFacet, SchemaDatasetFacet, SchemaField, OutputStatisticsOutputDatasetFacet, SourceCodeLocationJobFacet, NominalTimeRunFacet, DataQualityMetricsInputDatasetFacet, ColumnMetric, ) import uuid from datetime import datetime, timezone, timedelta import time from random import random PRODUCER = f&quot;https://github.com/openlineage-user&quot; namespace = &quot;python_client&quot; dag_name = &quot;user_trends&quot; url = &quot;http://mymarquez.host:5000&quot; api_key = &quot;1234567890ckcu028rzu5l&quot; client = OpenLineageClient( url=url, # optional api key in case marquez requires it. When running marquez in # your local environment, you usually do not need this. options=OpenLineageClientOptions(api_key=api_key), ) # generates job facet def job(job_name, sql, location): facets = {&quot;sql&quot;: SqlJobFacet(sql)} if location != None: facets.update( {&quot;sourceCodeLocation&quot;: SourceCodeLocationJobFacet(&quot;git&quot;, location)} ) return Job(namespace=namespace, name=job_name, facets=facets) # geneartes run racet def run(run_id, hour): return Run( runId=run_id, facets={ &quot;nominalTime&quot;: NominalTimeRunFacet( nominalStartTime=f&quot;2022-04-14T{twoDigits(hour)}:12:00Z&quot; ) }, ) # generates dataset def dataset(name, schema=None, ns=namespace): if schema == None: facets = {} else: facets = {&quot;schema&quot;: schema} return Dataset(namespace, name, facets) # generates output dataset def outputDataset(dataset, stats): output_facets = {&quot;stats&quot;: stats, &quot;outputStatistics&quot;: stats} return OutputDataset(dataset.namespace, dataset.name, dataset.facets, output_facets) # generates input dataset def inputDataset(dataset, dq): input_facets = { &quot;dataQuality&quot;: dq, } return InputDataset(dataset.namespace, dataset.name, dataset.facets, input_facets) def twoDigits(n): if n &lt; 10: result = f&quot;0{n}&quot; elif n &lt; 100: result = f&quot;{n}&quot; else: raise f&quot;error: {n}&quot; return result now = datetime.now(timezone.utc) # generates run Event def runEvents(job_name, sql, inputs, outputs, hour, min, location, duration): run_id = str(uuid.uuid4()) myjob = job(job_name, sql, location) myrun = run(run_id, hour) st = now + timedelta(hours=hour, minutes=min, seconds=20 + round(random() * 10)) end = st + timedelta(minutes=duration, seconds=20 + round(random() * 10)) started_at = st.isoformat() ended_at = end.isoformat() return ( RunEvent( eventType=RunState.START, eventTime=started_at, run=myrun, job=myjob, producer=PRODUCER, inputs=inputs, outputs=outputs, ), RunEvent( eventType=RunState.COMPLETE, eventTime=ended_at, run=myrun, job=myjob, producer=PRODUCER, inputs=inputs, outputs=outputs, ), ) # add run event to the events list def addRunEvents( events, job_name, sql, inputs, outputs, hour, minutes, location=None, duration=2 ): (start, complete) = runEvents( job_name, sql, inputs, outputs, hour, minutes, location, duration ) events.append(start) events.append(complete) events = [] # create dataset data for i in range(0, 5): user_counts = dataset(&quot;tmp_demo.user_counts&quot;) user_history = dataset( &quot;temp_demo.user_history&quot;, SchemaDatasetFacet( fields=[ SchemaField(name=&quot;id&quot;, type=&quot;BIGINT&quot;, description=&quot;the user id&quot;), SchemaField( name=&quot;email_domain&quot;, type=&quot;VARCHAR&quot;, description=&quot;the user id&quot; ), SchemaField(name=&quot;status&quot;, type=&quot;BIGINT&quot;, description=&quot;the user id&quot;), SchemaField( name=&quot;created_at&quot;, type=&quot;DATETIME&quot;, description=&quot;date and time of creation of the user&quot;, ), SchemaField( name=&quot;updated_at&quot;, type=&quot;DATETIME&quot;, description=&quot;the last time this row was updated&quot;, ), SchemaField( name=&quot;fetch_time_utc&quot;, type=&quot;DATETIME&quot;, description=&quot;the time the data was fetched&quot;, ), SchemaField( name=&quot;load_filename&quot;, type=&quot;VARCHAR&quot;, description=&quot;the original file this data was ingested from&quot;, ), SchemaField( name=&quot;load_filerow&quot;, type=&quot;INT&quot;, description=&quot;the row number in the original file&quot;, ), SchemaField( name=&quot;load_timestamp&quot;, type=&quot;DATETIME&quot;, description=&quot;the time the data was ingested&quot;, ), ] ), &quot;snowflake://&quot;, ) create_user_counts_sql = &quot;&quot;&quot;CREATE OR REPLACE TABLE TMP_DEMO.USER_COUNTS AS ( SELECT DATE_TRUNC(DAY, created_at) date, COUNT(id) as user_count FROM TMP_DEMO.USER_HISTORY GROUP BY date )&quot;&quot;&quot; # location of the source code location = &quot;https://github.com/some/airflow/dags/example/user_trends.py&quot; # run simulating Airflow DAG with snowflake operator addRunEvents( events, dag_name + &quot;.create_user_counts&quot;, create_user_counts_sql, [user_history], [user_counts], i, 11, location, ) for event in events: from openlineage.client.serde import Serde print(event) print(Serde.to_json(event)) # time.sleep(1) client.emit(event)  The resulting lineage events received by Marquez would look like this.  "},{"title":"Common","type":0,"sectionRef":"#","url":"/docs/development/developing/python/tests/common","content":"Common info This page needs your contribution! Please contribute new examples using the edit link at the bottom. There are unit tests available for OpenLineage common package. You can run them with a simple pytest command with directory set to package base path.","keywords":""},{"title":"Dagster","type":0,"sectionRef":"#","url":"/docs/development/developing/python/tests/dagster","content":"Dagster info This page needs your contribution! Please contribute new examples using the edit link at the bottom. There are unit tests available for Dagster integration. You can run them with a simple pytest command with directory set to integration base path.","keywords":""},{"title":"Frequently Asked Questions","type":0,"sectionRef":"#","url":"/docs/faq","content":"","keywords":""},{"title":"Is OpenLineage a metadata server?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#is-openlineage-a-metadata-server","content":"No. OpenLineage is, at its core, a specification for lineage metadata. But it also contains a collection of integrations, examples, and tools. If you are looking for a metadata server that can receive and analyze OpenLineage events, check out Marquez. "},{"title":"Is there room for another question on this page?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/faq#is-there-room-for-another-question-on-this-page","content":"You bet! There's always room. Submit an issue or pull request using the edit button at the bottom. "},{"title":"About These Guides","type":0,"sectionRef":"#","url":"/docs/guides/about","content":"About These Guides The following tutorials take you through the process of exploiting the lineage metadata provided by Marquez and OpenLineage to solve common data engineering problems and make new analytical and historical insights into your pipelines. The first tutorial, &quot;Using OpenLineage with Spark,&quot; provides an introduction to OpenLineage's integration with Apache Spark. You will learn how to use Marquez and the OpenLineage standard to produce lineage metadata about jobs and datasets created using Spark and BigQuery in a Jupyter notebook environment. The second tutorial, &quot;Using OpenLineage with Airflow,&quot; shows you how to use OpenLineage on Apache Airflow to produce data lineage on supported operators to emit lineage events to Marquez backend. The tutorial also introduces you to the OpenLineage proxy to monitor the event data being emitted. The third tutorial, &quot;Backfilling Airflow DAGs Using Marquez,&quot; shows you how to use Marquez's Airflow integration and the Marquez CLI to backfill failing runs with the help of lineage metadata. You will learn how data lineage can be used to automate the backfilling process. The fourth tutorial, &quot;Using Marquez with dbt,&quot; takes you through the process of setting up Marquez's dbt integration to harvest metadata produced by dbt. You will learn how to create a Marquez instance, install the integration, configure your dbt installation, and test the configuration using dbt.","keywords":""},{"title":"OpenLineage Proxy","type":0,"sectionRef":"#","url":"/docs/development/ol-proxy","content":"","keywords":""},{"title":"Accessing the proxy​","type":1,"pageTitle":"OpenLineage Proxy","url":"/docs/development/ol-proxy#accessing-the-proxy","content":"OpenLineage proxy can be obtained via github: git clone https://github.com/OpenLineage/OpenLineage.git cd OpenLineage/proxy  "},{"title":"Building the proxy​","type":1,"pageTitle":"OpenLineage Proxy","url":"/docs/development/ol-proxy#building-the-proxy","content":"To build the proxy jar, run $ ./gradlew build  The packaged jar file can be found under ./build/libs/ "},{"title":"Running the proxy​","type":1,"pageTitle":"OpenLineage Proxy","url":"/docs/development/ol-proxy#running-the-proxy","content":"OpenLineage Proxy requires configuration file named proxy.yml. There is an example that you can copy and name it as proxy.yml. cp proxy.example.yml proxy.yml  By default, the OpenLineage proxy uses the following ports: TCP port 8080 is available for the HTTP API server.TCP port 8081 is available for the admin interface. You can then run the proxy using gradlew: $ ./gradlew runShadow  "},{"title":"Monitoring OpenLineage events via Proxy​","type":1,"pageTitle":"OpenLineage Proxy","url":"/docs/development/ol-proxy#monitoring-openlineage-events-via-proxy","content":"When proxy is running, you can start sending your OpenLineage events just as the same way as you would be sending to any OpenLineage backend server. For example, in your URL for the OpenLineage backend, you can specify it as http://localhost:8080/api/v1/lineage. Once the message is sent to the proxy, you will see the OpenLineage message content (JSON) to the console output of the proxy. You can also specify in the configuration to store the messages into the log file. You might have noticed that OpenLineage client (python, java) simply requires http://localhost:8080 as the URL endpoint. This is possible because the client code adds the /api/v1/lineage internally before it makes the request. If you are not using OpenLineage client library to emit OpenLineage events, you must use the full URL in order for the proxy to receive the data correctly. "},{"title":"Forwarding the data​","type":1,"pageTitle":"OpenLineage Proxy","url":"/docs/development/ol-proxy#forwarding-the-data","content":"Not only the OpenLineage proxy is useful in receiving the monitoring the OpenLineage events, it can also be used to relay the events to other endpoints. Please see the example of how to set the proxy to relay the events via Kafka topic or HTTP endpoint. "},{"title":"Other ways to run OpenLineage Proxy​","type":1,"pageTitle":"OpenLineage Proxy","url":"/docs/development/ol-proxy#other-ways-to-run-openlineage-proxy","content":"You do not have to clone the git repo and build all the time. OpenLineage proxy is published and available in Maven Repository.You can also run OpenLineage Proxy as a docker container.There is also a helm chart for Kubernetes available. "},{"title":"Example Lineage Events","type":0,"sectionRef":"#","url":"/docs/development/examples","content":"","keywords":""},{"title":"Simple Examples​","type":1,"pageTitle":"Example Lineage Events","url":"/docs/development/examples#simple-examples","content":""},{"title":"START event with single input​","type":1,"pageTitle":"Example Lineage Events","url":"/docs/development/examples#start-event-with-single-input","content":"This is a START event with a single PostgreSQL input dataset. { &quot;eventType&quot;: &quot;START&quot;, &quot;eventTime&quot;: &quot;2020-12-28T19:52:00.001+10:00&quot;, &quot;run&quot;: { &quot;runId&quot;: &quot;d46e465b-d358-4d32-83d4-df660ff614dd&quot; }, &quot;job&quot;: { &quot;namespace&quot;: &quot;workshop&quot;, &quot;name&quot;: &quot;process_taxes&quot; }, &quot;inputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://workshop-db:None&quot;, &quot;name&quot;: &quot;workshop.public.taxes&quot; }], &quot;producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client&quot; }  "},{"title":"COMPLETE event with single output​","type":1,"pageTitle":"Example Lineage Events","url":"/docs/development/examples#complete-event-with-single-output","content":"This is a COMPLETE event with a single PostgreSQL output dataset. { &quot;eventType&quot;: &quot;COMPLETE&quot;, &quot;eventTime&quot;: &quot;2020-12-28T20:52:00.001+10:00&quot;, &quot;run&quot;: { &quot;runId&quot;: &quot;d46e465b-d358-4d32-83d4-df660ff614dd&quot; }, &quot;job&quot;: { &quot;namespace&quot;: &quot;workshop&quot;, &quot;name&quot;: &quot;process_taxes&quot; }, &quot;outputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://workshop-db:None&quot;, &quot;name&quot;: &quot;workshop.public.unpaid_taxes&quot; }], &quot;producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client&quot; }  "},{"title":"Complex Examples​","type":1,"pageTitle":"Example Lineage Events","url":"/docs/development/examples#complex-examples","content":""},{"title":"START event with Facets (run and job)​","type":1,"pageTitle":"Example Lineage Events","url":"/docs/development/examples#start-event-with-facets-run-and-job","content":"This is a START event with run and job facets of Apache Airflow. { &quot;eventType&quot;: &quot;START&quot;, &quot;eventTime&quot;: &quot;2020-12-28T19:52:00.001+10:00&quot;, &quot;run&quot;: { &quot;runId&quot;: &quot;d46e465b-d358-4d32-83d4-df660ff614dd&quot; &quot;facets&quot;: { &quot;airflow_runArgs&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.10.0/integration/airflow&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/BaseFacet&quot;, &quot;externalTrigger&quot;: true }, &quot;nominalTime&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.10.0/integration/airflow&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/NominalTimeRunFacet&quot;, &quot;nominalStartTime&quot;: &quot;2022-07-29T14:14:31.458067Z&quot; }, &quot;parentRun&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.10.0/integration/airflow&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/ParentRunFacet&quot;, &quot;job&quot;: { &quot;name&quot;: &quot;etl_orders&quot;, &quot;namespace&quot;: &quot;cosmic_energy&quot; }, &quot;run&quot;: { &quot;runId&quot;: &quot;1ba6fdaa-fb80-36ce-9c5b-295f544ec462&quot; } } } }, &quot;job&quot;: { &quot;namespace&quot;: &quot;workshop&quot;, &quot;name&quot;: &quot;process_taxes&quot;, &quot;facets&quot;: { &quot;documentation&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.10.0/integration/airflow&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/DocumentationJobFacet&quot;, &quot;description&quot;: &quot;Process taxes.&quot; }, &quot;sql&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.10.0/integration/airflow&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/SqlJobFacet&quot;, &quot;query&quot;: &quot;INSERT into taxes values(1, 100, 1000, 4000);&quot; } }, }, &quot;inputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://workshop-db:None&quot;, &quot;name&quot;: &quot;workshop.public.taxes&quot; }], &quot;producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client&quot; }  "},{"title":"COMPLETE event with Facets (dataset)​","type":1,"pageTitle":"Example Lineage Events","url":"/docs/development/examples#complete-event-with-facets-dataset","content":"This is a COMPLETE event with dataset facet of Database table. { &quot;eventType&quot;: &quot;COMPLETE&quot;, &quot;eventTime&quot;: &quot;2020-12-28T20:52:00.001+10:00&quot;, &quot;run&quot;: { &quot;runId&quot;: &quot;d46e465b-d358-4d32-83d4-df660ff614dd&quot; }, &quot;job&quot;: { &quot;namespace&quot;: &quot;workshop&quot;, &quot;name&quot;: &quot;process_taxes&quot; }, &quot;outputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://workshop-db:None&quot;, &quot;name&quot;: &quot;workshop.public.unpaid_taxes&quot;, &quot;facets&quot;: { &quot;dataSource&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.10.0/integration/airflow&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/DataSourceDatasetFacet&quot;, &quot;name&quot;: &quot;postgres://workshop-db:None&quot;, &quot;uri&quot;: &quot;workshop-db&quot; }, &quot;schema&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.10.0/integration/airflow&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/SchemaDatasetFacet&quot;, &quot;fields&quot;: [ { &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;SERIAL PRIMARY KEY&quot; }, { &quot;name&quot;: &quot;tax_dt&quot;, &quot;type&quot;: &quot;TIMESTAMP NOT NULL&quot; }, { &quot;name&quot;: &quot;tax_item_id&quot;, &quot;type&quot;: &quot;INTEGER REFERENCES tax_itemsid&quot; }, { &quot;name&quot;: &quot;amount&quot;, &quot;type&quot;: &quot;INTEGER NOT NULL&quot; }, { &quot;name&quot;: &quot;ref_id&quot;, &quot;type&quot;: &quot;INTEGER REFERENCES refid&quot; }, { &quot;name&quot;: &quot;comment&quot;, &quot;type&quot;: &quot;TEXT&quot; } ] } } }], &quot;producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client&quot; }  "},{"title":"Backfilling Airflow DAGs Using Marquez","type":0,"sectionRef":"#","url":"/docs/guides/airflow-backfill-dags","content":"","keywords":""},{"title":"Exploring Lineage Metadata using Marquez​","type":1,"pageTitle":"Backfilling Airflow DAGs Using Marquez","url":"/docs/guides/airflow-backfill-dags#exploring-lineage-metadata-using-marquez","content":""},{"title":"Prerequisites​","type":1,"pageTitle":"Backfilling Airflow DAGs Using Marquez","url":"/docs/guides/airflow-backfill-dags#prerequisites","content":"Sample data (for the dataset used here, follow the instructions in the Write Sample Lineage Metadata to Marquez section of Marquez's quickstart guide)Docker 17.05+Docker DesktopDocker Composejq info If you are using macOS Monterey (macOS 12), port 5000 will have to be released by disabling the AirPlay Receiver. Also, port 3000 will need to be free if access to the Marquez web UI is desired. "},{"title":"Query the Lineage Graph​","type":1,"pageTitle":"Backfilling Airflow DAGs Using Marquez","url":"/docs/guides/airflow-backfill-dags#query-the-lineage-graph","content":"After running the seed command in the quickstart guide, check to make sure Marquez is up by visiting http://localhost:3000. The page should display an empty Marquez instance and a message saying there is no data. Also, it should be possible to see the server output from requests in the terminal window where Marquez is running. This window should remain open. As you progress through the tutorial, feel free to experiment with the web UI. Use truncated strings (e.g., &quot;example.etl_orders_7_days&quot; instead of &quot;job:food_delivery:example.etl_orders_7_days&quot;) to find the datasets referenced below. In Marquez, each dataset and job has its own globally unique node ID that can be used to query the lineage graph. The LineageAPI returns a set of nodes consisting of edges. An edge is directed and has a defined origin and destination. A lineage graph may contain the following node types: dataset:&lt;namespace&gt;:&lt;dataset&gt;, job:&lt;namespace&gt;:&lt;job&gt;. Start by querying the lineage graph of the seed data via the CLI. The etl_orders_7_days DAG has the node ID job:food_delivery:example.etl_orders_7_days. To see the graph, run the following in a new terminal window: $ curl -X GET &quot;http://localhost:5000/api/v1-beta/lineage?nodeId=job:food_delivery:example.etl_orders_7_days&quot;  Notice in the returned lineage graph that the DAG input datasets are public.categories, public.orders, and public.menus, while public.orders_7_days is the output dataset. The response should look something like this: { &quot;graph&quot;: [{ &quot;id&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;, &quot;type&quot;: &quot;JOB&quot;, &quot;data&quot;: { &quot;type&quot;: &quot;BATCH&quot;, &quot;id&quot;: { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;example.etl_orders_7_days&quot; }, &quot;name&quot;: &quot;example.etl_orders_7_days&quot;, &quot;createdAt&quot;: &quot;2021-06-06T14:50:13.931946Z&quot;, &quot;updatedAt&quot;: &quot;2021-06-06T14:57:54.037399Z&quot;, &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;inputs&quot;: [ {&quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.categories&quot;}, {&quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.menu_items&quot;}, {&quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.orders&quot;}, {&quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.menus&quot;} ], &quot;outputs&quot;: [ {&quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.orders_7_days&quot;} ], &quot;location&quot;: &quot;https://github.com/example/jobs/blob/2294bc15eb49071f38425dc927e48655530a2f2e/etl_orders_7_days.py&quot;, &quot;context&quot;: { &quot;sql&quot;: &quot;INSERT INTO orders_7_days (order_id, placed_on, discount_id, menu_id, restaurant_id, menu_item_id, category_id)\\n SELECT o.id AS order_id, o.placed_on, o.discount_id, m.id AS menu_id, m.restaurant_id, mi.id AS menu_item_id, c.id AS category_id\\n FROM orders AS o\\n INNER JOIN menu_items AS mi\\n ON menu_items.id = o.menu_item_id\\n INNER JOIN categories AS c\\n ON c.id = mi.category_id\\n INNER JOIN menu AS m\\n ON m.id = c.menu_id\\n WHERE o.placed_on &gt;= NOW() - interval '7 days';&quot; }, &quot;description&quot;: &quot;Loads newly placed orders weekly.&quot;, &quot;latestRun&quot;: { &quot;id&quot;: &quot;5c7f0dc4-d3c1-4f16-9ac3-dc86c5da37cc&quot;, &quot;createdAt&quot;: &quot;2021-06-06T14:50:36.853459Z&quot;, &quot;updatedAt&quot;: &quot;2021-06-06T14:57:54.037399Z&quot;, &quot;nominalStartTime&quot;: &quot;2021-06-06T14:54:00Z&quot;, &quot;nominalEndTime&quot;: &quot;2021-06-06T14:57:00Z&quot;, &quot;state&quot;: &quot;FAILED&quot;, &quot;startedAt&quot;: &quot;2021-06-06T14:54:14.037399Z&quot;, &quot;endedAt&quot;: &quot;2021-06-06T14:57:54.037399Z&quot;, &quot;durationMs&quot;: 220000, &quot;args&quot;: {}, &quot;location&quot;: &quot;https://github.com/example/jobs/blob/2294bc15eb49071f38425dc927e48655530a2f2e/etl_orders_7_days.py&quot;, &quot;context&quot;: { &quot;sql&quot;: &quot;INSERT INTO orders_7_days (order_id, placed_on, discount_id, menu_id, restaurant_id, menu_item_id, category_id)\\n SELECT o.id AS order_id, o.placed_on, o.discount_id, m.id AS menu_id, m.restaurant_id, mi.id AS menu_item_id, c.id AS category_id\\n FROM orders AS o\\n INNER JOIN menu_items AS mi\\n ON menu_items.id = o.menu_item_id\\n INNER JOIN categories AS c\\n ON c.id = mi.category_id\\n INNER JOIN menu AS m\\n ON m.id = c.menu_id\\n WHERE o.placed_on &gt;= NOW() - interval '7 days';&quot; }, &quot;facets&quot;: {} } }, &quot;inEdges&quot;: [ {&quot;origin&quot;: &quot;dataset:food_delivery:public.categories&quot;, &quot;destination&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;}, &quot;destination&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;}, {&quot;origin&quot;: &quot;dataset:food_delivery:public.orders&quot;, &quot;destination&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;}, {&quot;origin&quot;: &quot;dataset:food_delivery:public.menus&quot;, &quot;destination&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;} ], &quot;outEdges&quot;: [ {&quot;origin&quot;: &quot;job:food_delivery:example.etl_orders_7_days&quot;, &quot;destination&quot;: &quot;dataset:food_delivery:public.orders_7_days&quot;} ] } }, ...] }  To see a visualization of the graph, search the web UI with public.delivery_7_days. "},{"title":"Backfill a DAG Run​","type":1,"pageTitle":"Backfilling Airflow DAGs Using Marquez","url":"/docs/guides/airflow-backfill-dags#backfill-a-dag-run","content":" Figure 1: Backfilled daily table partitions To run a backfill for example.etl_orders_7_days using the DAG lineage metadata stored in Marquez, query the lineage graph for the upstream DAG where an error originated. In this case, the example.etl_orders DAG upstream of example.etl_orders_7_days failed to write some of the daily table partitions needed for the weekly food order trends report. To fix the weekly trends report, backfill the missing daily table partitions public.orders_2021_06_04, public.orders_2021_06_05, and public.orders_2021_06_06 using the Airflow CLI: # Backfill daily food orders $ airflow dags backfill \\ --start-date 2021-06-04 \\ --end-date 2021-06-06 \\ example.etl_orders   Figure 2: Airflow inter-DAG dependencies Then, using the script backfill.sh defined below, we can easily backfill all DAGs downstream of example.etl_orders: (Note: Make sure you have jq installed before running backfill.sh.) #!/bin/bash # # Backfill DAGs automatically using lineage metadata stored in Marquez. # # Usage: $ ./backfill.sh &lt;start-date&gt; &lt;end-date&gt; &lt;dag-id&gt; ​ set -e ​ # Backfills DAGs downstream of the given node ID, recursively. backfill_downstream_of() { node_id=&quot;${1}&quot; # Get out edges for node ID out_edges=($(echo $lineage_graph \\ | jq -r --arg NODE_ID &quot;${node_id}&quot; '.graph[] | select(.id==$NODE_ID) | .outEdges[].destination')) for out_edge in &quot;${out_edges[@]}&quot;; do # Run backfill if out edge is a job node (i.e. &lt;dataset&gt; =&gt; &lt;job&gt;) if [[ &quot;${out_edge}&quot; = job:* ]]; then dag_id=&quot;${out_edge##*:}&quot; echo &quot;backfilling ${dag_id}...&quot; airflow backfill --start_date &quot;${start_date}&quot; --end_date &quot;${start_date}&quot; &quot;${dag_id}&quot; fi # Follow out edges downstream, recursively backfill_downstream_of &quot;${out_edge}&quot; done } ​ start_date=&quot;${1}&quot; end_date=&quot;${2}&quot; dag_id=&quot;${3}&quot; ​ # (1) Build job node ID (format: 'job:&lt;namespace&gt;:&lt;job&gt;') node_id=&quot;job:food_delivery:${dag_id}&quot; ​ # (2) Get lineage graph lineage_graph=$(curl -s -X GET &quot;http://localhost:5000/api/v1-beta/lineage?nodeId=${node_id}&quot;) ​ # (3) Run backfill backfill_downstream_of &quot;${node_id}&quot;  When run, the script should output all backfilled DAGs to the console: $ ./backfill.sh 2021-06-06 2021-06-06 example.etl_orders backfilling example.etl_orders_7_days... backfilling example.etl_delivery_7_days... backfilling example.delivery_times_7_days...  "},{"title":"Conclusion​","type":1,"pageTitle":"Backfilling Airflow DAGs Using Marquez","url":"/docs/guides/airflow-backfill-dags#conclusion","content":"The lineage metadata provided by Marquez can make the task of backfilling much easier. But lineage metadata can also help avoid the need to backfill altogether. Since Marquez collects DAG run metadata that can be viewed using the Runs API, building automated processes to check DAG run states and notify teams of upstream data quality issues is just one possible preventive measure. Explore Marquez's opinionated Metadata API and define your own automated process(es) for analyzing lineage metadata! Also, join our Slack channel or reach out to us on Twitter if you have questions. "},{"title":"Using the OpenLineage Proxy with Airflow","type":0,"sectionRef":"#","url":"/docs/guides/airflow_proxy","content":"","keywords":""},{"title":"Table of Contents​","type":1,"pageTitle":"Using the OpenLineage Proxy with Airflow","url":"/docs/guides/airflow_proxy#table-of-contents","content":"Setting up a Local Airflow Environment using Docker ComposeSetting up MarquezRunning EverythingAccessing the Airflow UIRunning an Example DAG "},{"title":"Setting up a Local Airflow Environment using Docker Compose​","type":1,"pageTitle":"Using the OpenLineage Proxy with Airflow","url":"/docs/guides/airflow_proxy#setting-up-a-local-airflow-environment-using-docker-compose","content":"Airflow has a convenient way to set up and run a fully functional environment using Docker Compose. The following are therefore required to be installed before we begin this tutorial. "},{"title":"Prerequisites​","type":1,"pageTitle":"Using the OpenLineage Proxy with Airflow","url":"/docs/guides/airflow_proxy#prerequisites","content":"Docker 20.10.0+Docker DesktopDocker ComposeJava 11 info If you are using MacOS Monterey (MacOS 12), port 5000 will have to be released by disabling the AirPlay Receiver. Also, port 3000 will need to be free if access to the Marquez Web UI is desired. Use the following instructions to set up and run Airflow using Docker Compose. First, let's start out by creating a new directory that will contain all of our work. mkdir ~/airflow-ol &amp;&amp; cd ~/airflow-ol  Then, let's download the Docker Compose file that we'll be running in it. curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.3.3/docker-compose.yaml'  This will allow a new environment variable OPENLINEAGE_URL to be passed to the Docker containers, which is needed for OpenLineage to work. Then, let's create the following directories that will be mounted and used by the Docker Compose that will start Airflow. mkdir dags &amp;&amp; mkdir logs &amp;&amp; mkdir plugins  Also, create a file .env that will contain an environment variable that is going to be used by Airflow to install additional Python packages that are needed. In this tutorial, the openlineage-airflow package will be installed. echo &quot;_PIP_ADDITIONAL_REQUIREMENTS=openlineage-airflow&quot; &gt; .env  You also need to let OpenLineage know where to send lineage data. echo &quot;OPENLINEAGE_URL=http://host.docker.internal:4433&quot; &gt;&gt; .env  The reason why we are setting the backend to host.docker.internal is that we are going to be running the OpenLineage Proxy outside Airflow's Docker environment on the host machine itself. Port 4433 is where the proxy will be listening for lineage data. "},{"title":"Setting up OpenLineage Proxy as Receiving End​","type":1,"pageTitle":"Using the OpenLineage Proxy with Airflow","url":"/docs/guides/airflow_proxy#setting-up-openlineage-proxy-as-receiving-end","content":"The OpenLineage Proxy is a simple tool that you can easily set up and run to receive OpenLineage data. The proxy does not do anything other than display what it receives. Optionally, it can also forward data to any OpenLineage-compatible backend via HTTP. Let's download the proxy code from git and build it: cd ~ &amp;&amp; git clone https://github.com/OpenLineage/OpenLineage.git &amp;&amp; cd OpenLineage/proxy/backend &amp;&amp; ./gradlew build  Now, copy proxy.dev.yml and edit its content as the following, and save it as proxy.yml. # Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an &quot;AS IS&quot; BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. server: applicationConnectors: - type: http port: ${OPENLINEAGE_PROXY_PORT:-4433} adminConnectors: - type: http port: ${OPENLINEAGE_PROXY_ADMIN_PORT:-4434} logging: level: ${LOG_LEVEL:-INFO} appenders: - type: console proxy: source: openLineageProxyBackend streams: - type: Console - type: Http url: http://localhost:5000/api/v1/lineage  "},{"title":"Setting up Marquez​","type":1,"pageTitle":"Using the OpenLineage Proxy with Airflow","url":"/docs/guides/airflow_proxy#setting-up-marquez","content":"The last piece of the setup is the Marquez backend. Using Marquez's quickstart document, set up the Marquez environment. cd ~ &amp;&amp; git clone https://github.com/MarquezProject/marquez.git  In marquez/docker-compose.dev.yml, change the ports for pghero to free up port 8080 for Airflow: version: &quot;3.7&quot; services: api: build: . seed_marquez: build: . pghero: image: ankane/pghero container_name: pghero ports: - &quot;8888:8888&quot; environment: DATABASE_URL: postgres://postgres:password@db:5432  "},{"title":"Running Everything​","type":1,"pageTitle":"Using the OpenLineage Proxy with Airflow","url":"/docs/guides/airflow_proxy#running-everything","content":""},{"title":"Running Marquez​","type":1,"pageTitle":"Using the OpenLineage Proxy with Airflow","url":"/docs/guides/airflow_proxy#running-marquez","content":"Start Docker Desktop, then: cd ~/marquez &amp;&amp; ./docker/up.sh  "},{"title":"Running OpenLineage proxy​","type":1,"pageTitle":"Using the OpenLineage Proxy with Airflow","url":"/docs/guides/airflow_proxy#running-openlineage-proxy","content":"cd ~/OpenLineage/proxy/backend &amp;&amp; ./gradlew runShadow  "},{"title":"Running Airflow​","type":1,"pageTitle":"Using the OpenLineage Proxy with Airflow","url":"/docs/guides/airflow_proxy#running-airflow","content":"cd ~/airflow-ol docker-compose up   At this point, Apache Airflow should be running and able to send lineage data to the OpenLineage Proxy, with the OpenLineage Proxy forwarding the data to Marquez. Consequently, we can both inspect data payloads and see lineage data in graph form. "},{"title":"Accessing the Airflow UI​","type":1,"pageTitle":"Using the OpenLineage Proxy with Airflow","url":"/docs/guides/airflow_proxy#accessing-the-airflow-ui","content":"With everything up and running, we can now login to Airflow's UI by opening up a browser and accessing http://localhost:8080. Initial ID and password to login would be airflow/airflow. "},{"title":"Running an Example DAG​","type":1,"pageTitle":"Using the OpenLineage Proxy with Airflow","url":"/docs/guides/airflow_proxy#running-an-example-dag","content":"When you log into Airflow UI, you will notice that there are several example DAGs already populated when it started up. We can start running some of them to see the OpenLineage events they generate. "},{"title":"Running Bash Operator​","type":1,"pageTitle":"Using the OpenLineage Proxy with Airflow","url":"/docs/guides/airflow_proxy#running-bash-operator","content":"In the DAGs page, locate the example_bash_operator.  Clicke the ► button at the right, which will show up a popup. Select Trigger DAG to trigger and run the DAG manually. You should see DAG running, and eventually completing. "},{"title":"Check the OpenLineage events​","type":1,"pageTitle":"Using the OpenLineage Proxy with Airflow","url":"/docs/guides/airflow_proxy#check-the-openlineage-events","content":"Once everything is finished, you should be able to see a number of JSON data payloads output in OpenLineage proxy's console. INFO [2022-08-16 21:39:41,411] io.openlineage.proxy.api.models.ConsoleLineageStream: { &quot;eventTime&quot; : &quot;2022-08-16T21:39:40.854926Z&quot;, &quot;eventType&quot; : &quot;START&quot;, &quot;inputs&quot; : [ ], &quot;job&quot; : { &quot;facets&quot; : { }, &quot;name&quot; : &quot;example_bash_operator.runme_2&quot;, &quot;namespace&quot; : &quot;default&quot; }, &quot;outputs&quot; : [ ], &quot;producer&quot; : &quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0/integration/airflow&quot;, &quot;run&quot; : { &quot;facets&quot; : { &quot;airflow_runArgs&quot; : { &quot;_producer&quot; : &quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0/integration/airflow&quot;, &quot;_schemaURL&quot; : &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/BaseFacet&quot;, &quot;externalTrigger&quot; : true }, &quot;airflow_version&quot; : { &quot;_producer&quot; : &quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0/integration/airflow&quot;, &quot;_schemaURL&quot; : &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/BaseFacet&quot;, &quot;airflowVersion&quot; : &quot;2.3.3&quot;, &quot;openlineageAirflowVersion&quot; : &quot;0.12.0&quot;, &quot;operator&quot; : &quot;airflow.operators.bash.BashOperator&quot;, &quot;taskInfo&quot; : &quot;{'_BaseOperator__init_kwargs': {'task_id': 'runme_2', 'params': &lt;***.models.param.ParamsDict object at 0xffff7467b610&gt;, 'bash_command': 'echo \\&quot;example_bash_operator__runme_2__20220816\\&quot; &amp;&amp; sleep 1'}, '_BaseOperator__from_mapped': False, 'task_id': 'runme_2', 'task_group': &lt;weakproxy at 0xffff74676ef0 to TaskGroup at 0xffff7467ba50&gt;, 'owner': '***', 'email': None, 'email_on_retry': True, 'email_on_failure': True, 'execution_timeout': None, 'on_execute_callback': None, 'on_failure_callback': None, 'on_success_callback': None, 'on_retry_callback': None, '_pre_execute_hook': None, '_post_execute_hook': None, 'executor_config': {}, 'run_as_user': None, 'retries': 0, 'queue': 'default', 'pool': 'default_pool', 'pool_slots': 1, 'sla': None, 'trigger_rule': &lt;TriggerRule.ALL_SUCCESS: 'all_success'&gt;, 'depends_on_past': False, 'ignore_first_depends_on_past': True, 'wait_for_downstream': False, 'retry_delay': datetime.timedelta(seconds=300), 'retry_exponential_backoff': False, 'max_retry_delay': None, 'params': &lt;***.models.param.ParamsDict object at 0xffff7467b4d0&gt;, 'priority_weight': 1, 'weight_rule': &lt;WeightRule.DOWNSTREAM: 'downstream'&gt;, 'resources': None, 'max_active_tis_per_dag': None, 'do_xcom_push': True, 'doc_md': None, 'doc_json': None, 'doc_yaml': None, 'doc_rst': None, 'doc': None, 'upstream_task_ids': set(), 'downstream_task_ids': {'run_after_loop'}, 'start_date': DateTime(2021, 1, 1, 0, 0, 0, tzinfo=Timezone('UTC')), 'end_date': None, '_dag': &lt;DAG: example_bash_operator&gt;, '_log': &lt;Logger ***.task.operators (INFO)&gt;, 'inlets': [], 'outlets': [], '_inlets': [], '_outlets': [], '_BaseOperator__instantiated': True, 'bash_command': 'echo \\&quot;example_bash_operator__runme_2__20220816\\&quot; &amp;&amp; sleep 1', 'env': None, 'output_encoding': 'utf-8', 'skip_exit_code': 99, 'cwd': None, 'append_env': False}&quot; }, &quot;nominalTime&quot; : { &quot;_producer&quot; : &quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0/integration/airflow&quot;, &quot;_schemaURL&quot; : &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/NominalTimeRunFacet&quot;, &quot;nominalStartTime&quot; : &quot;2022-08-16T21:39:38.005668Z&quot; }, &quot;parentRun&quot; : { &quot;_producer&quot; : &quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0/integration/airflow&quot;, &quot;_schemaURL&quot; : &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/ParentRunFacet&quot;, &quot;job&quot; : { &quot;name&quot; : &quot;example_bash_operator&quot;, &quot;namespace&quot; : &quot;default&quot; }, &quot;run&quot; : { &quot;runId&quot; : &quot;39ad10d1-72d9-3fe9-b2a4-860c651b98b7&quot; } } }, &quot;runId&quot; : &quot;313b4e71-9cde-4c83-b641-dd6773bf114b&quot; } }  "},{"title":"Check Marquez​","type":1,"pageTitle":"Using the OpenLineage Proxy with Airflow","url":"/docs/guides/airflow_proxy#check-marquez","content":"You can also open up the browser and visit http://localhost:3000 to access Marquez UI, and take a look at the OpenLineage events originating from Airflow.  "},{"title":"Running other DAGs​","type":1,"pageTitle":"Using the OpenLineage Proxy with Airflow","url":"/docs/guides/airflow_proxy#running-other-dags","content":"Due to the length of this tutorial, we are not going to be running additional example DAGs, but you can try running them and it would be interesting to see how each of them are going to be emitting OpenLineage events. Please try running other examples like example_python_operator which will also emit OpenLineage events. Normally, DataLineage will be much more complete and useful if a DAG run involves certain datasets that either get used or created during the runtime of it. When you run those DAGs, you will be able to see the connection between different DAGs and Tasks touching the same dataset that will eventually turn into Data Lineage graph that may look something like this:  Currently, these are the Airflow operators that have extractors that can extract and emit OpenLineage events. PostgresOperatorMySqlOperatorBigQueryOperatorSnowflakeOperatorGreatExpectationsOperatorPythonOperator See additional Apache Examples for DAGs that you can run in Airflow for OpenLineage. "},{"title":"Troubleshooting​","type":1,"pageTitle":"Using the OpenLineage Proxy with Airflow","url":"/docs/guides/airflow_proxy#troubleshooting","content":"You might not see any data going through the proxy or via Marquez. In that case, please check the task log of Airflow and see if you see the following message: [2022-08-16, 21:23:19 UTC] {factory.py:122} ERROR - Did not find openlineage.yml and OPENLINEAGE_URL is not set. In that case, it means that the environment variable OPENLINEAGE_URL was not set properly, thus OpenLineage was not able to emit any events. Please make sure to follow instructions in setting up the proper environment variable when setting up the Airflow via docker compose.Sometimes, Marquez would not respond and fail to receive any data via its API port 5000. You should be able to notice that if you start receiving response code 500 from Marquez or the Marquez UI hangs. In that case, simply stop and restart Marquez. "},{"title":"Conclusion​","type":1,"pageTitle":"Using the OpenLineage Proxy with Airflow","url":"/docs/guides/airflow_proxy#conclusion","content":"In this short tutorial, we have learned how to setup and run a simple Apache Airflow environment that can emit OpenLineage events during its DAG run. We have also monitored and received the lineage events using combination of OpenLineage proxy and Marquez. We hope this tutorial was helpful in understanding how Airflow could be setup with OpenLineage and how you can easily monitor its data and end result using proxy and Marquez. "},{"title":"Using Marquez with dbt","type":0,"sectionRef":"#","url":"/docs/guides/dbt","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Using Marquez with dbt","url":"/docs/guides/dbt#prerequisites","content":"dbtDocker DesktopgitGoogle Cloud Service account Google Cloud Service account JSON key file Note: your Google Cloud account should have access to BigQuery and read/write access to your GCS bucket. Giving your key file an easy-to-remember name (bq-dbt-demo.json) is recommended. Finally, if using macOS Monterey (macOS 12), you will need to release port 5000 by disabling the AirPlay Receiver. "},{"title":"Instructions​","type":1,"pageTitle":"Using Marquez with dbt","url":"/docs/guides/dbt#instructions","content":"First, run through this excellent dbt tutorial. It explains how to create a BigQuery project, provision a service account, download a JSON key, and set up a local dbt environment. The rest of this example assumes the existence of a BigQuery project where models can be run, as well as proper configuration of dbt to connect to the project. Next, start a local Marquez instance to store lineage metadata. Make sure Docker is running, and then clone the Marquez repository: git clone https://github.com/MarquezProject/marquez.git &amp;&amp; cd marquez ./docker/up.sh  Check to make sure Marquez is up by visiting http://localhost:3000. The page should display an empty Marquez instance and a message saying there is no data. Also, it should be possible to see the server output from requests in the terminal window where Marquez is running. This window should remain open. Now, in a new terminal window/pane, clone the following GitHub project, which contains some database models: git clone https://github.com/rossturk/stackostudy.git &amp;&amp; cd stackostudy  Now it is time to install dbt and its integration with OpenLineage. Doing this in a Python virtual environment is recommended. To create one and install necessary packages, run the following commands: python -m venv virtualenv source virtualenv/bin/activate pip install dbt dbt-openlineage  Keep in mind that dbt learns how to connect to a BigQuery project by looking for a matching profile in ~/.dbt/profiles.yml. Create or edit this file so it contains a section with the project's BigQuery connection details. Also, point to the location of the JSON key for the service account. Consult this section in the dbt documentation for more help with dbt profiles. At this point, profiles.yml should look something like this: stackostudy: target: dev outputs: dev: type: bigquery method: service-account keyfile: /Users/rturk/.dbt/dbt-example.json project: dbt-example dataset: stackostudy threads: 1 timeout_seconds: 300 location: US priority: interactive  The dbt debug command checks to see that everything has been configured correctly. Running it now should produce output like the following: % dbt debug Running with dbt=0.20.1 dbt version: 0.20.1 python version: 3.8.12 python path: /opt/homebrew/Cellar/dbt/0.20.1_1/libexec/bin/python3 os info: macOS-11.5.2-arm64-arm-64bit Using profiles.yml file at /Users/rturk/.dbt/profiles.yml Using dbt_project.yml file at /Users/rturk/projects/stackostudy/dbt_project.yml ​ Configuration: profiles.yml file [OK found and valid] dbt_project.yml file [OK found and valid] ​ Required dependencies: - git [OK found] ​ Connection: method: service-account database: stacko-study schema: stackostudy location: US priority: interactive timeout_seconds: 300 maximum_bytes_billed: None Connection test: OK connection ok  "},{"title":"Important Details​","type":1,"pageTitle":"Using Marquez with dbt","url":"/docs/guides/dbt#important-details","content":"Some important conventions should be followed when designing dbt models for use with OpenLineage. Following these conventions will help ensure that OpenLineage collects the most complete metadata possible. First, any datasets existing outside the dbt project should be defined in a schema YAML file inside the models/ directory: version: 2 ​ sources: - name: stackoverflow database: bigquery-public-data schema: stackoverflow tables: - name: posts_questions - name: posts_answers - name: users - name: votes  This contains the name of the external dataset - in this case, bigquery-public-datasets - and lists the tables that are used by the models in this project. The name of the file does not matter, as long as it ends with .yml and is inside models/. Hardcoding dataset and table names into queries can result in incomplete data. When writing queries, be sure to use the {{ ref() }} and {{ source() }} jinja functions when referring to data sources. The {{ ref() }} function can be used to refer to tables within the same model, and the {{ source() }} function refers to tables we have defined in schema.yml. That way, dbt will properly keep track of the relationships between datasets. For example, to select from both an external dataset and one in this model: select * from {{ source('stackoverflow', 'posts_answers') }} where parent_id in (select id from {{ ref('filtered_questions') }} )  "},{"title":"Logging","type":0,"sectionRef":"#","url":"/docs/development/developing/python/troubleshooting/logging","content":"","keywords":""},{"title":"Further readings​","type":1,"pageTitle":"Logging","url":"/docs/development/developing/python/troubleshooting/logging#further-readings","content":"https://docs.python.org/3/library/logging.htmlhttps://realpython.com/python-logging/ "},{"title":"Understanding and Using Facets","type":0,"sectionRef":"#","url":"/docs/guides/facets","content":"","keywords":""},{"title":"Standard Facets​","type":1,"pageTitle":"Understanding and Using Facets","url":"/docs/guides/facets#standard-facets","content":"Run Facets​ nominalTime: Captures the time this run is scheduled for. This is a typical usage for time based scheduled job. The job has a nominal schedule time that will be different from the actual time it is running at. parent: Captures the parent job and Run when the run was spawn from a parent run. For example in the case of Airflow, there's a run for the DAG that then spawns runs for individual tasks that would refer to the parent run as the DAG run. Similarly when a SparkOperator starts a Spark job, this creates a separate run that refers to the task run as its parent. errorMessage: Captures potential error message, programming language - and optionally stack trace - with which the run failed. Job Facets​ sourceCodeLocation: Captures the source code location and version (e.g., the git sha) of the job. sourceCode: Captures the language (e.g., Python) and actual source code of the job. sql: Capture the SQL query if this job is a SQL query. ownership: Captures the owners of the job. Dataset Facets​ schema: Captures the schema of the dataset. dataSource: Captures the database instance containing this dataset (e.g., Database schema, Object store bucket, etc.) lifecycleStateChange: Captures the lifecycle states of the dataset (e.g., alter, create, drop, overwrite, rename, truncate). version: Captures the dataset version when versioning is defined by database (e.g., Iceberg snapshot ID). columnLineage: Captures the column-level lineage. ownership: Captures the owners of the dataset. Input Dataset Facets​ dataQualityMetrics: Captures dataset-level and column-level data quality metrics when scanning a dataset whith a DataQuality library (row count, byte size, null count, distinct count, average, min, max, quantiles). dataQualityAssertions: Captures the result of running data tests on a dataset or its columns. Output Dataset Facets​ outputStatistics: Captures the size of the output written to a dataset (row count and byte size). "},{"title":"Getting Started with Airflow and OpenLineage+Marquez","type":0,"sectionRef":"#","url":"/docs/guides/airflow-quickstart","content":"","keywords":""},{"title":"You’ll Learn How To:​","type":1,"pageTitle":"Getting Started with Airflow and OpenLineage+Marquez","url":"/docs/guides/airflow-quickstart#youll-learn-how-to","content":"configure Airflow to send OpenLineage events to Marquezwrite OpenLineage-enabled DAGstroubleshoot a failing DAG using Marquez "},{"title":"Table of Contents​","type":1,"pageTitle":"Getting Started with Airflow and OpenLineage+Marquez","url":"/docs/guides/airflow-quickstart#table-of-contents","content":"Step 1: Configure Your Astro ProjectStep 2: Add Marquez Services Using Docker ComposeStep 3: Start Airflow with MarquezStep 4: Write Airflow DAGsStep 5: View Collected MetadataStep 6: Troubleshoot a Failing DAG with Marquez "},{"title":"Prerequisites​","type":1,"pageTitle":"Getting Started with Airflow and OpenLineage+Marquez","url":"/docs/guides/airflow-quickstart#prerequisites","content":"Before you begin, make sure you have installed: Docker 17.05+Astro CLIcurl Note: We recommend that you have allocated at least 2 CPUs and 8 GB of memory to Docker. "},{"title":"Configure Your Astro Project​","type":1,"pageTitle":"Getting Started with Airflow and OpenLineage+Marquez","url":"/docs/guides/airflow-quickstart#configure-your-astro-project","content":"Use the Astro CLI to create and run an Airflow project locally that will integrate with Marquez. In your project directory, create a new Astro project: $ .. $ mkdir astro-marquez-tutorial &amp;&amp; cd astro-marquez-tutorial $ astro dev init Using curl, change into new directory docker and download some scripts required by Marquez services: $ mkdir docker &amp;&amp; cd docker $ curl -O &quot;https://raw.githubusercontent.com/MarquezProject/marquez/main/docker/{entrypoint.sh,wait-for-it.sh}&quot; $ .. After executing the above, your project directory should look like this: $ ls -a . Dockerfile packages.txt .. README.md plugins .astro airflow_settings.yaml requirements.txt .dockerignore dags tests .env docker .gitignore include Add the OpenLineage Airflow Provider and the Common SQL Provider to the requirements.txt file: apache-airflow-providers-common-sql==1.7.2 apache-airflow-providers-openlineage==1.1.0 For details about the Provider and its minimum requirements, see the Airflow docs. To configure Astro to send lineage metadata to Marquez, add the following environment variables below to your Astro project's .env file: OPENLINEAGE_URL=http://host.docker.internal:5000 OPENLINEAGE_NAMESPACE=example AIRFLOW_CONN_EXAMPLE_DB=postgres://example:example@host.docker.internal:7654/example These variables allow Airflow to connect with the OpenLineage API and send events to Marquez. It is a good idea to have Airflow use a different port for Postgres than the default 5432, so run the following command to use port 5678 instead: astro config set postgres.port 5678 Check the Dockerfile to verify that your installed version of the Astro Runtime is 9.0.0+ (to ensure that you will be using Airflow 2.7.0+). For example: FROM quay.io/astronomer/astro-runtime:9.1.0  "},{"title":"Add Marquez and Database Services Using Docker Compose​","type":1,"pageTitle":"Getting Started with Airflow and OpenLineage+Marquez","url":"/docs/guides/airflow-quickstart#add-marquez-and-database-services-using-docker-compose","content":"Astro supports manual configuration of services via Docker Compose using YAML. Create new file docker-compose.override.yml in your project and copy/paste the following into the file: version: &quot;3.1&quot; services: web: image: marquezproject/marquez-web:latest container_name: marquez-web environment: - MARQUEZ_HOST=api - MARQUEZ_PORT=5000 ports: - &quot;3000:3000&quot; depends_on: - api db: image: postgres:14.9 container_name: marquez-db ports: - &quot;6543:6543&quot; environment: - POSTGRES_USER=marquez - POSTGRES_PASSWORD=marquez - POSTGRES_DB=marquez example-db: image: postgres:14.9 container_name: example-db ports: - &quot;7654:5432&quot; environment: - POSTGRES_USER=example - POSTGRES_PASSWORD=example - POSTGRES_DB=example api: image: marquezproject/marquez:latest container_name: marquez-api environment: - MARQUEZ_PORT=5000 - MARQUEZ_ADMIN_PORT=5001 ports: - &quot;5000:5000&quot; - &quot;5001:5001&quot; volumes: - ./docker/wait-for-it.sh:/usr/src/app/wait-for-it.sh links: - &quot;db:postgres&quot; depends_on: - db entrypoint: [&quot;/bin/bash&quot;, &quot;./wait-for-it.sh&quot;, &quot;db:6543&quot;, &quot;--&quot;, &quot;./entrypoint.sh&quot;] redis: image: bitnami/redis:6.0.6 environment: - ALLOW_EMPTY_PASSWORD=yes  The above adds the Marquez API, database and Web UI, along with an additional Postgres database for the DAGs used in this example, to Astro's Docker container and configures them to use the scripts in the docker directory you previously downloaded from Marquez. "},{"title":"Start Airflow with Marquez​","type":1,"pageTitle":"Getting Started with Airflow and OpenLineage+Marquez","url":"/docs/guides/airflow-quickstart#start-airflow-with-marquez","content":"Now you can start all services. To do so, execute the following: $ astro dev start  The above command will: start Airflowstart Marquez, including its API, database and UIcreate and start a Postgres server for DAG tasks To view the Airflow UI and verify it's running, open http://localhost:8080. Then, log in using the username and password admin / admin. You can also browse to http://localhost:3000 to view the Marquez UI. "},{"title":"Write Airflow DAGs​","type":1,"pageTitle":"Getting Started with Airflow and OpenLineage+Marquez","url":"/docs/guides/airflow-quickstart#write-airflow-dags","content":"In this step, you will create two new Airflow DAGs that perform simple tasks. The counter DAG adds 1 to a column every minute, while the sum DAG calculates a sum every five minutes. This will result in a simple pipeline containing two jobs and two datasets. "},{"title":"Create a counter DAG​","type":1,"pageTitle":"Getting Started with Airflow and OpenLineage+Marquez","url":"/docs/guides/airflow-quickstart#create-a-counter-dag","content":"In dags/, create a file named counter.py and add the following code: from airflow import DAG from airflow.decorators import task from airflow.providers.postgres.operators.postgres import PostgresOperator from airflow.utils.dates import days_ago with DAG( 'counter', start_date=days_ago(1), schedule='*/1 * * * *', catchup=False, is_paused_upon_creation=False, max_active_runs=1, description='DAG that generates a new count value equal to 1.' ): query1 = PostgresOperator( task_id='if_not_exists', postgres_conn_id='example_db', sql=''' CREATE TABLE IF NOT EXISTS counts ( value INTEGER );''' ) query2 = PostgresOperator( task_id='inc', postgres_conn_id='example_db', sql=''' INSERT INTO counts (value) VALUES (1) ''' ) query1 &gt;&gt; query2  "},{"title":"Create a sum DAG​","type":1,"pageTitle":"Getting Started with Airflow and OpenLineage+Marquez","url":"/docs/guides/airflow-quickstart#create-a-sum-dag","content":"In dags/, create a file named sum.py and add the following code: from airflow import DAG from airflow.providers.postgres.operators.postgres import PostgresOperator from airflow.utils.dates import days_ago with DAG( 'sum', start_date=days_ago(1), schedule='*/5 * * * *', catchup=False, is_paused_upon_creation=False, max_active_runs=1, description='DAG that sums the total of generated count values.' ): query1 = PostgresOperator( task_id='if_not_exists', postgres_conn_id='example_db', sql=''' CREATE TABLE IF NOT EXISTS sums ( value INTEGER );''' ) query2 = PostgresOperator( task_id='total', postgres_conn_id='example_db', sql=''' INSERT INTO sums (value) SELECT SUM(value) FROM counts; ''' ) query1 &gt;&gt; query2  "},{"title":"View Collected Metadata​","type":1,"pageTitle":"Getting Started with Airflow and OpenLineage+Marquez","url":"/docs/guides/airflow-quickstart#view-collected-metadata","content":"To ensure that Airflow is executing counter and sum, navigate to the DAGs tab in Airflow and verify that they are both enabled and are in a running state:  To view DAG metadata collected by Marquez from Airflow, browse to the Marquez UI by visiting http://localhost:3000. Then, use the search bar in the upper right-side of the page and search for the counter.inc job. To view lineage metadata for counter.inc, click on the job from the drop-down list: Note: If the counter.inc job is not in the drop-down list, check to see if Airflow has successfully executed the DAG.  If you take a quick look at the lineage graph for counter.inc, you should see example.public.counts as an output dataset and sum.total as a downstream job!  "},{"title":"Troubleshoot a Failing DAG with Marquez​","type":1,"pageTitle":"Getting Started with Airflow and OpenLineage+Marquez","url":"/docs/guides/airflow-quickstart#troubleshoot-a-failing-dag-with-marquez","content":"In this step, let's quickly walk through a simple troubleshooting scenario where the DAG sum begins to fail as the result of an upstream schema change for table counts. Tip: It's helpful to apply the same code changes outlined below to your Airflow DAGs defined in Step 6. Let's say team A owns the DAG counter. Team A decides to update the tasks in counter to rename the values column in the counts table to value_1_to_10 (without properly communicating the schema change!): query1 = PostgresOperator( - task_id='if_not_exists', + task_id='alter_name_of_column', postgres_conn_id='example_db', sql=''' - CREATE TABLE IF NOT EXISTS counts ( - value INTEGER - );''', + ALTER TABLE &quot;counts&quot; RENAME COLUMN &quot;value&quot; TO &quot;value_1_to_10&quot;; + ''' )  query2 = PostgresOperator( task_id='inc', postgres_conn_id='example_db', sql=''' - INSERT INTO counts (value) + INSERT INTO counts (value_1_to_10) VALUES (1) ''', )  Team B, unaware of the schema change, owns the DAG sum and begins to see DAG run metadata with failed run states:  But, team B is not sure what might have caused the DAG failure as no recent code changes have been made to the DAG. So, team B decides to check the schema of the input dataset:  Team B soon realizes that the schema has changed recently for the counts table! To fix the DAG, team B updates the t2 task that calcuates the count total to use the new column name: query2 = PostgresOperator( task_id='total', postgres_conn_id='example_db', sql=''' INSERT INTO sums (value) - SELECT SUM(value) FROM counts; + SELECT SUM(value_1_to_10) FROM counts; ''' )  With the code change, the DAG sum begins to run successfully:  Congrats! You successfully step through a troubleshooting scenario of a failing DAG using metadata collected with Marquez! You can now add your own DAGs to dags/ to build more complex data lineage graphs. "},{"title":"Next Steps​","type":1,"pageTitle":"Getting Started with Airflow and OpenLineage+Marquez","url":"/docs/guides/airflow-quickstart#next-steps","content":"Review the Marquez HTTP API used to collect Airflow DAG metadata and learn how to build your own integrations using OpenLineage.Take a look at openlineage-spark integration that can be used with Airflow. "},{"title":"Feedback​","type":1,"pageTitle":"Getting Started with Airflow and OpenLineage+Marquez","url":"/docs/guides/airflow-quickstart#feedback","content":"What did you think of this example? You can reach out to us on Slack and leave us feedback, or open a pull request with your suggested changes! "},{"title":"Apache Airflow","type":0,"sectionRef":"#","url":"/docs/integrations/airflow/","content":"","keywords":""},{"title":"How does Airflow work with OpenLineage?​","type":1,"pageTitle":"Apache Airflow","url":"/docs/integrations/airflow/#how-does-airflow-work-with-openlineage","content":"Understanding complex inter-DAG dependencies and providing up-to-date runtime visibility into DAG execution can be challenging. OpenLineage integrates with Airflow to collect DAG lineage metadata so that inter-DAG dependencies are easily maintained and viewable via a lineage graph, while also keeping a catalog of historical runs of DAGs.  The DAG metadata collected can answer questions like: Why has a DAG failed?Why has the DAG runtime increased after a code change?What are the upstream dependencies of a DAG? "},{"title":"How can I use this integration?​","type":1,"pageTitle":"Apache Airflow","url":"/docs/integrations/airflow/#how-can-i-use-this-integration","content":"To instrument your Airflow instance with OpenLineage, follow these instructions. "},{"title":"How to add lineage coverage for more operators?​","type":1,"pageTitle":"Apache Airflow","url":"/docs/integrations/airflow/#how-to-add-lineage-coverage-for-more-operators","content":"OpenLineage provides a set of extractors that extract lineage from operators. If you want to add lineage coverage for your own custom operators, follow these instructions to add lineage to operators. If you want to add coverage for operators you can not modify, follow instructions to add custom extractors. If you want to expose lineage as a one off in your workflow, you can also manually annotate the tasks in your DAG. "},{"title":"Where can I learn more?​","type":1,"pageTitle":"Apache Airflow","url":"/docs/integrations/airflow/#where-can-i-learn-more","content":"Take a look at Marquez's Airflow example to learn how to enable OpenLineage metadata collection for Airflow DAGs and troubleshoot failing DAGs using Marquez.Watch Data Lineage with OpenLineage and Airflow "},{"title":"Feedback​","type":1,"pageTitle":"Apache Airflow","url":"/docs/integrations/airflow/#feedback","content":"You can reach out to us on slack and leave us feedback! "},{"title":"OpenLineage Integrations","type":0,"sectionRef":"#","url":"/docs/integrations/about","content":"","keywords":""},{"title":"Capability Matrix​","type":1,"pageTitle":"OpenLineage Integrations","url":"/docs/integrations/about#capability-matrix","content":"caution This matrix is not yet complete. The matrix below shows the relationship between an input facet and various mechanisms OpenLineage uses to gather metadata. Not all mechanisms collect data to fill in all facets, and some facets are specific to one integration. ✔️: The mechanism does implement this facet. ✖️: The mechanism does not implement this facet. An empty column means it is not yet documented if the mechanism implements this facet. Mechanism\tIntegration\tMetadata Gathered\tInputDatasetFacet\tOutputDatasetFacet\tSqlJobFacet\tSchemaDatasetFacet\tDataSourceDatasetFacet\tDataQualityMetricsInputDatasetFacet\tDataQualityAssertionsDatasetFacet\tSourceCodeJobFacet\tExternalQueryRunFacet\tDocumentationDatasetFacet\tSourceCodeLocationJobFacet\tDocumentationJobFacet\tParentRunFacetSnowflakeOperator*\tAirflow Extractor\tLineage Job duration\t✔️\t✔️\t✔️\t✔️\t✔️\t✖️\t✖️ BigQueryOperator**\tAirflow Extractor\tLineage Schema details Job duration\t✔️\t✔️ ✔️ PostgresOperator*\tAirflow Extractor\tLineage Job duration\t✔️\t✔️\t✔️\t✔️\t✔️ SqlCheckOperators\tAirflow Extractor\tLineage Data quality assertions\t✔️\t✖️\t✔️\t✔️\t✔️\t✔️\t✔️ dbt\tdbt Project Files\tLineage Row count Byte count.\t✔️ Great Expectations\tAction\tData quality assertions\t✔️ ✔️\t✔️ Spark\tSparkListener\tSchema Row count Column lineage\t✔️ Snowflake***\tAccess History\tLineage  * Uses the Rest SQL parser ** Uses the BigQuery API *** Uses Snowflake query logs "},{"title":"Compatibility matrix​","type":1,"pageTitle":"OpenLineage Integrations","url":"/docs/integrations/about#compatibility-matrix","content":"This matrix shows which data sources are known to work with each integration, along with the minimum versions required in the target system or framework. Platform\tVersion\tData SourcesApache Airflow\t1.10+ 2.0+\tPostgreSQL MySQL Snowflake Amazon Athena Amazon Redshift Amazon SageMaker Amazon S3 Copy and Transform Google BigQuery Google Cloud Storage Great Expectations SFTP FTP Apache Spark\t2.4+\tJDBC HDFS Google Cloud Storage Google BigQuery Amazon S3 Azure Blob Storage Azure Data Lake Gen2 Azure Synapse dbt\t0.20+\tSnowflake Google BigQuery "},{"title":"Integration strategies​","type":1,"pageTitle":"OpenLineage Integrations","url":"/docs/integrations/about#integration-strategies","content":"info This section could use some more detail! You're welcome to contribute using the Edit link at the bottom. "},{"title":"Integrating with pipelines​","type":1,"pageTitle":"OpenLineage Integrations","url":"/docs/integrations/about#integrating-with-pipelines","content":" "},{"title":"Integrating with data sources​","type":1,"pageTitle":"OpenLineage Integrations","url":"/docs/integrations/about#integrating-with-data-sources","content":" "},{"title":"Using OpenLineage with Spark","type":0,"sectionRef":"#","url":"/docs/guides/spark","content":"","keywords":""},{"title":"Running Spark with OpenLineage​","type":1,"pageTitle":"Using OpenLineage with Spark","url":"/docs/guides/spark#running-spark-with-openlineage","content":""},{"title":"Prerequisites​","type":1,"pageTitle":"Using OpenLineage with Spark","url":"/docs/guides/spark#prerequisites","content":"Docker DesktopgitGoogle Cloud Service account Google Cloud Service account JSON key file Note: your Google Cloud account should have access to BigQuery and read/write access to your GCS bucket. Giving your key file an easy-to-remember name (bq-spark-demo.json) is recommended. Finally, if using macOS Monterey (macOS 12), port 5000 will have to be released by disabling the AirPlay Receiver. "},{"title":"Instructions​","type":1,"pageTitle":"Using OpenLineage with Spark","url":"/docs/guides/spark#instructions","content":"Clone the OpenLineage project, navigate to the spark directory, and create a directory for your Google Cloud Service credentials: git clone https://github.com/OpenLineage/OpenLineage cd integration/spark mkdir -p docker/notebooks/gcs  Copy your Google Cloud Service credentials file into that directory, then run: docker-compose up  This launches a Jupyter notebook with Spark as well as a Marquez API endpoint already installed to report lineage. Once the notebook server is up and running, you should see something like the following in the logs: notebook_1 | [I 21:43:39.014 NotebookApp] Jupyter Notebook 6.4.4 is running at: notebook_1 | [I 21:43:39.014 NotebookApp] http://082cb836f1ec:8888/?token=507af3cf9c22f627f6c5211d6861fe0804d9f7b19a93ca48 notebook_1 | [I 21:43:39.014 NotebookApp] or http://127.0.0.1:8888/?token=507af3cf9c22f627f6c5211d6861fe0804d9f7b19a93ca48 notebook_1 | [I 21:43:39.015 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).  Copy the URL with 127.0.0.1 as the hostname from your own log (the token will be different from this one) and paste it into your browser window. You should have a blank Jupyter notebook environment ready to go.  Click on the notebooks directory, then click on the New button to create a new Python 3 notebook.  In the first cell in the window paste the below text. Update the GCP project and bucket names and the service account credentials file, then run the code: from pyspark.sql import SparkSession import urllib.request # Download dependencies for BigQuery and GCS gc_jars = ['https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.1.1/gcs-connector-hadoop3-2.1.1-shaded.jar', 'https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/bigquery-connector/hadoop3-1.2.0/bigquery-connector-hadoop3-1.2.0-shaded.jar', 'https://repo1.maven.org/maven2/com/google/cloud/spark/spark-bigquery-with-dependencies_2.12/0.22.2/spark-bigquery-with-dependencies_2.12-0.22.2.jar'] files = [urllib.request.urlretrieve(url)[0] for url in gc_jars] # Set these to your own project and bucket project_id = 'bq-openlineage-spark-demo' gcs_bucket = 'bq-openlineage-spark-demo-bucket' credentials_file = '/home/jovyan/notebooks/gcs/bq-spark-demo.json' spark = (SparkSession.builder.master('local').appName('openlineage_spark_test') .config('spark.jars', &quot;,&quot;.join(files)) # Install and set up the OpenLineage listener .config('spark.jars.packages', 'io.openlineage:openlineage-spark:0.3.+') .config('spark.extraListeners', 'io.openlineage.spark.agent.OpenLineageSparkListener') .config('spark.openlineage.host', 'http://marquez-api:5000') .config('spark.openlineage.namespace', 'spark_integration') # Configure the Google credentials and project id .config('spark.executorEnv.GCS_PROJECT_ID', project_id) .config('spark.executorEnv.GOOGLE_APPLICATION_CREDENTIALS', '/home/jovyan/notebooks/gcs/bq-spark-demo.json') .config('spark.hadoop.google.cloud.auth.service.account.enable', 'true') .config('spark.hadoop.google.cloud.auth.service.account.json.keyfile', credentials_file) .config('spark.hadoop.fs.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem') .config('spark.hadoop.fs.AbstractFileSystem.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS') .config(&quot;spark.hadoop.fs.gs.project.id&quot;, project_id) .getOrCreate())  Most of this is boilerplate for installing the BigQuery and GCS libraries in the notebook environment. This also sets the configuration parameters to tell the libraries what GCP project to use and how to authenticate with Google. The parameters specific to OpenLineage are the four already mentioned: spark.jars.packages, spark.extraListeners, spark.openlineage.host, spark.openlineage.namespace. Here, the host has been configured to be the marquez-api container started by Docker. With OpenLineage configured, it's time to get some data. The below code populates Spark DataFrames with data from two COVID-19 public data sets. Create a new cell in the notebook and paste the following: from pyspark.sql.functions import expr, col mask_use = spark.read.format('bigquery') \\ .option('parentProject', project_id) \\ .option('table', 'bigquery-public-data:covid19_nyt.mask_use_by_county') \\ .load() \\ .select(expr(&quot;always + frequently&quot;).alias(&quot;frequent&quot;), expr(&quot;never + rarely&quot;).alias(&quot;rare&quot;), &quot;county_fips_code&quot;) opendata = spark.read.format('bigquery') \\ .option('parentProject', project_id) \\ .option('table', 'bigquery-public-data.covid19_open_data.covid19_open_data') \\ .load() \\ .filter(&quot;country_name == 'United States of America'&quot;) \\ .filter(&quot;date == '2021-10-31'&quot;) \\ .select(&quot;location_key&quot;, expr('cumulative_deceased/(population/100000)').alias('deaths_per_100k'), expr('cumulative_persons_fully_vaccinated/(population - population_age_00_09)').alias('vaccination_rate'), col('subregion2_code').alias('county_fips_code')) joined = mask_use.join(opendata, 'county_fips_code') joined.write.mode('overwrite').parquet(f'gs://{gcs_bucket}/demodata/covid_deaths_and_mask_usage/')  Some background on the above: the covid19_open_data table is being filtered to include only U.S. data and data for Halloween 2021. The deaths_per_100k data point is being calculated using the existing cumulative_deceased and population columns and the vaccination_rate using the total population, subtracting the 0-9 year olds, since they were ineligible for vaccination at the time. For the mask_use_by_county data, &quot;rarely&quot; and &quot;never&quot; data are being combined into a single number, as are &quot;frequently&quot; and &quot;always.&quot; The columns selected from the two datasets are then stored in GCS. Now, add a cell to the notebook and paste this line: spark.read.parquet(f'gs://{gcs_bucket}/demodata/covid_deaths_and_mask_usage/').count()  The notebook should print a warning and a stacktrace (probably a debug statement), then return a total of 3142 records. Now that the pipeline is operational it is available for lineage collection. The docker-compose.yml file that ships with the OpenLineage repo includes only the Jupyter notebook and the Marquez API. To explore the lineage visually, start up the Marquez web project. Without terminating the existing docker containers, run the following command in a new terminal: docker run --network spark_default -p 3000:3000 -e MARQUEZ_HOST=marquez-api -e MARQUEZ_PORT=5000 --link marquez-api:marquez-api marquezproject/marquez-web:0.19.1  Next, open a new browser tab and navigate to http://localhost:3000, which should look like this:  Note: the spark_integration namespace is automatically chosen because there are no other namespaces available. Three jobs are listed on the jobs page of the UI. They all start with openlineage_spark_test, which is the appName passed to the SparkSession when the first cell of the notebook was built. Each query execution or RDD action is represented as a distinct job and the name of the action is appended to the application name to form the name of the job. Clicking on the openlineage_spark_test.execute_insert_into_hadoop_fs_relation_command node calls up the lineage graph for our notebook:  The graph shows that the openlineage_spark_test.execute_insert_into_hadoop_fs_relation_command job reads from two input datasets, bigquery-public-data.covid19_nyt.mask_use_by_county and bigquery-public-data.covid19_open_data.covid19_open_data, and writes to a third dataset, /demodata/covid_deaths_and_mask_usage. The namespace is missing from that third dataset, but the fully qualified name is gs://&lt;your_bucket&gt;/demodata/covid_deaths_and_mask_usage. The bottom bar shows some interesting data that was collected from the Spark job. Dragging the bar up expands the view to offer a closer look.  Two facets always collected from Spark jobs are the spark_version and the spark.logicalPlan. The first simply reports what version of Spark was executing, as well as the version of the openlineage-spark library. This is helpful for debugging job runs. The second facet is the serialized optimized LogicalPlan Spark reports when the job runs. Spark’s query optimization can have dramatic effects on the execution time and efficiency of the query job. Tracking how query plans change over time can significantly aid in debugging slow queries or OutOfMemory errors in production. Clicking on the first BigQuery dataset provides information about the data:  One can see the schema of the dataset as well as the datasource. Similar information is available about the dataset written to in GCS:  As in the BigQuery dataset, one can see the output schema and the datasource — in this case, the gs:// scheme and the name of the bucket written to. In addition to the schema, one can also see a stats facet, reporting the number of output records and bytes as -1. The VERSIONS tab on the bottom bar would display multiple versions if there were any (not the case here). Clicking on the version shows the same schema and statistics facets, but they are specific to the version selected.  In production, this dataset would have many versions, as each time a job runs a new version of the dataset is created. This permits the tracking of changes to the statistics and schema over time, aiding in debugging slow jobs or data quality issues and job failures. The final job in the UI is a HashAggregate job. This represents the count() method called at the end to show the number of records in the dataset. Rather than a count(), this could easily be a toPandas() call or some other job that reads and processes that data -- perhaps one that stores output back into GCS or updates a Postgres database, publishes a new model, etc. Regardless of where the output gets stored, the OpenLineage integration allows one to see the entire lineage graph, unifying datasets in object stores, relational databases, and more traditional data warehouses. "},{"title":"Conclusion​","type":1,"pageTitle":"Using OpenLineage with Spark","url":"/docs/guides/spark#conclusion","content":"The Spark integration from OpenLineage offers users insights into graphs of datasets stored in object stores like S3, GCS, and Azure Blob Storage, as well as BigQuery and relational databases like Postgres. Now with support for Spark 3.1, OpenLineage offers visibility into more environments, such as Databricks, EMR, and Dataproc clusters. "},{"title":"Custom Extractors","type":0,"sectionRef":"#","url":"/docs/integrations/airflow/extractors/custom-extractors","content":"","keywords":""},{"title":"Interface​","type":1,"pageTitle":"Custom Extractors","url":"/docs/integrations/airflow/extractors/custom-extractors#interface","content":"Custom extractors have to derive from BaseExtractor. Extractors have three methods to implement: extract, extract_on_complete and get_operator_classnames. The last one is a classmethod that is used to provide list of operators that your extractor can get lineage from. For example: @classmethod def get_operator_classnames(cls) -&gt; List[str]: return ['PostgresOperator']  If the name of the operator matches one of the names on the list, the extractor will be instantiated - with operator provided in the extractor's self.operator property - and both extract and extract_on_complete methods will be called. They are used to provide actual information data. The difference is that extract is called before operator's executemethod, while extract_on_complete is called after. This can be used to extract any additional information that the operator sets on it's own properties. Good example is SnowflakeOperator that sets query_ids after execution. Both methods return TaskMetadata structure: @attr.s class TaskMetadata: name: str = attr.ib() # deprecated inputs: List[Dataset] = attr.ib(factory=list) outputs: List[Dataset] = attr.ib(factory=list) run_facets: Dict[str, BaseFacet] = attr.ib(factory=dict) job_facets: Dict[str, BaseFacet] = attr.ib(factory=dict)  Inputs and outputs are lists of plain OpenLineage datasets run_facets and job_facets are dictionaries of optional JobFacets and RunFacets that would be attached to the job - for example, you might want to attach SqlJobFacet if your operator is executing SQL. To learn more about facets in OpenLineage, please visit this section. "},{"title":"Registering custom extractor​","type":1,"pageTitle":"Custom Extractors","url":"/docs/integrations/airflow/extractors/custom-extractors#registering-custom-extractor","content":"OpenLineage integration does not know that you've provided an extractor unless you'll register it. The way to do that is to add them to OPENLINEAGE_EXTRACTORS environment variable. OPENLINEAGE_EXTRACTORS=full.path.to.ExtractorClass  If you have multiple custom extractors, separate the paths with comma (;) OPENLINEAGE_EXTRACTORS=full.path.to.ExtractorClass;full.path.to.AnotherExtractorClass  Optionally, you can separate them with whitespace. It's useful if you're providing them as part of some YAML file. OPENLINEAGE_EXTRACTORS: &gt;- full.path.to.FirstExtractor; full.path.to.SecondExtractor  Remember to make sure that the path is importable for scheduler and worker. "},{"title":"Adding extractor to OpenLineage Airflow integration package​","type":1,"pageTitle":"Custom Extractors","url":"/docs/integrations/airflow/extractors/custom-extractors#adding-extractor-to-openlineage-airflow-integration-package","content":"All Openlineage extractors are defined in this path. In order to add new extractor you should put your code in this directory. Additionally, you need to add the class to _extractors list in extractors.py, e.g.: _extractors = list( filter( lambda t: t is not None, [ try_import_from_string( 'openlineage.airflow.extractors.postgres_extractor.PostgresExtractor' ), ... # other extractors are listed here + try_import_from_string( + 'openlineage.airflow.extractors.new_extractor.ExtractorClass' + ), ] ) )  "},{"title":"Debugging issues​","type":1,"pageTitle":"Custom Extractors","url":"/docs/integrations/airflow/extractors/custom-extractors#debugging-issues","content":"There are two common problems associated with custom extractors. First, is wrong path provided to OPENLINEAGE_EXTRACTORS. The path needs to be exactly the same as one you'd use from your code. If the path is wrong or non-importable from worker, plugin will fail to load the extractors and proper OpenLineage events for that operator won't be emitted. Second one, and maybe more insidious, are imports from Airflow. Due to the fact that OpenLineage code gets instantiated when Airflow worker itself starts, any import from Airflow can be unnoticeably cyclical. This causes OpenLineage extraction to fail. To avoid this issue, import from Airflow only locally - in extract or extract_on_complete methods. If you need imports for type checking, guard them behind typing.TYPE_CHECKING. You can also check Development section to learn more about how to setup development environment and create tests. "},{"title":"Manually Annotated Lineage","type":0,"sectionRef":"#","url":"/docs/integrations/airflow/manual","content":"","keywords":""},{"title":"Example​","type":1,"pageTitle":"Manually Annotated Lineage","url":"/docs/integrations/airflow/manual#example","content":"An operator insider the Airflow DAG can be annotated with inlets and outlets like - &quot;&quot;&quot;Example DAG demonstrating the usage of the extraction via Inlets and Outlets.&quot;&quot;&quot; import pendulum import datetime from airflow import DAG from airflow.operators.bash import BashOperator from airflow.lineage.entities import Table, File def create_table(cluster, database, name): return Table( database=database, cluster=cluster, name=name, ) t1 = create_table(&quot;c1&quot;, &quot;d1&quot;, &quot;t1&quot;) t2 = create_table(&quot;c1&quot;, &quot;d1&quot;, &quot;t2&quot;) t3 = create_table(&quot;c1&quot;, &quot;d1&quot;, &quot;t3&quot;) t4 = create_table(&quot;c1&quot;, &quot;d1&quot;, &quot;t4&quot;) f1 = File(url = &quot;http://randomfile&quot;) with DAG( dag_id='example_operator', schedule_interval='0 0 * * *', start_date=pendulum.datetime(2021, 1, 1, tz=&quot;UTC&quot;), dagrun_timeout=datetime.timedelta(minutes=60), params={&quot;example_key&quot;: &quot;example_value&quot;}, ) as dag: task1 = BashOperator( task_id='task_1_with_inlet_outlet', bash_command='echo &quot;{{ task_instance_key_str }}&quot; &amp;&amp; sleep 1', inlets=[t1, t2], outlets=[t3], ) task2 = BashOperator( task_id='task_2_with_inlet_outlet', bash_command='echo &quot;{{ task_instance_key_str }}&quot; &amp;&amp; sleep 1', inlets=[t3, f1], outlets=[t4], ) task1 &gt;&gt; task2 if __name__ == &quot;__main__&quot;: dag.cli()   The corresponding lineage graph will be -  (The image is shown with the Marquez UI (metadata collector of OpenLineage events). More info can be found here. Also note that the File entity is not captured by the lineage event currently.  "},{"title":"Conversion from Airflow Table entity to Openlineage Dataset​","type":1,"pageTitle":"Manually Annotated Lineage","url":"/docs/integrations/airflow/manual#conversion-from-airflow-table-entity-to-openlineage-dataset","content":"The naming convention followed here is: CLUSTER of the table entity becomes the namespace of OpenLineage's DatasetThe name of the dataset is formed by {{DATABASE}}.{{NAME}} where DATABASE and NAME are attributes specified by Airflow's Table entity. "},{"title":"Using OpenLineage with Older Versions of Airflow","type":0,"sectionRef":"#","url":"/docs/integrations/airflow/older","content":"","keywords":""},{"title":"Airflow 2.1 - 2.2​","type":1,"pageTitle":"Using OpenLineage with Older Versions of Airflow","url":"/docs/integrations/airflow/older#airflow-21---22","content":"Integration for those versions has limitations: it does not support tracking failed jobs, and job starts are registered only when job ends. To make OpenLineage work, in addition to installing openlineage-airflow you need to set your LineageBackendin your airflow.cfg or via environmental variable AIRFLOW__LINEAGE__BACKEND to openlineage.lineage_backend.OpenLineageBackend  "},{"title":"Airflow <2.1​","type":1,"pageTitle":"Using OpenLineage with Older Versions of Airflow","url":"/docs/integrations/airflow/older#airflow-21","content":"OpenLineage does not work with versions older than Airflow 2.1. "},{"title":"Testing Custom Extractors","type":0,"sectionRef":"#","url":"/docs/integrations/airflow/extractors/extractor-testing","content":"","keywords":""},{"title":"Testing set-up​","type":1,"pageTitle":"Testing Custom Extractors","url":"/docs/integrations/airflow/extractors/extractor-testing#testing-set-up","content":"We’ll use the same extractor that we built in the blog post, the RedshiftDataExtractor. When testing an extractor, we want to verify a few different sets of assumptions. The first set of assumptions are about the TaskMetadata object being created, specifically verifying that the object is being built with the correct input and output datasets and relevant facets. This is done in OpenLineage via pytest, with appropriate mocking and patching for connections and objects. In the OpenLineage repository, extractor unit tests are found in under [integration/airflow/tests](https://github.com/OpenLineage/OpenLineage/tree/main/integration/airflow/tests). For custom extractors, these tests should go under a tests directory at the top level of your project hierarchy.  An Astro project directory structure, with extractors in an extractors/ folder under include/, and tests under a top-level tests/ folder. "},{"title":"Testing the TaskMetadata object​","type":1,"pageTitle":"Testing Custom Extractors","url":"/docs/integrations/airflow/extractors/extractor-testing#testing-the-taskmetadata-object","content":"For the RedshiftDataExtractor, this core extract test is actually run on extract_on_complete(), as the extract() method is empty. We’ll walk through a test function to see how we can ensure the output dataset is being built as expected (full test code here) # First, we add patching to mock our connection to Redshift. @mock.patch( &quot;airflow.providers.amazon.aws.operators.redshift_data.RedshiftDataOperator.hook&quot;, new_callable=PropertyMock, ) @mock.patch(&quot;botocore.client&quot;) def test_extract_e2e(self, mock_client, mock_hook): # Mock the descriptions we can expect from a real call. mock_client.describe_statement.return_value = self.read_file_json( &quot;tests/extractors/redshift_statement_details.json&quot; ) mock_client.describe_table.return_value = self.read_file_json( &quot;tests/extractors/redshift_table_details.json&quot; ) # Finish setting mock objects' expected values. job_id = &quot;test_id&quot; mock_client.execute_statement.return_value = {&quot;Id&quot;: job_id} mock_hook.return_value.conn = mock_client # Set the extractor and ensure that the extract() method is not returning anything, as expected. extractor = RedshiftDataExtractor(self.task) task_meta_extract = extractor.extract() assert task_meta_extract is None # Run an instance of RedshiftDataOperator with the predefined test values. self.ti.run() # Run extract_on_complete() with the task instance object. task_meta = extractor.extract_on_complete(self.ti) # Assert that the correct job_id was used in the client call. mock_client.describe_statement.assert_called_with(Id=job_id) # Assert there is a list of output datasets. assert task_meta.outputs # Assert there is only dataset in the list. assert len(task_meta.outputs) == 1 # Assert the output dataset name is the same as the table created by the operator query. assert task_meta.outputs[0].name == &quot;dev.public.fruit&quot; # Assert the output dataset has a parsed schema. assert task_meta.outputs[0].facets[&quot;schema&quot;].fields is not None # Assert the datasource is the correct Redshift URI. assert ( task_meta.outputs[0].facets[&quot;dataSource&quot;].name == f&quot;redshift://{CLUSTER_IDENTIFIER}.{REGION_NAME}:5439&quot; ) # Assert the uri is None (as it already exists in dataSource). assert task_meta.outputs[0].facets[&quot;dataSource&quot;].uri is None # Assert the schema fields match the numnber of fields of the table created by the operator query. assert len(task_meta.outputs[0].facets[&quot;schema&quot;].fields) == 3 # Assert the output statistics match the results of the operator query. assert ( OutputStatisticsOutputDatasetFacet( rowCount=1, size=11, ) == task_meta.outputs[0].facets['stats'] )  Most of the assertions above are straightforward, yet all are important in ensuring that no unexpected behavior occurs when building the metadata object. Testing each facet is important, as data or graphs in the UI can render incorrectly if the facets are wrong. For example, if the task_meta.outputs[0].facets[&quot;dataSource&quot;].name is created incorrectly in the extractor, then the operator’s task will not show up in the lineage graph, creating a gap in pipeline observability. "},{"title":"Testing private functions​","type":1,"pageTitle":"Testing Custom Extractors","url":"/docs/integrations/airflow/extractors/extractor-testing#testing-private-functions","content":"Private functions with any complexity beyond returning a string should be unit tested as well. An example of this is the _get_xcom_redshift_job_id() private function in the RedshiftDataExtractor. The unit test is shown below: @mock.patch(&quot;airflow.models.TaskInstance.xcom_pull&quot;) def test_get_xcom_redshift_job_id(self, mock_xcom_pull): self.extractor._get_xcom_redshift_job_id(self.ti) mock_xcom_pull.assert_called_once_with(task_ids=self.ti.task_id)  Unit tests do not have to be particularly complex, and in this instance the single assertion is enough to cover the expected behavior that the function was called only once. "},{"title":"Troubleshooting​","type":1,"pageTitle":"Testing Custom Extractors","url":"/docs/integrations/airflow/extractors/extractor-testing#troubleshooting","content":"Even with unit tests, an extractor may still not be operating as expected. The easiest way to tell if data isn’t coming through correctly is if the UI elements are not showing up correctly in the Lineage tab. When testing code locally, Marquez can be used to inspect the data being emitted—or not being emitted. Using Marquez will allow you to figure out if the error is being caused by the extractor or the API. If data is being emitted from the extractor as expected but isn’t making it to the UI, then the extractor is fine and an issue should be opened up in OpenLineage. However, if data is not being emitted properly, it is likely that more unit tests are needed to cover extractor behavior. Marquez can help you pinpoint which facets are not being formed properly so you know where to add test coverage. "},{"title":"Using the Airflow Integration","type":0,"sectionRef":"#","url":"/docs/integrations/airflow/usage","content":"","keywords":""},{"title":"Environment Variables​","type":1,"pageTitle":"Using the Airflow Integration","url":"/docs/integrations/airflow/usage#environment-variables","content":"The following environment variables are available specifically for the Airflow integration. Name\tDescription\tSinceOPENLINEAGE_AIRFLOW_DISABLE_SOURCE_CODE\tSet to False if you want source code of callables provided in PythonOperator or BashOperator NOT to be included in OpenLineage events. OPENLINEAGE_EXTRACTORS\tThe optional list of extractors class in case you need to use custom extractors. For example: OPENLINEAGE_EXTRACTORS=full.path.to.ExtractorClass;full.path.to.AnotherExtractorClass OPENLINEAGE_NAMESPACE\tThe optional namespace that the lineage data belongs to. If not specified, defaults to default.\t USAGE​ When enabled, the integration will: On TaskInstance start, collect metadata for each task.Collect task input / output metadata (source, schema, etc.).Collect task run-level metadata (execution time, state, parameters, etc.)On TaskInstance complete, also mark the task as complete in Marquez. "},{"title":"Exposing Lineage in Airflow Operators","type":0,"sectionRef":"#","url":"/docs/integrations/airflow/default-extractors","content":"","keywords":""},{"title":"Implementing lineage in an operator​","type":1,"pageTitle":"Exposing Lineage in Airflow Operators","url":"/docs/integrations/airflow/default-extractors#implementing-lineage-in-an-operator","content":"Not surprisingly, you will need an operator class to implement lineage collection in an operator. Here, we’ll use the DfToGcsOperator, a custom operator created by the Astronomer Data team to load arbitrary dataframes to our GCS bucket. We’ll implement both get_openlineage_facets_on_start() and get_openlineage_facets_on_complete() for our custom operator. The specific details of the implementation will vary from operator to operator, but there will always be five basic steps that these functions will share. Both the methods return an OperatorLineage object, which itself is a collection of facets. Four of the five steps mentioned above are creating these facets where necessary, and the fifth is creating the DataSourceDatasetFacet. First, though, we’ll need to import some OpenLineage objects: from openlineage.airflow.extractors.base import OperatorLineage from openlineage.client.facet import ( DataSourceDatasetFacet, SchemaDatasetFacet, SchemaField, ) from openlineage.client.run import Dataset  Now, we’ll start building the facets for the OperatorLineage object in the get_openlineage_facets_on_start() method. "},{"title":"1. DataSourceDatasetFacet​","type":1,"pageTitle":"Exposing Lineage in Airflow Operators","url":"/docs/integrations/airflow/default-extractors#1-datasourcedatasetfacet","content":"The DataSourceDatasestFacet is a simple object, containing two fields, name and uri, which should be populated with the unique name of the data source and the URI. We’ll make two of these objects, an input_source to specify where the data came from and an output_source to specify where the data is going. A quick note about the philosophy behind the name and uri in the OpenLineage spec: the uri is built from the namespace and the name, and each is expected to be unique with respect to its environment. This means a namespace should be globally unique in the OpenLineage universe, and the name unique within the namespace. The two are then concatenated to form the uri, so that uri = namespace + name. The full naming spec can be found here. In our case, the input name will be the table we are pulling data from, self.table, and the namespace will be our self.data_source. input_source = DataSourceDatasetFacet( name=self.table, uri=&quot;://&quot;.join([self.data_source, self.table]), )  The output data source object’s name will always be the base path given to the operator, self.base_path. The namespace is always in GCS, so we use the OpenLineage spec’s gs:// as the scheme and our bucket as the authority, giving us gs://{ol_bucket}. The uri is simply the concatenation of the two. if not self.bucket: ol_bucket = get_env_bucket() else: ol_bucket = self.bucket output_namespace = &quot;gs://&quot; + ol_bucket output_name = self.base_path output_uri = &quot;/&quot;.join( [ output_namespace, output_name, ] ) output_source = DataSourceDatasetFacet( name=output_name, uri=output_uri, )  "},{"title":"2. Inputs​","type":1,"pageTitle":"Exposing Lineage in Airflow Operators","url":"/docs/integrations/airflow/default-extractors#2-inputs","content":"Next we’ll create the input dataset object. As we are moving data from a dataframe to GCS in this operator, we’ll make sure that we are capturing all the info in the dataframe being extracted in a Dataset. To create the Dataset object, we’ll need namespace, name, and facets objects. The first two are strings, and facets is a dictionary. Our namespace will come from the operator, where we use self.data_source again. The name parameter for this facet will be the table, again coming from the operator’s parameter list. The facets will contain two entries, the first being our DataSourceDatasetFacet with the key &quot;datasource&quot; coming from the previous step and input_source being the value. The second has the key &quot;schema&quot;, with the value being a SchemaDatasetFacet, which itself is a collection of SchemaField objects, one for each column, created via a list comprehension over the operator's self.col_types parameter. The inputs parameter to OperatorLineage is a list of Dataset objects, so we’ll end up adding a single Dataset object to the list later. The creation of the Dataset object looks like the following: input_facet = { &quot;datasource&quot;: input_source, &quot;schema&quot;: SchemaDatasetFacet( fields=[ SchemaField(name=col_name, type=col_type) for col_name, col_type in self.col_types.items() ] ), } input = Dataset(namespace=self.data_source, name=self.table, facets=input_facet)  "},{"title":"3. Outputs​","type":1,"pageTitle":"Exposing Lineage in Airflow Operators","url":"/docs/integrations/airflow/default-extractors#3-outputs","content":"Our output facet will closely resemble the input facet, except it will use the output_source we previously created, and will also have a different namespace. Our output facet object will be built as follows: output_facet = { &quot;datasource&quot;: output_source, &quot;schema&quot;: SchemaDatasetFacet( fields=[ SchemaField(name=col_name, type=col_type) for col_name, col_type in self.col_types.items() ] ), } output = Dataset( namespace=output_namespace, name=output_name, facets=output_facet, )  "},{"title":"4. Job facets​","type":1,"pageTitle":"Exposing Lineage in Airflow Operators","url":"/docs/integrations/airflow/default-extractors#4-job-facets","content":"A Job in OpenLineage is a process definition that consumes and produces datasets. The Job evolves over time, and this change is captured when the Job runs. This means the facets we would want to capture in the Job level are independent of the state of the Job. Custom facets can be created to capture this Job data. For our operator, we went with pre-existing job facets, the DocumentationJobFacet and the OwnershipJobFacet: job_facets = { &quot;documentation&quot;: DocumentationJobFacet( description=f&quot;&quot;&quot; Takes data from the data source {input_uri} and puts it in GCS at the path: {output_uri} &quot;&quot;&quot; ), &quot;ownership&quot;: OwnershipJobFacet( owners=[OwnershipJobFacetOwners(name=self.owner, type=self.email)] ) }  "},{"title":"5. Run facets​","type":1,"pageTitle":"Exposing Lineage in Airflow Operators","url":"/docs/integrations/airflow/default-extractors#5-run-facets","content":"A Run is an instance of a Job execution. For example, when an Airflow Operator begins execution, the Run state of the OpenLineage Job transitions to Start, then to Running. When writing an emitter, this means a Run facet should contain information pertinent to the specific instance of the Job, something that could change every Run. In this example, we will output an error message when there is an empty dataframe, using the existing ErrorMessageRunFacet. starting_facets.run_facets = { &quot;errorMessage&quot;: ErrorMessageRunFacet( message=&quot;Empty dataframe, no artifact saved to GCS.&quot;, programmingLanguage=&quot;python&quot; ) }  "},{"title":"6. On complete​","type":1,"pageTitle":"Exposing Lineage in Airflow Operators","url":"/docs/integrations/airflow/default-extractors#6-on-complete","content":"Finally, we’ll implement the get_openlineage_metadata_on_complete() method. Most of our work has already been done for us, so we will start by calling get_openlineage_metadata_on_start() and then modifying the returned object slightly before returning it again. The two main additions here are replacing the original SchemaDatasetFacet fields and adding a potential error message to the run_facets. For the SchemaDatasetFacet update, we replace the old fields facet with updated ones based on the now-filled-out df_meta dict, which is populated during the operator’s execute() method and is therefore unavailable to get_openlineage_metadata_on_start(). Because df_meta is already a list of SchemaField objects, we can set the property directly. Although we use a for loop here, the operator ensures only one dataframe will ever be extracted per execution, so the for loop will only ever run once and we therefore do not have to worry about multiple input dataframes updating. The run_facets update is performed only if there is an error, which is a mutually exclusive event to updating the fields facets. We pass the same message to this facet that is printed in the execute() method when an empty dataframe is found. This error message does not halt operator execution, as it gets added *after* execution, but it does create an alert in the Marquez UI. def get_openlineage_facets_on_complete(self, task_instance): &quot;&quot;&quot;Add lineage to DfToGcsOperator on task completion.&quot;&quot;&quot; starting_facets = self.get_openlineage_facets_on_start() if task_instance.task.df_meta is not None: for i in starting_facets.inputs: i.facets[&quot;SchemaDatasetFacet&quot;].fields = task_instance.task.df_meta else: starting_facets.run_facets = { &quot;errorMessage&quot;: ErrorMessageRunFacet( message=&quot;Empty dataframe, no artifact saved to GCS.&quot;, programmingLanguage=&quot;python&quot; ) } return starting_facets  And with that final piece of the puzzle, we have a working implementation of lineage extraction from our custom operator! "},{"title":"Custom Facets​","type":1,"pageTitle":"Exposing Lineage in Airflow Operators","url":"/docs/integrations/airflow/default-extractors#custom-facets","content":"The OpenLineage spec might not contain all the facets you need to write your extractor, in which case you will have to make your own custom facets. More on creating custom facets can be found here. "},{"title":"Testing​","type":1,"pageTitle":"Exposing Lineage in Airflow Operators","url":"/docs/integrations/airflow/default-extractors#testing","content":"For information about testing your implementation, see the doc on testing custom extractors. "},{"title":"Great Expectations","type":0,"sectionRef":"#","url":"/docs/integrations/great-expectations","content":"","keywords":""},{"title":"How does Great Expectations work with OpenLineage?​","type":1,"pageTitle":"Great Expectations","url":"/docs/integrations/great-expectations#how-does-great-expectations-work-with-openlineage","content":"Great Expecations integrates with OpenLineage through the action list in a checkpoint. An OpenLineage action can be specified, which is triggered when all expectations are run. Data from the checkpoint is sent to OpenLineage, which can then be viewed in Marquez or Datakin. "},{"title":"Preparing a Great Expectations project for OpenLineage​","type":1,"pageTitle":"Great Expectations","url":"/docs/integrations/great-expectations#preparing-a-great-expectations-project-for-openlineage","content":"First, we specify where we want Great Expectations to send OpenLineage events by setting the OPENLINEAGE_URL environment variable. For example, to send OpenLineage events to a local instance of Marquez, use: OPENLINEAGE_URL=http://localhost:5000  If data is being sent to an endpoint with an API key, then that key must be supplied as well: OPENLINEAGE_API_KEY=123456789  We can optionally specify a namespace where the lineage events will be stored. For example, to use the namespace &quot;dev&quot;: OPENLINEAGE_NAMESPACE=dev  With these environment variables set, we can add the OpenLineage action to the action list of the Great Expecations checkpoint. Note: this must be done for each checkpoint. Note: when using the GreatExpectationsOperator&gt;=0.2.0 in Airflow, there is a boolean parameter, defaulting to True, that will automatically create this action list item when it detects the OpenLineage environment specified in the previous step. In a python checkpoint, this looks like: action_list = [ { &quot;name&quot;: &quot;store_validation_result&quot;, &quot;action&quot;: {&quot;class_name&quot;: &quot;StoreValidationResultAction&quot;}, }, { &quot;name&quot;: &quot;store_evaluation_params&quot;, &quot;action&quot;: {&quot;class_name&quot;: &quot;StoreEvaluationParametersAction&quot;}, }, { &quot;name&quot;: &quot;update_data_docs&quot;, &quot;action&quot;: {&quot;class_name&quot;: &quot;UpdateDataDocsAction&quot;, &quot;site_names&quot;: []}, }, { &quot;name&quot;: &quot;open_lineage&quot;, &quot;action&quot;: { &quot;class_name&quot;: &quot;OpenLineageValidationAction&quot;, &quot;module_name&quot;: &quot;openlineage.common.provider.great_expectations&quot;, &quot;openlineage_host&quot;: os.getenv(&quot;OPENLINEAGE_URL&quot;), &quot;openlineage_apiKey&quot;: os.getenv(&quot;OPENLINEAGE_API_KEY&quot;), &quot;openlineage_namespace&quot;: oss.getenv(&quot;OPENLINEAGE_NAMESPACE&quot;), &quot;job_name&quot;: &quot;openlineage_job&quot;, }, } ]  And in yaml: name: openlineage action: class_name: OpenLineageValidationAction module_name: openlineage.common.provider.great_expectations openlineage_host: &lt;HOST&gt; openlineage_apiKey: &lt;API_KEY&gt; openlineage_namespace: &lt;NAMESPACE_NAME&gt; # Replace with your job namespace; we recommend a meaningful namespace like `dev` or `prod`, etc. job_name: validate_my_dataset  Then run your Great Expecations checkpoint with the CLI or your integration of choice. "},{"title":"Feedback​","type":1,"pageTitle":"Great Expectations","url":"/docs/integrations/great-expectations#feedback","content":"What did you think of this guide? You can reach out to us on slack and leave us feedback! "},{"title":"dbt","type":0,"sectionRef":"#","url":"/docs/integrations/dbt","content":"","keywords":""},{"title":"How does dbt work with OpenLineage?​","type":1,"pageTitle":"dbt","url":"/docs/integrations/dbt#how-does-dbt-work-with-openlineage","content":"Fortunately, dbt already collects a lot of the data required to create and emit OpenLineage events. When it runs, it creates a target/manifest.json file containing information about jobs and the datasets they affect, and a target/run_results.json file containing information about the run-cycle. These files can be used to trace lineage and job performance. In addition, by using the create catalog command, a user can instruct dbt to create a target/catalog.json file containing information about dataset schemas. These files contain everything needed to trace lineage. However, the target/manifest.json and target/run_results.json files are only populated with comprehensive metadata after completion of a run-cycle. This integration is implemented as a wrapper script, dbt-ol, that calls dbt and, after the run has completed, collects information from the three json files and calls the OpenLineage API accordingly. For most users, enabling OpenLineage metadata collection can be accomplished by simply substituting dbt-ol for dbt when performing a run. "},{"title":"Preparing a dbt project for OpenLineage​","type":1,"pageTitle":"dbt","url":"/docs/integrations/dbt#preparing-a-dbt-project-for-openlineage","content":"First, we need to install the integration: pip3 install openlineage-dbt  Next, we specify where we want dbt to send OpenLineage events by setting the OPENLINEAGE_URL environment variable. For example, to send OpenLineage events to a local instance of Marquez, use: OPENLINEAGE_URL=http://localhost:5000  Finally, we can optionally specify a namespace where the lineage events will be stored. For example, to use the namespace &quot;dev&quot;: OPENLINEAGE_NAMESPACE=dev  "},{"title":"Running dbt with OpenLineage​","type":1,"pageTitle":"dbt","url":"/docs/integrations/dbt#running-dbt-with-openlineage","content":"To run your dbt project with OpenLineage collection, simply replace dbt with dbt-ol: dbt-ol run  The dbt-ol wrapper supports all of the standard dbt subcommands, and is safe to use as a substitutuon (i.e., in an alias). Once the run has completed, you will see output containing the number of events sent via the OpenLineage API: Completed successfully Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2 Emitted 4 openlineage events  "},{"title":"Where can I learn more?​","type":1,"pageTitle":"dbt","url":"/docs/integrations/dbt#where-can-i-learn-more","content":"Watch a short demonstration of the integration in action "},{"title":"Feedback​","type":1,"pageTitle":"dbt","url":"/docs/integrations/dbt#feedback","content":"What did you think of this guide? You can reach out to us on slack and leave us feedback! "},{"title":"Apache Flink","type":0,"sectionRef":"#","url":"/docs/integrations/flink","content":"","keywords":""},{"title":"Getting lineage from Flink​","type":1,"pageTitle":"Apache Flink","url":"/docs/integrations/flink#getting-lineage-from-flink","content":"OpenLineage utilizes Flink's JobListener interface. This interface is used by Flink to notify user of job submission, successful finish of job, or job failure. Implementations of this interface are executed on JobClient. When OpenLineage listener receives information that job was submitted, it extracts Transformations from job'sExecutionEnvironment. The Transformations represent logical operations in the dataflow graph; they are composed of both Flink's build-in operators, but also user-provided Sources, Sinks and functions. To get the lineage, OpenLineage integration processes dataflow graph. Currently, OpenLineage is interested only in information contained in Sources and Sinks, as they are the places where Flink interacts with external systems. After job submission, OpenLineage integration starts actively listening to checkpoints - this gives insight into whether the job runs properly. "},{"title":"Limitations​","type":1,"pageTitle":"Apache Flink","url":"/docs/integrations/flink#limitations","content":"Currently OpenLineage's Flink integration is limited to getting information from jobs running in Application Mode. OpenLineage integration extracts lineage only from following Sources and Sinks: Sources\tSinks KafkaSource\tKafkaSink (1) FlinkKafkaConsumer\tFlinkKafkaProducer IcebergFlinkSource\tIcebergFlinkSink We expect this list to grow as we add support for more connectors. (1) KafkaSink supports sinks that write to a single topic as well as multi topic sinks. The limitation for multi topic sink is that: topics need to have the same schema and implementation of KafkaRecordSerializationSchema must extend KafkaTopicsDescriptor. Methods isFixedTopics and getFixedTopics from KafkaTopicsDescriptor are used to extract multiple topics from a sink. "},{"title":"Usage​","type":1,"pageTitle":"Apache Flink","url":"/docs/integrations/flink#usage","content":"In your job, you need to set up OpenLineageFlinkJobListener. For example:  JobListener listener = JobListener listener = OpenLineageFlinkJobListener.builder() .executionEnvironment(streamExecutionEnvironment) .build(); streamExecutionEnvironment.registerJobListener(listener);  Also, OpenLineage needs certain parameters to be set in flink-conf.yaml: Configuration Key\tDescription\tExpected Value\tDefault execution.attached\tThis setting needs to be true if OpenLineage is to detect job start and failure\ttrue\tfalse OpenLineage jar needs to be present on JobManager. When the JobListener is configured, you need to point the OpenLineage integration where the events should end up. If you're using Marquez, simplest way to do that is to set up OPENLINEAGE_URL environment variable to Marquez URL. More advanced settings are in the client documentation.. "},{"title":"Configuring Openlineage connector​","type":1,"pageTitle":"Apache Flink","url":"/docs/integrations/flink#configuring-openlineage-connector","content":"Flink Openlineage connector utilizes standard Java client for Openlineageand allows all the configuration features present there to be used. The configuration can be passed with: openlineage.yml file with a environment property OPENLINEAGE_CONFIG being set and pointing to configuration file. File structure and allowed options are described here.Standard Flink configuration with the parameters defined below. "},{"title":"Flink Configuration parameters​","type":1,"pageTitle":"Apache Flink","url":"/docs/integrations/flink#flink-configuration-parameters","content":"The following parameters can be specified: Parameter\tDefinition\tExampleopenlineage.transport.type\tThe transport type used for event emit, default type is console\thttp openlineage.facets.disabled\tList of facets to disable, enclosed in [] (required from 0.21.x) and separated by ;\t[some_facet1;some_facet1] HTTP​ Parameter\tDefinition\tExampleopenlineage.transport.endpoint\tPath to resource\t/api/v1/lineage openlineage.transport.apiKey\tAn API key to be used when sending events to the OpenLineage server\tabcdefghijk openlineage.transport.timeout\tTimeout for sending OpenLineage info in milliseconds\t5000 openlineage.transport.urlParams.xyz\tA URL parameter (replace xyz) and value to be included in requests to the OpenLineage API server\tabcdefghijk openlineage.transport.url\tThe hostname of the OpenLineage API server where events should be reported, it can have other properties embeded\thttp://localhost:5000 openlineage.transport.headers.xyz\tRequest headers (replace xyz) and value to be included in requests to the OpenLineage API server\tabcdefghijk URL​ You can supply http parameters using values in url, the parsed openlineage.* properties are located in url as follows: {transport.url}/{transport.endpoint}/namespaces/{namespace}/jobs/{parentJobName}/runs/{parentRunId}?app_name={appName}&amp;api_key={transport.apiKey}&amp;timeout={transport.timeout}&amp;xxx={transport.urlParams.xxx} example: http://localhost:5000/api/v1/namespaces/ns_name/jobs/job_name/runs/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx?app_name=app&amp;api_key=abc&amp;timeout=5000&amp;xxx=xxx Kinesis​ If openlineage.transport.type is set to kinesis, then the below parameters would be read and used when building KinesisProducer. Also, KinesisTransport depends on you to provide artifact com.amazonaws:amazon-kinesis-producer:0.14.0 or compatible on your classpath. Parameter\tDefinition\tExampleopenlineage.transport.streamName\tRequired, the streamName of the Kinesis Stream\tsome-stream-name openlineage.transport.region\tRequired, the region of the stream\tus-east-2 openlineage.transport.roleArn\tOptional, the roleArn which is allowed to read/write to Kinesis stream\tsome-role-arn openlineage.transport.properties.[xxx]\tOptional, the [xxx] is property of Kinesis allowd properties\t1 Kafka​ If openlineage.transport.type is set to kafka, then the below parameters would be read and used when building KafkaProducer. Parameter\tDefinition\tExampleopenlineage.transport.topicName\tRequired, name of the topic\ttopic-name openlineage.transport.localServerId\tRequired, id of local server\txxxxxxxx openlineage.transport.properties.[xxx]\tOptional, the [xxx] is property of Kafka client\t1 Please note that configuration parameters provided via standard Flink configuration are translated to Openlineage Java client config entries and whenever new configuration feature is added to a Java client, it will be available for Flink users out of the box with no changes required. "},{"title":"Quickstart with Databricks","type":0,"sectionRef":"#","url":"/docs/integrations/spark/quickstart_databricks","content":"","keywords":""},{"title":"Enable OpenLineage​","type":1,"pageTitle":"Quickstart with Databricks","url":"/docs/integrations/spark/quickstart_databricks#enable-openlineage","content":"Follow the steps below to enable OpenLineage on Databricks. Build the jar via Gradle or download the latest release.Configure the Databricks CLI with your desired workspace: Azure Databricks CLIGCP Databricks CLIAWS Databricks CLI Run upload-to-databricks.sh or upload-to-databricks.ps1. This will: create a folder in DBFS to store the OpenLineage jar.copy the jar to the DBFS foldercopy the init script to the DBFS folder Create an interactive or job cluster with the relevant Spark configs: spark.openlineage.transport.type console spark.extraListeners io.openlineage.spark.agent.OpenLineageSparkListener spark.openlineage.version v1 Create manually open-lineage-init-script.sh through Workspace section in Databricks UI. Paste the script content from this file.Make the cluster init script to point to previously created file. For example, if you create open-lineage-init-script.sh within Shared, then init scripts should point to /Shared/open-lineage-init-script.sh. User's workspace may be used as well. Alternatively, init script can be located in S3. Please mind that DBFS located init script are no longer supported (starting September 2023). info Please note that the init script approach is currently obligatory to install OpenLineage on Databricks. The Openlineage integration relies on providing a custom extra listener class io.openlineage.spark.agent.OpenLineageSparkListener that has to be available on the classpath at the driver startup. Providing it with spark.jars.packages does not work on the Databricks platform as of August 2022. "},{"title":"Verify Initialization​","type":1,"pageTitle":"Quickstart with Databricks","url":"/docs/integrations/spark/quickstart_databricks#verify-initialization","content":"A successful initialization will emit logs in the Log4j output that look similar to the following: YY/MM/DD HH:mm:ss INFO SparkContext: Registered listener io.openlineage.spark.agent.OpenLineageSparkListener YY/MM/DD HH:mm:ss INFO OpenLineageContext: Init OpenLineageContext: Args: ArgumentParser(host=https://YOURHOST, version=v1, namespace=YOURNAMESPACE, jobName=default, parentRunId=null, apiKey=Optional.empty) URI: https://YOURHOST/api/v1/lineage YY/MM/DD HH:mm:ss INFO AsyncEventQueue: Process of event SparkListenerApplicationStart(Databricks Shell,Some(app-XXX-0000),YYYY,root,None,None,None) by listener OpenLineageSparkListener took Xs.  "},{"title":"Create a Dataset​","type":1,"pageTitle":"Quickstart with Databricks","url":"/docs/integrations/spark/quickstart_databricks#create-a-dataset","content":"Open a notebook and create an example dataset with: spark.createDataFrame([ {'a': 1, 'b': 2}, {'a': 3, 'b': 4} ]).write.mode(&quot;overwrite&quot;).saveAsTable(&quot;default.temp&quot;)  "},{"title":"Observe OpenLineage Events​","type":1,"pageTitle":"Quickstart with Databricks","url":"/docs/integrations/spark/quickstart_databricks#observe-openlineage-events","content":"To troubleshoot or observe OpenLineage information in Databricks, see the Log4j output in the Cluster definition's Driver Logs. The Log4j output should contain entries starting with a message INFO ConsoleTransport that contain generated OpenLineage events: {&quot;eventType&quot;:&quot;COMPLETE&quot;,&quot;eventTime&quot;:&quot;2022-08-01T08:36:21.633Z&quot;,&quot;run&quot;:{&quot;runId&quot;:&quot;64537bbd-00ac-498d-ad49-1c77e9c2aabd&quot;,&quot;facets&quot;:{&quot;spark_unknown&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-2/OpenLineage.json#/$defs/RunFacet&quot;,&quot;inputs&quot;:[{&quot;description&quot;:{&quot;@class&quot;:&quot;org.apache.spark.sql.catalyst.analysis.ResolvedTableName&quot;,&quot;id&quot;:1,&quot;traceEnabled&quot;:false,&quot;streaming&quot;:false,&quot;cacheId&quot;:{&quot;id&quot;:2,&quot;empty&quot;:true,&quot;defined&quot;:false},&quot;canonicalizedPlan&quot;:false,&quot;defaultTreePatternBits&quot;:{&quot;id&quot;:3}},&quot;inputAttributes&quot;:[],&quot;outputAttributes&quot;:[]},{&quot;description&quot;:{&quot;@class&quot;:&quot;org.apache.spark.sql.execution.LogicalRDD&quot;,&quot;id&quot;:1,&quot;streaming&quot;:false,&quot;traceEnabled&quot;:false,&quot;cacheId&quot;:{&quot;id&quot;:2,&quot;empty&quot;:true,&quot;defined&quot;:false},&quot;canonicalizedPlan&quot;:false,&quot;defaultTreePatternBits&quot;:{&quot;id&quot;:3}},&quot;inputAttributes&quot;:[],&quot;outputAttributes&quot;:[{&quot;name&quot;:&quot;a&quot;,&quot;type&quot;:&quot;long&quot;,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;b&quot;,&quot;type&quot;:&quot;long&quot;,&quot;metadata&quot;:{}}]}]},&quot;spark.logicalPlan&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-2/OpenLineage.json#/$defs/RunFacet&quot;,&quot;plan&quot;:[{&quot;class&quot;:&quot;org.apache.spark.sql.catalyst.plans.logical.ReplaceTableAsSelect&quot;,&quot;num-children&quot;:2,&quot;name&quot;:0,&quot;partitioning&quot;:[],&quot;query&quot;:1,&quot;tableSpec&quot;:null,&quot;writeOptions&quot;:null,&quot;orCreate&quot;:true},{&quot;class&quot;:&quot;org.apache.spark.sql.catalyst.analysis.ResolvedTableName&quot;,&quot;num-children&quot;:0,&quot;catalog&quot;:null,&quot;ident&quot;:null},{&quot;class&quot;:&quot;org.apache.spark.sql.execution.LogicalRDD&quot;,&quot;num-children&quot;:0,&quot;output&quot;:[[{&quot;class&quot;:&quot;org.apache.spark.sql.catalyst.expressions.AttributeReference&quot;,&quot;num-children&quot;:0,&quot;name&quot;:&quot;a&quot;,&quot;dataType&quot;:&quot;long&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{},&quot;exprId&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.expressions.ExprId&quot;,&quot;id&quot;:18,&quot;jvmId&quot;:&quot;481bebf6-f861-400e-bb00-ea105ed8afef&quot;},&quot;qualifier&quot;:[]}],[{&quot;class&quot;:&quot;org.apache.spark.sql.catalyst.expressions.AttributeReference&quot;,&quot;num-children&quot;:0,&quot;name&quot;:&quot;b&quot;,&quot;dataType&quot;:&quot;long&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{},&quot;exprId&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.expressions.ExprId&quot;,&quot;id&quot;:19,&quot;jvmId&quot;:&quot;481bebf6-f861-400e-bb00-ea105ed8afef&quot;},&quot;qualifier&quot;:[]}]],&quot;rdd&quot;:null,&quot;outputPartitioning&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning&quot;,&quot;numPartitions&quot;:0},&quot;outputOrdering&quot;:[],&quot;isStreaming&quot;:false,&quot;session&quot;:null}]},&quot;spark_version&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-2/OpenLineage.json#/$defs/RunFacet&quot;,&quot;spark-version&quot;:&quot;3.2.1&quot;,&quot;openlineage-spark-version&quot;:&quot;0.12.0-SNAPSHOT&quot;}}},&quot;job&quot;:{&quot;namespace&quot;:&quot;default&quot;,&quot;name&quot;:&quot;databricks_shell.atomic_replace_table_as_select&quot;,&quot;facets&quot;:{}},&quot;inputs&quot;:[],&quot;outputs&quot;:[{&quot;namespace&quot;:&quot;dbfs&quot;,&quot;name&quot;:&quot;/user/hive/warehouse/temp&quot;,&quot;facets&quot;:{&quot;dataSource&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet&quot;,&quot;name&quot;:&quot;dbfs&quot;,&quot;uri&quot;:&quot;dbfs&quot;},&quot;schema&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;a&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;b&quot;,&quot;type&quot;:&quot;long&quot;}]},&quot;storage&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/StorageDatasetFacet.json#/$defs/StorageDatasetFacet&quot;,&quot;storageLayer&quot;:&quot;delta&quot;,&quot;fileFormat&quot;:&quot;parquet&quot;},&quot;lifecycleStateChange&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet&quot;,&quot;lifecycleStateChange&quot;:&quot;OVERWRITE&quot;}},&quot;outputFacets&quot;:{}}],&quot;producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-2/OpenLineage.json#/$defs/RunEvent&quot;}  The generated JSON contains the output dataset name and location {&quot;namespace&quot;:&quot;dbfs&quot;,&quot;name&quot;:&quot;/user/hive/warehouse/temp&quot;&quot; metadata, schema fields [{&quot;name&quot;:&quot;a&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;b&quot;,&quot;type&quot;:&quot;long&quot;}], and more. "},{"title":"Apache Spark","type":0,"sectionRef":"#","url":"/docs/integrations/spark/","content":"","keywords":""},{"title":"Collecting Lineage in Spark​","type":1,"pageTitle":"Apache Spark","url":"/docs/integrations/spark/#collecting-lineage-in-spark","content":"Collecting lineage requires hooking into Spark's ListenerBus in the driver application and collecting and analyzing execution events as they happen. Both raw RDD and Dataframe jobs post events to the listener bus during execution. These events expose the structure of the job, including the optimized query plan, allowing the Spark integration to analyze the job for datasets consumed and produced, including attributes about the storage, such as location in GCS or S3, table names in a relational database or warehouse, such as Redshift or Bigquery, and schemas. In addition to dataset and job lineage, Spark SQL jobs also report logical plans, which can be compared across job runs to track important changes in query plans, which may affect the correctness or speed of a job. A single Spark application may execute multiple jobs. The Spark OpenLineage integration maps one Spark job to a single OpenLineage Job. The application will be assigned a Run id at startup and each job that executes will report the application's Run id as its parent job run. Thus, an application that reads one or more source datasets, writes an intermediate dataset, then transforms that intermediate dataset and writes a final output dataset will report three jobs- the parent application job, the initial job that reads the sources and creates the intermediate dataset, and the final job that consumes the intermediate dataset and produces the final output. As an image: "},{"title":"How to Use the Integration​","type":1,"pageTitle":"Apache Spark","url":"/docs/integrations/spark/#how-to-use-the-integration","content":"Adding OpenLineage metadata collection to existing Spark jobs was designed to be straightforward and unobtrusive to the application. "},{"title":"SparkListener​","type":1,"pageTitle":"Apache Spark","url":"/docs/integrations/spark/#sparklistener","content":"The SparkListener approach is very simple and covers most cases. The listener simply analyzes events, as they are posted by the SparkContext, and extracts job and dataset metadata that are exposed by the RDD and Dataframe dependency graphs. Most data sources, such as filesystem sources (including S3 and GCS), JDBC backends, and warehouses such as Redshift and Bigquery can be analyzed and reported in this way. Installation requires adding a following package: &lt;dependency&gt; &lt;groupId&gt;io.openlineage&lt;/groupId&gt; &lt;artifactId&gt;openlineage-spark&lt;/artifactId&gt; &lt;version&gt;{spark-openlineage-version}&lt;/version&gt; &lt;/dependency&gt;  or gradle: implementation 'io.openlineage:openlineage-spark:{spark-openlineage-version}'  spark-submit​ The listener can be enabled by adding the following configuration to a spark-submit command: spark-submit --conf &quot;spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener&quot; \\ --packages &quot;io.openlineage:openlineage-spark:&lt;spark-openlineage-version&gt;&quot; \\ --conf &quot;spark.openlineage.transport.type=http&quot; \\ --conf &quot;spark.openlineage.transport.url=http://{openlineage.client.host}/api/v1/namespaces/spark_integration/&quot; \\ --class com.mycompany.MySparkApp my_application.jar  The SparkListener reads its configuration from SparkConf parameters. These can be specified on the command line (e.g., --conf &quot;spark.openlineage.transport.url=http://{openlineage.client.host}/api/v1/namespaces/my_namespace/job/the_job&quot;) or from the conf/spark-defaults.conf file. Spark Config Parameters​ The following parameters can be specified: Parameter\tDefinition\tExamplespark.openlineage.transport.type\tThe transport type used for event emit, default type is console\thttp spark.openlineage.namespace\tThe default namespace to be applied for any jobs submitted\tMyNamespace spark.openlineage.parentJobName\tThe job name to be used for the parent job facet\tParentJobName spark.openlineage.parentRunId\tThe RunId of the parent job that initiated this Spark job\txxxx-xxxx-xxxx-xxxx spark.openlineage.appName\tCustom value overwriting Spark app name in events\tAppName spark.openlineage.facets.disabled\tList of facets to disable, enclosed in [] (required from 0.21.x) and separated by ;, default is [spark_unknown;] (currently must contain ;)\t[spark_unknown;spark.logicalPlan] spark.openlineage.capturedProperties\tcomma separated list of properties to be captured in spark properties facet (default spark.master, spark.app.name)\t&quot;spark.example1,spark.example2&quot; spark.openlineage.dataset.removePath.pattern\tJava regular expression that removes ?&lt;remove&gt; named group from dataset path. Can be used to last path subdirectories from paths like s3://my-whatever-path/year=2023/month=04\t(.*)(?&lt;remove&gt;\\/.*\\/.*) spark.openlineage.jobName.appendDatasetName\tDecides whether output dataset name should be appended to job name. By default true.\tfalse spark.openlineage.jobName.replaceDotWithUnderscore\tReplaces dots in job name with underscore. Can be used to mimic legacy behaviour on Databricks platform. By default false.\tfalse spark.openlineage.debugFacet\tDetermines whether debug facet shall be generated and included within the event. Set enabled to turn it on. By default, facet is disabled.\tenabled HTTP​ Parameter\tDefinition\tExamplespark.openlineage.transport.endpoint\tPath to resource\t/api/v1/lineage spark.openlineage.transport.auth.type\tThe type of authentication method to use\tapi_key spark.openlineage.transport.auth.apiKey\tAn API key to be used when sending events to the OpenLineage server\tabcdefghijk spark.openlineage.transport.timeout\tTimeout for sending OpenLineage info in milliseconds\t5000 spark.openlineage.transport.urlParams.xyz\tA URL parameter (replace xyz) and value to be included in requests to the OpenLineage API server\tabcdefghijk spark.openlineage.transport.url\tThe hostname of the OpenLineage API server where events should be reported, it can have other properties embeded\thttp://localhost:5000 spark.openlineage.transport.headers.xyz\tRequest headers (replace xyz) and value to be included in requests to the OpenLineage API server\tabcdefghijk URL​ You can supply http parameters using values in url, the parsed spark.openlineage.* properties are located in url as follows: {transport.url}/{transport.endpoint}/namespaces/{namespace}/jobs/{parentJobName}/runs/{parentRunId}?app_name={appName}&amp;api_key={transport.apiKey}&amp;timeout={transport.timeout}&amp;xxx={transport.urlParams.xxx} example: http://localhost:5000/api/v1/namespaces/ns_name/jobs/job_name/runs/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx?app_name=app&amp;api_key=abc&amp;timeout=5000&amp;xxx=xxx Kinesis​ If spark.openlineage.transport.type is set to kinesis, then the below parameters would be read and used when building KinesisProducer. Also, KinesisTransport depends on you to provide artifact com.amazonaws:amazon-kinesis-producer:0.14.0 or compatible on your classpath. Parameter\tDefinition\tExamplespark.openlineage.transport.streamName\tRequired, the streamName of the Kinesis Stream\tsome-stream-name spark.openlineage.transport.region\tRequired, the region of the stream\tus-east-2 spark.openlineage.transport.roleArn\tOptional, the roleArn which is allowed to read/write to Kinesis stream\tsome-role-arn spark.openlineage.transport.properties.[xxx]\tOptional, the [xxx] is property of Kinesis allowd properties\t1 Kafka​ If spark.openlineage.transport.type is set to kafka, then the below parameters would be read and used when building KafkaProducer. Parameter\tDefinition\tExamplespark.openlineage.transport.topicName\tRequired, name of the topic\ttopic-name spark.openlineage.transport.localServerId\tRequired, id of local server\txxxxxxxx spark.openlineage.transport.properties.[xxx]\tOptional, the [xxx] is property of Kafka client\t1 "},{"title":"Scheduling from Airflow​","type":1,"pageTitle":"Apache Spark","url":"/docs/integrations/spark/#scheduling-from-airflow","content":"The same parameters passed to spark-submit can be supplied from Airflow and other schedulers. If using the openlineage-airflow integration, each task in the DAG has its own Run id which can be connected to the Spark job run via the spark.openlineage.parentRunId parameter. For example, here is an example of a DataProcPySparkOperator that submits a Pyspark application on Dataproc: t1 = DataProcPySparkOperator( task_id=job_name, gcp_conn_id='google_cloud_default', project_id='project_id', cluster_name='cluster-name', region='us-west1', main='gs://bucket/your-prog.py', job_name=job_name, dataproc_pyspark_properties={ &quot;spark.extraListeners&quot;: &quot;io.openlineage.spark.agent.OpenLineageSparkListener&quot;, &quot;spark.jars.packages&quot;: &quot;io.openlineage:openlineage-spark:1.0.0+&quot;, &quot;spark.openlineage.transport.url&quot;: f&quot;{openlineage_url}/api/v1/namespaces/{openlineage_namespace}/jobs/dump_orders_to_gcs/runs/{{{{lineage_run_id(run_id, task)}}}}?api_key={api_key}&quot;, &quot;spark.openlineage.namespace&quot;: openlineage_namespace, &quot;spark.openlineage.parentJobName&quot;: job_name, &quot;spark.openlineage.parentRunId&quot;: f&quot;{{{{lineage_run_id(run_id, task)}}}} }, dag=dag)  "},{"title":"Quickstart with Jupyter","type":0,"sectionRef":"#","url":"/docs/integrations/spark/quickstart_local","content":"Quickstart with Jupyter Trying out the Spark integration is super easy if you already have Docker Desktop and git installed. Note: If you're on macOS Monterey (macOS 12) you'll have to release port 5000 before beginning by disabling the AirPlay Receiver. Check out the OpenLineage project into your workspace with: git clone https://github.com/OpenLineage/OpenLineage From the spark integration directory ($OPENLINEAGE_ROOT/integration/spark) execute docker-compose up This will start Marquez as an Openlineage client and Jupyter Spark notebook on localhost:8888. On startup, the notebook container logs will show a list of URLs including an access token, such as notebook_1 | To access the notebook, open this file in a browser: notebook_1 | file:///home/jovyan/.local/share/jupyter/runtime/nbserver-9-open.html notebook_1 | Or copy and paste one of these URLs: notebook_1 | http://abc12345d6e:8888/?token=XXXXXX notebook_1 | or http://127.0.0.1:8888/?token=XXXXXX Copy the URL with 127.0.0.1 as the hostname from your own log (the token will be different from mine) and paste it into your browser window. You should have a blank Jupyter notebook environment ready to go. Once your notebook environment is ready, click on the notebooks directory, then click on the New button to create a new Python 3 notebook. In the first cell in the window paste the following text: from pyspark.sql import SparkSession spark = (SparkSession.builder.master('local') .appName('sample_spark') .config('spark.extraListeners', 'io.openlineage.spark.agent.OpenLineageSparkListener') .config('spark.jars.packages', 'io.openlineage:openlineage-spark:1.7.0') .config('spark.openlineage.transport.type', 'console') .getOrCreate()) Once the Spark context is started, we adjust logging level to INFO with: spark.sparkContext.setLogLevel(&quot;INFO&quot;) and create some Spark table with: spark.createDataFrame([ {'a': 1, 'b': 2}, {'a': 3, 'b': 4} ]).write.mode(&quot;overwrite&quot;).saveAsTable(&quot;temp&quot;) The command should output OpenLineage event in a form of log: 22/08/01 06:15:49 INFO ConsoleTransport: {&quot;eventType&quot;:&quot;START&quot;,&quot;eventTime&quot;:&quot;2022-08-01T06:15:49.671Z&quot;,&quot;run&quot;:{&quot;runId&quot;:&quot;204d9c56-6648-4d46-b6bd-f4623255d324&quot;,&quot;facets&quot;:{&quot;spark_unknown&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-2/OpenLineage.json#/$defs/RunFacet&quot;,&quot;inputs&quot;:[{&quot;description&quot;:{&quot;@class&quot;:&quot;org.apache.spark.sql.execution.LogicalRDD&quot;,&quot;id&quot;:1,&quot;streaming&quot;:false,&quot;traceEnabled&quot;:false,&quot;canonicalizedPlan&quot;:false},&quot;inputAttributes&quot;:[],&quot;outputAttributes&quot;:[{&quot;name&quot;:&quot;a&quot;,&quot;type&quot;:&quot;long&quot;,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;b&quot;,&quot;type&quot;:&quot;long&quot;,&quot;metadata&quot;:{}}]}]},&quot;spark.logicalPlan&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-2/OpenLineage.json#/$defs/RunFacet&quot;,&quot;plan&quot;:[{&quot;class&quot;:&quot;org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand&quot;,&quot;num-children&quot;:1,&quot;table&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.catalog.CatalogTable&quot;,&quot;identifier&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.TableIdentifier&quot;,&quot;table&quot;:&quot;temp&quot;},&quot;tableType&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.catalog.CatalogTableType&quot;,&quot;name&quot;:&quot;MANAGED&quot;},&quot;storage&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat&quot;,&quot;compressed&quot;:false,&quot;properties&quot;:null},&quot;schema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[]},&quot;provider&quot;:&quot;parquet&quot;,&quot;partitionColumnNames&quot;:[],&quot;owner&quot;:&quot;&quot;,&quot;createTime&quot;:1659334549656,&quot;lastAccessTime&quot;:-1,&quot;createVersion&quot;:&quot;&quot;,&quot;properties&quot;:null,&quot;unsupportedFeatures&quot;:[],&quot;tracksPartitionsInCatalog&quot;:false,&quot;schemaPreservesCase&quot;:true,&quot;ignoredProperties&quot;:null},&quot;mode&quot;:null,&quot;query&quot;:0,&quot;outputColumnNames&quot;:&quot;[a, b]&quot;},{&quot;class&quot;:&quot;org.apache.spark.sql.execution.LogicalRDD&quot;,&quot;num-children&quot;:0,&quot;output&quot;:[[{&quot;class&quot;:&quot;org.apache.spark.sql.catalyst.expressions.AttributeReference&quot;,&quot;num-children&quot;:0,&quot;name&quot;:&quot;a&quot;,&quot;dataType&quot;:&quot;long&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{},&quot;exprId&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.expressions.ExprId&quot;,&quot;id&quot;:6,&quot;jvmId&quot;:&quot;6a1324ac-917e-4e22-a0b9-84a5f80694ad&quot;},&quot;qualifier&quot;:[]}],[{&quot;class&quot;:&quot;org.apache.spark.sql.catalyst.expressions.AttributeReference&quot;,&quot;num-children&quot;:0,&quot;name&quot;:&quot;b&quot;,&quot;dataType&quot;:&quot;long&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{},&quot;exprId&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.expressions.ExprId&quot;,&quot;id&quot;:7,&quot;jvmId&quot;:&quot;6a1324ac-917e-4e22-a0b9-84a5f80694ad&quot;},&quot;qualifier&quot;:[]}]],&quot;rdd&quot;:null,&quot;outputPartitioning&quot;:{&quot;product-class&quot;:&quot;org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning&quot;,&quot;numPartitions&quot;:0},&quot;outputOrdering&quot;:[],&quot;isStreaming&quot;:false,&quot;session&quot;:null}]},&quot;spark_version&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-2/OpenLineage.json#/$defs/RunFacet&quot;,&quot;spark-version&quot;:&quot;3.1.2&quot;,&quot;openlineage-spark-version&quot;:&quot;0.12.0-SNAPSHOT&quot;}}},&quot;job&quot;:{&quot;namespace&quot;:&quot;default&quot;,&quot;name&quot;:&quot;sample_spark.execute_create_data_source_table_as_select_command&quot;,&quot;facets&quot;:{}},&quot;inputs&quot;:[],&quot;outputs&quot;:[{&quot;namespace&quot;:&quot;file&quot;,&quot;name&quot;:&quot;/home/jovyan/notebooks/spark-warehouse/temp&quot;,&quot;facets&quot;:{&quot;dataSource&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet&quot;,&quot;name&quot;:&quot;file&quot;,&quot;uri&quot;:&quot;file&quot;},&quot;schema&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;a&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;b&quot;,&quot;type&quot;:&quot;long&quot;}]},&quot;lifecycleStateChange&quot;:{&quot;_producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;_schemaURL&quot;:&quot;https://openlineage.io/spec/facets/1-0-0/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet&quot;,&quot;lifecycleStateChange&quot;:&quot;CREATE&quot;}},&quot;outputFacets&quot;:{}}],&quot;producer&quot;:&quot;https://github.com/OpenLineage/OpenLineage/tree/0.12.0-SNAPSHOT/integration/spark&quot;,&quot;schemaURL&quot;:&quot;https://openlineage.io/spec/1-0-2/OpenLineage.json#/$defs/RunEvent&quot;} Generated JSON contains output dataset name and location {&quot;namespace&quot;:&quot;file&quot;,&quot;name&quot;:&quot;/home/jovyan/notebooks/spark-warehouse/temp&quot;, schema fields [{&quot;name&quot;:&quot;a&quot;,&quot;type&quot;:&quot;long&quot;},{&quot;name&quot;:&quot;b&quot;,&quot;type&quot;:&quot;long&quot;}], etc. More comprehensive demo, that integrates Spark events with Marquez backend can be found on our blog Tracing Data Lineage with OpenLineage and Apache Spark","keywords":""},{"title":"Column-level Lineage","type":0,"sectionRef":"#","url":"/docs/integrations/spark/spark_column_lineage","content":"","keywords":""},{"title":"Standard specification​","type":1,"pageTitle":"Column-level Lineage","url":"/docs/integrations/spark/spark_column_lineage#standard-specification","content":"Collected information is sent in OpenLineage event within columnLineage dataset facet described here. "},{"title":"Code architecture and its mechanics​","type":1,"pageTitle":"Column-level Lineage","url":"/docs/integrations/spark/spark_column_lineage#code-architecture-and-its-mechanics","content":"Column level lineage has been implemented separately from the rest of builders and visitors extracting lineage information from Spark logical plans. As a result the codebase is stored in io.openlineage.spark3.agent.lifecycle.plan.columnLineage package within classes responsible only for this feature. Class ColumnLevelLineageUtils.java is an entry point to run the mechanism and is used within OpenLineageRunEventBuilder. Classes ColumnLevelLineageUtilsNonV2CatalogTest and ColumnLevelLineageUtilsV2CatalogTest contain real-life test cases which run Spark jobs and get an access to the last query plan executed. They evaluate column level lineage based on the plan and expected output schema. Then, they verify if this meets the requirements. This allows testing column level lineage behavior in real scenarios. The more tests and scenarios put here, the better. Class ColumnLevelLineageBuilder is used when traversing logical plans to store all the information required to produce column lineage. It allows storing input/output columns. It also stores dependencies between the expressions contained in query plan. Once inputs, outputs and dependencies are filled, build method is used to produce output facet (ColumnLineageDatasetFacetFields). The mechanism gets output schema and logical plan as input. Output schemas are tightly coupled with root nodes of execution plans, however we do already have this information extracted within the other visitors and dataset input builders.OutputFieldsCollector class is used to traverse the plan to gather the outputs. Outputs can be extracted from Aggregate or Project and each output field has its ExprId (expression id) attached from the plan. InputFieldsCollector class is used to collect the inputs which can be extracted from DataSourceV2Relation, DataSourceV2ScanRelation, HiveTableRelation or LogicalRelation. Each input field has its ExprId within the plan. Each input is identified by DatasetIdentifier, which means it contains name and namespace, of a dataset and an input field. FieldDependenciesCollector traverses the plan to identify dependencies between different ExprId. Dependencies map parent expressions to children expressions'. This is used to identify inputs used to evaluate certain output. "},{"title":"Writing custom extensions​","type":1,"pageTitle":"Column-level Lineage","url":"/docs/integrations/spark/spark_column_lineage#writing-custom-extensions","content":"Spark framework is known for its great ability to be extended by custom libraries capable of reading or writing to anything. In case of having a custom implementation, we prepared an ability to extend column lineage implementation to be able to retrieve information from other input or output LogicalPlan nodes. Creating such an extension requires implementing a following interface: /** Interface for implementing custom collectors of column level lineage. */ interface CustomColumnLineageVisitor { /** * Collect inputs for a given {@link LogicalPlan}. Column level lineage mechanism traverses * LogicalPlan on its node. This method will be called for each traversed node. Input information * should be put into builder. * * @param node * @param builder */ void collectInputs(LogicalPlan node, ColumnLevelLineageBuilder builder); /** * Collect outputs for a given {@link LogicalPlan}. Column level lineage mechanism traverses * LogicalPlan on its node. This method will be called for each traversed node. Output information * should be put into builder. * * @param node * @param builder */ void collectOutputs(LogicalPlan node, ColumnLevelLineageBuilder builder); /** * Collect expressions for a given {@link LogicalPlan}. Column level lineage mechanism traverses * LogicalPlan on its node. This method will be called for each traversed node. Expression * dependency information should be put into builder. * * @param node * @param builder */ void collectExpressionDependencies(LogicalPlan node, ColumnLevelLineageBuilder builder); }  and making it available for Service Loader (implementation class name has to be put in a resource file META-INF/services/io.openlineage.spark.agent.lifecycle.plan.column.CustomColumnLineageVisitor). "},{"title":"Future work​","type":1,"pageTitle":"Column-level Lineage","url":"/docs/integrations/spark/spark_column_lineage#future-work","content":"Current version of the mechanism allows finding input fields that were used to produce the output field but does not determine how were they used. This simplification allowed us to built column lineage feature at the minimum amount of code written. Given an incredible amount of Spark's functions, operators and expressions, our implementation needs just to know it was UnaryOperator, BinaryOperator, etc. without a requirement to understand the operation it performs. This approach still has some room for extensions like: Being able to find out if an output field is a simple copy of input one or some modification has been applied.Assume there exist a mechanism within an organisation to blur personal data. Be able to extract information if the output still contains personal data or it got blurred. "},{"title":"0.10.0 - 2022-06-24","type":0,"sectionRef":"#","url":"/docs/releases/0_10_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.10.0 - 2022-06-24","url":"/docs/releases/0_10_0#added","content":"Add static code anlalysis tool mypy to run in CI for against all python modules (#802) @howardyooExtend SaveIntoDataSourceCommandVisitor to extract schema from LocalRelaiton and LogicalRdd in spark integration (#794) @pawel-big-lebowskiAdd InMemoryRelationInputDatasetBuilder for InMemory datasets to Spark integration (#818) @pawel-big-lebowskiAdd copyright to source files #755 @merobi-hubAdd SnowflakeOperatorAsync extractor support to Airflow integration #869 @merobi-hubAdd PMD analysis to proxy project (#889) @howardyoo "},{"title":"Changed​","type":1,"pageTitle":"0.10.0 - 2022-06-24","url":"/docs/releases/0_10_0#changed","content":"Skip FunctionRegistry.class serialization in Spark integration (#828) @mobuchowskiInstall new rust-based SQL parser by default in Airflow integration (#835) @mobuchowskiImprove overall pytest and integration tests for Airflow integration (#851,#858) @denimalpacaReduce OL event payload size by excluding local data and including output node in start events (#881) @collado-mikeSplit spark integration into submodules (#834, #890) @tnazarew @mobuchowski "},{"title":"Fixed​","type":1,"pageTitle":"0.10.0 - 2022-06-24","url":"/docs/releases/0_10_0#fixed","content":"Conditionally import sqlalchemy lib for Great Expectations integration (#826) @pawel-big-lebowskiAdd check for missing class org.apache.spark.sql.catalyst.plans.logical.CreateV2Table in Spark integration (#866) @pawel-big-lebowskiFix static code analysis issues (#867,#874) @pawel-big-lebowski "},{"title":"0.1.0 - 2021-08-13","type":0,"sectionRef":"#","url":"/docs/releases/0_1_0","content":"0.1.0 - 2021-08-13 OpenLineage is an Open Standard for lineage metadata collection designed to record metadata for a job in execution. The initial public release includes: An inital specification. The the inital version 1-0-0 of the OpenLineage specification defines the core model and facets.Integrations that collect lineage metadata as OpenLineage events: Apache Airflow with support for BigQuery, Great Expectations, Postgres, Redshift, SnowflakeApache Sparkdbt Clients that send OpenLineage events to an HTTP backend. Both java and python are initially supported.","keywords":""},{"title":"0.11.0 - 2022-07-07","type":0,"sectionRef":"#","url":"/docs/releases/0_11_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.11.0 - 2022-07-07","url":"/docs/releases/0_11_0#added","content":"HTTP option to override timeout and properly close connections in openlineage-java lib. #909 @mobuchowskiDynamic mapped tasks support to Airflow integration #906 @JDarDagranSqlExtractor to Airflow integration #907 @JDarDagranPMD to Java and Spark builds in CI #898 @merobi-hub "},{"title":"Changed​","type":1,"pageTitle":"0.11.0 - 2022-07-07","url":"/docs/releases/0_11_0#changed","content":"When testing extractors in the Airflow integration, set the extractor length assertion dynamic #882 @denimalpacaRender templates as start of integration tests for TaskListener in the Airflow integration #870 @mobuchowski  "},{"title":"Fixed​","type":1,"pageTitle":"0.11.0 - 2022-07-07","url":"/docs/releases/0_11_0#fixed","content":"Dependencies bundled with openlineage-java lib. #855 @collado-mikePMD reported issues #891 @pawel-big-lebowskiSpark casting error and session catalog support for iceberg in Spark integration #856 @wslulciuc "},{"title":"0.12.0 - 2022-08-01","type":0,"sectionRef":"#","url":"/docs/releases/0_12_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.12.0 - 2022-08-01","url":"/docs/releases/0_12_0#added","content":"Add Spark 3.3.0 support #950 @pawel-big-lebowskiAdd Apache Flink integration #951 @mobuchowskiAdd ability to extend column level lineage mechanism #922 @pawel-big-lebowskiAdd ErrorMessageRunFacet #897 @mobuchowskiAdd SQLCheckExtractors #717 @denimalpacaAdd RedshiftSQLExtractor &amp; RedshiftDataExtractor #930 @JDarDagranAdd dataset builder for AlterTableCommand #927 @tnazarew "},{"title":"Changed​","type":1,"pageTitle":"0.12.0 - 2022-08-01","url":"/docs/releases/0_12_0#changed","content":"Limit Delta events #905 @pawel-big-lebowskiAirflow integration: allow lineage metadata to flow through inlets and outlets #914 @fenil25 "},{"title":"Fixed​","type":1,"pageTitle":"0.12.0 - 2022-08-01","url":"/docs/releases/0_12_0#fixed","content":"Limit size of serialized plan #917 @pawel-big-lebowskiFix noclassdef error #942 @pawel-big-lebowski "},{"title":"0.13.1 - 2022-08-25","type":0,"sectionRef":"#","url":"/docs/releases/0_13_1","content":"","keywords":""},{"title":"Fixed​","type":1,"pageTitle":"0.13.1 - 2022-08-25","url":"/docs/releases/0_13_1#fixed","content":"Rename all parentRun occurrences to parent in Airflow integration 1037 @fm100 Changes the parentRun property name to parent in the Airflow integration to match the spec.Do not change task instance during on_running event 1028 @JDarDagran Fixes an issue in the Airflow integration with the on_running hook, which was changing the TaskInstance object along with the task attribute. "},{"title":"0.13.0 - 2022-08-22","type":0,"sectionRef":"#","url":"/docs/releases/0_13_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.13.0 - 2022-08-22","url":"/docs/releases/0_13_0#added","content":"Add BigQuery check support #960 @denimalpaca Adds logic and support for proper dynamic class inheritance for BigQuery-style operators. (BigQuery's extractor needed additional logic to support the forthcoming BigQueryColumnCheckOperator and BigQueryTableCheckOperator.)Add RUNNING EventType in spec and Python client #972 @mzareba382 Introduces a RUNNING event state in the OpenLineage spec to indicate a running task and adds a RUNNING event type in the Python API.Use databases &amp; schemas in SQL Extractors #974 @JDarDagran Allows the Airflow integration to differentiate between databases and schemas. (There was no notion of databases and schemas when querying and parsing results from information_schema tables.)Implement Event forwarding feature via HTTP protocol #995 @howardyoo Adds HttpLineageStream to forward a given OpenLineage event to any HTTP endpoint.Introduce SymlinksDatasetFacet to spec #936 @pawel-big-lebowski Creates a new facet, the SymlinksDatasetFacet, to support the storing of alternative dataset names.Add Azure Cosmos Handler to Spark integration #983 @hmoazam Defines a new interface, the RelationHandler, to support Spark data sources that do not have TableCatalog, Identifier, or TableProperties set, as is the case with the Azure Cosmos DB Spark connector.Support OL Datasets in manual lineage inputs/outputs #1015 @conorbev Allows Airflow users to create OpenLineage Dataset classes directly in DAGs with no conversion necessary. (Manual lineage definition required users to create an airflow.lineage.entities.Table, which was then converted to an OpenLineage Dataset.) Create ownership facets #996 @julienledem Adds an ownership facet to both Dataset and Job in the OpenLineage spec to capture ownership of jobs and datasets. "},{"title":"Changed​","type":1,"pageTitle":"0.13.0 - 2022-08-22","url":"/docs/releases/0_13_0#changed","content":"Use RUNNING EventType in Flink integration for currently running jobs #985 @mzareba382 Makes use of the new RUNNING event type in the Flink integration, changing events sent by Flink jobs from OTHER to this new type.Convert task objects to JSON-encodable objects when creating custom Airflow version facets #1018 @fm100 Implements a to_json_encodable function in the Airflow integration to make task objects JSON-encodable. "},{"title":"Fixed​","type":1,"pageTitle":"0.13.0 - 2022-08-22","url":"/docs/releases/0_13_0#fixed","content":"Add support for custom SQL queries in v3 Great Expectations API #1025 @collado-mike Fixes support for custom SQL statements in the Great Expectations provider. (The Great Expectations custom SQL datasource was not applied to the support for the V3 checkpoints API.) "},{"title":"0.14.0 - 2022-09-06","type":0,"sectionRef":"#","url":"/docs/releases/0_14_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.14.0 - 2022-09-06","url":"/docs/releases/0_14_0#added","content":"Support ABFSS and Hadoop Logical Relation in Column-level lineage #1008 @wjohnson Introduces an extractDatasetIdentifier that uses similar logic to InsertIntoHadoopFsRelationVisitor to pull out the path on the HDFS compliant file system; tested on ABFSS and DBFS (Databricks FileSystem) to prove that lineage could be extracted using non-SQL commands.Add Kusto relation visitor #939 @hmoazam Implements a KustoRelationVisitor to support lineage for Azure Kusto's Spark connector.Add ColumnLevelLineage facet doc #1020 @julienledem Adds documentation for the Column-level lineage facet.Include symlinks dataset facet #935 @pawel-big-lebowski Includes the recently introduced SymlinkDatasetFacet in generated OpenLineage events.Add support for dbt 1.3 beta's metadata changes #1051 @mobuchowski Makes projects that are composed of only SQL models work on 1.3 beta (dbt 1.3 renamed the compiled_sql field to compiled_code to support Python models). Does not provide support for dbt's Python models.Support Flink 1.15 #1009 @mzareba382 Adds support for Flink 1.15.Add Redshift dialect to the SQL integration #1066 @mobuchowski Adds support for Redshift's SQL dialect in OpenLineage's SQL parser, including quirks such as the use of square brackets in JSON paths. (Note, this does not add support for all of Redshift's custom syntax.) "},{"title":"Changed​","type":1,"pageTitle":"0.14.0 - 2022-09-06","url":"/docs/releases/0_14_0#changed","content":"Make the timeout configurable in the Spark integration #1050 @tnazarew Makes timeout configurable by the user. (In some cases, the time needed to send events was longer than 5 seconds, which exceeded the timeout value.) "},{"title":"Fixed​","type":1,"pageTitle":"0.14.0 - 2022-09-06","url":"/docs/releases/0_14_0#fixed","content":"Add a dialect parameter to Great Expectations SQL parser calls #1049 @collado-mike Specifies the dialect name from the SQL engine.Fix Delta 2.1.0 with Spark 3.3.0 #1065 @pawel-big-lebowski Allows delta support for Spark 3.3 and fixes potential issues. (The Openlineage integration for Spark 3.3 was turned on without delta support, as delta did not support Spark 3.3 at that time.) "},{"title":"0.14.1 - 2022-09-07","type":0,"sectionRef":"#","url":"/docs/releases/0_14_1","content":"","keywords":""},{"title":"Fixed​","type":1,"pageTitle":"0.14.1 - 2022-09-07","url":"/docs/releases/0_14_1#fixed","content":"Fix Spark integration issues including error when no openlineage.timeout #1069 @pawel-big-lebowski OpenlineageSparkListener was failing when no openlineage.timeout was provided. "},{"title":"0.16.1 - 2022-11-03","type":0,"sectionRef":"#","url":"/docs/releases/0_16_1","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.16.1 - 2022-11-03","url":"/docs/releases/0_16_1#added","content":"Airflow: add dag_run information to Airflow version run facet #1133 @fm100 Adds the Airflow DAG run ID to the taskInfo facet, making this additional information available to the integration.Airflow: add LoggingMixin to extractors #1149 @JDarDagran Adds a LoggingMixin class to the custom extractor to make the output consistent with general Airflow and OpenLineage logging settings.Airflow: add default extractor #1162 @mobuchowski Adds a DefaultExtractor to support the default implementation of OpenLineage for external operators without the need for custom extractors.Airflow: add on_complete argument in DefaultExtractor #1188 @JDarDagran Adds support for running another method on extract_on_complete.SQL: reorganize the library into multiple packages #1167 @StarostaGit @mobuchowski Splits the SQL library into a Rust implementation and foreign language bindings, easing the process of adding language interfaces. Also contains CI fix. "},{"title":"Changed​","type":1,"pageTitle":"0.16.1 - 2022-11-03","url":"/docs/releases/0_16_1#changed","content":"Airflow: move get_connection_uri as extractor's classmethod #1169 @JDarDagran The get_connection_uri method allowed for too many params, resulting in unnecessarily long URIs. This changes the logic to whitelisting per extractor.Airflow: change get_openlineage_facets_on_start/complete behavior #1201 @JDarDagran Splits up the method for greater legibility and easier maintenance. "},{"title":"Fixed​","type":1,"pageTitle":"0.16.1 - 2022-11-03","url":"/docs/releases/0_16_1#fixed","content":"Airflow: always send SQL in SqlJobFacet as a string #1143 @mobuchowski Changes the data type of query from array to string to an fix error in the RedshiftSQLOperator. Airflow: include __extra__ case when filtering URI query params #1144 @JDarDagran Includes the conn.EXTRA_KEY in the get_connection_uri method to avoid exposing secrets in URIs via the __extra__ key. Airflow: enforce column casing in SQLCheckExtractors #1159 @denimalpaca Uses the parent extractor's _is_uppercase_names property to determine if the column should be upper cased in the SQLColumnCheckExtractor's _get_input_facets() method.Spark: prevent exception when no schema provided #1180 @pawel-big-lebowski Prevents evalution of column lineage when the schemFacet is null.Great Expectations: add V3 API compatibility #1194 @denimalpaca Fixes the Pandas datasource to make it V3 API-compatible. "},{"title":"Removed​","type":1,"pageTitle":"0.16.1 - 2022-11-03","url":"/docs/releases/0_16_1#removed","content":"Airflow: remove support for Airflow 1.10 #1128 @mobuchowski Removes the code structures and tests enabling support for Airflow 1.10. "},{"title":"0.15.1 - 2022-10-05","type":0,"sectionRef":"#","url":"/docs/releases/0_15_1","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.15.1 - 2022-10-05","url":"/docs/releases/0_15_1#added","content":"Airflow: improve development experience #1101 @JDarDagran Adds an interactive development environment to the Airflow integration and improves integration testing.Spark: add description for URL parameters in readme, change overwriteName to appName #1130 @tnazarew Adds more information about passing arguments with spark.openlineage.url and changes overwriteName to appName for clarity.Documentation: update issue templates for proposal &amp; add new integration template #1116 @rossturk Adds a YAML issue template for new integrations and fixes a bug in the proposal template. "},{"title":"Changed​","type":1,"pageTitle":"0.15.1 - 2022-10-05","url":"/docs/releases/0_15_1#changed","content":"Airflow: lazy load BigQuery client #1119 @mobuchowski Moves import of the BigQuery client from top level to local level to decrease DAG import time. "},{"title":"Fixed​","type":1,"pageTitle":"0.15.1 - 2022-10-05","url":"/docs/releases/0_15_1#fixed","content":"Airflow: fix UUID generation conflict for Airflow DAGs with same name #1056 @collado-mike Adds a namespace to the UUID calculation to avoid conflicts caused by DAGs having the same name in different namespaces in Airflow deployments.Spark/BigQuery: fix issue with spark-bigquery-connector &gt;=0.25.0 #1111 @pawel-big-lebowski Makes the Spark integration compatible with the latest connector.Spark: fix column lineage #1069 @pawel-big-lebowski Fixes a null pointer exception error and an error when openlineage.timeout is not provided.Spark: set log level of Init OpenLineageContext to DEBUG #1064 @varuntestaz Prevents sensitive information from being logged unless debug mode is used.Java client: update version of SnakeYAML #1090 @TheSpeedding Bumps the SnakeYAML library version to include a key bug fix. dbt: remove requirement for OPENLINEAGE_URL to be set #1107 @mobuchowski Removes erroneous check for OPENLINEAGE_URL in the dbt integration.Python client: remove potentially cyclic import #1126 @mobuchowski Hides imports to remove potentially cyclic import.CI: build macos release package on medium resource class #1131 @mobuchowski Fixes failing build due to resource class being too large. "},{"title":"0.17.0 - 2022-11-16","type":0,"sectionRef":"#","url":"/docs/releases/0_17_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.17.0 - 2022-11-16","url":"/docs/releases/0_17_0#added","content":"Spark: support latest Spark 3.3.1 #1183 @pawel-big-lebowski Adds support for the latest version of Spark.Spark: add Kinesis Transport and support config Kinesis in Spark integration #1200 @yogayang Adds support for sending to Kinesis from the Spark integration. Spark: Disable specified facets #1271 @pawel-big-lebowski Adds the ability to disable specified facets from generated OpenLineage events.Python: add facets implementation to Python client #1233 @pawel-big-lebowski Adds missing facets to the Python client.SQL: add Rust parser interface #1172 @StarostaGit @mobuchowski Implements a Java interface in the Rust SQL parser, including a build script, native library loading mechanism, CI support and build fixes.Proxy: add helm chart for the proxy backed #1068 @wslulciuc Adds a helm chart for deploying the proxy backend on Kubernetes.Spec: include possible facets usage in spec #1249 @pawel-big-lebowski Extends the facets definition with a list of available facets.Website: publish YML version of spec to website #1300 @rossturk Adds configuration necessary to make the OpenLineage website auto-generate openAPI docs when the spec is published there.Docs: update language on nominating new committers #1270 @rossturk Updates the governance language to reflect the new policy on nominating committers. "},{"title":"Changed​","type":1,"pageTitle":"0.17.0 - 2022-11-16","url":"/docs/releases/0_17_0#changed","content":"Website: publish spec into new website repo location #1295 @rossturk Creates a new deploy key, adds it to CircleCI &amp; GitHub, and makes the necessary changes to the release.sh script.Airflow: change how pip installs packages in tox environments #1302 @JDarDagran Use deprecated resolver and constraints files provided by Airflow to avoid potential issues caused by pip's new resolver. "},{"title":"Fixed​","type":1,"pageTitle":"0.17.0 - 2022-11-16","url":"/docs/releases/0_17_0#fixed","content":"Airflow: fix README for running integration test #1238 @sekikn Updates the README for consistency with supported Airflow versions.Airflow: add task_instance argument to get_openlineage_facets_on_complete #1269 @JDarDagran Adds the task_instance argument to DefaultExtractor.Java client: fix up all artifactory paths #1290 @harels Not all artifactory paths were changed in the build CI script in a previous PR.Python client: fix Mypy errors and adjust to PEP 484 #1264 @JDarDagran Adds a --no-namespace-packages argument to the Mypy command and adjusts code to PEP 484.Website: release all specs since last_spec_commit_id, not just HEAD~1 #1298 @rossturk The script now ships all specs that have changed since .last_spec_commit_id. "},{"title":"Removed​","type":1,"pageTitle":"0.17.0 - 2022-11-16","url":"/docs/releases/0_17_0#removed","content":"Deprecate HttpTransport.Builder in favor of HttpConfig #1287 @collado-mike Deprecates the Builder in favor of HttpConfig only and replaces the existing Builder implementation by delegating to the HttpConfig. "},{"title":"0.18.0 - 2022-12-08","type":0,"sectionRef":"#","url":"/docs/releases/0_18_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.18.0 - 2022-12-08","url":"/docs/releases/0_18_0#added","content":"Airflow: support SQLExecuteQueryOperator #1379 @JDarDagran Changes the SQLExtractor and adds support for the dynamic assignment of extractors based on conn_type.Airflow: introduce a new extractor for SFTPOperator #1263 @sekikn Adds an extractor for tracing file transfers between local file systems.Airflow: add Sagemaker extractors #1136 @fhoda Creates extractors for SagemakeProcessingOperator and SagemakerTransformOperator.Airflow: add S3 extractor for Airflow operators #1166 @fhoda Creates an extractor for the S3CopyObject in the Airflow integration.Spec: add spec file for ExternalQueryRunFacet #1262 @howardyoo Adds a spec file to make this facet available for the Java client. Includes a README.Docs: add a TSC doc #1303 @merobi-hub Adds a document listing the members of the Technical Steering Committee. "},{"title":"Fixed​","type":1,"pageTitle":"0.18.0 - 2022-12-08","url":"/docs/releases/0_18_0#fixed","content":"Spark: improve Databricks to send better events #1330 @pawel-big-lebowski Filters unwanted events and provides a meaningful job name.Spark-Bigquery: fix a few of the common errors #1377 @mobuchowski Fixes a few of the common issues with the Spark-Bigquery integration and adds an integration test and configures CI.Python: validate eventTime field in Python client #1355 @pawel-big-lebowskiValidates the eventTime of a RunEvent within the client library.Databricks: Handle Databricks Runtime 11.3 changes to DbFsUtils constructor #1351 @wjohnson Recaptures lost mount point information from the DatabricksEnvironmentFacetBuilder and environment-properties facet by looking at the number of parameters in the DbFsUtils constructor to determine the runtime version. "},{"title":"0.19.2 - 2023-01-04","type":0,"sectionRef":"#","url":"/docs/releases/0_19_2","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.19.2 - 2023-01-04","url":"/docs/releases/0_19_2#added","content":"Airflow: add Trino extractor #1288 @sekikn Adds a Trino extractor to the Airflow integration.Airflow: add S3FileTransformOperator extractor #1450 @sekikn Adds an S3FileTransformOperator extractor to the Airflow integration.Airflow: add standardized run facet #1413 @JDarDagran Creates one standardized run facet for the Airflow integration.Airflow: add NominalTimeRunFacet and OwnershipJobFacet #1410 @JDarDagran Adds nominalEndTime and OwnershipJobFacet fields to the Airflow integration.dbt: add support for postgres datasources #1417 @julienledem Adds the previously unsupported postgres datasource type.Proxy: add client-side proxy (skeletal version) #1439 #1420 @fm100 Implements a skeletal version of a client-side proxy.Proxy: add CI job to publish Docker image #1086 @wslulciuc Includes a script to build and tag the image plus jobs to verify the build on every CI run and publish to Docker Hub.SQL: add ExtractionErrorRunFacet #1442 @mobuchowski Adds a facet to the spec to reflect internal processing errors, especially failed or incomplete parsing of SQL jobs.SQL: add column-level lineage to SQL parser #1432 #1461 @mobuchowski @StarostaGit Adds support for extracting column-level lineage from SQL statements in the parser, including adjustments to Rust-Python and Rust-Java interfaces and the Airflow integration's SQL extractor to make use of the feature. Also includes more tests, removal of the old parser, and removal of the common-build cache in CI (which was breaking the parser).Spark: pass config parameters to the OL client #1383 @tnazarew Adds a mechanism for making new lineage consumers transparent to the integration, easing the process of setting up new types of consumers. "},{"title":"Fixed​","type":1,"pageTitle":"0.19.2 - 2023-01-04","url":"/docs/releases/0_19_2#fixed","content":"Airflow: fix collect_ignore, add flags to Pytest for cleaner output #1437 @JDarDagran Removes the extractors directory from the ignored list, improving unit testing.Spark &amp; Java client: fix README typos @versaurabh Fixes typos in the SPDX license headers. "},{"title":"0.2.0 - 2021-08-23","type":0,"sectionRef":"#","url":"/docs/releases/0_2_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.2.0 - 2021-08-23","url":"/docs/releases/0_2_0#added","content":"Parse dbt command line arguments when invoking dbt-ol @mobuchowski. For example: $ dbt-ol run --project-dir path/to/dir Set UnknownFacet for spark (captures metadata about unvisited nodes from spark plan not yet supported) @OleksandrDvornik "},{"title":"Changed​","type":1,"pageTitle":"0.2.0 - 2021-08-23","url":"/docs/releases/0_2_0#changed","content":"Remove model from dbt job name @mobuchowskiDefault dbt job namespace to output dataset namespace @mobuchowskiRename openlineage.spark.* to io.openlineage.spark.* @OleksandrDvornik "},{"title":"Fixed​","type":1,"pageTitle":"0.2.0 - 2021-08-23","url":"/docs/releases/0_2_0#fixed","content":"Remove instance references to extractors from DAG and avoid copying log property for serializability @collado-mike "},{"title":"0.2.1 - 2021-08-27","type":0,"sectionRef":"#","url":"/docs/releases/0_2_1","content":"","keywords":""},{"title":"Fixed​","type":1,"pageTitle":"0.2.1 - 2021-08-27","url":"/docs/releases/0_2_1#fixed","content":"dbt: default --project-dir argument to current directory in dbt-ol script @mobuchowski "},{"title":"0.2.2 - 2021-09-08","type":0,"sectionRef":"#","url":"/docs/releases/0_2_2","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.2.2 - 2021-09-08","url":"/docs/releases/0_2_2#added","content":"Implement OpenLineageValidationAction for Great Expectations @collado-mikefacet: add expectations assertions facet @mobuchowski "},{"title":"Fixed​","type":1,"pageTitle":"0.2.2 - 2021-09-08","url":"/docs/releases/0_2_2#fixed","content":"airflow: pendulum formatting fix, add tests @mobuchowskidbt: do not emit events if run_result file was not updated @mobuchowski "},{"title":"0.2.3 - 2021-10-07","type":0,"sectionRef":"#","url":"/docs/releases/0_2_3","content":"","keywords":""},{"title":"Fixed​","type":1,"pageTitle":"0.2.3 - 2021-10-07","url":"/docs/releases/0_2_3#fixed","content":"dbt: add dbt v3 manifest support @mobuchowski "},{"title":"0.20.4 - 2023-02-07","type":0,"sectionRef":"#","url":"/docs/releases/0_20_4","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.20.4 - 2023-02-07","url":"/docs/releases/0_20_4#added","content":"Airflow: add new extractor for GCSToGCSOperator #1495 @sekikn Adds a new extractor for this operator.Flink: resolve topic names from regex, support 1.16.0 #1522 @pawel-big-lebowski Adds support for Flink 1.16.0 and makes the integration resolve topic names from Kafka topic patterns.Proxy: implement lineage event validator for client proxy #1469 @fm100 Implements logic in the proxy (which is still in development) for validating and handling lineage events. "},{"title":"Changed​","type":1,"pageTitle":"0.20.4 - 2023-02-07","url":"/docs/releases/0_20_4#changed","content":"CI: use ruff instead of flake8, isort, etc., for linting and formatting #1526 @mobuchowski Adopts the ruff package, which combines several linters and formatters into one fast binary. "},{"title":"Fixed​","type":1,"pageTitle":"0.20.4 - 2023-02-07","url":"/docs/releases/0_20_4#fixed","content":"Airflow: make the Trino catalog non-mandatory #1572 @JDarDagran Makes the Trino catalog optional in the Trino extractor.Common: add explicit SQL dependency #1532 @mobuchowski Addresses a 0.19.2 breaking change to the GE integration by including the SQL dependency explicitly.DBT: adjust tqdm logging in dbt-ol #1549 @JdarDagran Adjusts tqdm to show the correct number of iterations and adds START events for parent runs.DBT: fix typo in log output #1493 @denimalpaca Fixes 'emittled' typo in log output.Great Expectations/Airflow: follow Snowflake dataset naming rules #1527 @mobuchowski Normalizes Snowflake dataset and datasource naming rules among DBT/Airflow/GE; canonizes old Snowflake account paths around making them all full-size with account, region and cloud names.Java and Python Clients: Kafka does not initialize properties if they are empty; check and notify about Confluent-Kafka requirement #1556 @mobuchowski Fixes the failure to initialize KafkaTransport in the Java client and adds an exception if the required confluent-kafka module is missing from the Python client.Spark: add square brackets for list-based Spark configs #1507 @Varunvaruns9 Adds a condition to treat configs with [] as lists. Note: [] will be required for list-based configs starting with 0.21.0.Spark: fix several Spark/BigQuery-related issues #1557 @mobuchowski Fixes the assumption that a version is always a number; adds support for HadoopMapReduceWriteConfigUtil; makes the integration access BigQueryUtil and getTableId using reflection, which supports all BigQuery versions; makes logs provide the full serialized LogicalPlan on debug.SQL: only report partial failures `#1479 @mobuchowski Changes the parser so it reports partial failures instead of failing the whole extraction. "},{"title":"0.20.6 - 2023-02-10","type":0,"sectionRef":"#","url":"/docs/releases/0_20_6","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.20.6 - 2023-02-10","url":"/docs/releases/0_20_6#added","content":"Airflow: add new extractor for FTPFileTransmitOperator #1603 @sekikn Adds a new extractor for this Airflow operator serving legacy systems. "},{"title":"Changed​","type":1,"pageTitle":"0.20.6 - 2023-02-10","url":"/docs/releases/0_20_6#changed","content":"Airflow: make extractors for async operators work #1601 @JDarDagran Sends a deterministic Run UUID for Airflow runs. "},{"title":"Fixed​","type":1,"pageTitle":"0.20.6 - 2023-02-10","url":"/docs/releases/0_20_6#fixed","content":"dbt: render actual profile only in profiles.yml #1599 @mobuchowski Adds an include_section argument for the Jinja render method to include only one profile if needed.dbt: make compiled_code optional #1595 @JDarDagran Makes compiled_code optional for manifest &gt; v7. "},{"title":"0.21.1 - 2023-03-02","type":0,"sectionRef":"#","url":"/docs/releases/0_21_1","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.21.1 - 2023-03-02","url":"/docs/releases/0_21_1#added","content":"Clients: add DEBUG logging of events to transports #1633 @mobuchowski Ensures that the DEBUG loglevel on properly configured loggers will always log events, regardless of the chosen transport.Spark: add CustomEnvironmentFacetBuilder class #1545 New contributor @Anirudh181001 Enables the capture of custom environment variables from Spark.Spark: introduce the new output visitors AlterTableAddPartitionCommandVisitor and AlterTableSetLocationCommandVisitor #1629 New contributor @nataliezeller1 Adds visitors for extracting table names from the Spark commands AlterTableAddPartitionCommand and AlterTableSetLocationCommand. The intended use case is a custom transport for the OpenMetadata lineage API.Spark: add column lineage for JDBC relations #1636 @tnazarew Adds column lineage information to JDBC events with data extracted from query by the SQL parser.SQL: add linux-aarch64 native library to Java SQL parser #1664 @mobuchowski Adds a Linux-ARM version of the native library. The Java SQL parser interface had only Linux-x64 and MacOS universal binary variants previously. "},{"title":"Changed​","type":1,"pageTitle":"0.21.1 - 2023-03-02","url":"/docs/releases/0_21_1#changed","content":"Airflow: get table database in Athena extractor #1631 New contributor @rinzool Changes the extractor to get a table's database from the table.schema field or the operator default if the field is None. "},{"title":"Fixed​","type":1,"pageTitle":"0.21.1 - 2023-03-02","url":"/docs/releases/0_21_1#fixed","content":"dbt: add dbt seed to the list of dbt-ol events #1649 New contributor @pohek321 Ensures that dbt-ol test no longer fails when run against an event seed.Spark: make column lineage extraction in Spark support caching #1634 @pawel-big-lebowski Collect column lineage from Spark logical plans that contain cached datasets.Spark: add support for a deprecated config #1586 @tnazarew Maps the deprecated spark.openlineage.url to spark.openlineage.transport.url.Spark: add error message in case of null in url #1590 @tnazarew Improves error logging in the case of undefined URLs.Spark: collect complete event for really quick Spark jobs #1650 @pawel-big-lebowski Improves the collecting of OpenLineage events on SQL complete in the case of quick operations.Spark: fix input/outputs for one node LogicalRelation plans #1668 @pawel-big-lebowski For simple queries like select col1, col2 from my_db.my_table that do not write output, the Spark plan contained just a single node, which was wrongly treated as both an input and output dataset.SQL: fix file existence check in build script for openlineage-sql-java #1613 @sekikn Ensures that the build script works if the library is compiled solely for Linux. "},{"title":"Removed​","type":1,"pageTitle":"0.21.1 - 2023-03-02","url":"/docs/releases/0_21_1#removed","content":"Airflow: remove JobIdMapping and update macros to better support Airflow version 2+ #1645 @JDarDagran Updates macros to use OpenLineageAdapter's method to generate deterministic run UUIDs because using the JobIdMapping utility is incompatible with Airflow 2+. "},{"title":"0.24.0 - 2023-05-03","type":0,"sectionRef":"#","url":"/docs/releases/0_24_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.24.0 - 2023-05-03","url":"/docs/releases/0_24_0#added","content":"Support custom transport types #1795 @nataliezeller1 Adds a new interface, TransportBuilder, for creating custom transport types without having to modify core components of OpenLineage.Airflow: dbt Cloud integration #1418 @howardyoo Adds a new OpenLineage extractor for dbt Cloud that uses the dbt Cloud hook provided by Airflow to communicate with dbt Cloud via its API.Spark: support dataset name modification using regex #1796 @pawel-big-lebowski It is a common scenario to write Spark output datasets with a location path ending with /year=2023/month=04. The Spark parameter spark.openlineage.dataset.removePath.pattern introduced here allows for removing certain elements from a path with a regex pattern. "},{"title":"Fixed​","type":1,"pageTitle":"0.24.0 - 2023-05-03","url":"/docs/releases/0_24_0#fixed","content":"Spark: catch exception when trying to obtain details of non-existing table. #1798 @pawel-big-lebowski This mostly happens when getting table details on START event while the table is still not created.Spark: LogicalPlanSerializer #1792 @pawel-big-lebowski Changes LogicalPlanSerializer to make use of non-shaded Jackson classes in order to serialize LogicalPlans. Note: class names are no longer serialized. Flink: fix Flink CI #1801 @pawel-big-lebowski Specifies an older image version that succeeds on CI in order to fix the Flink integration. "},{"title":"0.23.0 - 2023-04-20","type":0,"sectionRef":"#","url":"/docs/releases/0_23_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.23.0 - 2023-04-20","url":"/docs/releases/0_23_0#added","content":"SQL: parser improvements to support: copy into, create stage, pivot #1742 @pawel-big-lebowski Adds support for additional syntax available in sqlparser-rs.dbt: add support for snapshots #1787 @JDarDagran Adds support for this special kind of table representing type-2 Slowly Changing Dimensions. "},{"title":"Changed​","type":1,"pageTitle":"0.23.0 - 2023-04-20","url":"/docs/releases/0_23_0#changed","content":"Spark: change custom column lineage visitors #1788 @pawel-big-lebowski Makes the CustomColumnLineageVisitor interface public to support custom column lineage. "},{"title":"Fixed​","type":1,"pageTitle":"0.23.0 - 2023-04-20","url":"/docs/releases/0_23_0#fixed","content":"Spark: fix null pointer in JobMetricsHolder #1786 @pawel-big-lebowski Adds a null check before running put to fix a NPE occurring in JobMetricsHolderSQL: fix query with table generator #1783 @pawel-big-lebowski Allows TableFactor::TableFunction to support queries containing table functions.SQL: fix rust code style bug #1785 @pawel-big-lebowski Fixes a minor style issue in visitor.rs. "},{"title":"Removed​","type":1,"pageTitle":"0.23.0 - 2023-04-20","url":"/docs/releases/0_23_0#removed","content":"Airflow: Remove explicit pass from several extract_on_complete methods #1771 @JDarDagran Removes the code from three extractors. "},{"title":"0.22.0 - 2023-04-03","type":0,"sectionRef":"#","url":"/docs/releases/0_22_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.22.0 - 2023-04-03","url":"/docs/releases/0_22_0#added","content":"Spark: properties facet #1717 @tnazarew Adds a new facet to capture specified Spark properties.SQL: SQLParser supports alter, truncate and drop statements #1695 @pawel-big-lebowski Adds support for the statements to the parser.Common/SQL: provide public interface for openlineage_sql package #1727 @JDarDagran Provides a .pyi public interface file for providing typing hints.Java client: add configurable headers to HTTP transport #1718 @tnazarew Adds custom header handling to HttpTransport and the Spark integration.Python client: create client from dictionary #1745 @JDarDagran Adds a new from_dict method to the Python client to support creating it from a dictionary. "},{"title":"Changed​","type":1,"pageTitle":"0.22.0 - 2023-04-03","url":"/docs/releases/0_22_0#changed","content":"Spark: remove URL parameters for JDBC namespaces #1708 @tnazarew Makes the namespace value from an event conform to the naming convention specified in Naming.md.Make OPENLINEAGE_DISABLED case-insensitive #1705 @jedcunningham Makes the environment variable for disabling OpenLineage in the Python client and Airflow integration case-insensitive. "},{"title":"Fixed​","type":1,"pageTitle":"0.22.0 - 2023-04-03","url":"/docs/releases/0_22_0#fixed","content":"Spark: fix missing BigQuery class in column lineage #1698 @pawel-big-lebowski The Spark integration now checks if the BigQuery classes are available on the classpath before attempting to use them.DBT: throw UnsupportedDbtCommand when finding unsupported entry in args.which #1724 @JDarDagran Adjusts the dbt-ol script to detect DBT commands in run_results.json only. "},{"title":"Removed​","type":1,"pageTitle":"0.22.0 - 2023-04-03","url":"/docs/releases/0_22_0#removed","content":"Spark: remove unnecessary warnings for column lineage #1700 @pawel-big-lebowski Removes the warnings about OneRowRelation and LocalRelation nodes.Spark: remove deprecated configs #1711 @tnazarew Removes support for deprecated configs. "},{"title":"0.26.0 - 2023-05-18","type":0,"sectionRef":"#","url":"/docs/releases/0_26_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.26.0 - 2023-05-18","url":"/docs/releases/0_26_0#added","content":"Proxy: Fluentd proxy support (experimental) #1757 @pawel-big-lebowski Adds a Fluentd data collector as a proxy to buffer Openlineage events and send them to multiple backends (among many other purposes). Also implements a Fluentd Openlineage parser to validate incoming HTTP events at the beginning of the pipeline. See the readme file for more details. "},{"title":"Changed​","type":1,"pageTitle":"0.26.0 - 2023-05-18","url":"/docs/releases/0_26_0#changed","content":"Python client: use Hatchling over setuptools to orchestrate Python env setup #1856 @gaborbernat Replaces setuptools with Hatchling for building the backend. Also includes a number of fixes, including to type definitions in transport and elsewhere. "},{"title":"Fixed​","type":1,"pageTitle":"0.26.0 - 2023-05-18","url":"/docs/releases/0_26_0#fixed","content":"Spark: support single file datasets #1855 @pawel-big-lebowski Fixes the naming of single file datasets so they are no longer named using the parent directory's path: spark.read.csv('file.csv').Spark: fix logicalPlan serialization issue on Databricks #1858 @pawel-big-lebowski Disables the spark_unknown facet by default to turn off serialization of logicalPlan. "},{"title":"0.25.0 - 2023-05-15","type":0,"sectionRef":"#","url":"/docs/releases/0_25_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.25.0 - 2023-05-15","url":"/docs/releases/0_25_0#added","content":"Spark: add Spark/Delta merge into support #1823 @pawel-big-lebowski Adds support for merge into queries. "},{"title":"Fixed​","type":1,"pageTitle":"0.25.0 - 2023-05-15","url":"/docs/releases/0_25_0#fixed","content":"Spark: fix JDBC query handling #1808 @nataliezeller1 Makes query handling more tolerant of variations in syntax and formatting.Spark: filter Delta adaptive plan events #1830 @pawel-big-lebowski Extends the DeltaEventFilter class to filter events in cases where rewritten queries in adaptive Spark plans generate extra events.Spark: fix Java class cast exception #1844 @Anirudh181001 Fixes the error caused by the OpenLineageRunEventBuilder when it cast the Spark scheduler's ShuffleMapStage to boolean.Flink: include missing fields of Openlineage events #1840 @pawel-big-lebowskiEnriches Flink events so that missing eventTime, runId and job elements no longer produce errors. "},{"title":"0.27.2 - 2023-06-06","type":0,"sectionRef":"#","url":"/docs/releases/0_27_2","content":"","keywords":""},{"title":"Fixed​","type":1,"pageTitle":"0.27.2 - 2023-06-06","url":"/docs/releases/0_27_2#fixed","content":"Python client: deprecate client.from_environment, do not skip loading config #1908 @mobuchowski Deprecates the OpenLineage.from_environment method and recommends using the constructor instead. "},{"title":"0.27.1 - 2023-06-05","type":0,"sectionRef":"#","url":"/docs/releases/0_27_1","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.27.1 - 2023-06-05","url":"/docs/releases/0_27_1#added","content":"Python client: add emission filtering mechanism and exact, regex filters #1878 @mobuchowski Adds configurable job-name filtering to the Python client. Filters can be exact-match- or regex-based. Events will not be sent in the case of matches. "},{"title":"Fixed​","type":1,"pageTitle":"0.27.1 - 2023-06-05","url":"/docs/releases/0_27_1#fixed","content":"Spark: fix column lineage for aggregate queries on databricks #1867 @pawel-big-lebowski Aggregate queries on databricks did not return column lineage.Airflow: fix unquoted [ and ] in Snowflake URIs #1883 @JDarDagran Snowflake connections containing one of [ or ] were causing urllib.parse.urlparse to fail. "},{"title":"0.28.0 - 2023-06-12","type":0,"sectionRef":"#","url":"/docs/releases/0_28_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.28.0 - 2023-06-12","url":"/docs/releases/0_28_0#added","content":"dbt: add Databricks compatibility #1829 @Ines70 Enables launching OpenLineage with a Databricks profile. "},{"title":"Fixed​","type":1,"pageTitle":"0.28.0 - 2023-06-12","url":"/docs/releases/0_28_0#fixed","content":"Fix type-checked marker and packaging #1913 @gaborbernat The client was not marking itself as type-annotated.Python client: add schemaURL to run event #1917 @gaborbernat Adds the missing schemaURL to the client's RunState class. "},{"title":"0.29.2 - 2023-06-30","type":0,"sectionRef":"#","url":"/docs/releases/0_29_2","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.29.2 - 2023-06-30","url":"/docs/releases/0_29_2#added","content":"Flink: support Flink version 1.17.1 #1947 @pawel-big-lebowski Support Flink versions: 1.15.4, 1.16.2 and 1.17.1.Spark: support Spark 3.4 #1790 @pawel-big-lebowski Introduce support for latest Spark version 3.4.0, along with 3.2.4 and 3.3.2.Spark: add Databricks platform integration test #1928 @pawel-big-lebowski Spark integration test to verify behaviour on databricks platform to be run manually in CircleCI when needed.Spec: add static lineage event types #1880 @pawel-big-lebowski As a first step in implementing static lineage, this adds new DatasetEvent and JobEvent types to the spec, along with support for the new types in the Python client. "},{"title":"Removed​","type":1,"pageTitle":"0.29.2 - 2023-06-30","url":"/docs/releases/0_29_2#removed","content":"Proxy: remove unused Golang client approach #1926 @mobuchowski Removes the unused Golang proxy, rendered redundant by the fluentd proxy.Req: bump minimum supported Python version to 3.8 #1950 @mobuchowski Python 3.7 is at EOL. This bumps the minimum supported version to 3.8 to keep the project aligned with the Python EOL schedule. "},{"title":"0.3.0 - 2021-12-03","type":0,"sectionRef":"#","url":"/docs/releases/0_3_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.3.0 - 2021-12-03","url":"/docs/releases/0_3_0#added","content":"Spark3 support @OleksandrDvornik / @collado-mikeLineageBackend for Airflow 2 @mobuchowskiAdding custom spark version facet to spark integration @OleksandrDvornikAdding dbt version facet @mobuchowskiAdded support for Redshift profile @AlessandroLollo "},{"title":"Fixed​","type":1,"pageTitle":"0.3.0 - 2021-12-03","url":"/docs/releases/0_3_0#fixed","content":"Sanitize JDBC URLs @OleksandrDvornikstrip openlineage url in python client @OleksandrDvornikdeploy spec if spec file changes @mobuchowski "},{"title":"0.3.1 - 2021-12-03","type":0,"sectionRef":"#","url":"/docs/releases/0_3_1","content":"","keywords":""},{"title":"Fixed​","type":1,"pageTitle":"0.3.1 - 2021-12-03","url":"/docs/releases/0_3_1#fixed","content":"fix import in spark3 visitor @mobuchowski "},{"title":"0.4.0 - 2021-12-13","type":0,"sectionRef":"#","url":"/docs/releases/0_4_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.4.0 - 2021-12-13","url":"/docs/releases/0_4_0#added","content":"Spark output metrics @OleksandrDvornikSeparated tests between Spark 2 &amp; 3 @pawel-big-lebowskiDatabricks install README and init scripts @wjohnsonIceberg integration with unit tests @pawel-big-lebowskiKafka read and write support @OleksandrDvornik / @collado-mikeArbitrary parameters supported in HTTP URL construction @wjohnsonIncreased visitor coverage for Spark commands @mobuchowski / @pawel-big-lebowski "},{"title":"Fixed​","type":1,"pageTitle":"0.4.0 - 2021-12-13","url":"/docs/releases/0_4_0#fixed","content":"dbt: column descriptions are properly filled from metadata.json @mobuchowskidbt: allow parsing artifacts with version higher than officially supported @mobuchowskidbt: dbt build command is supported @mobuchowskidbt: fix crash when build command is used with seeds in dbt 1.0.0rc3 @mobuchowskispark: increase logical plan visitor coverage @mobuchowski spark: fix logical serialization recursion issue @OleksandrDvornikUse URL#getFile to fix build on Windows @mobuchowski "},{"title":"0.30.1 - 2023-07-25","type":0,"sectionRef":"#","url":"/docs/releases/0_30_1","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.30.1 - 2023-07-25","url":"/docs/releases/0_30_1#added","content":"Flink: support Iceberg sinks #1960 @pawel-big-lebowski Detects output datasets when using an Iceberg table as a sink.Spark: column-level lineage for merge into on Delta tables #1958 @pawel-big-lebowski Makes column-level lineage support merge into on Delta tables. Also refactors column-level lineage to deal with multiple Spark versions.Spark: column-level lineage for merge into on Iceberg tables #1971 @pawel-big-lebowski Makes column-level lineage support merge into on Iceberg tables.Spark: add supprt for Iceberg REST catalog #1963 @juancappi Adds rest to the existing options of hive and hadoop in IcebergHandler.getDatasetIdentifier() to add support for Iceberg's RestCatalog.Airflow: add possibility to force direct-execution based on environment variable #1934 @mobuchowski Adds the option to use the direct-execution method on the Airflow listener when the existence of a non-SQLAlchemy-based Airflow event mechanism is confirmed. This happens when using Airflow 2.6 or when the OPENLINEAGE_AIRFLOW_ENABLE_DIRECT_EXECUTION environment variable exists.SQL: add support for Apple Silicon to openlineage-sql-java #1981 @davidjgoss Expands the OS/architecture checks when compiling to produce a specific file for Apple Silicon. Also expands the corresponding OS/architecture checks when loading the binary at runtime from Java code.Spec: add facet deletion #1975 @julienledem In order to add a mechanism for deleting job and dataset facets, adds a { _deleted: true } object that can take the place of any job or dataset facet (but not run or input/output facets, which are valid only for a specific run).Client: add a file transport #1891 @Alexkuva Creates a FileTransport and its configuration classes supporting append mode or write-new-file mode, which is especially useful when an object store does not support append mode, e.g. in the case of Databricks DBFS FUSE. "},{"title":"Changed​","type":1,"pageTitle":"0.30.1 - 2023-07-25","url":"/docs/releases/0_30_1#changed","content":"Airflow: do not run plugin if OpenLineage provider is installed #1999 @JDarDagran Sets OPENLINEAGE_DISABLED to true if the provider is installed.Python: rename config to config_class #1998 @mobuchowski Renames the config class variable to config_class to avoid potential conflict with the config instance. "},{"title":"Fixed​","type":1,"pageTitle":"0.30.1 - 2023-07-25","url":"/docs/releases/0_30_1#fixed","content":"Airflow: add workaround for airflow-sqlalchemy event mechanism bug #1959 @mobuchowski Due to known issues with the fork and thread model in the Airflow-SQLAlchemy-based event-delivery mechanism, a Kafka producer left alone does not emit a `COMPLETE`` event. This creates a producer for each event when we detect that we're under Airflow 2.3 - 2.5.Spark: fix custom environment variables facet #1973 @pawel-big-lebowski Enables sending the Spark environment variables facet in a non-deterministic way.Spark: filter unwanted Delta events #1968 @pawel-big-lebowski Clears events generated by logical plans having Project node as root.Python: allow modification of openlineage.* logging levels via environment variables #1974 @JDarDagran Adds OPENLINEAGE_{CLIENT/AIRFLOW/DBT}_LOGGING environment variables that can be set according to module logging levels and cleans up some logging calls in openlineage-airflow. "},{"title":"0.5.1 - 2022-01-18","type":0,"sectionRef":"#","url":"/docs/releases/0_5_1","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.5.1 - 2022-01-18","url":"/docs/releases/0_5_1#added","content":"Support for dbt-spark adapter @mobuchowskiNew backend to proxy OpenLineage events to one or more event streams 🎉 @mandy-chessell @wslulciucAdd Spark extensibility API with support for custom Dataset and custom facet builders @collado-mike "},{"title":"Fixed​","type":1,"pageTitle":"0.5.1 - 2022-01-18","url":"/docs/releases/0_5_1#fixed","content":"airflow: fix import failures when dependencies for bigquery, dbt, great_expectations extractors are missing @lukaszlaszkoFixed openlineage-spark jar to correctly rename bundled dependencies @collado-mike "},{"title":"0.5.2 - 2022-02-10","type":0,"sectionRef":"#","url":"/docs/releases/0_5_2","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.5.2 - 2022-02-10","url":"/docs/releases/0_5_2#added","content":"Proxy backend example using Kafka @wslulciucSupport Databricks Delta Catalog naming convention with DatabricksDeltaHandler @wjohnsonAdd javadoc as part of build task @mobuchowskiInclude TableStateChangeFacet in non V2 commands for Spark @mr-yusupovSupport for SqlDWRelation on Databricks' Azure Synapse/SQL DW Connector @wjohnsonImplement input visitors for v2 commands @pawel-big-lebowskiEnabled SparkListenerJobStart events to trigger open lineage events @collado-mike "},{"title":"Fixed​","type":1,"pageTitle":"0.5.2 - 2022-02-10","url":"/docs/releases/0_5_2#fixed","content":"dbt: job namespaces for given dbt run match each other @mobuchowskiFix Breaking SnowflakeOperator Changes from OSS Airflow @denimalpacaMade corrections to account for DeltaDataSource handling @collado-mike "},{"title":"0.6.0 - 2022-03-04","type":0,"sectionRef":"#","url":"/docs/releases/0_6_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.6.0 - 2022-03-04","url":"/docs/releases/0_6_0#added","content":"Extract source code of PythonOperator code similar to SQL facet @mobuchowskiAdd DatasetLifecycleStateDatasetFacet to spec @pawel-big-lebowskiAirflow: extract source code from BashOperator @mobuchowskiAdd generic facet to collect environmental properties (EnvironmentFacet) @harishsuneOpenLineage sensor for OpenLineage-Dagster integration @dalinkimJava-client: make generator generate enums as well @pawel-big-lebowskiAdded UnknownOperatorAttributeRunFacet to Airflow integration to record operators that don't produce lineage @collado-mike "},{"title":"Fixed​","type":1,"pageTitle":"0.6.0 - 2022-03-04","url":"/docs/releases/0_6_0#fixed","content":"Airflow: increase import timeout in tests, fix exit from integration @mobuchowskiReduce logging level for import errors to info @rossturkRemove AWS secret keys and extraneous Snowflake parameters from connection uri @collado-mikeConvert to LifecycleStateChangeDatasetFacet @pawel-big-lebowski "},{"title":"0.6.2 - 2022-03-16","type":0,"sectionRef":"#","url":"/docs/releases/0_6_2","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.6.2 - 2022-03-16","url":"/docs/releases/0_6_2#added","content":"CI: add integration tests for Airflow's SnowflakeOperator and dbt-snowflake @mobuchowskiIntroduce DatasetVersion facet in spec @pawel-big-lebowskiAirflow: add external query id facet @mobuchowski "},{"title":"Fixed​","type":1,"pageTitle":"0.6.2 - 2022-03-16","url":"/docs/releases/0_6_2#fixed","content":"Complete Fix of Snowflake Extractor get_hook() Bug @denimalpacaUpdate artwork @rossturkAirflow tasks in a DAG now report a common ParentRunFacet @collado-mike "},{"title":"0.6.1 - 2022-03-07","type":0,"sectionRef":"#","url":"/docs/releases/0_6_1","content":"","keywords":""},{"title":"Fixed​","type":1,"pageTitle":"0.6.1 - 2022-03-07","url":"/docs/releases/0_6_1#fixed","content":"Catch possible failures when emitting events and log them @mobuchowskidbt: jinja2 code using do extensions does not crash @mobuchowski "},{"title":"0.7.1 - 2022-04-19","type":0,"sectionRef":"#","url":"/docs/releases/0_7_1","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.7.1 - 2022-04-19","url":"/docs/releases/0_7_1#added","content":"Python implements Transport interface - HTTP and Kafka transports are available (#530) @mobuchowskiAdd UnknownOperatorAttributeRunFacet and support in lineage backend (#547) @collado-mikeSupport Spark 3.2.1 (#607) @pawel-big-lebowskiAdd StorageDatasetFacet to spec (#620) @pawel-big-lebowskiAirflow: custom extractors lookup uses only get_operator_classnames method (#656) @mobuchowskiREADME.md created at OpenLineage/integrations for compatibility matrix (#663) @howardyoo "},{"title":"Fixed​","type":1,"pageTitle":"0.7.1 - 2022-04-19","url":"/docs/releases/0_7_1#fixed","content":"Dagster: handle updated PipelineRun in OpenLineage sensor unit test (#624) @dominiquetiptonDelta improvements (#626) @collado-mikeFix SqlDwDatabricksVisitor for Spark2 (#630) @wjohnsonAirflow: remove redundant logging from GE import (#657) @mobuchowskiFix Shebang issue in Spark's wait-for-it.sh (#658) @mobuchowskiUpdate parent_run_id to be a uuid from the dag name and run_id (#664) @collado-mikeSpark: fix time zone inconsistency in testSerializeRunEvent (#681) @sekikn "},{"title":"0.8.2 - 2022-05-19","type":0,"sectionRef":"#","url":"/docs/releases/0_8_2","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.8.2 - 2022-05-19","url":"/docs/releases/0_8_2#added","content":"openlineage-airflow now supports getting credentials from Airflows secrets backend (#723) @mobuchowskiopenlineage-spark now supports Azure Databricks Credential Passthrough (#595) @wjohnsonopenlineage-spark detects datasets wrapped by ExternalRDDs (#746) @collado-mike "},{"title":"Fixed​","type":1,"pageTitle":"0.8.2 - 2022-05-19","url":"/docs/releases/0_8_2#fixed","content":"PostgresOperator fails to retrieve host and conn during extraction (#705) @sekiknSQL parser accepts lists of sql statements (#734) @mobuchowskiMissing schema when writing to Delta tables in Databricks (#748) @collado-mike "},{"title":"0.8.1 - 2022-04-29","type":0,"sectionRef":"#","url":"/docs/releases/0_8_1","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.8.1 - 2022-04-29","url":"/docs/releases/0_8_1#added","content":"Airflow integration uses new TaskInstance listener API for Airflow 2.3+ (#508) @mobuchowskiSupport for HiveTableRelation as input source in Spark integration (#683) @collado-mikeAdd HTTP and Kafka Client to openlineage-java lib (#480) @wslulciuc, @mobuchowskiNew SQL parser, used by Postgres, Snowflake, Great Expectations integrations (#644) @mobuchowski "},{"title":"Fixed​","type":1,"pageTitle":"0.8.1 - 2022-04-29","url":"/docs/releases/0_8_1#fixed","content":"GreatExpectations: Fixed bug when invoking GreatExpectations using v3 API (#683) @collado-mike "},{"title":"0.9.0 - 2022-06-03","type":0,"sectionRef":"#","url":"/docs/releases/0_9_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"0.9.0 - 2022-06-03","url":"/docs/releases/0_9_0#added","content":"Add static code anlalysis tool mypy to run in CI for against all python modules (#802) @howardyooExtend SaveIntoDataSourceCommandVisitor to extract schema from LocalRelaiton and LogicalRdd in spark integration (#794) @pawel-big-lebowskiAdd InMemoryRelationInputDatasetBuilder for InMemory datasets to Spark integration (#818) @pawel-big-lebowskiAdd copyright to source files #755 @merobi-hubAdd SnowflakeOperatorAsync extractor support to Airflow integration #869 @merobi-hubAdd PMD analysis to proxy project (#889) @howardyoo "},{"title":"Changed​","type":1,"pageTitle":"0.9.0 - 2022-06-03","url":"/docs/releases/0_9_0#changed","content":"Skip FunctionRegistry.class serialization in Spark integration (#828) @mobuchowskiInstall new rust-based SQL parser by default in Airflow integration (#835) @mobuchowskiImprove overall pytest and integration tests for Airflow integration (#851,#858) @denimalpacaReduce OL event payload size by excluding local data and including output node in start events (#881) @collado-mikeSplit spark integration into submodules (#834, #890) @tnazarew @mobuchowski "},{"title":"Fixed​","type":1,"pageTitle":"0.9.0 - 2022-06-03","url":"/docs/releases/0_9_0#fixed","content":"Conditionally import sqlalchemy lib for Great Expectations integration (#826) @pawel-big-lebowskiAdd check for missing class org.apache.spark.sql.catalyst.plans.logical.CreateV2Table in Spark integration (#866) @pawel-big-lebowskiFix static code analysis issues (#867,#874) @pawel-big-lebowski "},{"title":"1.0.0 - 2023-08-01","type":0,"sectionRef":"#","url":"/docs/releases/1_0_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"1.0.0 - 2023-08-01","url":"/docs/releases/1_0_0#added","content":"Airflow: convert lineage from legacy File definition #2006 @mobuchowski Adds coverage for File entity definition to enhance backwards compatibility. "},{"title":"Removed​","type":1,"pageTitle":"1.0.0 - 2023-08-01","url":"/docs/releases/1_0_0#removed","content":"Spec: remove facet ref from core #1997 @JDarDagran Removes references to facets from the core spec that broke compatibility with JSON schema specification. "},{"title":"Changed​","type":1,"pageTitle":"1.0.0 - 2023-08-01","url":"/docs/releases/1_0_0#changed","content":"Airflow: change log level to DEBUG when extractor isn't found #2012 @kaxil Changes log level from WARNING to DEBUG when an extractor is not available.Airflow: make sure we cannot fail in thread despite direct execution #2010 @mobuchowski Ensures the listener is not failing tasks, even in unlikely scenarios. "},{"title":"Fixed​","type":1,"pageTitle":"1.0.0 - 2023-08-01","url":"/docs/releases/1_0_0#fixed","content":"Airflow: stop using reusable session by default, do not send full event on Snowflake complete #2025 @mobuchowski Fixes the issue of the Snowflake connector clashing with HttpTransport by disabling automatic requests session reuse and not running SnowflakeExtractor again on job completion.Client: fix error message to avoid confusion #2001 @mars-lan Fixes the error message in HttpTransport in the case of a null URL. "},{"title":"1.1.0 - 2023-08-23","type":0,"sectionRef":"#","url":"/docs/releases/1_1_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"1.1.0 - 2023-08-23","url":"/docs/releases/1_1_0#added","content":"Flink: create Openlineage configuration based on Flink configuration #2033 @pawel-big-lebowski Flink configuration entries starting with openlineage.* are passed to the Openlineage client.Java: add Javadocs to the Java client #2004 @julienledem The client was missing some Javadocs.Spark: append output dataset name to a job name #2036 @pawel-big-lebowski Solves the problem of multiple jobs writing to different datasets while having the same job name. The feature is enabled by default and results in different job names. It can be disabled by setting spark.openlineage.jobName.appendDatasetName to false. Unifies job names generated on the Databricks platform (using a dot job part separator instead of an underscore). The default behaviour can be altered with spark.openlineage.jobName.replaceDotWithUnderscore.Spark: support Spark 3.4.1 #2057 @pawel-big-lebowski Bumps the latest Spark version to be covered in integration tests. "},{"title":"Fixed​","type":1,"pageTitle":"1.1.0 - 2023-08-23","url":"/docs/releases/1_1_0#fixed","content":"Airflow: do not use database as fallback when no schema parsed #2023 @mobuchowski Sets the schema to None in TablesHierarchy to skip filtering on the schema level in the information schema query.Flink: fix a bug when getting schema for KafkaSink #2042 @pentium3 Fixes the incomplete schema from KafkaSinkVisitor by changing the KafkaSinkWrapper to catch schemas of type AvroSerializationSchema.Spark: filter CreateView events #1968#1987 @pawel-big-lebowski Clears events generated by logical plans having CreateView nodes as root.Spark: fix MERGE INTO for delta tables identified by physical locations #2026 @pawel-big-lebowski Delta tables identified by physical locations were not properly recognized.Spark: fix incorrect naming of JDBC datasets #2035 @mobuchowski Makes the namespace generated by the JDBC/Spark connector conform to the naming schema in the spec. Spark: fix ignored event adaptive_spark_plan in Databricks #2061 @algorithmy1 Removes adaptive_spark_plan from the excludedNodes in DatabricksEventFilter. "},{"title":"1.2.2 - 2023-09-20","type":0,"sectionRef":"#","url":"/docs/releases/1_2_2","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"1.2.2 - 2023-09-20","url":"/docs/releases/1_2_2#added","content":"Spark: publish the ProcessingEngineRunFacet as part of the normal operation of the OpenLineageSparkEventListener #2089 @d-m-h Publishes the spec-defined ProcessEngineRunFacet alongside the custom SparkVersionFacet (for now).The SparkVersionFacet is deprecated and will be removed in a future release.Spark: capture and emit spark.databricks.clusterUsageTags.clusterAllTags variable from databricks environment #2099 @Anirudh181001 Adds spark.databricks.clusterUsageTags.clusterAllTags to the list of environment variables captured from databricks. "},{"title":"Fixed​","type":1,"pageTitle":"1.2.2 - 2023-09-20","url":"/docs/releases/1_2_2#fixed","content":"Common: support parsing dbt_project.yml without target-path #2106 @tatiana As of dbt v1.5, usage of target-path in the dbt_project.yml file has been deprecated, now preferring a CLI flag or env var. It will be removed in a future version. This allows users to run DbtLocalArtifactProcessor in dbt projects that do not declare target-path.Proxy: fix Proxy chart #2091 @harels Includes the proper image to deploy in the helm chart.Python: fix serde filtering #2044 @xli-1026 Fixes the bug causing values in list objects to be filtered accidentally.Python: use non-deprecated apiKey if loading it from env variables @2029 @mobuchowski Changes api_key to apiKey in create_token_provider.Spark: Improve RDDs on S3 integration. #2039 @pawel-big-lebowski Prepares integration test to access S3, fixes input dataset duplicates and includes other minor fixes.Flink: prevent sending running events after job completes #2075 @pawel-big-lebowski Flink checkpoint tracking thread was not getting stopped properly on job complete.Spark &amp; Flink: Unify dataset naming from URI objects #2083 @pawel-big-lebowski Makes sure Spark and Flink generate same dataset identifiers for the same datasets by having a single implementation to generate dataset namespace and name.Spark: Databricks improvements #2076 @pawel-big-lebowski Filters unwanted events on databricks and adds an integration test to verify this. Adds integration tests to verify dataset naming on databricks runtime is correct when table location is specified. Adds integration test for wide transformation on delta tables. "},{"title":"Removed​","type":1,"pageTitle":"1.2.2 - 2023-09-20","url":"/docs/releases/1_2_2#removed","content":"SQL: remove sqlparser dependency from iface-java and iface-py #2090 @JDarDagran Removes the dependency due to a breaking change in the latest release of the parser. "},{"title":"1.3.1 - 2023-10-03","type":0,"sectionRef":"#","url":"/docs/releases/1_3_1","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"1.3.1 - 2023-10-03","url":"/docs/releases/1_3_1#added","content":"Airflow: add some basic stats to the Airflow integration #1845 @harels Uses the statsd component that already exists in the Airflow codebase and wraps the section that emits to event with a timer, as well as emitting a counter for exceptions in sending the event.Airflow: add columns as schema facet for airflow.lineage.Table (if defined) #2138 @erikalfthan Adds columns (if set) from airflow.lineage.Table inlets/outlets to the OpenLineage Dataset.DBT: add SQLSERVER to supported dbt profile types #2136 @erikalfthan Adds support for dbt-sqlserver, solving #2129.Spark: support for latest 3.5 #2118 @pawel-big-lebowski Integration tests are now run on Spark 3.5. Also upgrades 3.3 branch to 3.3.3. Please note that delta and iceberg are not supported for Spark 3.5 at this time. "},{"title":"Fixed​","type":1,"pageTitle":"1.3.1 - 2023-10-03","url":"/docs/releases/1_3_1#fixed","content":"Airflow: fix find-links path in tox #2139 @JDarDagran Fixes a broken link.Airflow: add more graceful logging when no OpenLineage provider installed #2141 @JDarDagran Recognizes a failed import of airflow.providers.openlineage and adds more graceful logging to fix a corner case.Spark: fix bug in PathUtils' prepareDatasetIdentifierFromDefaultTablePath(CatalogTable) to correctly preserve scheme from CatalogTable's location #2142 @d-m-h Previously, the prepareDatasetIdentifierFromDefaultTablePath method would override the scheme with the value of &quot;file&quot; when constructing a dataset identifier. It now uses the scheme of the CatalogTable's URI for this. Thank you @pawel-big-lebowski for the quick triage and suggested fix. "},{"title":"1.5.0 - 2023-11-02","type":0,"sectionRef":"#","url":"/docs/releases/1_5_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"1.5.0 - 2023-11-02","url":"/docs/releases/1_5_0#added","content":"Flink: add Flink lineage for Cassandra Connectors #2175 @HuangZhenQiu Adds Flink Cassandra source and sink visitors and Flink Cassandra Integration test.Spark: support rdd and toDF operations available in Spark Scala API #2188 @pawel-big-lebowski Includes the first Scala integration test, fixes ExternalRddVisitor and adds support for extracting inputs from MapPartitionsRDD and ParallelCollectionRDD plan nodes.Spark: support Databricks Runtime 13.3 #2185 @pawel-big-lebowski Modifies the Spark integration to support the latest Databricks Runtime version. "},{"title":"Changed​","type":1,"pageTitle":"1.5.0 - 2023-11-02","url":"/docs/releases/1_5_0#changed","content":"Airflow: loosen attrs and requests versions #2107 @JDarDagran Lowers the version requirements for attrs and requests and removes an unnecessary dependency.dbt: render yaml configs lazily #2221 @JDarDagran Don't render each entry in yaml files at start.  "},{"title":"Fixed​","type":1,"pageTitle":"1.5.0 - 2023-11-02","url":"/docs/releases/1_5_0#fixed","content":"Airflow/Athena: change dataset name to its location #2167 @sophiely Replaces the dataset and namespace with the data's physical location for more complete lineage across integrations.Python client: skip redaction in column lineage facet #2177 @JDarDagran Redacted fields in ColumnLineageDatasetFacetFieldsAdditionalInputFields are now skipped.Spark: unify dataset naming for RDD jobs and Spark SQL #2181 @pawel-big-lebowski Use the same mechanism for RDD jobs to extract dataset identifier as used for Spark SQL.Spark: ensure a single START and a single COMPLETE event are sent #2103 @pawel-big-lebowski For Spark SQL at least four events are sent triggered by different SparkListener methods. Each of them is required and used to collect facets unavailable elsewhere. However, there should be only one START and COMPLETE events emitted. Other events should be sent as RUNNING. Please keep in mind that Spark integration remains stateless to limit the memory footprint, and it is the backend responsibility to merge several Openlineage events into a meaningful snapshot of metadata changes. "},{"title":"1.4.1 - 2023-10-09","type":0,"sectionRef":"#","url":"/docs/releases/1_4_1","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"1.4.1 - 2023-10-09","url":"/docs/releases/1_4_1#added","content":"Client: allow setting client's endpoint via environment variable #2151 @mars-lan Enables setting this endpoint via environment variable because creating the client manually in Airflow is not possible.Flink: expand Iceberg source types #2149 @HuangZhenQiu Adds support for FlinkIcebergSource and FlinkIcebergTableSource for Flink Iceberg lineage.Spark: add debug facet #2147 @pawel-big-lebowski An extra run facet containing some system details (e.g., OS, Java, Scala version), classpath (e.g., package versions, jars included in the Spark job), SparkConf (like openlineage entries except auth, specified extensions, etc.) and LogicalPlan details (execution tree nodes' names) are added to events emitted. SparkConf setting spark.openlineage.debugFacet=enabled needs to be set to include the facet. By default, the debug facet is disabled.Spark: enable Nessie REST catalog #2165 @julwin Adds support for Nessie catalog in Spark. "},{"title":"1.6.2 - 2023-12-07","type":0,"sectionRef":"#","url":"/docs/releases/1_6_2","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"1.6.2 - 2023-12-07","url":"/docs/releases/1_6_2#added","content":"Dagster: support Dagster 1.5.x #2220 @tsungchih Gets event records for each target Dagster event type to support Dagster version 0.15.0+.Dbt: add a new command dbt-ol send-events to send metadata of the last run without running the job #2285 @sophiely Adds a new command to send events to OpenLineage according to the latest metadata generated without running any dbt command.Flink: add option for Flink job listener to read from Flink conf #2229 @ensctom Adds option for the Flink job listener to read jobnames and namespaces from Flink conf.Spark: get column-level lineage from JDBC dbtable option #2284 @mobuchowski Adds support for dbtable, enables lineage in the case of single input columns, and improves dataset naming.Spec: introduce JobTypeJobFacet to contain additional job related information#2241 @pawel-big-lebowski New JobTypeJobFacet contains the processing type such as BATCH|STREAMING, integration via SPARK|FLINK|... and job type in QUERY|COMMAND|DAG|....SQL: add quote information from sqlparser-rs #2259 @JDarDagran Adds quote information from sqlparser-rs. "},{"title":"Fixed​","type":1,"pageTitle":"1.6.2 - 2023-12-07","url":"/docs/releases/1_6_2#fixed","content":"Spark: update Jackson dependency to resolve CVE-2022-1471 #2185 @pawel-big-lebowski Updates Gradle for Spark and Flink to 8.1.1. Upgrade Jackson 2.15.3.Flink: avoid relying on Guava which can be missing during production runtime #2296 @pawel-big-lebowski Removes usage of Guava ImmutableList.Spark: exclude commons-logging transitive dependency from published jar #2297 @pawel-big-lebowski Ensures commons-logging is not shipped as this can lead to a version mismatch on the user's side. "},{"title":"1.7.0 - 2023-12-21","type":0,"sectionRef":"#","url":"/docs/releases/1_7_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"1.7.0 - 2023-12-21","url":"/docs/releases/1_7_0#added","content":"Airflow: add parent run facet to COMPLETE and FAIL events in Airflow integration #2320 @kacpermuda Adds a parent run facet to all events in the Airflow integration. "},{"title":"Fixed​","type":1,"pageTitle":"1.7.0 - 2023-12-21","url":"/docs/releases/1_7_0#fixed","content":"Airflow: repair up.sh for MacOS #2316 #2318 @kacpermuda Some scripts were not working well on MacOS. This adjusts them.Airflow: repair run_id for FAIL event in Airflow 2.6+ #2305 @kacpermuda The Run_id in a FAIL event was different than in the START event for Airflow 2.6+.Flink: open Iceberg TableLoader before loading a table #2314 @pawel-big-lebowski Fixes a potential NullPointerException in 1.17 when dealing with Iceberg sinks.Flink: name Kafka datasets according to the naming convention #2321 @pawel-big-lebowski Adds a kafka:// prefix to Kafka topic datasets' namespaces.Flink: fix properties within JobTypeJobFacet #2325 @pawel-big-lebowski Fixes properties assignment in the Flink visitor.Spark: fix commons-logging relocate in target jar #2319 @pawel-big-lebowski Avoids relocating a dependency that was getting excluded from the jar.Spec: fix inconsistency with Redshift authority format #2315 @davidjgoss Amends the Authority format for consistency with other references in the same section. "},{"title":"Removed​","type":1,"pageTitle":"1.7.0 - 2023-12-21","url":"/docs/releases/1_7_0#removed","content":"Airflow: remove Airflow 2.8+ support #2330 @kacpermuda If the Airflow version is &gt;=2.8.0, the Airflow integration's plugin does not import the integration's listener, disabling the external integration. Please use the OpenLineage Airflow Provider instead. "},{"title":"1.8.0 - 2024-01-22","type":0,"sectionRef":"#","url":"/docs/releases/1_8_0","content":"","keywords":""},{"title":"Added​","type":1,"pageTitle":"1.8.0 - 2024-01-22","url":"/docs/releases/1_8_0#added","content":"Flink: support Flink 1.18 #2366 @HuangZhenQiu Adds support for the latest Flink version with 1.17 used for Iceberg Flink runtime and Cassandra Connector as these do not yet support 1.18.Spark: add Gradle plugins to simplify the build process to support Scala 2.13 #2376 @d-m-h *Defines a set of Gradle plugins to configure the modules and reduce duplication.Spark: support multiple Scala versions LogicalPlan implementation #2361 @mattiabertorello In the LogicalPlanSerializerTest class, the implementation of the LogicalPlan interface is different between Scala 2.12 and Scala 2.13. In detail, the IndexedSeq changes package from the scala.collection to scala.collection.immutable. This implements both of the methods necessary in the two versions.Spark: Use ScalaConversionUtils to convert Scala and Java collections #2357 @mattiabertorello This initial step is to start supporting compilation for Scala 2.13 in the 3.2+ Spark versions. Scala 2.13 changed the default collection to immutable, the methods to create an empty collection, and the conversion between Java and Scala. This causes the code to not compile between 2.12 and 2.13. This replaces the usage of direct Scala collection methods (like creating an empty object) and conversions utils with ScalaConversionUtils methods that will support cross-compilation.Spark: support MERGE INTO queries on Databricks #2348 @pawel-big-lebowski Supports custom plan nodes used when running MERGE INTO queries on Databricks runtime.Spark: Support Glue catalog in iceberg #2283 @nataliezeller1 Adds support for the Glue catalog based on the 'catalog-impl' property (in this case we will not have a 'type' property). "},{"title":"Changed​","type":1,"pageTitle":"1.8.0 - 2024-01-22","url":"/docs/releases/1_8_0#changed","content":"Spark: Move Spark 3.1 code from the spark3 project #2365 @mattiabertorello Moves the Spark 3.1-related code to a specific project, spark31, so the spark3 project can be compiled with any Spark 3.x version. "},{"title":"Fixed​","type":1,"pageTitle":"1.8.0 - 2024-01-22","url":"/docs/releases/1_8_0#fixed","content":"Airflow: add database information to SnowflakeExtractor #2364 @kacpermuda Fixes missing database information in SnowflakeExtractor.Airflow: add dag_id to task_run_id to avoid duplicates #2358 @kacpermuda The lack of dag_id in task_run_id can cause duplicates in run_id across different dags.Airflow: Add tests for column lineage facet and sql parser #2373 @kacpermuda Improves naming (database.schema.table) in SQLExtractor's column lineage facet and adds some unit tests.Spark: fix removePathPattern behaviour #2350 @pawel-big-lebowski The removepath pattern feature is not applied all the time. The method is called when constructing DatasetIdentifier through PathUtils which is not the case all the time. This moves removePattern to another place in the codebase that is always run.Spark: fix a type incompatibility in RddExecutionContext between Scala 2.12 and 2.13 #2360 @mattiabertorello The function from the ResultStage.func() object change type in Spark between Scala 2.12 and 2.13 makes the compilation fail. This avoids getting the function with an explicit type; instead, it gets it every time it is needed from the ResultStage object. This PR is part of the effort to support Scala 2.13 in the Spark integration.Spark: Fix removePathPattern feature #2350 @pawel-big-lebowski Refactors code to make sure that all datasets sent are processed through removePathPattern if configured to do so.Spark: Clean up the individual build.gradle files in preparation for Scala 2.13 support #2377 @d-m-h Cleans up the build.gradle files, consolidating the custom plugin and removing unused and unnecessary configuration.Spark: refactor the Gradle plugins to make it easier to define Scala variants per module #2383 @d-m-h The third of several PRs to support producing Scala 2.12 and Scala 2.13 variants of the OpenLineage Spark integration. This PR refactors the custom Gradle plugins in order to make supporting multiple variants per module easier. This is necessary because the shared module fails its tests when consuming the Scala 2.13 variants of Apache Spark. "},{"title":"Dataset Facets","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/","content":"Dataset Facets Dataset Facets are generally consisted of common facet that is used both in inputs and outputs of the OpenLineage event. There are facets that exist specifically for input or output datasets. { ... &quot;inputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://workshop-db:None&quot;, &quot;name&quot;: &quot;workshop.public.taxes-in&quot;, &quot;facets&quot;: { # This is where the common dataset facets are located }, &quot;inputFacets&quot;: { # This is where the input dataset facets are located } }], &quot;outputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://workshop-db:None&quot;, &quot;name&quot;: &quot;workshop.public.taxes-out&quot;, &quot;facets&quot;: { # This is where the common dataset facets are located }, &quot;outputFacets&quot;: { # This is where the output dataset facets are located } }], ... } In the above Example, Notice that there is a distinction of facets that are common for both input and output dataset, and input or output specific datasets. As for the common datasets, they all reside under the facets property. However, input or output specific facets are located either in inputFacets or outputFacets property.","keywords":""},{"title":"Data Quality Assertions Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/data_quality_assertions","content":"Data Quality Assertions Facet Example: { ... &quot;inputs&quot;: { &quot;facets&quot;: { &quot;dataQualityAssertions&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/DataQualityAssertionsDatasetFacet.json&quot;, &quot;assertions&quot;: [ { &quot;assertion&quot;: &quot;not_null&quot;, &quot;success&quot;: true, &quot;column&quot;: &quot;user_name&quot; }, { &quot;assertion&quot;: &quot;is_string&quot;, &quot;success&quot;: true, &quot;column&quot;: &quot;user_name&quot; } ] } } } ... } The facet specification can be found here.","keywords":""},{"title":"Facets & Extensibility","type":0,"sectionRef":"#","url":"/docs/spec/facets/","content":"Facets &amp; Extensibility Facets provide context to the OpenLineage events. Generally, an OpenLineage event contains the type of the event, who created it, and when the event happened. In addition to the basic information related to the event, it provides facets for more details in four general categories: job: What kind of activity ranrun: How it raninputs: What was used during its runoutputs: What was the outcome of the run Here is an example of the four facets in action. Notice the element facets under each of the four categories of the OpenLineage event: { &quot;eventType&quot;: &quot;START&quot;, &quot;eventTime&quot;: &quot;2020-12-28T19:52:00.001+10:00&quot;, &quot;run&quot;: { &quot;runId&quot;: &quot;d46e465b-d358-4d32-83d4-df660ff614dd&quot;, &quot;facets&quot;: { &quot;parent&quot;: { &quot;job&quot;: { &quot;name&quot;: &quot;dbt-execution-parent-job&quot;, &quot;namespace&quot;: &quot;dbt-namespace&quot; }, &quot;run&quot;: { &quot;runId&quot;: &quot;f99310b4-3c3c-1a1a-2b2b-c1b95c24ff11&quot; } } } }, &quot;job&quot;: { &quot;namespace&quot;: &quot;workshop&quot;, &quot;name&quot;: &quot;process_taxes&quot;, &quot;facets&quot;: { &quot;sql&quot;: { &quot;query&quot;: &quot;insert into taxes_out select id, name, is_active from taxes_in&quot; } } }, &quot;inputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://workshop-db:None&quot;, &quot;name&quot;: &quot;workshop.public.taxes-in&quot;, &quot;facets&quot;: { &quot;schema&quot;: { &quot;fields&quot;: [ { &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;int&quot;, &quot;description&quot;: &quot;Customer's identifier&quot; }, { &quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;Customer's name&quot; }, { &quot;name&quot;: &quot;is_active&quot;, &quot;type&quot;: &quot;boolean&quot;, &quot;description&quot;: &quot;Has customer completed activation process&quot; } ] } } }], &quot;outputs&quot;: [{ &quot;namespace&quot;: &quot;postgres://workshop-db:None&quot;, &quot;name&quot;: &quot;workshop.public.taxes-out&quot;, &quot;facets&quot;: { &quot;schema&quot;: { &quot;fields&quot;: [ { &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;int&quot;, &quot;description&quot;: &quot;Customer's identifier&quot; }, { &quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;Customer's name&quot; }, { &quot;name&quot;: &quot;is_active&quot;, &quot;type&quot;: &quot;boolean&quot;, &quot;description&quot;: &quot;Has customer completed activation process&quot; } ] } } }], &quot;producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client&quot; } For more information of what kind of facets are available as part of OpenLineage spec, please refer to the sub sections Run Facets, Job Facets, and Dataset Facets of this document.","keywords":""},{"title":"Column Level Lineage Dataset Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/column_lineage_facet","content":"Column Level Lineage Dataset Facet Column level lineage provides fine grained information on datasets' dependencies. Not only we know the dependency exist, but we are also able to understand which input columns are used to produce output columns. This allows answering questions like Which root input columns are used to construct column x? For example, a Job might executes the following query: INSERT INTO top_delivery_times ( order_id, order_placed_on, order_delivered_on, order_delivery_time ) SELECT order_id, order_placed_on, order_delivered_on, DATEDIFF(minute, order_placed_on, order_delivered_on) AS order_delivery_time, FROM delivery_7_days ORDER BY order_delivery_time DESC LIMIT 1; This would establish the following relationships between the delivery_7_days and top_delivery_times tables: An OpenLinage run state update that represent this query using column-level lineage facets might look like: { &quot;eventType&quot;: &quot;START&quot;, &quot;eventTime&quot;: &quot;2020-02-22T22:42:42.000Z&quot;, &quot;run&quot;: ..., &quot;job&quot;: ..., &quot;inputs&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot; } ], &quot;outputs&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.top_delivery_times&quot;, &quot;facets&quot;: { &quot;columnLineage&quot;: { &quot;_producer&quot;: &quot;https://github.com/MarquezProject/marquez/blob/main/docker/metadata.json&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-1/ColumnLineageDatasetFacet.json&quot;, &quot;fields&quot;: { &quot;order_id&quot;: { &quot;inputFields&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot;, &quot;field&quot;: &quot;order_id&quot; } ] }, &quot;order_placed_on&quot;: { &quot;inputFields&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot;, &quot;field&quot;: &quot;order_placed_on&quot; } ] }, &quot;order_delivered_on&quot;: { &quot;inputFields&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot;, &quot;field&quot;: &quot;order_delivered_on&quot; } ] }, &quot;order_delivery_time&quot;: { &quot;inputFields&quot;: [ { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot;, &quot;field&quot;: &quot;order_placed_on&quot; }, { &quot;namespace&quot;: &quot;food_delivery&quot;, &quot;name&quot;: &quot;public.delivery_7_days&quot;, &quot;field&quot;: &quot;order_delivered_on&quot; } ] } } } } } ], ... } The facet specification can be found here.","keywords":""},{"title":"Datasource Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/data_source","content":"Datasource Facet Example: { ... &quot;inputs&quot;: { &quot;facets&quot;: { &quot;dataSource&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/DatasourceDatasetFacet.json&quot;, &quot;name&quot;: &quot;datasource_one&quot;, &quot;url&quot;: &quot;https://some.location.com/datsource/one&quot; } } } ... } The facet specification can be found here.","keywords":""},{"title":"Data Quality Metrics Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/input-dataset-facets/data_quality_metrics","content":"Data Quality Metrics Facet Example: { ... &quot;inputs&quot;: { &quot;inputFacets&quot;: { &quot;dataQualityMetrics&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/DataQualityMetricsInputDatasetFacet.json&quot;, &quot;rowCount&quot;: 123, &quot;bytes&quot;: 35602, &quot;columnMetrics&quot;: { &quot;column_one&quot;: { &quot;nullCount&quot;: 132, &quot;distincCount&quot;: 11, &quot;sum&quot;: 500, &quot;count&quot;: 234, &quot;min&quot;: 111, &quot;max&quot;: 3234, &quot;quantiles&quot;: { &quot;0.1&quot;: 12, &quot;0.5&quot;: 22, &quot;1&quot;: 123, &quot;2&quot;: 11 } }, &quot;column_two&quot;: { &quot;nullCount&quot;: 132, &quot;distinctCount&quot;: 11, &quot;sum&quot;: 500, &quot;count&quot;: 234, &quot;min&quot;: 111, &quot;max&quot;: 3234, &quot;quantiles&quot;: { &quot;0.1&quot;: 12, &quot;0.5&quot;: 22, &quot;1&quot;: 123, &quot;2&quot;: 11 } }, &quot;column_three&quot;: { &quot;nullCount&quot;: 132, &quot;distincCount&quot;: 11, &quot;sum&quot;: 500, &quot;count&quot;: 234, &quot;min&quot;: 111, &quot;max&quot;: 3234, &quot;quantiles&quot;: { &quot;0.1&quot;: 12, &quot;0.5&quot;: 22, &quot;1&quot;: 123, &quot;2&quot;: 11 } } } } } } ... } The facet specification can be found here.","keywords":""},{"title":"Lifecycle State Change Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/lifecycle_state_change","content":"Lifecycle State Change Facet Example: { ... &quot;outputs&quot;: { &quot;facets&quot;: { &quot;lifecycleStateChange&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/LifecycleStateChangeDatasetFacet.json&quot;, &quot;lifecycleStateChange&quot;: &quot;CREATE&quot; } } } ... } { ... &quot;outputs&quot;: { &quot;facets&quot;: { &quot;lifecycleStateChange&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/LifecycleStateChangeDatasetFacet.json&quot;, &quot;lifecycleStateChange&quot;: &quot;RENAME&quot;, &quot;previousIdentifier&quot;: { &quot;namespace&quot;: &quot;example_namespace&quot;, &quot;name&quot;: &quot;example_table_1&quot; } } } } ... } The facet specification can be found here.","keywords":""},{"title":"Output Statistics Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/output-dataset-facets/output_statistics","content":"Output Statistics Facet Example: { ... &quot;outputs&quot;: { &quot;outputFacets&quot;: { &quot;outputStatistics&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/OutputStatisticsOutputDatasetFacet.json&quot;, &quot;rowCount&quot;: 123, &quot;size&quot;: 35602 } } } ... } The facet specification can be found here.","keywords":""},{"title":"Ownership Dataset Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/ownership","content":"Ownership Dataset Facet Example: { ... &quot;inputs&quot;: { &quot;facets&quot;: { &quot;ownership&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/OwnershipDatasetFacet.json&quot;, &quot;owners&quot;: [ { &quot;name&quot;: &quot;maintainer_one&quot;, &quot;type&quot;: &quot;MAINTAINER&quot; } ] } } } ... } The facet specification can be found here.","keywords":""},{"title":"Schema Dataset Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/schema","content":"Schema Dataset Facet The schema dataset facet contains the schema of a particular dataset. Besides a name, it provides an optional type and description of each field. Example: { ... &quot;inputs&quot;: { &quot;facets&quot;: { &quot;schema&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/SchemaDatasetFacet.json&quot;, &quot;fields&quot;: [ { &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;int&quot;, &quot;description&quot;: &quot;Customer's identifier&quot; }, { &quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;Customer's name&quot; }, { &quot;name&quot;: &quot;is_active&quot;, &quot;type&quot;: &quot;boolean&quot;, &quot;description&quot;: &quot;Has customer completed activation process&quot; } ] } } } ... } The facet specification can be found here.","keywords":""},{"title":"Storage Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/storage","content":"Storage Facet Example: { ... &quot;inputs&quot;: { &quot;facets&quot;: { &quot;storage&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/StorageDatasetFacet.json&quot;, &quot;storageLayer&quot;: &quot;iceberg&quot;, &quot;fileFormat&quot;: &quot;csv&quot; } } } ... } The facet specification can be found here.","keywords":""},{"title":"Version Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/version_facet","content":"Version Facet Example: { ... &quot;inputs&quot;: { &quot;facets&quot;: { &quot;version&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/DatasetVersionDatasetFacet.json&quot;, &quot;datasetVersion&quot;: &quot;1&quot; } } } ... } The facet specification can be found here.","keywords":""},{"title":"Symlinks Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/dataset-facets/symlinks","content":"Symlinks Facet Example: { ... &quot;inputs&quot;: { &quot;facets&quot;: { &quot;symlinks&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/SymlinksDatasetFacet.json&quot;, &quot;identifiers&quot;: [ &quot;namespace&quot;: &quot;example_namespace&quot;, &quot;name&quot;: &quot;example_dataset_1&quot;, &quot;type&quot;: &quot;table&quot; ] } } } ... } The facet specification can be found here.","keywords":""},{"title":"Documentation Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/job-facets/documentation","content":"Documentation Facet Contains the documentation or description of the job. Example: { ... &quot;job&quot;: { &quot;facets&quot;: { &quot;documentation&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/main/spec/facets/DocumentationJobFacet.json&quot;, &quot;description&quot;: &quot;This is the documentation of something.&quot; } } } ... } The facet specification can be found here","keywords":""},{"title":"Job type Job Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/job-facets/job-type","content":"Job type Job Facet Facet to contain job properties like: processingType which can be STREAMING or BATCH,integration which can be SPARK|DBT|AIRFLOW|FLINK,jobType which can be QUERY|COMMAND|DAG|TASK|JOB|MODEL. Example: { ... &quot;job&quot;: { &quot;facets&quot;: { &quot;jobType&quot;: { &quot;jobType&quot;: { &quot;processingType&quot;: &quot;BATCH&quot;, &quot;integration&quot;: &quot;SPARK&quot;, &quot;jobType&quot;: &quot;QUERY&quot;, &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/v1-0-0/client&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json&quot; } } } ... } The examples for specific integrations: Integration: SPARK Processing type: STREAM|BATCHJob type: JOB|COMMAND Integration: AIRFLOW Processing type: BATCHJob type: DAG|TASK Integration: DBT ProcessingType: BATCHJobType: PROJECT|MODEL Integration: FLINK Processing type: STREAMING|BATCHJob type: JOB","keywords":""},{"title":"Job Facets","type":0,"sectionRef":"#","url":"/docs/spec/facets/job-facets/","content":"Job Facets Job Facets apply to a distinct instance of a job: an abstract process that consumes, executes, and produces datasets (defined as its inputs and outputs). It is identified by a unique name within a namespace. The Job evolves over time and this change is captured during the job runs.","keywords":""},{"title":"Source Code Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/job-facets/source-code","content":"Source Code Facet The source code of a particular job (e.g. Python script) Example: { ... &quot;job&quot;: { &quot;facets&quot;: { &quot;sourceCode&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/main/spec/facets/SourceCodeJobFacet.json&quot;, &quot;language&quot;: &quot;python&quot;, &quot;sourceCode&quot;: &quot;print('hello, OpenLineage!')&quot; } } } ... } The facet specification can be found here","keywords":""},{"title":"Source Code Location Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/job-facets/source-code-location","content":"Source Code Location Facet The facet that indicates where the source code is located. Example: { ... &quot;job&quot;: { &quot;facets&quot;: { &quot;sourceCodeLocation&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/main/spec/facets/SourceCodeLocationJobFacet.json&quot;, &quot;type&quot;: &quot;git|svn&quot;, &quot;url&quot;: &quot;https://github.com/MarquezProject/marquez-airflow-quickstart/blob/693e35482bc2e526ced2b5f9f76ef83dec6ec691/dags/hello.py&quot;, &quot;repoUrl&quot;: &quot;git@github.com:{org}/{repo}.git or https://github.com/{org}/{repo}.git|svn://&lt;your_ip&gt;/&lt;repository_name&gt;&quot;, &quot;path&quot;: &quot;path/to/my/dags&quot;, &quot;version&quot;: &quot;git: the git sha | Svn: the revision number&quot;, &quot;tag&quot;: &quot;example&quot;, &quot;branch&quot; &quot;main&quot; } } } ... } The facet specification can be found here","keywords":""},{"title":"Ownership Job Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/job-facets/ownership","content":"Ownership Job Facet The facet that contains the information regarding users or group who owns this particular job. Example: { ... &quot;job&quot;: { &quot;facets&quot;: { &quot;ownership&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://openlineage.io/spec/facets/1-0-0/OwnershipJobFacet.json&quot;, &quot;owners&quot;: [ { &quot;name&quot;: &quot;maintainer_one&quot;, &quot;type&quot;: &quot;MAINTAINER&quot; } ] } } } ... } The facet specification can be found here","keywords":""},{"title":"Custom Facets","type":0,"sectionRef":"#","url":"/docs/spec/facets/custom-facets","content":"","keywords":""},{"title":"Example of creating your first custom facet​","type":1,"pageTitle":"Custom Facets","url":"/docs/spec/facets/custom-facets#example-of-creating-your-first-custom-facet","content":"Let's look at this sample OpenLineage client code written in python, that defines and uses a custom facet called my-facet. #!/usr/bin/env python3 from openlineage.client.run import ( RunEvent, RunState, Run, Job, Dataset, OutputDataset, InputDataset, ) from openlineage.client.client import OpenLineageClient, OpenLineageClientOptions from openlineage.client.facet import ( BaseFacet, SqlJobFacet, SchemaDatasetFacet, SchemaField, SourceCodeLocationJobFacet, NominalTimeRunFacet, ) import uuid from datetime import datetime, timezone, timedelta from typing import List import attr from random import random import logging, os logging.basicConfig(level=logging.DEBUG) PRODUCER = f&quot;https://github.com/openlineage-user&quot; namespace = &quot;python_client&quot; url = &quot;http://localhost:5000&quot; api_key = &quot;1234567890ckcu028rzu5l&quot; client = OpenLineageClient( url=url, # optional api key in case the backend requires it options=OpenLineageClientOptions(api_key=api_key), ) # generates job facet def job(job_name, sql, location): facets = { &quot;sql&quot;: SqlJobFacet(sql) } if location != None: facets.update( {&quot;sourceCodeLocation&quot;: SourceCodeLocationJobFacet(&quot;git&quot;, location)} ) return Job(namespace=namespace, name=job_name, facets=facets) @attr.s class MyFacet(BaseFacet): name: str = attr.ib() age: str = attr.ib() email: str = attr.ib() _additional_skip_redact: List[str] = ['name', 'age', 'email'] def __init__(self, name, age, email): super().__init__() self.name = name self.age = age self.email = email # geneartes run racet def run(run_id, hour, name, age, email): return Run( runId=run_id, facets={ &quot;nominalTime&quot;: NominalTimeRunFacet( nominalStartTime=f&quot;2022-04-14T{twoDigits(hour)}:12:00Z&quot; ), &quot;my_facet&quot;: MyFacet(name, age, email) }, ) # generates dataset def dataset(name, schema=None, ns=namespace): if schema == None: facets = {} else: facets = {&quot;schema&quot;: schema} return Dataset(namespace, name, facets) # generates output dataset def outputDataset(dataset, stats): output_facets = {&quot;stats&quot;: stats, &quot;outputStatistics&quot;: stats} return OutputDataset(dataset.namespace, dataset.name, dataset.facets, output_facets) # generates input dataset def inputDataset(dataset, dq): input_facets = { &quot;dataQuality&quot;: dq, } return InputDataset(dataset.namespace, dataset.name, dataset.facets, input_facets) def twoDigits(n): if n &lt; 10: result = f&quot;0{n}&quot; elif n &lt; 100: result = f&quot;{n}&quot; else: raise f&quot;error: {n}&quot; return result now = datetime.now(timezone.utc) # generates run Event def runEvents(job_name, sql, inputs, outputs, hour, min, location, duration): run_id = str(uuid.uuid4()) myjob = job(job_name, sql, location) myrun = run(run_id, hour, 'user_1', 25, 'user_1@email.com') st = now + timedelta(hours=hour, minutes=min, seconds=20 + round(random() * 10)) end = st + timedelta(minutes=duration, seconds=20 + round(random() * 10)) started_at = st.isoformat() ended_at = end.isoformat() return ( RunEvent( eventType=RunState.START, eventTime=started_at, run=myrun, job=myjob, producer=PRODUCER, inputs=inputs, outputs=outputs, ), RunEvent( eventType=RunState.COMPLETE, eventTime=ended_at, run=myrun, job=myjob, producer=PRODUCER, inputs=inputs, outputs=outputs, ), ) # add run event to the events list def addRunEvents( events, job_name, sql, inputs, outputs, hour, minutes, location=None, duration=2 ): (start, complete) = runEvents( job_name, sql, inputs, outputs, hour, minutes, location, duration ) events.append(start) events.append(complete) events = [] # create dataset data for i in range(0, 5): user_counts = dataset(&quot;tmp_demo.user_counts&quot;) user_history = dataset( &quot;temp_demo.user_history&quot;, SchemaDatasetFacet( fields=[ SchemaField(name=&quot;id&quot;, type=&quot;BIGINT&quot;, description=&quot;the user id&quot;), SchemaField( name=&quot;email_domain&quot;, type=&quot;VARCHAR&quot;, description=&quot;the user id&quot; ), SchemaField(name=&quot;status&quot;, type=&quot;BIGINT&quot;, description=&quot;the user id&quot;), SchemaField( name=&quot;created_at&quot;, type=&quot;DATETIME&quot;, description=&quot;date and time of creation of the user&quot;, ), SchemaField( name=&quot;updated_at&quot;, type=&quot;DATETIME&quot;, description=&quot;the last time this row was updated&quot;, ), SchemaField( name=&quot;fetch_time_utc&quot;, type=&quot;DATETIME&quot;, description=&quot;the time the data was fetched&quot;, ), SchemaField( name=&quot;load_filename&quot;, type=&quot;VARCHAR&quot;, description=&quot;the original file this data was ingested from&quot;, ), SchemaField( name=&quot;load_filerow&quot;, type=&quot;INT&quot;, description=&quot;the row number in the original file&quot;, ), SchemaField( name=&quot;load_timestamp&quot;, type=&quot;DATETIME&quot;, description=&quot;the time the data was ingested&quot;, ), ] ), &quot;snowflake://&quot;, ) create_user_counts_sql = &quot;&quot;&quot;CREATE OR REPLACE TABLE TMP_DEMO.USER_COUNTS AS ( SELECT DATE_TRUNC(DAY, created_at) date, COUNT(id) as user_count FROM TMP_DEMO.USER_HISTORY GROUP BY date )&quot;&quot;&quot; # location of the source code location = &quot;https://github.com/some/airflow/dags/example/user_trends.py&quot; # run simulating Airflow DAG with snowflake operator addRunEvents( events, &quot;create_user_counts&quot;, create_user_counts_sql, [user_history], [user_counts], i, 11, location, ) for event in events: from openlineage.client.serde import Serde # print(Serde.to_json(event)) # time.sleep(1) client.emit(event)  As you can see in the source code, there is a class called MyFacet which extends from the BaseFacet of OpenLineage, having three attributes of name, age, and email. @attr.s class MyFacet(BaseFacet): name: str = attr.ib() age: str = attr.ib() email: str = attr.ib() _additional_skip_redact: List[str] = ['name', 'age', 'email'] def __init__(self, name, age, email): super().__init__() self.name = name self.age = age self.email = email  And, when the application is generating a Run data, you can see the instantiation of MyFacet, having the name my_facet. def run(run_id, hour, name, age, email): return Run( runId=run_id, facets={ &quot;nominalTime&quot;: NominalTimeRunFacet( nominalStartTime=f&quot;2022-04-14T{twoDigits(hour)}:12:00Z&quot; ), &quot;my_facet&quot;: MyFacet(name, age, email) }, )  When you run this application with python (and please make sure you have installed openlineage-python using pip before running it), you will see a series of JSON output that represents the OpenLineage events being submitted. Here is one example. { &quot;eventTime&quot;: &quot;2022-12-09T09:17:28.239394+00:00&quot;, &quot;eventType&quot;: &quot;COMPLETE&quot;, &quot;inputs&quot;: [ { &quot;facets&quot;: { &quot;schema&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.18.0/client/python&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/SchemaDatasetFacet&quot;, &quot;fields&quot;: [ { &quot;description&quot;: &quot;the user id&quot;, &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;BIGINT&quot; }, { &quot;description&quot;: &quot;the user id&quot;, &quot;name&quot;: &quot;email_domain&quot;, &quot;type&quot;: &quot;VARCHAR&quot; }, { &quot;description&quot;: &quot;the user id&quot;, &quot;name&quot;: &quot;status&quot;, &quot;type&quot;: &quot;BIGINT&quot; }, { &quot;description&quot;: &quot;date and time of creation of the user&quot;, &quot;name&quot;: &quot;created_at&quot;, &quot;type&quot;: &quot;DATETIME&quot; }, { &quot;description&quot;: &quot;the last time this row was updated&quot;, &quot;name&quot;: &quot;updated_at&quot;, &quot;type&quot;: &quot;DATETIME&quot; }, { &quot;description&quot;: &quot;the time the data was fetched&quot;, &quot;name&quot;: &quot;fetch_time_utc&quot;, &quot;type&quot;: &quot;DATETIME&quot; }, { &quot;description&quot;: &quot;the original file this data was ingested from&quot;, &quot;name&quot;: &quot;load_filename&quot;, &quot;type&quot;: &quot;VARCHAR&quot; }, { &quot;description&quot;: &quot;the row number in the original file&quot;, &quot;name&quot;: &quot;load_filerow&quot;, &quot;type&quot;: &quot;INT&quot; }, { &quot;description&quot;: &quot;the time the data was ingested&quot;, &quot;name&quot;: &quot;load_timestamp&quot;, &quot;type&quot;: &quot;DATETIME&quot; } ] } }, &quot;name&quot;: &quot;temp_demo.user_history&quot;, &quot;namespace&quot;: &quot;python_client&quot; } ], &quot;job&quot;: { &quot;facets&quot;: { &quot;sourceCodeLocation&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.18.0/client/python&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/SourceCodeLocationJobFacet&quot;, &quot;type&quot;: &quot;git&quot;, &quot;url&quot;: &quot;https://github.com/some/airflow/dags/example/user_trends.py&quot; }, &quot;sql&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.18.0/client/python&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/SqlJobFacet&quot;, &quot;query&quot;: &quot;CREATE OR REPLACE TABLE TMP_DEMO.USER_COUNTS AS (\\n\\t\\t\\tSELECT DATE_TRUNC(DAY, created_at) date, COUNT(id) as user_count\\n\\t\\t\\tFROM TMP_DEMO.USER_HISTORY\\n\\t\\t\\tGROUP BY date\\n\\t\\t\\t)&quot; } }, &quot;name&quot;: &quot;create_user_counts&quot;, &quot;namespace&quot;: &quot;python_client&quot; }, &quot;outputs&quot;: [ { &quot;facets&quot;: {}, &quot;name&quot;: &quot;tmp_demo.user_counts&quot;, &quot;namespace&quot;: &quot;python_client&quot; } ], &quot;producer&quot;: &quot;https://github.com/openlineage-user&quot;, &quot;run&quot;: { &quot;facets&quot;: { &quot;my_facet&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.18.0/client/python&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/BaseFacet&quot;, &quot;age&quot;: 25, &quot;email&quot;: &quot;user_1@email.com&quot;, &quot;name&quot;: &quot;user_1&quot; }, &quot;nominalTime&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.18.0/client/python&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/NominalTimeRunFacet&quot;, &quot;nominalStartTime&quot;: &quot;2022-04-14T04:12:00Z&quot; } }, &quot;runId&quot;: &quot;7886a902-8fec-422f-9ee4-818489e59f5f&quot; } }  Notice the facet information my_facet that has is now part of the OpenLineage event.  ... &quot;run&quot;: { &quot;facets&quot;: { &quot;my_facet&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.18.0/client/python&quot;, &quot;_schemaURL&quot;: &quot;https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/BaseFacet&quot;, &quot;age&quot;: 25, &quot;email&quot;: &quot;user_1@email.com&quot;, &quot;name&quot;: &quot;user_1&quot; }, ...  OpenLineage backend should be able to store this information when submitted, and later, when you access the Lineage, you should be able to view the facet information that you submitted, along with your custom facet that you made. Below is the screen shot of one of the OpenLineage backend called Marquez, that shows th custom facet that the application has submitted.  You might have noticed the schema URL is actually that of BaseFacet. By default, if the facet class did not specify its own schema URL, that value would be that of BaseFacet. From the view of OpenLineage specification, this is legal. However, if you have your own JSON spec defined, and has it publically accessible, you can specify it by overriding the _get_schema function as such: @attr.s class MyFacet(BaseFacet): name: str = attr.ib() age: str = attr.ib() email: str = attr.ib() _additional_skip_redact: List[str] = ['name', 'age', 'email'] def __init__(self, name, age, email): super().__init__() self.name = name self.age = age self.email = email @staticmethod def _get_schema() -&gt; str: return &quot;https://somewhere/schemas/myfacet.json#/definitions/MyFacet&quot;  And the _schemaURL of the OpenLineage event would now reflect the change as such:  &quot;run&quot;: { &quot;facets&quot;: { &quot;my_facet&quot;: { &quot;_producer&quot;: &quot;https://github.com/OpenLineage/OpenLineage/tree/0.18.0/client/python&quot;, &quot;_schemaURL&quot;: &quot;https://somewhere/schemas/myfacet.json#/definitions/MyFacet&quot;, &quot;age&quot;: 25, &quot;email&quot;: &quot;user_1@email.com&quot;, &quot;name&quot;: &quot;user_1&quot; },  "},{"title":"SQL Job Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/job-facets/sql","content":"SQL Job Facet The SQL Job Facet contains a SQL query that was used in a particular job. Example: { ... &quot;job&quot;: { &quot;facets&quot;: { &quot;sql&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/main/spec/facets/SQLJobFacet.json&quot;, &quot;query&quot;: &quot;select id, name from schema.table where id = 1&quot; } } } ... } The facet specification can be found here","keywords":""},{"title":"Run Facets","type":0,"sectionRef":"#","url":"/docs/spec/facets/run-facets/","content":"Run Facets Run Facets apply to a specific instance of a particular running job. Every run will have a uniquely identifiable run ID that is usually in UUID format, that can later be tracked.","keywords":""},{"title":"Error Message Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/run-facets/error_message","content":"Error Message Facet The facet to contain information about the failures during the run of the job. A typical payload would be the message, stack trace, etc. Example: { ... &quot;run&quot;: { &quot;facets&quot;: { &quot;errorMessage&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/main/spec/facets/ErrorMessageRunFacet.json&quot;, &quot;message&quot;: &quot;org.apache.spark.sql.AnalysisException: Table or view not found: wrong_table_name; line 1 pos 14&quot;, &quot;programmingLanguage&quot;: &quot;JAVA&quot;, &quot;stackTrace&quot;: &quot;Exception in thread \\&quot;main\\&quot; java.lang.RuntimeException: A test exception\\nat io.openlineage.SomeClass.method(SomeClass.java:13)\\nat io.openlineage.SomeClass.anotherMethod(SomeClass.java:9)&quot; } } } ... } The facet specification can be found here","keywords":""},{"title":"External Query Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/run-facets/external_query","content":"External Query Facet The facet that describes the identification of the query that the run is related to which was executed by external systems. Even though the query itself is not contained, using this facet, the user should be able to access the query and its details. Example: { ... &quot;run&quot;: { &quot;facets&quot;: { &quot;externalQuery&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/main/spec/facets/ExternalQueryRunFacet.json&quot;, &quot;externalQueryId&quot;: &quot;my-project-1234:US.bquijob_123x456_123y123z123c&quot;, &quot;source&quot;: &quot;bigquery&quot; } } } ... } The facet specification can be found here","keywords":""},{"title":"Parent Run Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/run-facets/parent_run","content":"Parent Run Facet Commonly, scheduler systems like Apache Airflow will trigger processes on remote systems, such as on Apache Spark or Apache Beam jobs. Those systems might have their own OpenLineage integration and report their own job runs and dataset inputs/outputs. The ParentRunFacet allows those downstream jobs to report which jobs spawned them to preserve job hierarchy. To do that, the scheduler system should have a way to pass its own job and run id to the child job. Example: { ... &quot;run&quot;: { &quot;facets&quot;: { &quot;parent&quot;: { &quot;job&quot;: { &quot;name&quot;: &quot;the-execution-parent-job&quot;, &quot;namespace&quot;: &quot;the-namespace&quot; }, &quot;run&quot;: { &quot;runId&quot;: &quot;f99310b4-3c3c-1a1a-2b2b-c1b95c24ff11&quot; } } } } ... } The facet specification can be found here.","keywords":""},{"title":"Naming Conventions","type":0,"sectionRef":"#","url":"/docs/spec/naming","content":"","keywords":""},{"title":"Dataset Naming​","type":1,"pageTitle":"Naming Conventions","url":"/docs/spec/naming#dataset-naming","content":"A dataset, or table, is organized according to a producer, namespace, database and (optionally) schema. Producer\tNamespace\tName\tExample Namespace\tExample NamePostgres\tpostgres + host + port\tdatabase + schema + table\tpostgres://db.foo.com:6543\tmetrics.sales.orders MySQL\tmysql + host + port\tdatabase + table\tmysql://db.foo.com:6543\tmetrics.orders S3\ts3 + bucket\tpath\ts3://sales-metrics\torders.csv GCS\tgcs + bucket\tpath\tgcs://sales-metrics\torders.csv HDFS\thdfs + host + port\tpath\thdfs://stg.foo.com:3000\tsalesorders.csv BigQuery\tbigquery\tproject + dataset + table\tbigquery\tmetrics.sales.orders Redshift\tredshift + host + port\tdatabase + schema + table\tredshift://examplecluster.XXXXXXXXXXXX.us-west-2.redshift.amazonaws.com:5439\tmetrics.sales.orders Athena\tawsathena + host\tcatalog + database + table\tawsathena://athena.us-west-2.amazonaws.com\tmetrics.sales.orders Azure Synapse\tproducer + host + port\tdatabase + schema + table\tsqlserver://XXXXXXXXXXXX.sql.azuresynapse.net:1433\tSQLPool1/sales.orders Azure Cosmos DB\tproducer + host\tdatabase + 'colls' + table\tazurecosmos://XXXXXXXXXXXX.documents.azure.com/dbs\tmetrics.colls.orders "},{"title":"Job Naming​","type":1,"pageTitle":"Naming Conventions","url":"/docs/spec/naming#job-naming","content":"A Job is a recurring data transformation with inputs and outputs. Each execution is captured as a Run with corresponding metadata. A Run event identifies the Job it instances by providing the job’s unique identifier. The Job identifier is composed of a Namespace and Name. The job name is unique within its namespace. Producer\tFormula\tExampleAirflow\tnamespace + DAG + task\tairflow-staging.orders_etl.count_orders SQL\tnamespace + name\tgx.validate_datasets "},{"title":"Run Naming​","type":1,"pageTitle":"Naming Conventions","url":"/docs/spec/naming#run-naming","content":"Runs are named using client-generated UUIDs. The OpenLineage client is responsible for generating them and maintaining them throughout the duration of the runcycle. from openlineage.client.run import Run run = Run(str(uuid4()))  "},{"title":"Why Naming Matters​","type":1,"pageTitle":"Naming Conventions","url":"/docs/spec/naming#why-naming-matters","content":"Naming enables focused insight into data flows, even when datasets and workflows are distributed across an organization. This focus enabled by naming is key to the production of useful lineage.  "},{"title":"Additional Resources​","type":1,"pageTitle":"Naming Conventions","url":"/docs/spec/naming#additional-resources","content":"The OpenLineage Naming SpecWhat's in a Namespace Blog Post "},{"title":"Nominal Time Facet","type":0,"sectionRef":"#","url":"/docs/spec/facets/run-facets/nominal_time","content":"Nominal Time Facet The facet to describe the nominal start and end time of the run. The nominal usually means the time the job run was expected to run (like a scheduled time), and the actual time can be different. Example: { ... &quot;run&quot;: { &quot;facets&quot;: { &quot;nominalTime&quot;: { &quot;_producer&quot;: &quot;https://some.producer.com/version/1.0&quot;, &quot;_schemaURL&quot;: &quot;https://github.com/OpenLineage/OpenLineage/blob/main/spec/facets/SQLJobFacet.json&quot;, &quot;nominalStartTime&quot;: &quot;2020-12-17T03:00:00.000Z&quot;, &quot;nominalEndTime&quot;: &quot;2020-12-17T03:05:00.000Z&quot; } } } ... } The facet specification can be found here","keywords":""},{"title":"Producers","type":0,"sectionRef":"#","url":"/docs/spec/producers","content":"Producers info This page could use some extra detail! You're welcome to contribute using the Edit link at the bottom. The _producer value is included in an OpenLineage request as a way to know how the metadata was generated. It is a URI that links to a source code SHA or the location where a package can be found. For example, this field is populated by many of the common integrations. For example, the dbt integration will set this value to https://github.com/OpenLineage/OpenLineage/tree/{__version__}/integration/dbt and the Python client will set it to https://github.com/OpenLineage/OpenLineage/tree/{__version__}/client/python.","keywords":""},{"title":"Object Model","type":0,"sectionRef":"#","url":"/docs/spec/object-model","content":"","keywords":""},{"title":"Run State Update​","type":1,"pageTitle":"Object Model","url":"/docs/spec/object-model#run-state-update","content":"A Run State Update is prepared and sent when something important occurs within your pipeline, and each one can be thought of as a distinct observation. This commonly happens when a Job starts or finishes. The run state itself refers to a stage within the run cycle of the current run. Usually, the first Run State for a Job would be START and the last would be COMPLETE. A run cycle is likely to have at least two Run State Updates, and perhaps more. Each one will also have timestamp of when this particular state change happened.  Each Run State Update can include detail about the Job, the Run, and the input and output Datasets involved in the run. Subsequent updates are additive: input Datasets, for example, can be specified along with START, along with COMPLETE, or both. This accommodates situations where information is only available at certain times. Each of these three core entities can also be extended through the use of facets, some of which are documented in the relevant sections below. "},{"title":"Job​","type":1,"pageTitle":"Object Model","url":"/docs/spec/object-model#job","content":"A Job is a process that consumes or produces Datasets. This is abstract, and can map to different things in different operational contexts. For example, a job could be a task in a workflow orchestration system. It could also be a model, a query, or a checkpoint. Depending on the system under observation, a Job can represent a small or large amount of work. A Job is the part of the object model that represents a discrete bit of defined work. If, for example, you have cron running a Python script that executes a CREATE TABLE x AS SELECT * FROM y query every day, the Python script is the Job. Jobs are identified by a unique name within a namespace. They are expected to evolve over time and their changes can be captured through Run State Updates. "},{"title":"Job Facets​","type":1,"pageTitle":"Object Model","url":"/docs/spec/object-model#job-facets","content":"Facets that can be used to augment the metadata of a Job include: sourceCodeLocation: Captures the source code location and version (e.g., the git SHA) of the job. sourceCode: Captures the language (e.g. python) and complete source code of the job. Using this source code, users can gain useful information about what the job does. For more details, please refer to the Job Facets. "},{"title":"Run​","type":1,"pageTitle":"Object Model","url":"/docs/spec/object-model#run","content":"A Run is an instance of a Job that represents one of its occurrences in time. Each run will have a uniquely identifiable runId that is generated by the client in UUID format. The client is responsible for maintaining the runId between different Run State Updates in the same Run. Runs can be used to observe changes in Jobs between their instances. If, for example, you have cron running a Python script that repeats a query every day, this should resuilt in a separate Run for each day. "},{"title":"Run Facets​","type":1,"pageTitle":"Object Model","url":"/docs/spec/object-model#run-facets","content":"Facets that can be used to augment the metadata of a Run include: nominalTime: Captures the time this run is scheduled for. This is typically used for scheduled jobs. The job has a nominally scheduled time that will be different from the actual time it ran. parent: Captures the parent Job and Run, for instances where this Run was spawned from a parent Run. For example in the case of Airflow, there's a Run that represents the DAG itself that is the parent of the individual Runs that represent the tasks it spawns. Similarly when a SparkOperator starts a Spark job, this creates a separate run that refers to the task run as its parent. errorMessage: Captures potential error messages - and optionally stack traces - with which the run failed. sql: Captures the SQL query, if this job runs one. For more details, please refer to the Run Facets. "},{"title":"Dataset​","type":1,"pageTitle":"Object Model","url":"/docs/spec/object-model#dataset","content":"A Dataset is an abstract representation of data. This can refer to a small amount or large amount of data, as long as it's discrete. For databases, this should be a table. For cloud storage, this is often an object in a bucket. This can represent a directory of a filesystem. It has a unique name within a namespace derived from its physical location (i.e., db.host.database.schema.table). The combined namespace and name for a Dataset should be enough to uniquely identify it within a data ecosystem. Typically, a Dataset changes when a job writing to it completes. Similarly to the Job and Run distinction, metadata that is more static from Run to Run is captured in a DatasetFacet - for example, the schema that does not change every run). What changes every Run is captured as an InputFacet or an OutputFacet - for example, a time partition indicating the subset of the data set that was read or written). A Dataset is the part of the object model that represents a discrete collection of data. If, for example, you have cron running a Python script that executes a CREATE TABLE x AS SELECT * FROM y query every day, the x and y tables are Datasets. "},{"title":"Dataset Facets​","type":1,"pageTitle":"Object Model","url":"/docs/spec/object-model#dataset-facets","content":"Facets that can be used to augment the metadata of a Dataset include: schema: Captures the schema of the dataset dataSource: Captures the database instance containing this Dataset (e.g., database schema, object store bucket) lifecycleStateChange: Captures the lifecycle states of the Dataset (e.g., alter, create, drop, overwrite, rename, truncate) version: Captures the dataset version when versioning is defined by the data store (e.g.. Iceberg snapshot ID) Input Datasets have the following facets: dataQualityMetrics: Captures dataset-level and column-level data quality metrics (row count, byte size, null count, distinct count, average, min, max, quantiles) dataQualityAssertions: Captures the result of running data tests on dataset or its columns Output Datasets have the following facets: outputStatistics: Captures the size of the output written to a dataset (e.g., row count and byte size) For more details, please refer to the Dataset Facets. "},{"title":"The Run Cycle","type":0,"sectionRef":"#","url":"/docs/spec/run-cycle","content":"","keywords":""},{"title":"Run States​","type":1,"pageTitle":"The Run Cycle","url":"/docs/spec/run-cycle#run-states","content":"There are six run states currently defined in the OpenLineage spec: START to indicate the beginning of a Job RUNNING to provide additional information about a running Job COMPLETE to signify that execution of the Job has concluded ABORT to signify that the Job has been stopped abnormally FAIL to signify that the Job has failed OTHER to send additional metadata outside standard run cycle We assume events describing a single run are accumulative andCOMPLETE, ABORT and FAIL are terminal events. Sending any of terminal events means no other events related to this run will be emitted. Additionally, we allow OTHER to be sent anytime before the terminal states, also before START. The purpose of this is the agility to send additional metadata outside standard run cycle - e.g., on a run that hasn't yet started but is already awaiting the resources.  "},{"title":"Typical Scenarios​","type":1,"pageTitle":"The Run Cycle","url":"/docs/spec/run-cycle#typical-scenarios","content":"A batch Job - e.g., an Airflow task or a dbt model - will typically be represented as a START event followed by a COMPLETE event. Occasionally, an ABORT or FAIL event will be sent when a job does not complete successfully.  A long-running Job - e.g., a microservice or a stream - will typically be represented by a START event followed by a series of RUNNING events that report changes in the run or emit performance metrics. Occasionally, a COMPLETE, ABORT, or FAIL event will occur, often followed by a START event as the job is reinitiated.  "},{"title":"Working with Schemas","type":0,"sectionRef":"#","url":"/docs/spec/schemas","content":"","keywords":""},{"title":"Create a new issue with label spec​","type":1,"pageTitle":"Working with Schemas","url":"/docs/spec/schemas#create-a-new-issue-with-label-spec","content":"Before you decide to make any changes, it is best advised that you first label your issue with spec. This will indicate the the issue is related to any changes in the current OpenLineage spec. "},{"title":"Make changes to the spec's version​","type":1,"pageTitle":"Working with Schemas","url":"/docs/spec/schemas#make-changes-to-the-specs-version","content":"Whenever there is a change to existing spec file (JSON), you need to bump up the version of the existing current spec, so that the changes can go through the code generation and gradle build. Consider the following spec file, where you will see the URL in $id that shows what is the current spec version the file currently is. { &quot;$schema&quot;: &quot;https://json-schema.org/draft/2020-12/schema&quot;, &quot;$id&quot;: &quot;https://openlineage.io/spec/facets/1-0-1/ColumnLineageDatasetFacet.json&quot;, &quot;$defs&quot;: {  In this example, bumping up the version to the new value, should be changed from 1-0-1 to 1-0-2. { &quot;$schema&quot;: &quot;https://json-schema.org/draft/2020-12/schema&quot;, &quot;$id&quot;: &quot;https://openlineage.io/spec/facets/1-0-2/ColumnLineageDatasetFacet.json&quot;, &quot;$defs&quot;: {  If you do not bump the version to higher number, the code generation of Java client will fail. "},{"title":"Python client's codes need to be manually updated​","type":1,"pageTitle":"Working with Schemas","url":"/docs/spec/schemas#python-clients-codes-need-to-be-manually-updated","content":"Java client's build process does involve code generation that automatically produces OpenLineage classes derived from the spec files, so you do not need to do anything in terms of coding the client. However, python client libraries does not depend on the spec files to be generated, so you have to make sure to add changes to the python code in order for it to know and use the changes. As for the facets, they are implemented here, so generally, you need to apply necessary changes to it. As for the general structure of OpenLineage's run events, it can be found here. "},{"title":"Add test cases​","type":1,"pageTitle":"Working with Schemas","url":"/docs/spec/schemas#add-test-cases","content":"Make sure to add changes to the unit tests for python and java to make sure the unit test can be performed against your new SPEC changes. Refer to existing test codes to add yours in. "},{"title":"Test the SPEC change using code generation and integration tests​","type":1,"pageTitle":"Working with Schemas","url":"/docs/spec/schemas#test-the-spec-change-using-code-generation-and-integration-tests","content":"When you have modified the SPEC file(s), always make sure to perform code generation and unit tests by going into client/java and running ./gradlew generateCode and ./gradlew test. As for python, cd into client/python and run pytest. Note: Some of the tests may fail due to the fact that they require external systems like kafka. You can ignore those errors. "}]